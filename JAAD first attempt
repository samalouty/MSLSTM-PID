{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11201333,"sourceType":"datasetVersion","datasetId":6993690},{"sourceId":11201362,"sourceType":"datasetVersion","datasetId":6993708},{"sourceId":11201388,"sourceType":"datasetVersion","datasetId":6993722},{"sourceId":11201422,"sourceType":"datasetVersion","datasetId":6993740},{"sourceId":11201506,"sourceType":"datasetVersion","datasetId":6993794},{"sourceId":11201543,"sourceType":"datasetVersion","datasetId":6993809},{"sourceId":11255589,"sourceType":"datasetVersion","datasetId":7034191},{"sourceId":11382982,"sourceType":"datasetVersion","datasetId":7127490},{"sourceId":11402679,"sourceType":"datasetVersion","datasetId":7142036},{"sourceId":11684148,"sourceType":"datasetVersion","datasetId":7333398},{"sourceId":11720877,"sourceType":"datasetVersion","datasetId":7357780},{"sourceId":11801892,"sourceType":"datasetVersion","datasetId":7411543},{"sourceId":302300,"sourceType":"modelInstanceVersion","modelInstanceId":258142,"modelId":279383},{"sourceId":307831,"sourceType":"modelInstanceVersion","modelInstanceId":262207,"modelId":283333},{"sourceId":316944,"sourceType":"modelInstanceVersion","modelInstanceId":267476,"modelId":288527},{"sourceId":329886,"sourceType":"modelInstanceVersion","modelInstanceId":276781,"modelId":297682},{"sourceId":329908,"sourceType":"modelInstanceVersion","modelInstanceId":276800,"modelId":297702},{"sourceId":352620,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":294156,"modelId":314775},{"sourceId":391172,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":322103,"modelId":342770},{"sourceId":391205,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":322120,"modelId":342793}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/aras62/PIE.git\n!unzip /content/PIE/annotations/annotations.zip -d /content/PIE\n!unzip /content/PIE/annotations/annotations_vehicle.zip -d /content/PIE\n!mkdir /kaggle/working/PIE/content\n#!git clone https://github.com/hustvl/YOLOP.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:27:07.771729Z","iopub.execute_input":"2025-04-19T13:27:07.772029Z","iopub.status.idle":"2025-04-19T13:27:08.263278Z","shell.execute_reply.started":"2025-04-19T13:27:07.771999Z","shell.execute_reply":"2025-04-19T13:27:08.262097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/ykotseruba/JAAD.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:49:18.677075Z","iopub.execute_input":"2025-05-13T23:49:18.677562Z","iopub.status.idle":"2025-05-13T23:49:25.784752Z","shell.execute_reply.started":"2025-05-13T23:49:18.677528Z","shell.execute_reply":"2025-05-13T23:49:25.783349Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'JAAD'...\nremote: Enumerating objects: 6155, done.\u001b[K\nremote: Counting objects: 100% (724/724), done.\u001b[K\nremote: Compressing objects: 100% (72/72), done.\u001b[K\nremote: Total 6155 (delta 672), reused 695 (delta 652), pack-reused 5431 (from 1)\u001b[K\nReceiving objects: 100% (6155/6155), 42.16 MiB | 17.61 MiB/s, done.\nResolving deltas: 100% (5491/5491), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q ultralytics opencv-python-headless ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport xml.etree.ElementTree as ET\nimport os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\nfrom tqdm.notebook import tqdm\nimport random\nimport math\nimport zipfile\nimport cv2\nfrom ultralytics import YOLO\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:10.154360Z","iopub.execute_input":"2025-04-15T19:55:10.154719Z","iopub.status.idle":"2025-04-15T19:55:16.590012Z","shell.execute_reply.started":"2025-04-15T19:55:10.154687Z","shell.execute_reply":"2025-04-15T19:55:16.589278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + 'annotations'):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.590843Z","iopub.execute_input":"2025-04-15T19:55:16.591210Z","iopub.status.idle":"2025-04-15T19:55:16.596284Z","shell.execute_reply.started":"2025-04-15T19:55:16.591187Z","shell.execute_reply":"2025-04-15T19:55:16.595261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations_vehicle.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + 'annotations_vehicle'):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.597390Z","iopub.execute_input":"2025-04-15T19:55:16.597839Z","iopub.status.idle":"2025-04-15T19:55:16.618074Z","shell.execute_reply.started":"2025-04-15T19:55:16.597803Z","shell.execute_reply":"2025-04-15T19:55:16.617129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations_attributes.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + \"annotations_attributes\"):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.619029Z","iopub.execute_input":"2025-04-15T19:55:16.619360Z","iopub.status.idle":"2025-04-15T19:55:16.635884Z","shell.execute_reply.started":"2025-04-15T19:55:16.619328Z","shell.execute_reply":"2025-04-15T19:55:16.634975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # -----------------------------------------------------------------------------\n# # CELL 1: DATA PREPARATION & BALANCING  (run once before training)\n# # -----------------------------------------------------------------------------\n# #  This cell:\n# #    1. Loads (or regenerates) the PIE database\n# #    2. Computes per-signal standardisation scalers\n# #    3. Extracts ALL training sequences for every stream\n# #    4. Balances the dataset 50 / 50 on the crossing label\n# #    5. Writes two pickles:\n# #         - /kaggle/working/balanced_train_data.pkl\n# #         - /kaggle/working/scalers.pkl\n# # -----------------------------------------------------------------------------\n\n# import os\n# import sys\n# import time\n# import pickle\n# import gc\n# from pathlib import Path\n\n# import cv2                               # used internally by PIE utilities\n# import numpy as np\n# import torch\n# from torch.utils.data import Dataset\n# from tqdm.notebook import tqdm\n\n# # -----------------------------------------------------------------------------#\n# #                                PIE utilities                                 #\n# # -----------------------------------------------------------------------------#\n# pie_utilities_path = \"/kaggle/working/PIE/utilities\"\n# if pie_utilities_path not in sys.path:\n#     sys.path.insert(0, pie_utilities_path)\n\n# try:\n#     from pie_data import PIE\n# except ImportError as e:\n#     print(\n#         f\"[WARN] Could not import PIE from {pie_utilities_path}. \"\n#         f\"If the DB cache already exists this is fine.\\n→ {e}\"\n#     )\n#     PIE = None\n\n# # -----------------------------------------------------------------------------#\n# #                              configuration                                   #\n# # -----------------------------------------------------------------------------#\n# PIE_ROOT_PATH           = \"/kaggle/working/PIE\"\n# POSE_DATA_DIR           = \"/kaggle/input/pose-data/extracted_poses2\"\n# PIE_DATABASE_CACHE_PATH = \"/kaggle/input/pie-database/pie_database.pkl\"\n\n# TRAIN_SETS_STR = [\"set01\", \"set02\", \"set04\"]\n\n# BALANCED_DATA_PKL_PATH  = \"/kaggle/working/balanced_train_data.pkl\"\n# SCALERS_PKL_PATH        = \"/kaggle/working/scalers.pkl\"\n\n# # Streams used throughout the project ----------------------------------------\n# ALL_POSSIBLE_STREAMS = [\n#     \"bbox\",\n#     \"pose\",\n#     \"ego_speed\",\n#     \"ego_acc\",\n#     \"ego_gyro\",\n#     \"ped_action\",\n#     \"ped_look\",\n#     \"ped_occlusion\",\n#     \"traffic_light\",\n#     \"static_context\",\n# ]\n\n# # Feature sizes & categorical constants --------------------------------------\n# SEQ_LEN, PRED_LEN = 30, 1\n\n# INPUT_SIZE_BBOX       = 4\n# INPUT_SIZE_POSE       = 34\n# INPUT_SIZE_EGO_SPEED  = 1\n# INPUT_SIZE_EGO_ACC    = 2\n# INPUT_SIZE_EGO_GYRO   = 1\n# INPUT_SIZE_PED_ACTION = 1\n# INPUT_SIZE_PED_LOOK   = 1\n# INPUT_SIZE_PED_OCC    = 1\n# INPUT_SIZE_TL_STATE   = 4\n\n# NUM_SIGNALIZED_CATS   = 4\n# NUM_INTERSECTION_CATS = 5\n# NUM_AGE_CATS          = 4\n# NUM_GENDER_CATS       = 3\n# NUM_TRAFFIC_DIR_CATS  = 2\n\n# LANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7: 4, 8: 4}\n# NUM_LANE_CATS   = len(set(LANE_CATEGORIES.values()))\n\n# INPUT_SIZE_STATIC = (\n#     NUM_SIGNALIZED_CATS\n#     + NUM_INTERSECTION_CATS\n#     + NUM_AGE_CATS\n#     + NUM_GENDER_CATS\n#     + NUM_TRAFFIC_DIR_CATS\n#     + NUM_LANE_CATS\n# )  # → 23\n\n# TL_STATE_MAP = {\"__undefined__\": 0, \"red\": 1, \"yellow\": 2, \"green\": 3}\n# NUM_TL_STATES = len(TL_STATE_MAP)\n\n# # -----------------------------------------------------------------------------#\n# #                               helper utils                                   #\n# # -----------------------------------------------------------------------------#\n\n\n# def to_one_hot(index: int, num_classes: int) -> np.ndarray:\n#     vec = np.zeros(num_classes, dtype=np.float32)\n#     vec[int(np.clip(index, 0, num_classes - 1))] = 1.0\n#     return vec\n\n\n# def balance_samples_count(seq_data: dict, label_key: str, seed: int = 42) -> dict:\n#     \"\"\"Undersample majority class so positive and negative labels are equal.\"\"\"\n#     labels = [lbl[0] for lbl in seq_data[label_key]]\n#     n_pos  = int(np.sum(labels))\n#     n_neg  = len(labels) - n_pos\n\n#     if n_pos == n_neg:\n#         print(\"Dataset already balanced.\")\n#         return seq_data.copy()\n\n#     majority_label    = 0 if n_neg > n_pos else 1\n#     minority_count    = min(n_pos, n_neg)\n#     majority_indices  = np.where(np.array(labels) == majority_label)[0]\n#     minority_indices  = np.where(np.array(labels) != majority_label)[0]\n\n#     rng = np.random.default_rng(seed)\n#     keep_majority = rng.choice(majority_indices, size=minority_count, replace=False)\n#     final_indices = np.concatenate([minority_indices, keep_majority])\n#     rng.shuffle(final_indices)\n\n#     balanced = {}\n#     for k, v in seq_data.items():\n#         balanced[k] = [v[i] for i in final_indices]\n\n#     print(f\"Balanced: 1s={minority_count} | 0s={minority_count}\")\n#     return balanced\n\n\n# # -----------------------------------------------------------------------------#\n# #                                PIEDataset                                    #\n# # -----------------------------------------------------------------------------#\n# class PIEDataset(Dataset):\n#     \"\"\"\n#     Lightweight dataset that can generate any subset of the PIE feature streams.\n#     \"\"\"\n\n#     def __init__(\n#         self,\n#         pie_db: dict,\n#         set_names: list[str],\n#         pose_dir: str,\n#         seq_len: int,\n#         pred_len: int,\n#         scalers: dict,\n#         streams_to_generate: list[str],\n#     ):\n#         self.pie_db            = pie_db\n#         self.set_names         = set_names\n#         self.pose_dir          = pose_dir\n#         self.seq_len           = seq_len\n#         self.pred_len          = pred_len\n#         self.scalers           = scalers\n#         self.streams           = streams_to_generate\n#         self._input_sizes      = self._build_input_size_map()\n#         self.all_pose_data     = {}\n#         self.sequences         = []\n\n#         if \"pose\" in self.streams:\n#             self._load_pose_pkls()\n#         self._enumerate_sequences()\n\n#     # ------------------------ internal helpers -------------------------------\n#     def _build_input_size_map(self) -> dict:\n#         special = {\n#             \"TRAFFIC_LIGHT\": \"TL_STATE\",\n#             \"STATIC_CONTEXT\": \"STATIC\",\n#             \"EGO_SPEED\": \"EGO_SPEED\",\n#             \"EGO_ACC\": \"EGO_ACC\",\n#             \"EGO_GYRO\": \"EGO_GYRO\",\n#             \"PED_ACTION\": \"PED_ACTION\",\n#             \"PED_LOOK\": \"PED_LOOK\",\n#             \"PED_OCCLUSION\": \"PED_OCC\",\n#         }\n#         sizes = {}\n#         for s in ALL_POSSIBLE_STREAMS:\n#             const = f\"INPUT_SIZE_{special.get(s.upper(), s.upper())}\"\n#             if s == \"bbox\":\n#                 const = \"INPUT_SIZE_BBOX\"\n#             elif s == \"pose\":\n#                 const = \"INPUT_SIZE_POSE\"\n#             sizes[s] = globals().get(const, 1)\n#         return sizes\n\n#     def _load_pose_pkls(self):\n#         print(\"Loading pose PKLs …\")\n#         for set_id in self.set_names:\n#             set_dir = Path(self.pose_dir) / set_id\n#             if not set_dir.is_dir():\n#                 continue\n#             self.all_pose_data[set_id] = {}\n#             for pkl_path in tqdm(set_dir.glob(f\"{set_id}_*_poses.pkl\"), leave=False):\n#                 try:\n#                     with open(pkl_path, \"rb\") as fp:\n#                         loaded = pickle.load(fp)\n#                 except Exception as e:\n#                     print(f\"[pose load] {pkl_path}: {e}\")\n#                     continue\n\n#                 if len(loaded) != 1:\n#                     continue\n#                 (key, data), *_ = loaded.items()\n#                 vid = \"_\".join(key.split(\"_\")[1:])\n#                 if vid in self.pie_db.get(set_id, {}):\n#                     self.all_pose_data[set_id][vid] = data\n\n#     def _enumerate_sequences(self):\n#         print(\"Enumerating sequences …\")\n#         for set_id in self.set_names:\n#             for vid, vdb in self.pie_db.get(set_id, {}).items():\n#                 for pid, pdb in vdb.get(\"ped_annotations\", {}).items():\n#                     frames = pdb.get(\"frames\", [])\n#                     if len(frames) < self.seq_len + self.pred_len:\n#                         continue\n#                     frames = sorted(frames)\n#                     for i in range(len(frames) - self.seq_len - self.pred_len + 1):\n#                         start = frames[i]\n#                         obs_end = frames[i + self.seq_len - 1]\n#                         if obs_end - start != self.seq_len - 1:\n#                             continue\n#                         target = frames[i + self.seq_len + self.pred_len - 1]\n#                         if target - obs_end != self.pred_len:\n#                             continue\n#                         self.sequences.append((set_id, vid, pid, start))\n#         print(f\"Total sequences: {len(self.sequences)}\")\n\n#     # ------------------ Dataset API ------------------------------------------\n#     def __len__(self):\n#         return len(self.sequences)\n\n#     def __getitem__(self, idx: int):\n#         set_id, vid, pid, start = self.sequences[idx]\n#         vdb  = self.pie_db[set_id][vid]\n#         pdb  = vdb[\"ped_annotations\"][pid]\n#         ego  = vdb.get(\"vehicle_annotations\", {})\n#         tldb = vdb.get(\"traffic_annotations\", {})\n\n#         frame_nums = list(range(start, start + self.seq_len))\n#         target_f   = start + self.seq_len + self.pred_len - 1\n\n#         # label ---------------------------------------------------------------\n#         label = 0\n#         if (\n#             \"frames\" in pdb\n#             and \"behavior\" in pdb\n#             and \"cross\" in pdb[\"behavior\"]\n#             and target_f in pdb[\"frames\"]\n#         ):\n#             try:\n#                 j = pdb[\"frames\"].index(target_f)\n#                 label = pdb[\"behavior\"][\"cross\"][j]\n#                 if label == -1:\n#                     label = 0\n#             except (ValueError, IndexError):\n#                 pass\n\n#         # static context ------------------------------------------------------\n#         static_vec = np.zeros(INPUT_SIZE_STATIC, np.float32)\n#         if \"static_context\" in self.streams:\n#             attr  = pdb.get(\"attributes\", {})\n#             sig   = attr.get(\"signalized\", 0)\n#             intr  = attr.get(\"intersection\", 0)\n#             age   = attr.get(\"age\", 2)\n#             gen   = attr.get(\"gender\", 0)\n#             tdir  = int(attr.get(\"traffic_direction\", 0))\n#             ln    = attr.get(\"num_lanes\", 2)\n#             lncat = LANE_CATEGORIES.get(ln, LANE_CATEGORIES[max(LANE_CATEGORIES)])\n#             static_vec = np.concatenate(\n#                 [\n#                     to_one_hot(sig,  NUM_SIGNALIZED_CATS),\n#                     to_one_hot(intr, NUM_INTERSECTION_CATS),\n#                     to_one_hot(age,  NUM_AGE_CATS),\n#                     to_one_hot(gen,  NUM_GENDER_CATS),\n#                     to_one_hot(tdir, NUM_TRAFFIC_DIR_CATS),\n#                     to_one_hot(lncat, NUM_LANE_CATS),\n#                 ]\n#             ).astype(np.float32)\n\n#         # per-frame feature assembly -----------------------------------------\n#         feats = {s: [] for s in self.streams}\n\n#         for fn in frame_nums:\n#             fidx = -1\n#             if \"frames\" in pdb:\n#                 try:\n#                     fidx = pdb[\"frames\"].index(fn)\n#                 except ValueError:\n#                     pass\n\n#             ego_f = ego.get(fn, {})\n\n#             # bbox ----------------------------------------------------------\n#             if \"bbox\" in self.streams:\n#                 bb = np.zeros(INPUT_SIZE_BBOX, np.float32)\n#                 if (\n#                     fidx != -1\n#                     and \"bbox\" in pdb\n#                     and len(pdb[\"bbox\"]) > fidx\n#                 ):\n#                     try:\n#                         x1, y1, x2, y2 = pdb[\"bbox\"][fidx]\n#                         w_img = vdb.get(\"width\", 1920)\n#                         h_img = vdb.get(\"height\", 1080)\n#                         if w_img > 0 and h_img > 0:\n#                             cx = ((x1 + x2) / 2) / w_img\n#                             cy = ((y1 + y2) / 2) / h_img\n#                             w  = (x2 - x1) / w_img\n#                             h  = (y2 - y1) / h_img\n#                             if 0 < w and 0 < h and 0 <= cx <= 1 and 0 <= cy <= 1:\n#                                 bb = np.array([cx, cy, w, h], np.float32)\n#                     except Exception:\n#                         pass\n#                 feats[\"bbox\"].append(bb)\n\n#             # pose ----------------------------------------------------------\n#             if \"pose\" in self.streams:\n#                 pvec = np.zeros(INPUT_SIZE_POSE, np.float32)\n#                 pose_set = self.all_pose_data.get(set_id, {}).get(vid, {})\n#                 p_loaded = pose_set.get(fn, {}).get(pid)\n#                 if (\n#                     isinstance(p_loaded, np.ndarray)\n#                     and p_loaded.shape == (INPUT_SIZE_POSE,)\n#                 ):\n#                     pvec = p_loaded\n#                 feats[\"pose\"].append(pvec)\n\n#             # ego signals ---------------------------------------------------\n#             if \"ego_speed\" in self.streams:\n#                 s = ego_f.get(\"OBD_speed\", 0.0) or ego_f.get(\"GPS_speed\", 0.0)\n#                 s = (s - self.scalers.get(\"ego_speed_mean\", 0.0)) / self.scalers.get(\n#                     \"ego_speed_std\", 1.0\n#                 )\n#                 feats[\"ego_speed\"].append([s])\n\n#             if \"ego_acc\" in self.streams:\n#                 ax = ego_f.get(\"accX\", 0.0)\n#                 ay = ego_f.get(\"accY\", 0.0)\n#                 ax = (ax - self.scalers.get(\"accX_mean\", 0.0)) / self.scalers.get(\n#                     \"accX_std\", 1.0\n#                 )\n#                 ay = (ay - self.scalers.get(\"accY_mean\", 0.0)) / self.scalers.get(\n#                     \"accY_std\", 1.0\n#                 )\n#                 feats[\"ego_acc\"].append([ax, ay])\n\n#             if \"ego_gyro\" in self.streams:\n#                 gz = ego_f.get(\"gyroZ\", 0.0)\n#                 gz = (gz - self.scalers.get(\"gyroZ_mean\", 0.0)) / self.scalers.get(\n#                     \"gyroZ_std\", 1.0\n#                 )\n#                 feats[\"ego_gyro\"].append([gz])\n\n#             # pedestrian behaviour -----------------------------------------\n#             if \"ped_action\" in self.streams:\n#                 action = (\n#                     pdb[\"behavior\"][\"action\"][fidx]\n#                     if fidx != -1\n#                     and \"behavior\" in pdb\n#                     and \"action\" in pdb[\"behavior\"]\n#                     and len(pdb[\"behavior\"][\"action\"]) > fidx\n#                     else 0\n#                 )\n#                 feats[\"ped_action\"].append([float(action)])\n\n#             if \"ped_look\" in self.streams:\n#                 look = (\n#                     pdb[\"behavior\"][\"look\"][fidx]\n#                     if fidx != -1\n#                     and \"behavior\" in pdb\n#                     and \"look\" in pdb[\"behavior\"]\n#                     and len(pdb[\"behavior\"][\"look\"]) > fidx\n#                     else 0\n#                 )\n#                 feats[\"ped_look\"].append([float(look)])\n\n#             if \"ped_occlusion\" in self.streams:\n#                 occ = (\n#                     float(pdb[\"occlusion\"][fidx]) / 2.0\n#                     if fidx != -1\n#                     and \"occlusion\" in pdb\n#                     and len(pdb[\"occlusion\"]) > fidx\n#                     else 0.0\n#                 )\n#                 feats[\"ped_occlusion\"].append([occ])\n\n#             # traffic light -------------------------------------------------\n#             if \"traffic_light\" in self.streams:\n#                 tl_state = 0\n#                 for obj in tldb.values():\n#                     if obj.get(\"obj_class\") != \"traffic_light\":\n#                         continue\n#                     if \"frames\" not in obj or \"state\" not in obj:\n#                         continue\n#                     try:\n#                         j = obj[\"frames\"].index(fn)\n#                         if obj[\"state\"][j] != 0:\n#                             tl_state = obj[\"state\"][j]\n#                             break\n#                     except (ValueError, IndexError):\n#                         continue\n#                 feats[\"traffic_light\"].append(to_one_hot(tl_state, NUM_TL_STATES))\n\n#             # static context -----------------------------------------------\n#             if \"static_context\" in self.streams:\n#                 feats[\"static_context\"].append(static_vec)\n\n#         # numpy → torch ------------------------------------------------------\n#         out = {\n#             s: torch.tensor(np.asarray(feats[s], np.float32), dtype=torch.float32)\n#             for s in self.streams\n#         }\n#         return out, torch.tensor(label, dtype=torch.long)\n\n\n# # =============================================================================\n# #                       MAIN: build balanced training set\n# # =============================================================================\n# if __name__ == \"__main__\" and '__file__' not in globals(): # Avoid running this if imported\n#     print(\"\\n--- CELL 1: DATA PREPARATION ---\")\n\n#     # 1) load / regenerate PIE DB -------------------------------------------\n#     cache = Path(PIE_DATABASE_CACHE_PATH)\n#     if cache.is_file():\n#         print(\"Loading PIE database cache …\")\n#         with cache.open(\"rb\") as fp:\n#             pie_db = pickle.load(fp)\n#         print(\"✓ PIE DB loaded.\")\n#     else:\n#         if PIE is None:\n#             raise RuntimeError(\"PIE class unavailable: cannot rebuild database.\")\n#         print(\"Cache not found – regenerating PIE DB …\")\n#         pie_db = PIE(data_path=PIE_ROOT_PATH, regen_database=True).generate_database()\n#         if not pie_db:\n#             raise RuntimeError(\"PIE DB generation failed.\")\n#         print(\"✓ PIE DB generated.\")\n\n#     # 2) compute scalers -----------------------------------------------------\n#     print(\"\\nComputing scalers …\")\n#     spd, accx, accy, gyz = [], [], [], []\n#     for sid in TRAIN_SETS_STR:\n#         for vid, vdb in pie_db.get(sid, {}).items():\n#             for frame, e in vdb.get(\"vehicle_annotations\", {}).items():\n#                 s  = e.get(\"OBD_speed\", 0.0) or e.get(\"GPS_speed\", 0.0)\n#                 spd.append(s)\n#                 accx.append(e.get(\"accX\", 0.0))\n#                 accy.append(e.get(\"accY\", 0.0))\n#                 gyz.append(e.get(\"gyroZ\", 0.0))\n\n#     scalers = {}\n#     if spd:\n#         scalers[\"ego_speed_mean\"] = float(np.mean(spd))\n#         scalers[\"ego_speed_std\"]  = float(max(np.std(spd), 1e-6))\n#     if accx:\n#         scalers[\"accX_mean\"] = float(np.mean(accx))\n#         scalers[\"accX_std\"]  = float(max(np.std(accx), 1e-6))\n#         scalers[\"accY_mean\"] = float(np.mean(accy))\n#         scalers[\"accY_std\"]  = float(max(np.std(accy), 1e-6))\n#     if gyz:\n#         scalers[\"gyroZ_mean\"] = float(np.mean(gyz))\n#         scalers[\"gyroZ_std\"]  = float(max(np.std(gyz), 1e-6))\n\n#     print(\"Scalers:\", scalers)\n\n#     # 3) extract full training dataset --------------------------------------\n#     print(\"\\nExtracting training sequences (all streams) …\")\n#     full_ds = PIEDataset(\n#         pie_db,\n#         TRAIN_SETS_STR,\n#         POSE_DATA_DIR,\n#         SEQ_LEN,\n#         PRED_LEN,\n#         scalers,\n#         ALL_POSSIBLE_STREAMS,\n#     )\n\n#     train_dict = {s: [] for s in ALL_POSSIBLE_STREAMS}\n#     train_dict[\"label\"] = []\n\n#     for i in tqdm(range(len(full_ds)), desc=\"seq\"):\n#         feat, lbl = full_ds[i]\n#         for s in ALL_POSSIBLE_STREAMS:\n#             train_dict[s].append(feat[s].numpy())\n#         train_dict[\"label\"].append([lbl.item()])\n\n#     print(f\"Raw training samples: {len(train_dict['label'])}\")\n\n#     # 4) balance -------------------------------------------------------------\n#     balanced = balance_samples_count(train_dict, \"label\")\n#     del train_dict, full_ds\n#     gc.collect()\n\n#     # 5) write pickles -------------------------------------------------------\n#     print(\"\\nSaving balanced data …\")\n#     with open(BALANCED_DATA_PKL_PATH, \"wb\") as fp:\n#         pickle.dump(balanced, fp, pickle.HIGHEST_PROTOCOL)\n#     print(f\"✓ {BALANCED_DATA_PKL_PATH}\")\n\n#     print(\"Saving scalers …\")\n#     with open(SCALERS_PKL_PATH, \"wb\") as fp:\n#         pickle.dump(scalers, fp, pickle.HIGHEST_PROTOCOL)\n#     print(f\"✓ {SCALERS_PKL_PATH}\")\n\n#     del pie_db\n#     gc.collect()\n\n#     print(\"\\n--- CELL 1: DATA PREPARATION COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:48:03.811065Z","iopub.execute_input":"2025-05-11T15:48:03.811411Z","execution_failed":"2025-05-11T16:04:33.218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- CELL 2: ABLATION STUDY – MODEL TRAINING AND EVALUATION (with Weighted Average Fusion) ---\n\n# import os\n# import sys\n# import gc\n# import time\n# import math\n# import random\n# import pickle\n# import torch\n# import numpy as np\n# import pandas as pd                      # results-summary table\n# import torch.nn as nn\n# import torch.optim as optim\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# from tqdm.notebook import tqdm\n# from torch.utils.data import Dataset, DataLoader\n# from sklearn.metrics import (\n#     accuracy_score,\n#     precision_recall_fscore_support,\n#     roc_auc_score,\n#     confusion_matrix,\n#     ConfusionMatrixDisplay,\n#     # f1_score # Explicitly import if used directly, or use from precision_recall_fscore_support\n# )\n\n# # --- Add PIE utilities path if necessary (adjust path) ------------------------\n# pie_utilities_path = \"/kaggle/working/PIE/utilities\"\n# if pie_utilities_path not in sys.path:\n#     sys.path.insert(0, pie_utilities_path)\n\n# try:\n#     from pie_data import PIE\n# except ImportError as e:\n#     print(f\"Warn: Could not import PIE class: {e}\")\n#     PIE = None\n\n# # --- Configuration ------------------------------------------------------------\n# PIE_ROOT_PATH = \"/kaggle/working/PIE\"\n# POSE_DATA_DIR = \"/kaggle/input/pose-data/extracted_poses2\"\n# PIE_DATABASE_CACHE_PATH = \"/kaggle/input/pie-database/pie_database.pkl\"\n\n# # --- Define ALL possible streams (used by Dataset class) ----------------------\n# ALL_POSSIBLE_STREAMS_CELL2 = [ # Renamed to avoid conflict if cell1 not run\n#     \"bbox\",\n#     \"pose\",\n#     \"ego_speed\",\n#     \"ego_acc\",\n#     \"ego_gyro\",\n#     \"ped_action\",\n#     \"ped_look\",\n#     \"ped_occlusion\",\n#     \"traffic_light\",\n#     \"static_context\",\n# ]\n\n# # --- *** CHOOSE ACTIVE STREAMS FOR THIS EXPERIMENT *** ------------------------\n# ACTIVE_STREAMS = [\n#     \"bbox\",\n#     \"ped_action\",\n#     \"ped_look\",\n#     \"ego_speed\",\n#     \"ego_acc\",\n#     \"static_context\",\n# ]\n# # ------------------------------------------------------------------------------\n\n# print(f\"--- Running Weighted Average Fusion With Active Streams: {ACTIVE_STREAMS} ---\")\n\n# # --- Model Hyper-parameters ---------------------------------------------------\n# SEQ_LEN, PRED_LEN = 30, 1\n# INPUT_SIZE_BBOX = 4\n# INPUT_SIZE_POSE = 34\n# INPUT_SIZE_EGO_SPEED = 1\n# INPUT_SIZE_EGO_ACC = 2\n# INPUT_SIZE_EGO_GYRO = 1\n# INPUT_SIZE_PED_ACTION = 1\n# INPUT_SIZE_PED_LOOK = 1\n# INPUT_SIZE_PED_OCC = 1\n# INPUT_SIZE_TL_STATE = 4\n# NUM_SIGNALIZED_CATS = 4\n# NUM_INTERSECTION_CATS = 5\n# NUM_AGE_CATS = 4\n# NUM_GENDER_CATS = 3\n# NUM_TRAFFIC_DIR_CATS = 2\n# LANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7: 4, 8: 4}\n# NUM_LANE_CATS = len(set(LANE_CATEGORIES.values()))\n# INPUT_SIZE_STATIC = (\n#     NUM_SIGNALIZED_CATS\n#     + NUM_INTERSECTION_CATS\n#     + NUM_AGE_CATS\n#     + NUM_GENDER_CATS\n#     + NUM_TRAFFIC_DIR_CATS\n#     + NUM_LANE_CATS\n# )\n\n# LSTM_HIDDEN_SIZE = 256\n# NUM_LSTM_LAYERS = 2\n# DROPOUT_RATE = 0.3\n# NUM_CLASSES = 2\n# ATTENTION_DIM = 128\n\n# # --- Training Hyper-parameters ------------------------------------------------\n# LEARNING_RATE = 1e-4\n# BATCH_SIZE = 32\n# NUM_EPOCHS = 5\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {DEVICE}\")\n\n# # --- Dataset splits -----------------------------------------------------------\n# VAL_SETS_STR = [\"set05\", \"set06\"]\n\n# # --- Paths for pre-processed data --------------------------------------------\n# BALANCED_DATA_PKL_PATH = \"/kaggle/working/balanced_train_data.pkl\"\n# SCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\"\n\n# # -----------------------------------------------------------------------------#\n# #                               Helper classes (Dataset)                       #\n# # -----------------------------------------------------------------------------\n\n# # Using to_one_hot from Cell 1. If Cell 1 is not run, define it here:\n# if 'to_one_hot' not in globals():\n#     def to_one_hot(index, num_classes):\n#         vec = np.zeros(num_classes, dtype=np.float32)\n#         safe_index = int(np.clip(index, 0, num_classes - 1))\n#         vec[safe_index] = 1.0\n#         return vec\n\n# class PIEDataset_Cell2(Dataset): # Renamed to avoid conflict if Cell 1 is run in same notebook\n#     \"\"\"\n#     Dataset that can dynamically enable/disable streams. (Copied from original cell 2)\n#     \"\"\"\n#     def __init__(\n#         self,\n#         pie_database,\n#         set_names,\n#         pose_data_dir,\n#         seq_len,\n#         pred_len,\n#         scalers=None,\n#         active_streams=None, # This should be ALL_POSSIBLE_STREAMS_CELL2 for val\n#     ):\n#         self.pie_db = pie_database\n#         self.set_names = set_names\n#         self.pose_data_dir = pose_data_dir\n#         self.seq_len = seq_len\n#         self.pred_len = pred_len\n#         self.scalers = scalers or {}\n#         # This dataset needs to be able to generate ALL streams,\n#         # then the loader or model will select the ACTIVE_STREAMS.\n#         # However, for efficiency, it's better if it only processes active_streams.\n#         # The original code passed ALL_POSSIBLE_STREAMS to PIEDataset for validation.\n#         # Let's assume it should prepare data for any of ALL_POSSIBLE_STREAMS_CELL2\n#         # but only return those specified by a different active_streams parameter later.\n#         # For now, let's assume active_streams passed here are the ones to process.\n#         self.streams_to_extract = active_streams or ALL_POSSIBLE_STREAMS_CELL2\n#         self.sequences = []\n#         self.all_pose_data = {}\n\n#         self._input_sizes_for_error = self._get_input_sizes_dict()\n\n#         if \"pose\" in self.streams_to_extract:\n#             self._load_pose_data()\n\n#         self._generate_sequence_list()\n#         if not self.sequences:\n#             # Make this a warning or handle it, as val set might be empty based on set_names\n#             print(f\"Warning: PIEDataset_Cell2 init: No sequences for {self.set_names}\")\n\n\n#     def _get_input_sizes_dict(self):\n#         input_sizes = {}\n#         special_cases = {\n#             \"TRAFFIC_LIGHT\": \"TL_STATE\", \"STATIC_CONTEXT\": \"STATIC\",\n#             \"EGO_SPEED\": \"EGO_SPEED\", \"EGO_ACC\": \"EGO_ACC\", \"EGO_GYRO\": \"EGO_GYRO\",\n#             \"PED_ACTION\": \"PED_ACTION\", \"PED_LOOK\": \"PED_LOOK\", \"PED_OCCLUSION\": \"PED_OCC\",\n#         }\n#         for stream in ALL_POSSIBLE_STREAMS_CELL2: # Check against all possible for safety\n#             size_constant_name = f\"INPUT_SIZE_{stream.upper()}\"\n#             suffix = special_cases.get(stream.upper())\n#             if suffix: size_constant_name = f\"INPUT_SIZE_{suffix}\"\n#             elif stream == \"bbox\": size_constant_name = \"INPUT_SIZE_BBOX\"\n#             elif stream == \"pose\": size_constant_name = \"INPUT_SIZE_POSE\"\n#             input_sizes[stream] = globals().get(size_constant_name, 1)\n#         return input_sizes\n\n#     def _load_pose_data(self):\n#         sets_loaded_count = 0\n#         for set_id in self.set_names:\n#             self.all_pose_data[set_id] = {}\n#             pose_set_path = os.path.join(self.pose_data_dir, set_id)\n#             if not os.path.isdir(pose_set_path): continue\n#             pkl_files_in_set = [f for f in os.listdir(pose_set_path) if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")]\n#             if not pkl_files_in_set: continue\n#             loaded_video_count = 0\n#             for pkl_filename in pkl_files_in_set:\n#                 pkl_file_path = os.path.join(pose_set_path, pkl_filename)\n#                 try:\n#                     with open(pkl_file_path, \"rb\") as f: loaded_pkl_content = pickle.load(f)\n#                 except FileNotFoundError: continue\n#                 except Exception as e: print(f\"Error loading pose PKL {pkl_file_path}: {e}\"); continue\n#                 if len(loaded_pkl_content) != 1: continue\n#                 unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n#                 video_id = \"_\".join(unique_video_key.split(\"_\")[1:])\n#                 if video_id in self.pie_db.get(set_id, {}):\n#                     self.all_pose_data[set_id][video_id] = video_data\n#                     loaded_video_count += 1\n#             if loaded_video_count > 0: sets_loaded_count += 1\n\n#     def _generate_sequence_list(self):\n#         sequence_count = 0\n#         for set_id in self.set_names:\n#             if set_id not in self.pie_db: continue\n#             for video_id, video_data in self.pie_db[set_id].items():\n#                 if \"ped_annotations\" not in video_data: continue\n#                 for ped_id, ped_data in video_data[\"ped_annotations\"].items():\n#                     frames = ped_data.get(\"frames\", [])\n#                     if len(frames) < self.seq_len + self.pred_len: continue\n#                     frames_sorted = sorted(frames)\n#                     for i in range(len(frames_sorted) - self.seq_len - self.pred_len + 1):\n#                         start_f, obs_end_f = frames_sorted[i], frames_sorted[i + self.seq_len - 1]\n#                         if obs_end_f - start_f != self.seq_len - 1: continue\n#                         target_idx = i + self.seq_len + self.pred_len - 1\n#                         if target_idx >= len(frames_sorted): continue\n#                         target_f = frames_sorted[target_idx]\n#                         if target_f - obs_end_f != self.pred_len: continue\n#                         self.sequences.append((set_id, video_id, ped_id, start_f))\n#                         sequence_count += 1\n#         print(f\"PIEDataset_Cell2 initialized with {sequence_count} sequences for sets {self.set_names}.\")\n\n#     def __len__(self): return len(self.sequences)\n\n#     def __getitem__(self, idx):\n#         set_id, video_id, ped_id, start_frame = self.sequences[idx]\n#         frame_nums = list(range(start_frame, start_frame + self.seq_len))\n#         target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n#         video_db, ped_db = self.pie_db.get(set_id, {}).get(video_id, {}), {}\n#         if video_db: ped_db = video_db.get(\"ped_annotations\", {}).get(ped_id, {})\n#         ego_db = video_db.get(\"vehicle_annotations\", {}) if video_db else {}\n#         traffic_db = video_db.get(\"traffic_annotations\", {}) if video_db else {}\n#         ped_attributes = ped_db.get(\"attributes\", {}) if ped_db else {}\n\n#         feature_sequences = {s: [] for s in self.streams_to_extract}\n#         label = 0\n#         if ped_db and \"frames\" in ped_db and \"behavior\" in ped_db and \"cross\" in ped_db[\"behavior\"]:\n#             try:\n#                 target_idx = ped_db[\"frames\"].index(target_frame_num)\n#                 label = ped_db[\"behavior\"][\"cross\"][target_idx]\n#                 if label == -1: label = 0\n#             except (ValueError, IndexError): pass\n\n#         static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n#         if \"static_context\" in self.streams_to_extract:\n#             sig_idx, int_idx = ped_attributes.get(\"signalized\", 0), ped_attributes.get(\"intersection\", 0)\n#             age_idx, gen_idx = ped_attributes.get(\"age\", 2), ped_attributes.get(\"gender\", 0)\n#             td_idx = int(ped_attributes.get(\"traffic_direction\", 0))\n#             nl_val = ped_attributes.get(\"num_lanes\", 2)\n#             nl_cat_idx = LANE_CATEGORIES.get(nl_val, LANE_CATEGORIES[max(LANE_CATEGORIES.keys())])\n#             static_vec = np.concatenate([\n#                 to_one_hot(sig_idx, NUM_SIGNALIZED_CATS), to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n#                 to_one_hot(age_idx, NUM_AGE_CATS), to_one_hot(gen_idx, NUM_GENDER_CATS),\n#                 to_one_hot(td_idx, NUM_TRAFFIC_DIR_CATS), to_one_hot(nl_cat_idx, NUM_LANE_CATS),\n#             ])\n#             if static_vec.shape[0] != INPUT_SIZE_STATIC: static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n\n#         for frame_num in frame_nums:\n#             frame_db_idx = -1\n#             if ped_db and \"frames\" in ped_db:\n#                 try: frame_db_idx = ped_db[\"frames\"].index(frame_num)\n#                 except ValueError: pass\n#             ego_frame_data = ego_db.get(frame_num, {})\n\n#             if \"bbox\" in self.streams_to_extract:\n#                 bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32)\n#                 if ped_db and frame_db_idx!=-1 and \"bbox\" in ped_db and len(ped_db[\"bbox\"]) > frame_db_idx:\n#                     try:\n#                         x1,y1,x2,y2 = ped_db[\"bbox\"][frame_db_idx]\n#                         img_w, img_h = (video_db.get(\"width\",1920) if video_db else 1920), (video_db.get(\"height\",1080) if video_db else 1080)\n#                         if img_w > 0 and img_h > 0:\n#                             cx,cy,w,h = ((x1+x2)/2)/img_w, ((y1+y2)/2)/img_h, (x2-x1)/img_w, (y2-y1)/img_h\n#                             if 0<w and 0<h and 0<=cx<=1 and 0<=cy<=1: bbox_norm=np.array([cx,cy,w,h],dtype=np.float32)\n#                     except Exception: pass\n#                 feature_sequences[\"bbox\"].append(bbox_norm)\n#             if \"pose\" in self.streams_to_extract:\n#                 pose_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n#                 vid_pose = self.all_pose_data.get(set_id,{}).get(video_id,{})\n#                 loaded_pose = vid_pose.get(frame_num,{}).get(ped_id)\n#                 if isinstance(loaded_pose,np.ndarray) and loaded_pose.shape==(INPUT_SIZE_POSE,): pose_vector=loaded_pose\n#                 feature_sequences[\"pose\"].append(pose_vector)\n#             if \"ego_speed\" in self.streams_to_extract:\n#                 speed = ego_frame_data.get(\"OBD_speed\",0.0) or ego_frame_data.get(\"GPS_speed\",0.0)\n#                 speed_scaled = (speed - self.scalers.get(\"ego_speed_mean\",0.0)) / self.scalers.get(\"ego_speed_std\",1.0)\n#                 feature_sequences[\"ego_speed\"].append([speed_scaled])\n#             if \"ego_acc\" in self.streams_to_extract:\n#                 ax,ay = ego_frame_data.get(\"accX\",0.0), ego_frame_data.get(\"accY\",0.0)\n#                 ax_s = (ax - self.scalers.get(\"accX_mean\",0.0))/self.scalers.get(\"accX_std\",1.0)\n#                 ay_s = (ay - self.scalers.get(\"accY_mean\",0.0))/self.scalers.get(\"accY_std\",1.0)\n#                 feature_sequences[\"ego_acc\"].append([ax_s, ay_s])\n#             if \"ego_gyro\" in self.streams_to_extract:\n#                 gz = ego_frame_data.get(\"gyroZ\",0.0)\n#                 gz_s = (gz - self.scalers.get(\"gyroZ_mean\",0.0))/self.scalers.get(\"gyroZ_std\",1.0)\n#                 feature_sequences[\"ego_gyro\"].append([gz_s])\n#             if \"ped_action\" in self.streams_to_extract:\n#                 action=0\n#                 if ped_db and frame_db_idx!=-1 and \"behavior\" in ped_db and \"action\" in ped_db[\"behavior\"] and len(ped_db[\"behavior\"][\"action\"])>frame_db_idx:\n#                     action = ped_db[\"behavior\"][\"action\"][frame_db_idx]\n#                 feature_sequences[\"ped_action\"].append([float(action)])\n#             if \"ped_look\" in self.streams_to_extract:\n#                 look=0\n#                 if ped_db and frame_db_idx!=-1 and \"behavior\" in ped_db and \"look\" in ped_db[\"behavior\"] and len(ped_db[\"behavior\"][\"look\"])>frame_db_idx:\n#                     look = ped_db[\"behavior\"][\"look\"][frame_db_idx]\n#                 feature_sequences[\"ped_look\"].append([float(look)])\n#             if \"ped_occlusion\" in self.streams_to_extract:\n#                 occ=0.0\n#                 if ped_db and frame_db_idx!=-1 and \"occlusion\" in ped_db and len(ped_db[\"occlusion\"])>frame_db_idx:\n#                     occ = float(ped_db[\"occlusion\"][frame_db_idx])/2.0\n#                 feature_sequences[\"ped_occlusion\"].append([occ])\n#             if \"traffic_light\" in self.streams_to_extract:\n#                 state_int=0\n#                 for _,obj_data in traffic_db.items():\n#                     if obj_data.get(\"obj_class\")==\"traffic_light\" and \"frames\" in obj_data and \"state\" in obj_data:\n#                         try:\n#                             tl_idx = obj_data[\"frames\"].index(frame_num)\n#                             if obj_data[\"state\"][tl_idx]!=0: state_int=obj_data[\"state\"][tl_idx]; break\n#                         except (ValueError,IndexError): continue\n#                 feature_sequences[\"traffic_light\"].append(to_one_hot(state_int,INPUT_SIZE_TL_STATE))\n#             if \"static_context\" in self.streams_to_extract:\n#                 feature_sequences[\"static_context\"].append(static_vec)\n#         features = {}\n#         try:\n#             for name in self.streams_to_extract:\n#                 features[name] = torch.tensor(np.asarray(feature_sequences[name],dtype=np.float32),dtype=torch.float32)\n#         except Exception as e:\n#             print(f\"Error converting features idx {idx} ({set_id},{video_id},{ped_id},{start_frame}): {e}. Ret zeros.\")\n#             features = {name:torch.zeros((self.seq_len,self._input_sizes_for_error.get(name,1)),dtype=torch.float32) for name in self.streams_to_extract}\n#         return features, torch.tensor(label,dtype=torch.long)\n\n\n# class BalancedDataset(Dataset):\n#     def __init__(self, data_dict, active_streams, label_key=\"label\"):\n#         self.active_streams = active_streams\n#         self.label_key = label_key\n#         if self.label_key not in data_dict or not data_dict[self.label_key]:\n#             raise ValueError(f\"Label key '{self.label_key}' missing/empty.\")\n#         self.num_samples = len(data_dict[self.label_key])\n#         if self.num_samples == 0: print(\"Warning: BalancedDataset initialized with zero samples.\")\n#         self.features = {}\n#         for stream in self.active_streams:\n#             if stream in data_dict and data_dict[stream]:\n#                 try: self.features[stream] = torch.tensor(np.asarray(data_dict[stream]),dtype=torch.float32)\n#                 except ValueError as e: raise ValueError(f\"Error converting stream '{stream}': {e}\")\n#             else: raise KeyError(f\"Stream '{stream}' missing or empty in data.\")\n#         try: self.labels = torch.tensor([lbl[0] for lbl in data_dict[self.label_key]],dtype=torch.long)\n#         except (IndexError, TypeError) as e: raise ValueError(f\"Error converting labels: {e}\")\n#         for stream in self.active_streams:\n#             if len(self.features[stream]) != self.num_samples:\n#                 raise ValueError(f\"Len mismatch: '{stream}' ({len(self.features[stream])}) vs labels ({self.num_samples})\")\n#     def __len__(self): return self.num_samples\n#     def __getitem__(self, idx):\n#         feature_dict = {s: self.features[s][idx] for s in self.active_streams}\n#         label = self.labels[idx]\n#         return feature_dict, label\n\n\n# class Attention(nn.Module):\n#     def __init__(self, hidden_dim, attention_dim):\n#         super().__init__()\n#         self.attention_net = nn.Sequential(\n#             nn.Linear(hidden_dim, attention_dim), nn.Tanh(), nn.Linear(attention_dim, 1),\n#         )\n#     def forward(self, lstm_output):\n#         att_scores = self.attention_net(lstm_output).squeeze(2)\n#         att_weights = torch.softmax(att_scores, dim=1)\n#         context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1)\n#         return context_vector, att_weights\n\n# # -----------------------------------------------------------------------------#\n# #                  ***  MODEL WITH WEIGHTED AVERAGE FUSION  ***                #\n# # -----------------------------------------------------------------------------\n# class MultiStreamWeightedAvgLSTM(nn.Module):\n#     def __init__(\n#         self, input_sizes, lstm_hidden_size, num_lstm_layers,\n#         num_classes, attention_dim, dropout_rate, stream_names=None,\n#     ):\n#         super().__init__()\n#         if not stream_names: raise ValueError(\"stream_names cannot be empty.\")\n#         self.stream_names = stream_names\n#         self.num_active_streams = len(stream_names)\n#         self.lstm_output_dim = lstm_hidden_size * 2\n#         self.lstms, self.attentions = nn.ModuleDict(), nn.ModuleDict()\n#         print(f\"Initializing Weighted-Avg model with streams: {self.stream_names}\")\n#         for name in self.stream_names:\n#             if name not in input_sizes: raise KeyError(f\"Input size for stream '{name}' not provided.\")\n#             in_size = input_sizes[name]\n#             print(f\"  – Adding stream '{name}' (input {in_size})\")\n#             self.lstms[name] = nn.LSTM(\n#                 in_size, lstm_hidden_size, num_lstm_layers, batch_first=True,\n#                 dropout=dropout_rate if num_lstm_layers > 1 else 0, bidirectional=True,\n#             )\n#             self.attentions[name] = Attention(self.lstm_output_dim, attention_dim)\n#         self.fusion_weights = nn.Parameter(torch.ones(self.num_active_streams))\n#         fused_dim = self.lstm_output_dim\n#         self.dropout = nn.Dropout(dropout_rate)\n#         inter_dim = max(num_classes * 4, fused_dim // 2)\n#         self.fc1 = nn.Linear(fused_dim, inter_dim)\n#         self.relu = nn.ReLU()\n#         self.fc2 = nn.Linear(inter_dim, num_classes)\n\n#     def forward(self, x):\n#         ctx_vecs = []\n#         for name in self.stream_names:\n#             if name not in x:\n#                 zero_ctx = torch.zeros(x[next(iter(x))].shape[0], self.lstm_output_dim, device=x[next(iter(x))].device)\n#                 ctx_vecs.append(zero_ctx)\n#                 continue\n#             lstm_out, _ = self.lstms[name](x[name])\n#             context_vector, _ = self.attentions[name](lstm_out)\n#             ctx_vecs.append(context_vector)\n#         if len(ctx_vecs) != self.num_active_streams:\n#             raise RuntimeError(f\"context_vectors({len(ctx_vecs)}) != num_streams({self.num_active_streams})\")\n#         stacked = torch.stack(ctx_vecs, dim=1)\n#         weights = torch.softmax(self.fusion_weights, dim=0).view(1, -1, 1)\n#         fused = torch.sum(stacked * weights, dim=1)\n#         out = self.dropout(fused)\n#         out = self.relu(self.fc1(out))\n#         out = self.dropout(out)\n#         logits = self.fc2(out)\n#         return logits\n\n# # -----------------------------------------------------------------------------#\n# #         Training / evaluation / Threshold Tuning helpers                     #\n# # -----------------------------------------------------------------------------\n\n# def get_all_probabilities_and_labels(model, dataloader, device):\n#     \"\"\"Gets true labels and predicted probabilities for the positive class.\"\"\"\n#     model.eval()\n#     labels_all, probs_all_positive_class = [], []\n#     active_streams = model.stream_names\n\n#     with torch.no_grad():\n#         for feats, labels in tqdm(dataloader, desc=\"Getting Probs & Labels for Threshold Tuning\", leave=False):\n#             inputs = {name: feats[name].to(device) for name in active_streams if name in feats}\n#             outputs = model(inputs)  # Logits\n#             probs = torch.softmax(outputs, dim=1)  # Probabilities\n\n#             labels_all.extend(labels.cpu().numpy())\n#             probs_all_positive_class.extend(probs[:, 1].cpu().numpy()) # Prob for class 1\n\n#     return np.asarray(labels_all), np.asarray(probs_all_positive_class)\n\n\n# def find_optimal_threshold(y_true, y_probs_positive_class, metric='f1', steps=100):\n#     \"\"\"Finds the optimal probability threshold for a binary classifier to maximize a metric.\"\"\"\n#     best_threshold = 0.5\n#     best_metric_val = -1.0\n\n#     if metric == 'f1':\n#         # precision_recall_fscore_support returns (precision, recall, f1, support)\n#         metric_func = lambda yt, yp: precision_recall_fscore_support(yt, yp, average='binary', pos_label=1, zero_division=0)[2]\n#     else:\n#         raise ValueError(f\"Unsupported metric for threshold tuning: {metric}\")\n\n#     thresholds = np.linspace(0.0, 1.0, steps + 1) # e.g., 0.0, 0.01, ..., 1.0\n\n#     for threshold in tqdm(thresholds, desc=f\"Tuning Threshold ({metric.upper()})\", leave=False):\n#         y_pred_tuned = (y_probs_positive_class >= threshold).astype(int)\n#         current_metric_val = metric_func(y_true, y_pred_tuned)\n\n#         if current_metric_val > best_metric_val:\n#             best_metric_val = current_metric_val\n#             best_threshold = threshold\n#         elif current_metric_val == best_metric_val: # Tie-breaking\n#             if abs(threshold - 0.5) < abs(best_threshold - 0.5):\n#                 best_threshold = threshold\n    \n#     print(f\"Optimal threshold found: {best_threshold:.4f} (max {metric.upper()}: {best_metric_val:.4f})\")\n#     return best_threshold, best_metric_val\n\n\n# def train_epoch(model, dataloader, optimizer, criterion, device):\n#     model.train()\n#     total_loss = 0.0\n#     all_preds, all_labels = [], []\n#     active = model.stream_names\n#     for feats, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n#         inputs = {n: feats[n].to(device) for n in active if n in feats}\n#         labels = labels.to(device)\n#         optimizer.zero_grad()\n#         outputs = model(inputs)\n#         loss = criterion(outputs, labels)\n#         loss.backward()\n#         optimizer.step()\n#         total_loss += loss.item()\n#         all_preds.extend(torch.argmax(outputs, 1).cpu().numpy())\n#         all_labels.extend(labels.cpu().numpy())\n#     return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n\n\n# def evaluate_epoch(model, dataloader, criterion, device):\n#     \"\"\"Evaluates model for one epoch, returns metrics at 0.5 threshold and raw probabilities.\"\"\"\n#     model.eval()\n#     total_loss = 0.0\n#     all_labels_list, all_preds_list, all_probs_list = [], [], [] # Changed to list for easier appending\n#     active = model.stream_names\n\n#     with torch.no_grad():\n#         for feats, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n#             inputs = {n: feats[n].to(device) for n in active if n in feats}\n#             labels_gpu = labels.to(device) # Ensure labels are on device for loss\n#             outputs = model(inputs)\n#             loss = criterion(outputs, labels_gpu) # Use labels_gpu for loss\n#             total_loss += loss.item()\n#             probs = torch.softmax(outputs, 1)\n#             preds = torch.argmax(probs, 1)\n#             all_labels_list.extend(labels.cpu().numpy()) # Use original labels for metrics\n#             all_preds_list.extend(preds.cpu().numpy())\n#             all_probs_list.extend(probs.cpu().numpy()) # Store all class probabilities\n\n#     avg_loss = total_loss / max(1, len(dataloader))\n#     all_labels = np.asarray(all_labels_list)\n#     all_preds = np.asarray(all_preds_list)\n#     all_probs_np = np.asarray(all_probs_list) # Probabilities for all classes\n\n#     acc = accuracy_score(all_labels, all_preds)\n#     prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", pos_label=1, zero_division=0)\n#     auc = roc_auc_score(all_labels, all_probs_np[:, 1]) if len(np.unique(all_labels)) > 1 else float(\"nan\")\n    \n#     return {\n#         \"loss\": avg_loss, \"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"auc\": auc,\n#         \"labels\": all_labels, \"probs_positive_class\": all_probs_np[:, 1] # Return these for potential later use\n#     }\n\n# # get_predictions_and_labels is used for CM after training. We can update it to use tuned threshold\n# # or create a new one. For now, the main block will handle predictions with tuned threshold.\n# # The original get_predictions_and_labels can be kept if needed for 0.5 threshold CM elsewhere.\n\n# def get_predictions_at_threshold(y_probs_positive_class, threshold):\n#     return (y_probs_positive_class >= threshold).astype(int)\n\n# # -----------------------------------------------------------------------------#\n# #                            Main execution block                              #\n# # -----------------------------------------------------------------------------\n\n# if __name__ == \"__main__\":\n#     print(\"--- CELL 2: Running Model Training/Evaluation with Weighted Fusion ---\")\n#     print(f\"Active Streams: {ACTIVE_STREAMS}\")\n\n#     print(\"\\nLoading balanced training data …\")\n#     try:\n#         with open(BALANCED_DATA_PKL_PATH, \"rb\") as f: balanced_train_data_dict = pickle.load(f)\n#         with open(SCALERS_PKL_PATH, \"rb\") as f: scalers = pickle.load(f)\n#         print(\"   ✓ pre-processed data loaded.\")\n#     except FileNotFoundError as e: print(f\"ERROR: {e}. Run cell 1 first.\"); sys.exit(1)\n#     except Exception as e: print(f\"Error loading pre-processed data: {e}\"); sys.exit(1)\n\n#     print(\"\\nLoading PIE database cache for validation …\")\n#     if not os.path.exists(PIE_DATABASE_CACHE_PATH): raise FileNotFoundError(\"PIE db cache not found.\")\n#     try:\n#         with open(PIE_DATABASE_CACHE_PATH, \"rb\") as f: pie_database = pickle.load(f)\n#     except Exception as e: raise RuntimeError(f\"Failed to load PIE database: {e}\")\n#     print(\"   ✓ PIE database loaded.\")\n\n#     print(\"\\nCreating Datasets and DataLoaders …\")\n#     try:\n#         train_dataset = BalancedDataset(balanced_train_data_dict, ACTIVE_STREAMS, label_key=\"label\")\n#         del balanced_train_data_dict\n#         # For validation, PIEDataset_Cell2 should be configured to extract ALL_POSSIBLE_STREAMS_CELL2\n#         # if the model might use any of them, or just ACTIVE_STREAMS if validation uses same streams.\n#         # Original code used ALL_POSSIBLE_STREAMS for val_dataset's PIEDataset.\n#         # Assuming val_dataset also processes only ACTIVE_STREAMS for consistency with model input.\n#         val_dataset = PIEDataset_Cell2(\n#             pie_database, VAL_SETS_STR, POSE_DATA_DIR, SEQ_LEN, PRED_LEN,\n#             scalers, active_streams=ALL_POSSIBLE_STREAMS_CELL2, # Val dataset prepares all, model selects\n#         )\n#     except Exception as e: print(f\"Error creating datasets: {e}\"); raise\n#     if len(train_dataset) == 0 : raise ValueError(\"Train dataset is empty!\")\n#     if len(val_dataset) == 0 : print(\"Warning: Validation dataset is empty! Check VAL_SETS_STR.\")\n\n\n#     train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n#     val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n#     print(\"   ✓ DataLoaders ready.\")\n#     del pie_database; gc.collect()\n\n#     print(\"\\nInitialising model …\")\n#     current_input_sizes = {}\n#     SPECIAL_CELL2 = { # Renamed for safety\n#         \"TRAFFIC_LIGHT\": \"TL_STATE\", \"STATIC_CONTEXT\": \"STATIC\", \"EGO_SPEED\": \"EGO_SPEED\",\n#         \"EGO_ACC\": \"EGO_ACC\", \"EGO_GYRO\": \"EGO_GYRO\", \"PED_ACTION\": \"PED_ACTION\",\n#         \"PED_LOOK\": \"PED_LOOK\", \"PED_OCCLUSION\": \"PED_OCC\",\n#     }\n#     for s in ACTIVE_STREAMS: # Model is built only for ACTIVE_STREAMS\n#         name = f\"INPUT_SIZE_{SPECIAL_CELL2.get(s.upper(), s.upper())}\"\n#         if s == \"bbox\": name = \"INPUT_SIZE_BBOX\"\n#         elif s == \"pose\": name = \"INPUT_SIZE_POSE\"\n#         if name not in globals(): raise ValueError(f\"Input-size const {name} not found.\")\n#         current_input_sizes[s] = globals()[name]\n\n#     model = MultiStreamWeightedAvgLSTM(\n#         current_input_sizes, LSTM_HIDDEN_SIZE, NUM_LSTM_LAYERS, NUM_CLASSES,\n#         ATTENTION_DIM, DROPOUT_RATE, stream_names=ACTIVE_STREAMS,\n#     ).to(DEVICE)\n#     print(\"\\n--- Model architecture ---\"); print(model)\n#     total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n#     print(f\"Trainable parameters: {total_params:,}\\n{'-'*30}\")\n\n#     print(\"\\nCalculating class weights …\")\n#     train_labels = train_dataset.labels.tolist()\n#     n0, n1 = train_labels.count(0), train_labels.count(1)\n#     total = len(train_labels)\n#     w0, w1 = (1.0,1.0) if total==0 else ( (total/(2.*n0), total/(2.*n1)) if n0>0 and n1>0 else ((0.,1.) if n0==0 else (1.,0.)) )\n#     class_weights = torch.tensor([w0, w1], dtype=torch.float32).to(DEVICE)\n#     print(f\"Loss weights → 0: {w0:.2f}, 1: {w1:.2f}\")\n#     criterion = nn.CrossEntropyLoss(weight=class_weights)\n#     optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n#     best_val_f1_at_0_5_thresh = -1.0 # F1 at 0.5 threshold for saving model\n#     history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": [], \"val_f1_0.5\": []}\n#     best_model_path = \"\"\n\n#     print(\"\\n--- Starting training ---\")\n#     for epoch in range(NUM_EPOCHS):\n#         t0 = time.time()\n#         train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n#         # evaluate_epoch returns metrics at 0.5 threshold\n#         epoch_eval_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n\n#         history[\"train_loss\"].append(train_loss)\n#         history[\"val_loss\"].append(epoch_eval_metrics[\"loss\"])\n#         history[\"train_acc\"].append(train_acc)\n#         history[\"val_acc\"].append(epoch_eval_metrics[\"accuracy\"])\n#         history[\"val_f1_0.5\"].append(epoch_eval_metrics[\"f1\"]) # F1 at 0.5\n\n#         print(f\"\\nEpoch {epoch + 1:02d}/{NUM_EPOCHS} – {time.time() - t0:.1f}s\")\n#         print(f\"  train loss {train_loss:.4f} | acc {train_acc:.4f}\")\n#         print(f\"  val   loss {epoch_eval_metrics['loss']:.4f} | acc {epoch_eval_metrics['accuracy']:.4f} (at 0.5 thresh)\")\n#         print(f\"           prec {epoch_eval_metrics['precision']:.4f} | rec {epoch_eval_metrics['recall']:.4f} | f1 {epoch_eval_metrics['f1']:.4f} | auc {epoch_eval_metrics['auc']:.4f} (at 0.5 thresh / auc indep.)\")\n\n#         if epoch_eval_metrics[\"f1\"] > best_val_f1_at_0_5_thresh:\n#             best_val_f1_at_0_5_thresh = epoch_eval_metrics[\"f1\"]\n#             best_model_path = f\"best_model_weighted_{'_'.join(sorted(ACTIVE_STREAMS))}_ep{epoch + 1}.pth\"\n#             torch.save(model.state_dict(), best_model_path)\n#             print(f\"  ✓ new best model saved → {best_model_path} (F1@0.5 {best_val_f1_at_0_5_thresh:.4f})\")\n#     print(\"\\n--- Training finished ---\")\n\n#     print(\"\\nPlotting training curves …\")\n#     fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n#     ax[0].plot(range(1, NUM_EPOCHS + 1), history[\"train_loss\"], label=\"Train Loss\")\n#     ax[0].plot(range(1, NUM_EPOCHS + 1), history[\"val_loss\"], label=\"Val Loss\")\n#     ax[0].set_xlabel(\"Epoch\"); ax[0].set_ylabel(\"Loss\"); ax[0].set_title(\"Loss curve\"); ax[0].legend(); ax[0].grid(True)\n#     ax[1].plot(range(1, NUM_EPOCHS + 1), history[\"train_acc\"], label=\"Train Acc\")\n#     ax[1].plot(range(1, NUM_EPOCHS + 1), history[\"val_acc\"], label=\"Val Acc (at 0.5)\")\n#     ax[1].plot(range(1, NUM_EPOCHS + 1), history[\"val_f1_0.5\"], \"--\", label=\"Val F1 (at 0.5)\")\n#     ax[1].set_xlabel(\"Epoch\"); ax[1].set_ylabel(\"Metric\"); ax[1].set_title(\"Accuracy & F1 (at 0.5 Thresh)\"); ax[1].legend(); ax[1].grid(True)\n#     plt.tight_layout(); plt.show()\n\n#     # ------------------- final evaluation (best model) & Threshold Tuning -----------------------\n#     print(\"\\n--- Final Evaluation on Validation set with Threshold Tuning ---\")\n#     if best_model_path and os.path.exists(best_model_path):\n#         try:\n#             model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n#             print(f\"Loaded best model: {best_model_path}\")\n#         except Exception as e: print(f\"Warning: could not load best model ({e}). Using last epoch params.\")\n#     else: print(\"Warning: best model not found or path empty, using last epoch parameters.\")\n\n#     # 1. Get probabilities and true labels from the validation set using the loaded best model\n#     y_true_val, y_probs_val_positive = get_all_probabilities_and_labels(model, val_loader, DEVICE)\n\n#     if len(y_true_val) == 0:\n#         print(\"Validation set is empty. Cannot perform threshold tuning or final evaluation.\")\n#     else:\n#         # 2. Find the optimal threshold using F1-score\n#         optimal_threshold, best_f1_at_optimal_thresh = find_optimal_threshold(y_true_val, y_probs_val_positive, metric='f1', steps=100)\n        \n#         # 3. Apply the optimal threshold to get new predictions\n#         y_pred_val_tuned = get_predictions_at_threshold(y_probs_val_positive, optimal_threshold)\n\n#         # 4. Calculate metrics using the tuned predictions\n#         final_accuracy_tuned = accuracy_score(y_true_val, y_pred_val_tuned)\n#         final_precision_tuned, final_recall_tuned, final_f1_tuned, _ = \\\n#             precision_recall_fscore_support(y_true_val, y_pred_val_tuned, average='binary', pos_label=1, zero_division=0)\n        \n#         final_auc_val = roc_auc_score(y_true_val, y_probs_val_positive) if len(np.unique(y_true_val)) > 1 else float('nan')\n\n#         cm_tuned = confusion_matrix(y_true_val, y_pred_val_tuned, labels=[0, 1])\n\n#         print(\"\\n--- Final metrics (Weighted Avg Fusion) with Tuned Threshold ---\")\n#         print(f\"{'Optimal Threshold':<20}: {optimal_threshold:.4f}\")\n#         print(f\"{'Accuracy':<20}: {final_accuracy_tuned:.4f}\")\n#         print(f\"{'Precision':<20}: {final_precision_tuned:.4f}\")\n#         print(f\"{'Recall':<20}: {final_recall_tuned:.4f}\")\n#         print(f\"{'F1-score':<20}: {final_f1_tuned:.4f} (Max F1 at optimal threshold)\")\n#         print(f\"{'AUC':<20}: {final_auc_val:.4f}\")\n#         print(f\"(Best F1@0.5 during training: {best_val_f1_at_0_5_thresh:.4f})\")\n\n#         ConfusionMatrixDisplay(cm_tuned, display_labels=[\"Not Crossing\", \"Crossing\"]).plot(cmap=plt.cm.Blues)\n#         plt.title(f\"Confusion Matrix (Optimal Threshold: {optimal_threshold:.2f})\")\n#         plt.show()\n\n#     if hasattr(model, \"fusion_weights\"):\n#         w = torch.softmax(model.fusion_weights, 0).detach().cpu().numpy()\n#         print(\"\\n--- Learned fusion weights (from best model) ---\")\n#         for stream, weight in zip(model.stream_names, w): print(f\"{stream:<15}: {weight:.4f}\")\n#         print(\"-\" * 30)\n\n#     print(\"\\n--- CELL 2: Script complete ---\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T16:04:33.220Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport time\nimport pickle\nimport gc\nfrom pathlib import Path\nimport shutil # For cleaning up image directories if needed\nimport random # For balancing\n\nimport cv2\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_recall_fscore_support,\n    roc_auc_score,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n)\nfrom ultralytics import YOLO\n\n# -----------------------------------------------------------------------------#\n#                                JAAD utilities                                #\n# -----------------------------------------------------------------------------#\nJAAD_REPO_PATH = \"/kaggle/working/JAAD\"\nif JAAD_REPO_PATH not in sys.path:\n    sys.path.insert(0, JAAD_REPO_PATH)\ntry:\n    from jaad_data import JAAD\nexcept ImportError as e:\n    print(f\"[ERROR] Could not import JAAD: {e}\")\n    JAAD = None; sys.exit(1)\n\n# -----------------------------------------------------------------------------#\n#                              Configuration                                   #\n# -----------------------------------------------------------------------------#\nJAAD_DATA_ROOT_PATH = \"/kaggle/working/JAAD\"\nJAAD_CLIPS_PATH = \"/kaggle/input/jaad-clips/JAAD_clips\" # Read-only\nJAAD_IMAGES_PATH = os.path.join(JAAD_DATA_ROOT_PATH, \"images_extracted_subset\") # Store subset here\n\n# --- Number of videos to process for this run ---\nNUM_JAAD_VIDEOS_TO_PROCESS = 5 # <<<<<<<<<  SET THIS TO A SMALL NUMBER (e.g., 5)\nFORCE_REEXTRACT_IMAGES_SUBSET = False # If True, will delete and re-extract images for the subset\n\nJAAD_BALANCED_SEQUENCES_PKL_PATH = f\"/kaggle/working/balanced_jaad_train_seq_v{NUM_JAAD_VIDEOS_TO_PROCESS}.pkl\"\nJAAD_VAL_SEQUENCES_PKL_PATH = f\"/kaggle/working/jaad_val_seq_v{NUM_JAAD_VIDEOS_TO_PROCESS}.pkl\"\nPRETRAINED_MODEL_PATH = '/kaggle/input/mslstm-pid/pytorch/default/1/best_model_weighted_bbox_ego_acc_ego_speed_ped_action_ped_look_static_context_ep2.pth'\n\nACTIVE_STREAMS_JAAD = [\"bbox\", \"ego_acc\", \"ego_speed\", \"ped_action\", \"ped_look\", \"static_context\", \"pose\"]\nprint(f\"--- Active streams for JAAD experiment: {ACTIVE_STREAMS_JAAD} ---\")\n\nSEQ_LEN, PRED_LEN = 30, 1\nINPUT_SIZE_BBOX_JAAD = 4; INPUT_SIZE_POSE_JAAD = 34\nINPUT_SIZE_EGO_SPEED_JAAD = 1; INPUT_SIZE_EGO_ACC_JAAD = 2\nINPUT_SIZE_PED_ACTION_JAAD = 1; INPUT_SIZE_PED_LOOK_JAAD = 1\nINPUT_SIZE_PED_OCC_JAAD = 1\n\nNUM_SIGNALIZED_CATS_JAAD_PADDED = 4; NUM_INTERSECTION_CATS_JAAD_PADDED = 5\nNUM_AGE_CATS_JAAD = 4; NUM_GENDER_CATS_JAAD = 3\nNUM_TRAFFIC_DIR_CATS_JAAD = 2\nLANE_CATEGORIES_JAAD = {1:0, 2:1, 3:2, 4:3, 5:4, 6:4, 7:4, 8:4, -1:1} # Default unknown to 2 lanes (idx 1)\nNUM_LANE_CATS_JAAD = 5\nINPUT_SIZE_STATIC_JAAD_PADDED = (\n    NUM_SIGNALIZED_CATS_JAAD_PADDED + NUM_INTERSECTION_CATS_JAAD_PADDED +\n    NUM_AGE_CATS_JAAD + NUM_GENDER_CATS_JAAD + NUM_TRAFFIC_DIR_CATS_JAAD + NUM_LANE_CATS_JAAD\n)\nif INPUT_SIZE_STATIC_JAAD_PADDED != 23:\n    print(f\"[WARNING] JAAD Static context size is {INPUT_SIZE_STATIC_JAAD_PADDED}, PIE model might expect 23.\")\n\nLEARNING_RATE = 1e-5 # Fine-tuning often uses smaller LR\nBATCH_SIZE = 8      # Smaller due to pose estimation overhead\nNUM_EPOCHS = 3      # For testing\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\nYOLO_POSE_MODEL_PATH = 'yolov8n-pose.pt'\n\n# (to_one_hot_jaad, JAADDataset_Cell2, Attention, MultiStreamWeightedAvgLSTM, train/eval functions - from your previous code)\n# Make sure these helper classes and functions are defined here or imported correctly.\n# For brevity, I'm assuming they are present as in your original combined script.\ndef to_one_hot_jaad(index: int, num_classes: int) -> np.ndarray:\n    vec = np.zeros(num_classes, dtype=np.float32)\n    safe_index = int(np.clip(index, 0, num_classes - 1))\n    vec[safe_index] = 1.0\n    return vec\n\nclass JAADDataset_Cell2(Dataset):\n    def __init__(\n        self,\n        jaad_db_obj: JAAD,\n        jaad_sequences: list,\n        seq_len: int,\n        pred_len: int,\n        active_streams: list[str],\n        yolo_pose_model,\n        all_jaad_annotations # Pass the full annotation dict here\n    ):\n        self.jaad_obj = jaad_db_obj\n        self.sequences = jaad_sequences\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.streams_to_extract = active_streams\n        self.yolo_pose_model = yolo_pose_model\n        self.total_pose_inference_time = 0.0\n        self.num_pose_inferences = 0\n        self.jaad_full_db = all_jaad_annotations # Use preloaded full DB\n\n        if not self.sequences:\n            print(f\"Warning: JAADDataset_Cell2 initialized with zero sequences.\")\n\n    def _get_input_sizes_dict(self):\n        return {\n            \"bbox\": INPUT_SIZE_BBOX_JAAD, \"pose\": INPUT_SIZE_POSE_JAAD,\n            \"ego_speed\": INPUT_SIZE_EGO_SPEED_JAAD, \"ego_acc\": INPUT_SIZE_EGO_ACC_JAAD,\n            \"ped_action\": INPUT_SIZE_PED_ACTION_JAAD, \"ped_look\": INPUT_SIZE_PED_LOOK_JAAD,\n            \"static_context\": INPUT_SIZE_STATIC_JAAD_PADDED,\n        }\n\n    def __len__(self): return len(self.sequences)\n    def __getitem__(self, idx: int):\n        vid_id, ped_id, start_frame, _ = self.sequences[idx] # Label is already in sequence tuple\n\n        video_db = self.jaad_full_db.get(vid_id)\n        if not video_db: raise ValueError(f\"Video ID {vid_id} not found in JAAD database.\")\n        ped_db = video_db.get(\"ped_annotations\", {}).get(ped_id)\n        if not ped_db: raise ValueError(f\"Pedestrian ID {ped_id} not found in video {vid_id}.\")\n\n        ped_attributes = ped_db.get(\"attributes\", {})\n        vehicle_annots_for_vid = video_db.get(\"vehicle_annotations\", {})\n        \n        ped_frames_list = ped_db.get(\"frames\", [])\n        try:\n            start_frame_idx_in_ped_frames = ped_frames_list.index(start_frame)\n        except ValueError:\n            raise ValueError(f\"start_frame {start_frame} not in ped {ped_id} frames for video {vid_id}\")\n\n        frame_nums_in_seq = []\n        for i in range(self.seq_len):\n            current_ped_frame_idx = start_frame_idx_in_ped_frames + i\n            if current_ped_frame_idx < len(ped_frames_list):\n                frame_nums_in_seq.append(ped_frames_list[current_ped_frame_idx])\n            else:\n                # This case should ideally be prevented by sequence enumeration logic\n                # If it occurs, pad with last valid frame's features or zeros\n                print(f\"Warning: Ran out of frames for ped {ped_id} in vid {vid_id} at seq index {i}. Will try to pad.\")\n                # For simplicity, we'll let the feature extraction loop handle missing frames by potentially using zeros\n                # A more robust solution would pad with the last valid frame's features.\n                # For now, ensure enough frames are requested in sequence generation.\n                # Let's assume sequence generation ensures this won't happen for valid sequences.\n                # If it must be handled, break or pad:\n                # frame_nums_in_seq.append(frame_nums_in_seq[-1] if frame_nums_in_seq else 0) # Crude padding\n                # This is an error in sequence generation if it happens.\n                raise IndexError(f\"Sequence generation error: Not enough frames for ped {ped_id} in vid {vid_id} starting at {start_frame}\")\n\n\n        # Label is already part of the sequence tuple\n        label = self.sequences[idx][3]\n\n        static_vec = np.zeros(INPUT_SIZE_STATIC_JAAD_PADDED, dtype=np.float32)\n        if \"static_context\" in self.streams_to_extract:\n            sig_idx   = ped_attributes.get(\"signalized\", 0)\n            intr_idx  = ped_attributes.get(\"intersection\", 0)\n            age_idx   = ped_attributes.get(\"age\", 2)\n            gen_idx   = ped_attributes.get(\"gender\", 0)\n            tdir_idx  = int(ped_attributes.get(\"traffic_direction\", 0))\n            nl_val    = ped_attributes.get(\"num_lanes\", 2)\n            nl_cat_idx = LANE_CATEGORIES_JAAD.get(nl_val, LANE_CATEGORIES_JAAD.get(2))\n            static_vec = np.concatenate([\n                to_one_hot_jaad(sig_idx, NUM_SIGNALIZED_CATS_JAAD_PADDED),\n                to_one_hot_jaad(intr_idx, NUM_INTERSECTION_CATS_JAAD_PADDED),\n                to_one_hot_jaad(age_idx, NUM_AGE_CATS_JAAD),\n                to_one_hot_jaad(gen_idx, NUM_GENDER_CATS_JAAD),\n                to_one_hot_jaad(tdir_idx, NUM_TRAFFIC_DIR_CATS_JAAD),\n                to_one_hot_jaad(nl_cat_idx, NUM_LANE_CATS_JAAD),\n            ]).astype(np.float32)\n\n        feature_sequences = {s: [] for s in self.streams_to_extract}\n\n        for frame_num_orig_video in frame_nums_in_seq:\n            try:\n                fidx_in_ped_frames = ped_frames_list.index(frame_num_orig_video)\n            except ValueError:\n                # This frame number is not in this pedestrian's list. Pad with zeros.\n                for stream_name_pad in self.streams_to_extract:\n                    if stream_name_pad == \"static_context\": feature_sequences[stream_name_pad].append(static_vec)\n                    else: feature_sequences[stream_name_pad].append(np.zeros(self._get_input_sizes_dict()[stream_name_pad], dtype=np.float32))\n                continue\n\n            if \"bbox\" in self.streams_to_extract:\n                bbox_raw = ped_db[\"bbox\"][fidx_in_ped_frames]\n                x1, y1, x2, y2 = bbox_raw\n                img_w, img_h = video_db[\"width\"], video_db[\"height\"]\n                cx = ((x1 + x2) / 2) / img_w; cy = ((y1 + y2) / 2) / img_h\n                w  = (x2 - x1) / img_w;     h  = (y2 - y1) / img_h\n                feature_sequences[\"bbox\"].append(np.array([cx, cy, w, h], dtype=np.float32))\n\n            if \"pose\" in self.streams_to_extract:\n                pose_vector = np.zeros(INPUT_SIZE_POSE_JAAD, dtype=np.float32)\n                img_path = self.jaad_obj._get_image_path(vid_id, frame_num_orig_video) # Uses overridden _images_path\n                if os.path.exists(img_path):\n                    img_cv = cv2.imread(img_path)\n                    if img_cv is not None:\n                        x1, y1, x2, y2 = map(int, ped_db[\"bbox\"][fidx_in_ped_frames])\n                        x1,y1 = max(0,x1), max(0,y1)\n                        x2,y2 = min(img_cv.shape[1],x2), min(img_cv.shape[0],y2)\n                        if x1 < x2 and y1 < y2:\n                            crop_img = img_cv[y1:y2, x1:x2]\n                            if crop_img.size > 0:\n                                t_start_pose = time.time()\n                                results = self.yolo_pose_model(crop_img, verbose=False, device=DEVICE) # Ensure YOLO uses correct device\n                                self.total_pose_inference_time += (time.time() - t_start_pose)\n                                self.num_pose_inferences += 1\n                                if results[0].keypoints and results[0].keypoints.data.numel() > 0 :\n                                    kpts_norm_crop = results[0].keypoints.xyn[0].cpu().numpy()\n                                    if kpts_norm_crop.size > 0:\n                                        flat_kpts = kpts_norm_crop.flatten()\n                                        # Pad or truncate if necessary\n                                        if len(flat_kpts) < INPUT_SIZE_POSE_JAAD:\n                                            pose_vector[:len(flat_kpts)] = flat_kpts\n                                        else:\n                                            pose_vector = flat_kpts[:INPUT_SIZE_POSE_JAAD]\n                feature_sequences[\"pose\"].append(pose_vector)\n            \n            ego_action_num = vehicle_annots_for_vid.get(frame_num_orig_video, 0)\n            if \"ego_speed\" in self.streams_to_extract:\n                feature_sequences[\"ego_speed\"].append([float(ego_action_num) / 4.0])\n            if \"ego_acc\" in self.streams_to_extract:\n                feature_sequences[\"ego_acc\"].append([0.0, 0.0])\n            \n            ped_beh = ped_db.get(\"behavior\", {})\n            if \"ped_action\" in self.streams_to_extract:\n                action_list = ped_beh.get(\"action\", [])\n                feature_sequences[\"ped_action\"].append([float(action_list[fidx_in_ped_frames]) if fidx_in_ped_frames < len(action_list) else 0.0])\n            if \"ped_look\" in self.streams_to_extract:\n                look_list = ped_beh.get(\"look\", [])\n                feature_sequences[\"ped_look\"].append([float(look_list[fidx_in_ped_frames]) if fidx_in_ped_frames < len(look_list) else 0.0])\n            if \"static_context\" in self.streams_to_extract:\n                feature_sequences[\"static_context\"].append(static_vec)\n\n        final_features = {}\n        for stream_name in self.streams_to_extract:\n            feat_array = np.asarray(feature_sequences[stream_name], dtype=np.float32)\n            expected_dim = self._get_input_sizes_dict()[stream_name]\n            if feat_array.ndim == 1 and expected_dim == 1: feat_array = feat_array.reshape(-1,1)\n            if feat_array.shape[0] != self.seq_len or (feat_array.ndim > 1 and feat_array.shape[1] != expected_dim):\n                print(f\"Shape mismatch for {stream_name} in {vid_id}/{ped_id}/f{start_frame}: got {feat_array.shape}, expected ({self.seq_len}, {expected_dim}). Using zeros.\")\n                feat_array = np.zeros((self.seq_len, expected_dim), dtype=np.float32)\n            final_features[stream_name] = torch.tensor(feat_array, dtype=torch.float32)\n        return final_features, torch.tensor(label, dtype=torch.long)\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim, attention_dim):\n        super().__init__()\n        self.attention_net = nn.Sequential(\n            nn.Linear(hidden_dim, attention_dim), nn.Tanh(), nn.Linear(attention_dim, 1),\n        )\n    def forward(self, lstm_output):\n        att_scores = self.attention_net(lstm_output).squeeze(-1)\n        att_weights = torch.softmax(att_scores, dim=1)\n        context_vector = torch.sum(lstm_output * att_weights.unsqueeze(-1), dim=1)\n        return context_vector, att_weights\n\nclass MultiStreamWeightedAvgLSTM(nn.Module):\n    def __init__(\n        self, input_sizes, lstm_hidden_size, num_lstm_layers,\n        num_classes, attention_dim, dropout_rate, stream_names=None,\n    ):\n        super().__init__()\n        if not stream_names: raise ValueError(\"stream_names cannot be empty.\")\n        self.stream_names = stream_names\n        self.num_active_streams = len(stream_names)\n        self.lstm_output_dim = lstm_hidden_size * 2 # Bidirectional\n        self.lstms, self.attentions = nn.ModuleDict(), nn.ModuleDict()\n        print(f\"Initializing Weighted-Avg model with streams: {self.stream_names}\")\n        for name in self.stream_names:\n            if name not in input_sizes: raise KeyError(f\"Input size for stream '{name}' not provided.\")\n            in_size = input_sizes[name]\n            print(f\"  – Adding stream '{name}' (input {in_size})\")\n            self.lstms[name] = nn.LSTM(\n                in_size, lstm_hidden_size, num_lstm_layers, batch_first=True,\n                dropout=dropout_rate if num_lstm_layers > 1 else 0, bidirectional=True,\n            )\n            self.attentions[name] = Attention(self.lstm_output_dim, attention_dim)\n        self.fusion_weights = nn.Parameter(torch.ones(self.num_active_streams))\n        fused_dim = self.lstm_output_dim\n        self.dropout = nn.Dropout(dropout_rate)\n        inter_dim = max(num_classes * 4, fused_dim // 2)\n        self.fc1 = nn.Linear(fused_dim, inter_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(inter_dim, num_classes)\n\n    def forward(self, x):\n        ctx_vecs = []\n        for name in self.stream_names:\n            if name not in x or x[name].numel() == 0 :\n                zero_ctx = torch.zeros(x[next(iter(x))].shape[0], self.lstm_output_dim, device=x[next(iter(x))].device)\n                ctx_vecs.append(zero_ctx)\n                continue\n            lstm_out, _ = self.lstms[name](x[name])\n            context_vector, _ = self.attentions[name](lstm_out)\n            ctx_vecs.append(context_vector)\n        if len(ctx_vecs) != self.num_active_streams:\n            raise RuntimeError(f\"Ctx vecs {len(ctx_vecs)} != num_streams {self.num_active_streams}\")\n        stacked_contexts = torch.stack(ctx_vecs, dim=1)\n        normalized_weights = torch.softmax(self.fusion_weights, dim=0)\n        reshaped_weights = normalized_weights.view(1, -1, 1)\n        fused_representation = torch.sum(stacked_contexts * reshaped_weights, dim=1)\n        out = self.dropout(fused_representation)\n        out = self.relu(self.fc1(out)); out = self.dropout(out)\n        logits = self.fc2(out)\n        return logits\n\ndef get_all_probabilities_and_labels(model, dataloader, device):\n    model.eval(); labels_all, probs_all_positive_class = [], []\n    active_streams = model.stream_names\n    with torch.no_grad():\n        for feats, labels in tqdm(dataloader, desc=\"Getting Probs & Labels\", leave=False):\n            inputs = {name: feats[name].to(device) for name in active_streams if name in feats and feats[name].numel() > 0}\n            if not inputs: continue\n            outputs = model(inputs); probs = torch.softmax(outputs, dim=1)\n            labels_all.extend(labels.cpu().numpy())\n            probs_all_positive_class.extend(probs[:, 1].cpu().numpy())\n    return np.asarray(labels_all), np.asarray(probs_all_positive_class)\n\ndef find_optimal_threshold(y_true, y_probs_positive_class, metric='f1', steps=100):\n    best_threshold, best_metric_val = 0.5, -1.0\n    metric_func = lambda yt, yp: precision_recall_fscore_support(yt, yp, average='binary', pos_label=1, zero_division=0)[2]\n    thresholds = np.linspace(0.0, 1.0, steps + 1)\n    for threshold in tqdm(thresholds, desc=f\"Tuning Thresh ({metric.upper()})\", leave=False):\n        y_pred_tuned = (y_probs_positive_class >= threshold).astype(int)\n        current_metric_val = metric_func(y_true, y_pred_tuned)\n        if current_metric_val > best_metric_val: best_metric_val, best_threshold = current_metric_val, threshold\n        elif current_metric_val == best_metric_val and abs(threshold - 0.5) < abs(best_threshold - 0.5): best_threshold = threshold\n    # print(f\"Optimal threshold found: {best_threshold:.4f} (max {metric.upper()}: {best_metric_val:.4f})\")\n    return best_threshold, best_metric_val\n\ndef train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train(); total_loss, all_preds, all_labels = 0.0, [], []\n    active = model.stream_names\n    for feats, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n        inputs = {n: feats[n].to(device) for n in active if n in feats and feats[n].numel() > 0}\n        if not inputs : continue\n        labels = labels.to(device); optimizer.zero_grad(); outputs = model(inputs); loss = criterion(outputs, labels)\n        loss.backward(); optimizer.step(); total_loss += loss.item()\n        all_preds.extend(torch.argmax(outputs, 1).cpu().numpy()); all_labels.extend(labels.cpu().numpy())\n    return total_loss/max(1,len(dataloader)), accuracy_score(all_labels,all_preds) if all_labels else 0.0\n\ndef evaluate_epoch(model, dataloader, criterion, device, threshold=0.5):\n    model.eval(); total_loss,all_labels_list,all_preds_list,all_probs_list = 0.0,[],[],[]\n    active = model.stream_names\n    with torch.no_grad():\n        for feats, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n            inputs = {n:feats[n].to(device) for n in active if n in feats and feats[n].numel()>0}\n            if not inputs: continue\n            labels_gpu=labels.to(device); outputs=model(inputs); loss=criterion(outputs,labels_gpu)\n            total_loss+=loss.item(); probs=torch.softmax(outputs,1); preds=(probs[:,1]>=threshold).int()\n            all_labels_list.extend(labels.cpu().numpy()); all_preds_list.extend(preds.cpu().numpy())\n            all_probs_list.extend(probs.cpu().numpy())\n    avg_loss=total_loss/max(1,len(dataloader))\n    all_labels,all_preds,all_probs_np = np.asarray(all_labels_list),np.asarray(all_preds_list),np.asarray(all_probs_list)\n    if len(all_labels)==0: return {\"loss\":avg_loss,\"accuracy\":0,\"precision\":0,\"recall\":0,\"f1\":0,\"auc\":0,\"labels\":[],\"probs_positive_class\":[]}\n    acc=accuracy_score(all_labels,all_preds)\n    prec,rec,f1,_ = precision_recall_fscore_support(all_labels,all_preds,average=\"binary\",pos_label=1,zero_division=0)\n    auc=roc_auc_score(all_labels,all_probs_np[:,1]) if len(np.unique(all_labels))>1 else float(\"nan\")\n    return {\"loss\":avg_loss,\"accuracy\":acc,\"precision\":prec,\"recall\":rec,\"f1\":f1,\"auc\":auc,\"labels\":all_labels,\"probs_positive_class\":all_probs_np[:,1]}\n\ndef get_predictions_at_threshold(y_probs_positive_class, threshold):\n    return (y_probs_positive_class >= threshold).astype(int)\n\n# -----------------------------------------------------------------------------#\n#                                Main Execution Block                          #\n# -----------------------------------------------------------------------------#\nif __name__ == \"__main__\":\n    print(f\"--- JAAD DATASET EXPERIMENT (Processing {NUM_JAAD_VIDEOS_TO_PROCESS} videos) ---\")\n\n    print(\"\\nInitializing JAAD object...\")\n    if not os.path.exists(JAAD_IMAGES_PATH):\n        os.makedirs(JAAD_IMAGES_PATH)\n        print(f\"Created JAAD images directory: {JAAD_IMAGES_PATH}\")\n\n    jaad_obj = JAAD(data_path=JAAD_DATA_ROOT_PATH)\n    jaad_obj._clips_path = JAAD_CLIPS_PATH\n    jaad_obj._images_path = JAAD_IMAGES_PATH\n\n    all_available_clip_basenames = sorted([f.split('.')[0] for f in os.listdir(JAAD_CLIPS_PATH) if f.endswith('.mp4')])\n    videos_for_extraction_and_processing = all_available_clip_basenames[:NUM_JAAD_VIDEOS_TO_PROCESS]\n    print(f\"Selected JAAD videos for this run: {videos_for_extraction_and_processing}\")\n\n    images_exist_for_subset = True\n    if FORCE_REEXTRACT_IMAGES_SUBSET: images_exist_for_subset = False\n    else:\n        for vid_check in videos_for_extraction_and_processing:\n            vid_img_dir = os.path.join(JAAD_IMAGES_PATH, vid_check)\n            if not os.path.exists(vid_img_dir) or not os.listdir(vid_img_dir):\n                images_exist_for_subset = False; print(f\"Images missing for {vid_check}.\"); break\n    \n    if not images_exist_for_subset or FORCE_REEXTRACT_IMAGES_SUBSET:\n        print(f\"Extracting images for {len(videos_for_extraction_and_processing)} JAAD videos...\")\n        if FORCE_REEXTRACT_IMAGES_SUBSET:\n            for vid_clean in videos_for_extraction_and_processing:\n                dir_to_clean = os.path.join(JAAD_IMAGES_PATH, vid_clean)\n                if os.path.exists(dir_to_clean): shutil.rmtree(dir_to_clean)\n        jaad_obj.extract_and_save_images(videos_to_process=videos_for_extraction_and_processing)\n        print(\"✓ JAAD images extracted for the subset.\")\n    else: print(\"JAAD images for the selected subset already exist.\")\n\n    print(\"\\nLoading/Generating JAAD database (annotations)...\")\n    jaad_full_db = jaad_obj.generate_database()\n    print(\"✓ JAAD database loaded/generated.\")\n\n    print(\"\\nEnumerating and balancing JAAD sequences for the subset...\")\n    if not os.path.exists(JAAD_BALANCED_SEQUENCES_PKL_PATH) or not os.path.exists(JAAD_VAL_SEQUENCES_PKL_PATH) or FORCE_REEXTRACT_IMAGES_SUBSET: # Regenerate if forced\n        full_train_video_ids_split = jaad_obj._get_video_ids_split('train', subset='default')\n        full_val_video_ids_split = jaad_obj._get_video_ids_split('val', subset='default')\n        \n        # Filter splits to only include the videos we're processing\n        train_video_ids = [vid for vid in full_train_video_ids_split if vid in videos_for_extraction_and_processing]\n        val_video_ids = [vid for vid in full_val_video_ids_split if vid in videos_for_extraction_and_processing]\n        print(f\"Effective train videos in subset: {train_video_ids}\")\n        print(f\"Effective val videos in subset: {val_video_ids}\")\n\n        def enumerate_sequences_for_split(video_ids_list, split_name):\n            sequences = []\n            for vid_id in tqdm(video_ids_list, desc=f\"Videos ({split_name})\"):\n                video_data = jaad_full_db.get(vid_id)\n                if not video_data: continue\n                for ped_id, ped_data in video_data.get(\"ped_annotations\", {}).items():\n                    if 'b' not in ped_id: continue\n                    frames = ped_data.get(\"frames\", [])\n                    if len(frames) < SEQ_LEN + PRED_LEN: continue\n                    \n                    crossing_attr = ped_data.get(\"attributes\", {}).get(\"crossing\", 0)\n                    # Binary label: 1 if crossing (crossing_attr == 1), 0 otherwise (includes not-crossing and irrelevant)\n                    label = 1 if crossing_attr == 1 else 0\n\n                    for i in range(len(frames) - SEQ_LEN - PRED_LEN + 1):\n                        start_frame = frames[i]\n                        # Check if sequence is continuous enough (simple check, can be improved)\n                        # For JAAD fstride=1 implies taking directly from the list of annotated frames\n                        # The frame numbers themselves might have gaps.\n                        # We take SEQ_LEN items from the *list* of annotated frames.\n                        sequences.append((vid_id, ped_id, start_frame, label))\n            return sequences\n\n        all_train_sequences_tuples = enumerate_sequences_for_split(train_video_ids, \"train\")\n        all_val_sequences_tuples = enumerate_sequences_for_split(val_video_ids, \"val\")\n        \n        train_labels = [s[3] for s in all_train_sequences_tuples]\n        if train_labels:\n            n_pos = sum(train_labels); n_neg = len(train_labels) - n_pos\n            print(f\"Unbalanced JAAD train (subset): Positive: {n_pos}, Negative: {n_neg}\")\n            if n_pos > 0 and n_neg > 0 and n_pos != n_neg:\n                minority_class = 1 if n_pos < n_neg else 0\n                minority_count = min(n_pos, n_neg)\n                minority_s = [s for s in all_train_sequences_tuples if s[3] == minority_class]\n                majority_s = [s for s in all_train_sequences_tuples if s[3] != minority_class]\n                random.shuffle(majority_s)\n                balanced_train_sequences = minority_s + majority_s[:minority_count]\n                random.shuffle(balanced_train_sequences)\n                print(f\"Balanced JAAD train (subset): Total {len(balanced_train_sequences)}, Positive: {sum(s[3] for s in balanced_train_sequences)}\")\n            else: balanced_train_sequences = all_train_sequences_tuples\n        else: balanced_train_sequences = []\n\n        with open(JAAD_BALANCED_SEQUENCES_PKL_PATH, \"wb\") as f: pickle.dump(balanced_train_sequences, f)\n        with open(JAAD_VAL_SEQUENCES_PKL_PATH, \"wb\") as f: pickle.dump(all_val_sequences_tuples, f)\n        print(\"✓ JAAD sequence lists for subset prepared and saved.\")\n    else:\n        print(\"Loading pre-prepared JAAD sequence lists for subset...\")\n        with open(JAAD_BALANCED_SEQUENCES_PKL_PATH, \"rb\") as f: balanced_train_sequences = pickle.load(f)\n        with open(JAAD_VAL_SEQUENCES_PKL_PATH, \"rb\") as f: all_val_sequences_tuples = pickle.load(f)\n        print(\"✓ JAAD sequence lists for subset loaded.\")\n\n    print(\"\\nLoading YOLOv8-pose model...\")\n    yolo_pose_model = YOLO(YOLO_POSE_MODEL_PATH).to(DEVICE) # Move model to device once\n    print(\"✓ YOLOv8-pose model loaded.\")\n\n    print(\"\\nCreating JAAD Datasets and DataLoaders for subset...\")\n    if not balanced_train_sequences: print(\"Warning: No training sequences available for JAAD subset.\")\n    if not all_val_sequences_tuples: print(\"Warning: No validation sequences available for JAAD subset.\")\n\n    train_jaad_dataset = JAADDataset_Cell2(jaad_obj, balanced_train_sequences,\n                                         SEQ_LEN, PRED_LEN, ACTIVE_STREAMS_JAAD, yolo_pose_model, jaad_full_db)\n    val_jaad_dataset = JAADDataset_Cell2(jaad_obj, all_val_sequences_tuples,\n                                       SEQ_LEN, PRED_LEN, ACTIVE_STREAMS_JAAD, yolo_pose_model, jaad_full_db)\n    \n    train_loader_jaad = DataLoader(train_jaad_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader_jaad = DataLoader(val_jaad_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    print(\"✓ JAAD DataLoaders for subset ready.\")\n\n    print(\"\\nInitializing model and loading pre-trained weights...\")\n    jaad_input_sizes = {s: globals()[f\"INPUT_SIZE_{s.upper()}_JAAD\"] if f\"INPUT_SIZE_{s.upper()}_JAAD\" in globals() else globals()[f\"INPUT_SIZE_{s.upper()}_JAAD_PADDED\"] for s in ACTIVE_STREAMS_JAAD}\n    model = MultiStreamWeightedAvgLSTM(\n        input_sizes=jaad_input_sizes, lstm_hidden_size=256, num_lstm_layers=2,\n        num_classes=2, attention_dim=128, dropout_rate=0.3, stream_names=ACTIVE_STREAMS_JAAD\n    ).to(DEVICE)\n    if os.path.exists(PRETRAINED_MODEL_PATH):\n        try: model.load_state_dict(torch.load(PRETRAINED_MODEL_PATH, map_location=DEVICE)); print(\"✓ Pre-trained weights loaded.\")\n        except Exception as e: print(f\"[ERROR] Load pre-trained: {e}. Using random init.\")\n    else: print(\"Pre-trained model not found. Using random init.\")\n    \n    train_labels_for_weight = [s[3] for s in balanced_train_sequences]\n    class_weights_jaad = torch.tensor([1.0,1.0], dtype=torch.float32).to(DEVICE)\n    if train_labels_for_weight:\n        n0,n1=train_labels_for_weight.count(0),train_labels_for_weight.count(1); total=len(train_labels_for_weight)\n        if n0>0 and n1>0: class_weights_jaad = torch.tensor([total/(2.*n0), total/(2.*n1)],dtype=torch.float32).to(DEVICE)\n    criterion = nn.CrossEntropyLoss(weight=class_weights_jaad); optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    print(\"\\n--- Starting training on JAAD subset ---\")\n    # (Training loop copied from your previous code, ensure it uses *_jaad variables)\n    best_val_f1_jaad = -1.0; history_jaad = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc_tuned\": [], \"val_f1_tuned\": []}; best_model_jaad_path = \"\"\n    for epoch in range(NUM_EPOCHS):\n        t0_epoch = time.time()\n        # Reset pose inference timers\n        train_jaad_dataset.total_pose_inference_time = 0.0; train_jaad_dataset.num_pose_inferences = 0\n        val_jaad_dataset.total_pose_inference_time = 0.0; val_jaad_dataset.num_pose_inferences = 0\n\n        train_loss, train_acc = train_epoch(model, train_loader_jaad, optimizer, criterion, DEVICE)\n        \n        val_labels_raw, val_probs_positive_raw = [], []\n        if len(val_loader_jaad) > 0 : # Only run if val_loader is not empty\n            val_labels_raw, val_probs_positive_raw = get_all_probabilities_and_labels(model, val_loader_jaad, DEVICE)\n        \n        optimal_val_threshold = 0.5\n        if len(val_labels_raw) > 0 and len(np.unique(val_labels_raw)) > 1:\n             optimal_val_threshold, _ = find_optimal_threshold(val_labels_raw, val_probs_positive_raw, metric='f1', steps=50)\n        \n        epoch_eval_metrics_tuned = {\"loss\": float('nan'), \"accuracy\": 0, \"precision\": 0, \"recall\": 0, \"f1\": 0, \"auc\": float('nan')}\n        if len(val_loader_jaad) > 0:\n            epoch_eval_metrics_tuned = evaluate_epoch(model, val_loader_jaad, criterion, DEVICE, threshold=optimal_val_threshold)\n        \n        history_jaad[\"train_loss\"].append(train_loss); history_jaad[\"val_loss\"].append(epoch_eval_metrics_tuned[\"loss\"])\n        history_jaad[\"train_acc\"].append(train_acc); history_jaad[\"val_acc_tuned\"].append(epoch_eval_metrics_tuned[\"accuracy\"])\n        history_jaad[\"val_f1_tuned\"].append(epoch_eval_metrics_tuned[\"f1\"])\n\n        print(f\"\\nEpoch {epoch+1:02d}/{NUM_EPOCHS} (JAAD) – {time.time()-t0_epoch:.1f}s \"\n              f\"| Train L: {train_loss:.4f} Acc: {train_acc:.4f}\")\n        if len(val_loader_jaad) > 0:\n             print(f\"  Val (Thresh:{optimal_val_threshold:.2f}) L: {epoch_eval_metrics_tuned['loss']:.4f} Acc: {epoch_eval_metrics_tuned['accuracy']:.4f} F1: {epoch_eval_metrics_tuned['f1']:.4f} AUC: {epoch_eval_metrics_tuned['auc']:.4f}\")\n        else: print(\"  Validation set empty, skipping val metrics.\")\n\n        if epoch_eval_metrics_tuned[\"f1\"] > best_val_f1_jaad:\n            best_val_f1_jaad = epoch_eval_metrics_tuned[\"f1\"]\n            best_model_jaad_path = f\"best_model_jaad_subset_ep{epoch+1}.pth\"\n            torch.save(model.state_dict(), best_model_jaad_path)\n            print(f\"  ✓ New best JAAD model (subset) saved: {best_model_jaad_path} (F1: {best_val_f1_jaad:.4f})\")\n\n    total_train_pose_time = train_jaad_dataset.total_pose_inference_time\n    total_train_pose_count = train_jaad_dataset.num_pose_inferences\n    avg_train_pose_time_ms = (total_train_pose_time / total_train_pose_count * 1000) if total_train_pose_count > 0 else 0\n    print(f\"\\nAvg YOLOv8-pose time (training, last epoch): {avg_train_pose_time_ms:.2f} ms ({total_train_pose_count} inferences)\")\n    \n    total_val_pose_time = val_jaad_dataset.total_pose_inference_time\n    total_val_pose_count = val_jaad_dataset.num_pose_inferences\n    avg_val_pose_time_ms = (total_val_pose_time / total_val_pose_count * 1000) if total_val_pose_count > 0 else 0\n    print(f\"Avg YOLOv8-pose time (validation, last epoch): {avg_val_pose_time_ms:.2f} ms ({total_val_pose_count} inferences)\")\n\n    print(\"\\n--- JAAD SUBSET SCRIPT COMPLETE ---\")\n    if JAAD_IMAGES_PATH.startswith(\"/kaggle/working/\") and os.path.exists(JAAD_IMAGES_PATH):\n        print(f\"\\nCleaning up extracted images at {JAAD_IMAGES_PATH}...\")\n        try: shutil.rmtree(JAAD_IMAGES_PATH); print(\"✓ Cleanup successful.\")\n        except Exception as e: print(f\"Error during image cleanup: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:25:59.254420Z","iopub.execute_input":"2025-05-22T17:25:59.254723Z","iopub.status.idle":"2025-05-22T17:26:04.347330Z","shell.execute_reply.started":"2025-05-22T17:25:59.254697Z","shell.execute_reply":"2025-05-22T17:26:04.345961Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n--- Active streams for JAAD experiment: ['bbox', 'ego_acc', 'ego_speed', 'ped_action', 'ped_look', 'static_context', 'pose'] ---\nUsing device: cuda\n--- JAAD DATASET EXPERIMENT (Processing 5 videos) ---\n\nInitializing JAAD object...\nSelected JAAD videos for this run: ['video_0001', 'video_0002', 'video_0003', 'video_0004', 'video_0005']\nImages missing for video_0001.\nExtracting images for 5 JAAD videos...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-eb8e61d68fb3>\u001b[0m in \u001b[0;36m<cell line: 379>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0mdir_to_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJAAD_IMAGES_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_to_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_to_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mjaad_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_and_save_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos_to_process\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideos_for_extraction_and_processing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✓ JAAD images extracted for the subset.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"JAAD images for the selected subset already exist.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: JAAD.extract_and_save_images() got an unexpected keyword argument 'videos_to_process'"],"ename":"TypeError","evalue":"JAAD.extract_and_save_images() got an unexpected keyword argument 'videos_to_process'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}