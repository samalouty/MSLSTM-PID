{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":302300,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":258142,"modelId":279383}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/aras62/PIE.git\n!unzip /content/PIE/annotations/annotations.zip -d /content/PIE\n!unzip /content/PIE/annotations/annotations_vehicle.zip -d /content/PIE\n!git clone https://github.com/hustvl/YOLOP.git\n!mkdir /kaggle/working/PIE/content\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T00:50:49.581711Z","iopub.execute_input":"2025-03-26T00:50:49.582030Z","iopub.status.idle":"2025-03-26T00:51:05.118691Z","shell.execute_reply.started":"2025-03-26T00:50:49.582004Z","shell.execute_reply":"2025-03-26T00:51:05.117661Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'PIE'...\nremote: Enumerating objects: 178, done.\u001b[K\nremote: Counting objects: 100% (93/93), done.\u001b[K\nremote: Compressing objects: 100% (73/73), done.\u001b[K\nremote: Total 178 (delta 33), reused 75 (delta 18), pack-reused 85 (from 1)\u001b[K\nReceiving objects: 100% (178/178), 144.63 MiB | 36.59 MiB/s, done.\nResolving deltas: 100% (74/74), done.\nUpdating files: 100% (41/41), done.\nunzip:  cannot find or open /content/PIE/annotations/annotations.zip, /content/PIE/annotations/annotations.zip.zip or /content/PIE/annotations/annotations.zip.ZIP.\nunzip:  cannot find or open /content/PIE/annotations/annotations_vehicle.zip, /content/PIE/annotations/annotations_vehicle.zip.zip or /content/PIE/annotations/annotations_vehicle.zip.ZIP.\nCloning into 'YOLOP'...\nremote: Enumerating objects: 431, done.\u001b[K\nremote: Counting objects: 100% (426/426), done.\u001b[K\nremote: Compressing objects: 100% (231/231), done.\u001b[K\nremote: Total 431 (delta 214), reused 335 (delta 186), pack-reused 5 (from 1)\u001b[K\nReceiving objects: 100% (431/431), 140.17 MiB | 27.21 MiB/s, done.\nResolving deltas: 100% (214/214), done.\nUpdating files: 100% (95/95), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!mkdir /kaggle/working/PIE/content/set01\n!wget -r --no-parent -P/kaggle/working/PIE/content/set01 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set01/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T00:51:10.362399Z","iopub.execute_input":"2025-03-26T00:51:10.362750Z","iopub.status.idle":"2025-03-26T00:53:04.336375Z","shell.execute_reply.started":"2025-03-26T00:51:10.362712Z","shell.execute_reply":"2025-03-26T00:53:04.335267Z"}},"outputs":[{"name":"stdout","text":"--2025-03-26 00:51:10--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set01/\nResolving data.nvision2.eecs.yorku.ca (data.nvision2.eecs.yorku.ca)... 130.63.94.247\nConnecting to data.nvision2.eecs.yorku.ca (data.nvision2.eecs.yorku.ca)|130.63.94.247|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‘/kaggle/working/PIE/content/set01/index.html’\n\nindex.html              [ <=>                ]     667  --.-KB/s    in 0s      \n\n2025-03-26 00:51:10 (176 MB/s) - ‘/kaggle/working/PIE/content/set01/index.html’ saved [667]\n\nLoading robots.txt; please ignore errors.\n--2025-03-26 00:51:10--  https://data.nvision2.eecs.yorku.ca/robots.txt\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 404 Not Found\n2025-03-26 00:51:10 ERROR 404: Not Found.\n\n--2025-03-26 00:51:10--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set01/video_0001.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1200536469 (1.1G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set01/video_0001.mp4’\n\nvideo_0001.mp4      100%[===================>]   1.12G  35.9MB/s    in 32s     \n\n2025-03-26 00:51:43 (35.5 MB/s) - ‘/kaggle/working/PIE/content/set01/video_0001.mp4’ saved [1200536469/1200536469]\n\n--2025-03-26 00:51:43--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set01/video_0002.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1200434097 (1.1G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set01/video_0002.mp4’\n\nvideo_0002.mp4      100%[===================>]   1.12G  36.2MB/s    in 32s     \n\n2025-03-26 00:52:15 (36.0 MB/s) - ‘/kaggle/working/PIE/content/set01/video_0002.mp4’ saved [1200434097/1200434097]\n\n--2025-03-26 00:52:15--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set01/video_0003.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1199604441 (1.1G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set01/video_0003.mp4’\n\nvideo_0003.mp4      100%[===================>]   1.12G  35.9MB/s    in 32s     \n\n2025-03-26 00:52:46 (36.0 MB/s) - ‘/kaggle/working/PIE/content/set01/video_0003.mp4’ saved [1199604441/1199604441]\n\n--2025-03-26 00:52:46--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set01/video_0004.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 656370477 (626M) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set01/video_0004.mp4’\n\nvideo_0004.mp4      100%[===================>] 625.96M  36.0MB/s    in 17s     \n\n2025-03-26 00:53:04 (36.1 MB/s) - ‘/kaggle/working/PIE/content/set01/video_0004.mp4’ saved [656370477/656370477]\n\nFINISHED --2025-03-26 00:53:04--\nTotal wall clock time: 1m 54s\nDownloaded: 5 files, 4.0G in 1m 53s (35.9 MB/s)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!mkdir /kaggle/working/PIE/content/set02\n!wget -r --no-parent -P/kaggle/working/PIE/content/set02 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set02/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T00:53:04.337777Z","iopub.execute_input":"2025-03-26T00:53:04.338117Z","iopub.status.idle":"2025-03-26T00:53:58.939532Z","shell.execute_reply.started":"2025-03-26T00:53:04.338081Z","shell.execute_reply":"2025-03-26T00:53:58.938712Z"}},"outputs":[{"name":"stdout","text":"--2025-03-26 00:53:04--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set02/\nResolving data.nvision2.eecs.yorku.ca (data.nvision2.eecs.yorku.ca)... 130.63.94.247\nConnecting to data.nvision2.eecs.yorku.ca (data.nvision2.eecs.yorku.ca)|130.63.94.247|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‘/kaggle/working/PIE/content/set02/index.html’\n\nindex.html              [ <=>                ]     548  --.-KB/s    in 0s      \n\n2025-03-26 00:53:04 (202 MB/s) - ‘/kaggle/working/PIE/content/set02/index.html’ saved [548]\n\nLoading robots.txt; please ignore errors.\n--2025-03-26 00:53:04--  https://data.nvision2.eecs.yorku.ca/robots.txt\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 404 Not Found\n2025-03-26 00:53:04 ERROR 404: Not Found.\n\n--2025-03-26 00:53:04--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set02/video_0001.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1200345018 (1.1G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set02/video_0001.mp4’\n\nvideo_0001.mp4      100%[===================>]   1.12G  53.3MB/s    in 22s     \n\n2025-03-26 00:53:26 (52.2 MB/s) - ‘/kaggle/working/PIE/content/set02/video_0001.mp4’ saved [1200345018/1200345018]\n\n--2025-03-26 00:53:26--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set02/video_0002.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1200625008 (1.1G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set02/video_0002.mp4’\n\nvideo_0002.mp4      100%[===================>]   1.12G  48.9MB/s    in 22s     \n\n2025-03-26 00:53:49 (51.0 MB/s) - ‘/kaggle/working/PIE/content/set02/video_0002.mp4’ saved [1200625008/1200625008]\n\n--2025-03-26 00:53:49--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set02/video_0003.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 465517511 (444M) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set02/video_0003.mp4’\n\nvideo_0003.mp4      100%[===================>] 443.95M  40.8MB/s    in 9.5s    \n\n2025-03-26 00:53:58 (46.8 MB/s) - ‘/kaggle/working/PIE/content/set02/video_0003.mp4’ saved [465517511/465517511]\n\nFINISHED --2025-03-26 00:53:58--\nTotal wall clock time: 54s\nDownloaded: 4 files, 2.7G in 54s (50.8 MB/s)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!mkdir /kaggle/working/PIE/content/set03\n!wget -r --no-parent -P/kaggle/working/PIE/content/set03 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set03/\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir /kaggle/working/PIE/content/set04\n!wget -r --no-parent -P/kaggle/working/PIE/content/set04 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set04/\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir /kaggle/working/PIE/content/set05\n!wget -r --no-parent -P/kaggle/working/PIE/content/set05 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set05/\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir /kaggle/working/PIE/content/set06\n!wget -r --no-parent -P/kaggle/working/PIE/content/set06 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T00:53:58.941224Z","iopub.execute_input":"2025-03-26T00:53:58.941571Z","iopub.status.idle":"2025-03-26T00:58:04.647229Z","shell.execute_reply.started":"2025-03-26T00:53:58.941534Z","shell.execute_reply":"2025-03-26T00:58:04.646185Z"}},"outputs":[{"name":"stdout","text":"--2025-03-26 00:53:59--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/\nResolving data.nvision2.eecs.yorku.ca (data.nvision2.eecs.yorku.ca)... 130.63.94.247\nConnecting to data.nvision2.eecs.yorku.ca (data.nvision2.eecs.yorku.ca)|130.63.94.247|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‘/kaggle/working/PIE/content/set06/index.html’\n\nindex.html              [ <=>                ]   1.23K  --.-KB/s    in 0s      \n\n2025-03-26 00:53:59 (285 MB/s) - ‘/kaggle/working/PIE/content/set06/index.html’ saved [1262]\n\nLoading robots.txt; please ignore errors.\n--2025-03-26 00:53:59--  https://data.nvision2.eecs.yorku.ca/robots.txt\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 404 Not Found\n2025-03-26 00:53:59 ERROR 404: Not Found.\n\n--2025-03-26 00:53:59--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0001.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1500150602 (1.4G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0001.mp4’\n\nvideo_0001.mp4      100%[===================>]   1.40G  52.5MB/s    in 27s     \n\n2025-03-26 00:54:26 (53.3 MB/s) - ‘/kaggle/working/PIE/content/set06/video_0001.mp4’ saved [1500150602/1500150602]\n\n--2025-03-26 00:54:26--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0002.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1500301047 (1.4G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0002.mp4’\n\nvideo_0002.mp4      100%[===================>]   1.40G  52.5MB/s    in 27s     \n\n2025-03-26 00:54:53 (52.5 MB/s) - ‘/kaggle/working/PIE/content/set06/video_0002.mp4’ saved [1500301047/1500301047]\n\n--2025-03-26 00:54:53--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0003.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1500582428 (1.4G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0003.mp4’\n\nvideo_0003.mp4      100%[===================>]   1.40G  51.8MB/s    in 27s     \n\n2025-03-26 00:55:20 (52.6 MB/s) - ‘/kaggle/working/PIE/content/set06/video_0003.mp4’ saved [1500582428/1500582428]\n\n--2025-03-26 00:55:20--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0004.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1349801835 (1.3G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0004.mp4’\n\nvideo_0004.mp4      100%[===================>]   1.26G  52.8MB/s    in 25s     \n\n2025-03-26 00:55:45 (52.1 MB/s) - ‘/kaggle/working/PIE/content/set06/video_0004.mp4’ saved [1349801835/1349801835]\n\n--2025-03-26 00:55:45--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0005.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1499881950 (1.4G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0005.mp4’\n\nvideo_0005.mp4      100%[===================>]   1.40G  52.9MB/s    in 27s     \n\n2025-03-26 00:56:12 (52.7 MB/s) - ‘/kaggle/working/PIE/content/set06/video_0005.mp4’ saved [1499881950/1499881950]\n\n--2025-03-26 00:56:12--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0006.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1500032594 (1.4G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0006.mp4’\n\nvideo_0006.mp4      100%[===================>]   1.40G  53.0MB/s    in 27s     \n\n2025-03-26 00:56:40 (52.8 MB/s) - ‘/kaggle/working/PIE/content/set06/video_0006.mp4’ saved [1500032594/1500032594]\n\n--2025-03-26 00:56:40--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0007.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1650227705 (1.5G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0007.mp4’\n\nvideo_0007.mp4      100%[===================>]   1.54G  53.0MB/s    in 30s     \n\n2025-03-26 00:57:09 (52.7 MB/s) - ‘/kaggle/working/PIE/content/set06/video_0007.mp4’ saved [1650227705/1650227705]\n\n--2025-03-26 00:57:09--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0008.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1499974602 (1.4G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0008.mp4’\n\nvideo_0008.mp4      100%[===================>]   1.40G  51.5MB/s    in 34s     \n\n2025-03-26 00:57:44 (41.5 MB/s) - ‘/kaggle/working/PIE/content/set06/video_0008.mp4’ saved [1499974602/1499974602]\n\n--2025-03-26 00:57:44--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0009.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1500368853 (1.4G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0009.mp4’\n\nvideo_0009.mp4       71%[=============>      ]   1.00G  51.6MB/s    in 20s     \n\n\nCannot write to ‘/kaggle/working/PIE/content/set06/video_0009.mp4’ (Success).\nFINISHED --2025-03-26 00:58:04--\nTotal wall clock time: 4m 5s\nDownloaded: 9 files, 11G in 3m 45s (51.0 MB/s)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport xml.etree.ElementTree as ET\nimport os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\nfrom tqdm.notebook import tqdm\nimport random\nimport math\nimport zipfile","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T00:58:43.262020Z","iopub.execute_input":"2025-03-26T00:58:43.262350Z","iopub.status.idle":"2025-03-26T00:58:43.266704Z","shell.execute_reply.started":"2025-03-26T00:58:43.262324Z","shell.execute_reply":"2025-03-26T00:58:43.265759Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extractall(extract_to)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T00:59:54.157618Z","iopub.execute_input":"2025-03-26T00:59:54.157915Z","iopub.status.idle":"2025-03-26T00:59:55.960281Z","shell.execute_reply.started":"2025-03-26T00:59:54.157873Z","shell.execute_reply":"2025-03-26T00:59:55.959623Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# --- Configuration ---\nBASE_DIR = '/kaggle/working/PIE/content'\nANNOTATION_DIR = '/kaggle/working/PIE/annotations/annotations'\n# CLIP_DIR = os.path.join(BASE_DIR, 'PIE_clips') # Not used directly in this version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:12:02.724483Z","iopub.execute_input":"2025-03-25T22:12:02.724845Z","iopub.status.idle":"2025-03-25T22:12:02.728739Z","shell.execute_reply.started":"2025-03-25T22:12:02.724819Z","shell.execute_reply":"2025-03-25T22:12:02.727748Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Model Hyperparameters\nSEQ_LEN = 15        # Number of past frames to observe\nPRED_LEN = 1        # Predict state at the end of sequence (relative to seq_len)\nINPUT_SIZE_BBOX = 4 # (center_x, center_y, width, height) - normalized\n# INPUT_SIZE_POSE = 34 # Example: 17 keypoints * 2 coords (if using pose)\n# INPUT_SIZE_FLOW = 10 # Example: Aggregated optical flow features (if using flow)\nLSTM_HIDDEN_SIZE = 128\nNUM_LSTM_LAYERS = 2\nDROPOUT_RATE = 0.3\nNUM_CLASSES = 2      # 0: not-crossing, 1: crossing\nATTENTION_DIM = 128 # Dimension for the attention mechanism","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:01:37.482266Z","iopub.execute_input":"2025-03-25T22:01:37.482620Z","iopub.status.idle":"2025-03-25T22:01:37.486781Z","shell.execute_reply.started":"2025-03-25T22:01:37.482587Z","shell.execute_reply":"2025-03-25T22:01:37.485840Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Training Hyperparameters\nLEARNING_RATE = 1e-4\nBATCH_SIZE = 64\nNUM_EPOCHS = 20 # Adjust as needed\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:01:40.365061Z","iopub.execute_input":"2025-03-25T22:01:40.365369Z","iopub.status.idle":"2025-03-25T22:01:40.420119Z","shell.execute_reply.started":"2025-03-25T22:01:40.365340Z","shell.execute_reply":"2025-03-25T22:01:40.419224Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"TRAIN_SETS = ['set06']\nVAL_SETS = ['set01', 'set02'] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:01:43.081114Z","iopub.execute_input":"2025-03-25T22:01:43.081429Z","iopub.status.idle":"2025-03-25T22:01:43.085614Z","shell.execute_reply.started":"2025-03-25T22:01:43.081404Z","shell.execute_reply":"2025-03-25T22:01:43.084618Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# --- Data Preprocessing ---\n\ndef parse_annotations(xml_file):\n    \"\"\"Parses a PIE annotation XML file.\"\"\"\n    try:\n        tree = ET.parse(xml_file)\n        root = tree.getroot()\n    except ET.ParseError:\n        print(f\"Error parsing {xml_file}\")\n        return None, None\n\n    img_width = int(root.find('.//original_size/width').text)\n    img_height = int(root.find('.//original_size/height').text)\n\n    ped_tracks = {} # {ped_id: {frame: {'bbox': [xtl,ytl,xbr,ybr], 'cross': label}}}\n\n    for track in root.findall('.//track[@label=\"pedestrian\"]'):\n        for box in track.findall('.//box'):\n            frame = int(box.get('frame'))\n            ped_id = box.find('.//attribute[@name=\"id\"]').text\n            xtl = float(box.get('xtl'))\n            ytl = float(box.get('ytl'))\n            xbr = float(box.get('xbr'))\n            ybr = float(box.get('ybr'))\n            occluded = int(box.get('occluded', 0)) # Handle missing occluded tag\n\n            # Only process non-occluded boxes for simplicity\n            if occluded > 0:\n                continue\n\n            cross_status = box.find('.//attribute[@name=\"cross\"]').text\n            # Map labels: 1 for crossing, 0 otherwise\n            cross_label = 1 if cross_status == 'crossing' else 0\n\n            if ped_id not in ped_tracks:\n                ped_tracks[ped_id] = {}\n\n            # Normalize bounding box: [center_x, center_y, width, height]\n            center_x = ((xtl + xbr) / 2) / img_width\n            center_y = ((ytl + ybr) / 2) / img_height\n            width = (xbr - xtl) / img_width\n            height = (ybr - ytl) / img_height\n\n            # Basic check for valid bbox dimensions\n            if width <= 0 or height <= 0 or not (0 <= center_x <= 1) or not (0 <= center_y <= 1):\n                continue # Skip invalid boxes\n\n            ped_tracks[ped_id][frame] = {\n                'bbox': [center_x, center_y, width, height],\n                'cross': cross_label\n            }\n\n    return ped_tracks, (img_width, img_height)\n\ndef create_sequences(ped_tracks, seq_len, pred_len):\n    \"\"\"Creates sequences of features and labels from pedestrian tracks.\"\"\"\n    sequences = []\n    for ped_id, frames_data in ped_tracks.items():\n        sorted_frames = sorted(frames_data.keys())\n\n        for i in range(len(sorted_frames) - seq_len - pred_len + 1):\n            seq_frames = sorted_frames[i : i + seq_len]\n            target_frame = sorted_frames[i + seq_len + pred_len - 1]\n\n            # Ensure frames are consecutive (or reasonably close)\n            # Allow for small gaps (e.g., 1-2 frames) if needed, but strict continuity is safer\n            is_continuous = all(seq_frames[j+1] - seq_frames[j] == 1 for j in range(len(seq_frames)-1))\n            is_target_continuous = (target_frame - seq_frames[-1] == pred_len)\n\n            if not (is_continuous and is_target_continuous):\n                continue\n\n            # Extract features (only bbox for now)\n            bbox_seq = [frames_data[f]['bbox'] for f in seq_frames]\n            # pose_seq = ... # Placeholder: Extract pose sequence if available\n            # flow_seq = ... # Placeholder: Extract flow sequence if available\n\n            # Get target label\n            label = frames_data[target_frame]['cross']\n\n            # Append sequence and label\n            # In a multi-stream setup, you'd store dict like {'bbox': bbox_seq, 'pose': pose_seq, ...}\n            sequences.append({'bbox': np.array(bbox_seq, dtype=np.float32), 'label': label})\n\n    return sequences\n\n\nclass PIEDataset(Dataset):\n    def __init__(self, annotation_dir, set_folders, seq_len, pred_len):\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.sequences = []\n\n        print(f\"Loading data from sets: {set_folders}\")\n        for set_folder in tqdm(set_folders):\n            set_path = os.path.join(annotation_dir, set_folder)\n            if not os.path.isdir(set_path):\n                print(f\"Warning: Annotation directory not found for {set_folder}\")\n                continue\n\n            xml_files = [f for f in os.listdir(set_path) if f.endswith('.xml')]\n            for xml_file in tqdm(xml_files, desc=f\"Processing {set_folder}\", leave=False):\n                file_path = os.path.join(set_path, xml_file)\n                ped_tracks, _ = parse_annotations(file_path)\n                if ped_tracks:\n                    video_sequences = create_sequences(ped_tracks, seq_len, pred_len)\n                    self.sequences.extend(video_sequences)\n\n        print(f\"Loaded {len(self.sequences)} sequences.\")\n        # Basic balancing (undersample majority class if highly imbalanced)\n        # More sophisticated balancing might be needed\n        labels = [s['label'] for s in self.sequences]\n        count_0 = labels.count(0)\n        count_1 = labels.count(1)\n        print(f\"Class distribution: 0={count_0}, 1={count_1}\")\n\n        # Example Undersampling (adjust ratio as needed)\n        # if count_0 > count_1 * 2: # If non-crossing is more than double crossing\n        #     print(\"Performing undersampling of class 0...\")\n        #     indices_0 = [i for i, label in enumerate(labels) if label == 0]\n        #     indices_1 = [i for i, label in enumerate(labels) if label == 1]\n        #     indices_0_sampled = random.sample(indices_0, count_1 * 2) # Keep double non-crossing\n        #     sampled_indices = sorted(indices_1 + indices_0_sampled)\n        #     self.sequences = [self.sequences[i] for i in sampled_indices]\n        #     print(f\"Reduced sequences to {len(self.sequences)}\")\n\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        sequence_data = self.sequences[idx]\n\n        # --- Modality Extraction ---\n        # Currently only BBox\n        bbox_features = torch.tensor(sequence_data['bbox'], dtype=torch.float32)\n\n        # Placeholder for other modalities\n        # pose_features = torch.tensor(...)\n        # flow_features = torch.tensor(...)\n\n        label = torch.tensor(sequence_data['label'], dtype=torch.long) # Use long for CrossEntropyLoss\n\n        # Return as a dictionary for clarity, especially with multiple streams\n        features = {\n            'bbox': bbox_features\n            # 'pose': pose_features,\n            # 'flow': flow_features\n        }\n        return features, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:01:45.490840Z","iopub.execute_input":"2025-03-25T22:01:45.491154Z","iopub.status.idle":"2025-03-25T22:01:45.504862Z","shell.execute_reply.started":"2025-03-25T22:01:45.491126Z","shell.execute_reply":"2025-03-25T22:01:45.503989Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# --- Model Architecture ---\n\nclass Attention(nn.Module):\n    \"\"\" Simple Dot-Product Attention or Learned Attention\"\"\"\n    def __init__(self, hidden_dim, attention_dim):\n        super(Attention, self).__init__()\n        # Option 1: Simple Linear Attention\n        self.attention_net = nn.Sequential(\n            nn.Linear(hidden_dim, attention_dim),\n            nn.Tanh(),\n            nn.Linear(attention_dim, 1)\n        )\n        # Option 2: Dot Product (if query is available, e.g., final hidden state)\n        # Or Multi-Head Attention (more complex)\n\n    def forward(self, lstm_output):\n        # lstm_output shape: (batch_size, seq_len, hidden_dim)\n        # Calculate attention scores\n        attention_scores = self.attention_net(lstm_output).squeeze(2) # (batch_size, seq_len)\n        # Normalize scores to get weights\n        attention_weights = torch.softmax(attention_scores, dim=1) # (batch_size, seq_len)\n        # Weighted sum of LSTM outputs\n        # unsqueeze to allow broadcasting: (batch_size, seq_len, 1)\n        context_vector = torch.sum(lstm_output * attention_weights.unsqueeze(2), dim=1) # (batch_size, hidden_dim)\n        return context_vector, attention_weights\n\n\nclass MultiStreamAdaptiveLSTM(nn.Module):\n    def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes,\n                 attention_dim, dropout_rate, stream_names=['bbox']): # Add 'pose', 'flow' etc. later\n        super(MultiStreamAdaptiveLSTM, self).__init__()\n        self.stream_names = stream_names\n        self.lstms = nn.ModuleDict()\n        self.attentions = nn.ModuleDict() # Attention per stream (temporal attention)\n\n        # --- Create LSTM Streams ---\n        for name in self.stream_names:\n            input_size = input_sizes[name]\n            self.lstms[name] = nn.LSTM(input_size, lstm_hidden_size, num_lstm_layers,\n                                       batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0,\n                                       bidirectional=False) # Consider bidirectional=True\n            # Temporal attention for each stream's output sequence\n            self.attentions[name] = Attention(lstm_hidden_size, attention_dim) # Assuming non-bidirectional LSTM\n\n        # --- Fusion Mechanism ---\n        # For now, simple concatenation or weighted sum after temporal attention.\n        # The \"Adaptive Fusion via Attention\" from the prompt would operate *across*\n        # the outputs of the stream-specific attentions (the context_vectors).\n        num_streams = len(self.stream_names)\n        combined_feature_dim = lstm_hidden_size * num_streams # If concatenating context vectors\n\n        # Placeholder for cross-stream attention mechanism\n        # self.fusion_attention = nn.Linear(combined_feature_dim, num_streams) # Example: Learns weights per stream\n\n        # --- Prediction Head ---\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc1 = nn.Linear(combined_feature_dim, combined_feature_dim // 2)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(combined_feature_dim // 2, num_classes)\n\n    def forward(self, x):\n        # x is a dictionary: {'bbox': tensor, 'pose': tensor, ...}\n        stream_outputs = {}\n        stream_context_vectors = []\n        stream_att_weights = {} # Store attention weights for inspection if needed\n\n        # --- Process each stream ---\n        for name in self.stream_names:\n            lstm_input = x[name] # Shape: (batch_size, seq_len, input_size)\n            lstm_out, _ = self.lstms[name](lstm_input) # lstm_out shape: (batch_size, seq_len, hidden_size)\n            # Apply temporal attention to each stream's output sequence\n            context_vector, attention_weights = self.attentions[name](lstm_out) # context_vector shape: (batch_size, hidden_size)\n            stream_context_vectors.append(context_vector)\n            stream_att_weights[name] = attention_weights # Store temporal attention weights\n\n        # --- Fusion ---\n        # Option 1: Simple Concatenation (as implemented here)\n        fused_features = torch.cat(stream_context_vectors, dim=1) # Shape: (batch_size, hidden_size * num_streams)\n\n        # Option 2: Adaptive Attention Fusion (More complex, requires defining attention mechanism over context_vectors)\n        # Example idea:\n        # combined_context = torch.stack(stream_context_vectors, dim=1) # (batch_size, num_streams, hidden_size)\n        # fusion_scores = self.fusion_attention(fused_features) # Or another attention mechanism on combined_context\n        # fusion_weights = torch.softmax(fusion_scores, dim=1).unsqueeze(2) # (batch_size, num_streams, 1)\n        # fused_features = torch.sum(combined_context * fusion_weights, dim=1) # (batch_size, hidden_size)\n\n        # --- Prediction ---\n        x = self.dropout(fused_features)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        logits = self.fc2(x) # Output logits\n\n        return logits # Return logits for CrossEntropyLoss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:01:51.228836Z","iopub.execute_input":"2025-03-25T22:01:51.229116Z","iopub.status.idle":"2025-03-25T22:01:51.238246Z","shell.execute_reply.started":"2025-03-25T22:01:51.229094Z","shell.execute_reply":"2025-03-25T22:01:51.237292Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# --- Training and Evaluation Functions ---\n\ndef train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n\n    for features, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n        # Move data to device\n        input_features = {name: features[name].to(device) for name in model.stream_names}\n        labels = labels.to(device)\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(input_features)\n\n        # Calculate loss\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        # Store predictions and labels for epoch metrics (optional during training)\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    avg_loss = total_loss / len(dataloader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    return avg_loss, accuracy\n\ndef evaluate_epoch(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_labels = []\n    all_preds = []\n    all_probs = [] # For AUC\n\n    with torch.no_grad():\n        for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n            # Move data to device\n            input_features = {name: features[name].to(device) for name in model.stream_names}\n            labels = labels.to(device)\n\n            # Forward pass\n            outputs = model(input_features) # Logits\n\n            # Calculate loss\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n\n            # Calculate probabilities and predictions\n            probs = torch.softmax(outputs, dim=1)\n            preds = torch.argmax(probs, dim=1)\n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy()) # Store probabilities\n\n    avg_loss = total_loss / len(dataloader)\n    all_probs = np.array(all_probs)\n    all_labels = np.array(all_labels)\n    all_preds = np.array(all_preds)\n\n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', pos_label=1, zero_division=0\n    )\n\n    # Calculate AUC using probabilities of the positive class (class 1)\n    if len(np.unique(all_labels)) > 1: # AUC requires both classes present\n       auc = roc_auc_score(all_labels, all_probs[:, 1]) # Probs for class 1\n    else:\n       auc = float('nan') # Not defined if only one class is present\n\n    metrics = {\n        'loss': avg_loss,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'auc': auc\n    }\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:01:55.778831Z","iopub.execute_input":"2025-03-25T22:01:55.779120Z","iopub.status.idle":"2025-03-25T22:01:55.788711Z","shell.execute_reply.started":"2025-03-25T22:01:55.779096Z","shell.execute_reply":"2025-03-25T22:01:55.787853Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# --- Main Execution ---\n\nif __name__ == '__main__':\n    # --- Initialize Datasets and Dataloaders ---\n    train_dataset = PIEDataset(ANNOTATION_DIR, TRAIN_SETS, SEQ_LEN, PRED_LEN)\n    val_dataset = PIEDataset(ANNOTATION_DIR, VAL_SETS, SEQ_LEN, PRED_LEN)\n\n    # Handle potential empty datasets\n    if not train_dataset or not val_dataset or len(train_dataset) == 0 or len(val_dataset) == 0:\n         raise ValueError(\"Dataset loading failed or resulted in empty datasets. Check paths and parsing.\")\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n    # --- Initialize Model, Loss, Optimizer ---\n    input_sizes = {'bbox': INPUT_SIZE_BBOX} # Add other modalities here if implemented\n    model = MultiStreamAdaptiveLSTM(\n        input_sizes=input_sizes,\n        lstm_hidden_size=LSTM_HIDDEN_SIZE,\n        num_lstm_layers=NUM_LSTM_LAYERS,\n        num_classes=NUM_CLASSES,\n        attention_dim=ATTENTION_DIM,\n        dropout_rate=DROPOUT_RATE,\n        stream_names=['bbox'] # Update if using more streams\n    ).to(DEVICE)\n\n    print(model)\n    # Calculate class weights for imbalanced data (optional but recommended)\n    # labels = np.array([s['label'] for s in train_dataset.sequences])\n    # class_counts = np.bincount(labels)\n    # class_weights = torch.tensor([len(labels) / c if c > 0 else 0 for c in class_counts], dtype=torch.float).to(DEVICE)\n    # print(f\"Using class weights: {class_weights}\")\n    # criterion = nn.CrossEntropyLoss(weight=class_weights)\n    criterion = nn.CrossEntropyLoss() # Standard loss\n\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5) # Optional LR scheduler\n\n    # --- Training Loop ---\n    best_val_f1 = -1.0\n    print(\"\\n--- Starting Training ---\")\n    for epoch in range(NUM_EPOCHS):\n        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n        val_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n        # if scheduler: scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}:\")\n        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        print(f\"  Val Loss:   {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n        print(f\"  Val Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n        print(f\"  Val AUC: {val_metrics['auc']:.4f}\")\n\n        # Save best model based on validation F1-score\n        if val_metrics['f1'] > best_val_f1:\n            best_val_f1 = val_metrics['f1']\n            torch.save(model.state_dict(), 'best_model.pth')\n            print(f\"  Saved new best model with F1: {best_val_f1:.4f}\")\n        print(\"-\" * 20)\n\n    print(\"--- Training Finished ---\")\n\n    # --- Final Evaluation on Validation Set (Set 6) ---\n    print(\"\\n--- Final Evaluation on Set 6 (Validation Set) using Best Model ---\")\n    # Load best model\n    model.load_state_dict(torch.load('best_model.pth', map_location=DEVICE))\n    final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n\n    print(\"Final Performance Metrics:\")\n    print(f\"  Accuracy:  {final_metrics['accuracy']:.4f}\")\n    print(f\"  Precision: {final_metrics['precision']:.4f}\")\n    print(f\"  Recall:    {final_metrics['recall']:.4f}\")\n    print(f\"  F1 Score:  {final_metrics['f1']:.4f}\")\n    print(f\"  AUC:       {final_metrics['auc']:.4f}\")\n    print(f\"  Loss:      {final_metrics['loss']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:12:10.041764Z","iopub.execute_input":"2025-03-25T22:12:10.042063Z","iopub.status.idle":"2025-03-25T22:14:36.950001Z","shell.execute_reply.started":"2025-03-25T22:12:10.042040Z","shell.execute_reply":"2025-03-25T22:14:36.948881Z"}},"outputs":[{"name":"stdout","text":"Loading data from sets: ['set06']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e47d74779d048a1abdf49355f0e024f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing set06:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Loaded 46323 sequences.\nClass distribution: 0=37978, 1=8345\nLoading data from sets: ['set01', 'set02']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3b47eb1b2c64dc4bfa217de16c3ce58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing set01:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing set02:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Loaded 64096 sequences.\nClass distribution: 0=49937, 1=14159\nMultiStreamAdaptiveLSTM(\n  (lstms): ModuleDict(\n    (bbox): LSTM(4, 128, num_layers=2, batch_first=True, dropout=0.3)\n  )\n  (attentions): ModuleDict(\n    (bbox): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=128, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc1): Linear(in_features=128, out_features=64, bias=True)\n  (relu): ReLU()\n  (fc2): Linear(in_features=64, out_features=2, bias=True)\n)\n\n--- Starting Training ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1/20:\n  Train Loss: 0.4273, Train Acc: 0.7971\n  Val Loss:   0.4452, Val Acc: 0.8000\n  Val Precision: 0.6000, Recall: 0.2838, F1: 0.3853\n  Val AUC: 0.7796\n  Saved new best model with F1: 0.3853\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2/20:\n  Train Loss: 0.3332, Train Acc: 0.8760\n  Val Loss:   0.4547, Val Acc: 0.7985\n  Val Precision: 0.5543, Recall: 0.4485, F1: 0.4958\n  Val AUC: 0.7796\n  Saved new best model with F1: 0.4958\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3/20:\n  Train Loss: 0.3093, Train Acc: 0.8813\n  Val Loss:   0.4580, Val Acc: 0.8120\n  Val Precision: 0.6110, Recall: 0.4096, F1: 0.4904\n  Val AUC: 0.7864\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4/20:\n  Train Loss: 0.2955, Train Acc: 0.8831\n  Val Loss:   0.4845, Val Acc: 0.8109\n  Val Precision: 0.6028, Recall: 0.4221, F1: 0.4965\n  Val AUC: 0.7899\n  Saved new best model with F1: 0.4965\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5/20:\n  Train Loss: 0.2912, Train Acc: 0.8831\n  Val Loss:   0.4967, Val Acc: 0.7965\n  Val Precision: 0.5448, Recall: 0.4788, F1: 0.5097\n  Val AUC: 0.7907\n  Saved new best model with F1: 0.5097\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6/20:\n  Train Loss: 0.2877, Train Acc: 0.8846\n  Val Loss:   0.5192, Val Acc: 0.7783\n  Val Precision: 0.4984, Recall: 0.5159, F1: 0.5070\n  Val AUC: 0.7919\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7/20:\n  Train Loss: 0.2852, Train Acc: 0.8851\n  Val Loss:   0.5518, Val Acc: 0.7769\n  Val Precision: 0.4955, Recall: 0.5433, F1: 0.5183\n  Val AUC: 0.7916\n  Saved new best model with F1: 0.5183\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8/20:\n  Train Loss: 0.2824, Train Acc: 0.8861\n  Val Loss:   0.5033, Val Acc: 0.7906\n  Val Precision: 0.5270, Recall: 0.5064, F1: 0.5165\n  Val AUC: 0.7980\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9/20:\n  Train Loss: 0.2783, Train Acc: 0.8859\n  Val Loss:   0.5502, Val Acc: 0.7791\n  Val Precision: 0.5000, Recall: 0.5678, F1: 0.5317\n  Val AUC: 0.7956\n  Saved new best model with F1: 0.5317\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10/20:\n  Train Loss: 0.2743, Train Acc: 0.8871\n  Val Loss:   0.5473, Val Acc: 0.7876\n  Val Precision: 0.5176, Recall: 0.5699, F1: 0.5425\n  Val AUC: 0.8023\n  Saved new best model with F1: 0.5425\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11/20:\n  Train Loss: 0.2711, Train Acc: 0.8871\n  Val Loss:   0.5078, Val Acc: 0.7930\n  Val Precision: 0.5312, Recall: 0.5356, F1: 0.5334\n  Val AUC: 0.8059\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12/20:\n  Train Loss: 0.2686, Train Acc: 0.8882\n  Val Loss:   0.6090, Val Acc: 0.7907\n  Val Precision: 0.5224, Recall: 0.6152, F1: 0.5650\n  Val AUC: 0.8078\n  Saved new best model with F1: 0.5650\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13/20:\n  Train Loss: 0.2647, Train Acc: 0.8883\n  Val Loss:   0.5367, Val Acc: 0.7968\n  Val Precision: 0.5385, Recall: 0.5619, F1: 0.5499\n  Val AUC: 0.8019\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 14/20:\n  Train Loss: 0.2629, Train Acc: 0.8886\n  Val Loss:   0.4895, Val Acc: 0.8171\n  Val Precision: 0.6051, Recall: 0.4948, F1: 0.5444\n  Val AUC: 0.8071\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 15/20:\n  Train Loss: 0.2622, Train Acc: 0.8887\n  Val Loss:   0.5955, Val Acc: 0.8040\n  Val Precision: 0.5565, Recall: 0.5551, F1: 0.5558\n  Val AUC: 0.7986\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 16/20:\n  Train Loss: 0.2602, Train Acc: 0.8890\n  Val Loss:   0.6451, Val Acc: 0.8037\n  Val Precision: 0.5552, Recall: 0.5608, F1: 0.5580\n  Val AUC: 0.7988\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 17/20:\n  Train Loss: 0.2583, Train Acc: 0.8886\n  Val Loss:   0.6282, Val Acc: 0.8066\n  Val Precision: 0.5635, Recall: 0.5522, F1: 0.5578\n  Val AUC: 0.7971\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 18/20:\n  Train Loss: 0.2557, Train Acc: 0.8887\n  Val Loss:   0.5783, Val Acc: 0.8156\n  Val Precision: 0.5891, Recall: 0.5462, F1: 0.5668\n  Val AUC: 0.8074\n  Saved new best model with F1: 0.5668\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 19/20:\n  Train Loss: 0.2532, Train Acc: 0.8885\n  Val Loss:   0.6383, Val Acc: 0.8031\n  Val Precision: 0.5516, Recall: 0.5822, F1: 0.5665\n  Val AUC: 0.7956\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 20/20:\n  Train Loss: 0.2495, Train Acc: 0.8878\n  Val Loss:   0.5901, Val Acc: 0.8023\n  Val Precision: 0.5509, Recall: 0.5688, F1: 0.5597\n  Val AUC: 0.8022\n--------------------\n--- Training Finished ---\n\n--- Final Evaluation on Set 6 (Validation Set) using Best Model ---\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-27-56c466e73d67>:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth', map_location=DEVICE))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Final Performance Metrics:\n  Accuracy:  0.8156\n  Precision: 0.5891\n  Recall:    0.5462\n  F1 Score:  0.5668\n  AUC:       0.8074\n  Loss:      0.5783\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Install necessary libraries (run this cell once)\n# !pip install -q openmim opencv-python-headless torch torchvision torchaudio # -headless avoids issues in some envs\n!mim install -q mmcv>=2.0.0 # Or mmcv-full if needed, but base should suffice for inference\n!mim install -q mmpose>=1.0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T01:29:28.203677Z","iopub.execute_input":"2025-03-26T01:29:28.204050Z","iopub.status.idle":"2025-03-26T01:31:55.959597Z","shell.execute_reply.started":"2025-03-26T01:29:28.204022Z","shell.execute_reply":"2025-03-26T01:31:55.958412Z"}},"outputs":[{"name":"stdout","text":"^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!pip install -U openmim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T01:32:07.712587Z","iopub.execute_input":"2025-03-26T01:32:07.712968Z","iopub.status.idle":"2025-03-26T01:32:11.305419Z","shell.execute_reply.started":"2025-03-26T01:32:07.712933Z","shell.execute_reply":"2025-03-26T01:32:11.304516Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: openmim in /usr/local/lib/python3.10/dist-packages (0.3.9)\nRequirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from openmim) (8.1.7)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from openmim) (0.4.6)\nRequirement already satisfied: model-index in /usr/local/lib/python3.10/dist-packages (from openmim) (0.1.11)\nRequirement already satisfied: opendatalab in /usr/local/lib/python3.10/dist-packages (from openmim) (0.0.10)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from openmim) (2.2.3)\nRequirement already satisfied: pip>=19.3 in /usr/local/lib/python3.10/dist-packages (from openmim) (24.1.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from openmim) (2.28.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from openmim) (13.4.2)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from openmim) (0.9.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (6.0.2)\nRequirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (3.7)\nRequirement already satisfied: ordered-set in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (4.1.0)\nRequirement already satisfied: pycryptodome in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim) (3.21.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim) (4.65.2)\nRequirement already satisfied: openxlab in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim) (0.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (1.26.20)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (2025.1.31)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2023.4)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2025.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->openmim) (0.1.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->openmim) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->openmim) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->openmim) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->openmim) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->openmim) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->openmim) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->openmim) (1.17.0)\nRequirement already satisfied: filelock~=3.14.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (3.14.0)\nRequirement already satisfied: oss2~=2.17.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (2.17.0)\nRequirement already satisfied: packaging~=24.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (24.2)\nRequirement already satisfied: setuptools~=60.2.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (60.2.0)\nRequirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.10/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (1.7)\nRequirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (2.16.5)\nRequirement already satisfied: aliyun-python-sdk-core>=2.13.12 in /usr/local/lib/python3.10/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (2.16.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas->openmim) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas->openmim) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas->openmim) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas->openmim) (2024.2.0)\nRequirement already satisfied: jmespath<1.0.0,>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (0.10.0)\nRequirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (44.0.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas->openmim) (2024.2.0)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (2.22)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# !mim install -q \"mmcv>=2.0.0\"\n# !mim install -q \"mmpose>=1.0.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T01:32:28.044709Z","iopub.execute_input":"2025-03-26T01:32:28.045124Z","iopub.status.idle":"2025-03-26T01:53:16.885659Z","shell.execute_reply.started":"2025-03-26T01:32:28.045088Z","shell.execute_reply":"2025-03-26T01:53:16.884560Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for mmcv (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# # Download RTMPose model (e.g., RTMPose-M for COCO)\n# # Create a directory for the model\n# # !mkdir -p /kaggle/working/checkpoints\n# # Download config file\n# !wget -q https://download.openmmlab.com/mmpose/v1/projects/rtmpose/rtmpose-m_8xb256-420e_coco-256x192.py -O /kaggle/working/rtmpose-m_8xb256-420e_coco-256x192.py\n# # Download checkpoint file\n# !wget -q https://download.openmmlab.com/mmpose/v1/projects/rtmpose/rtmpose-m_8xb256-420e_coco-256x192-4d311afd_20230531.pth -O /kaggle/working/checkpoints/rtmpose-m_8xb256-420e_coco-256x192-4d311afd_20230531.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T01:26:05.293460Z","iopub.execute_input":"2025-03-26T01:26:05.293806Z","iopub.status.idle":"2025-03-26T01:26:07.198393Z","shell.execute_reply.started":"2025-03-26T01:26:05.293776Z","shell.execute_reply":"2025-03-26T01:26:07.197227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Verify download\n# !ls -l /kaggle/working/checkpoints/\n# !ls -l /kaggle/working/rtmpose-m_8xb256-420e_coco-256x192.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T01:27:02.878478Z","iopub.execute_input":"2025-03-26T01:27:02.878861Z","iopub.status.idle":"2025-03-26T01:27:03.164356Z","shell.execute_reply.started":"2025-03-26T01:27:02.878829Z","shell.execute_reply":"2025-03-26T01:27:03.163347Z"}},"outputs":[{"name":"stdout","text":"total 0\n-rw-r--r-- 1 root root 0 Mar 26 01:26 rtmpose-m_8xb256-420e_coco-256x192-4d311afd_20230531.pth\n-rw-r--r-- 1 root root 0 Mar 26 01:26 /kaggle/working/rtmpose-m_8xb256-420e_coco-256x192.py\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!pip install -q ultralytics opencv-python-headless # ultralytics includes necessary dependencies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:26:58.843007Z","iopub.execute_input":"2025-03-26T02:26:58.843408Z","iopub.status.idle":"2025-03-26T02:27:03.316706Z","shell.execute_reply.started":"2025-03-26T02:26:58.843379Z","shell.execute_reply":"2025-03-26T02:27:03.315806Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.8/949.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed \n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport zipfile\nimport random\nimport math\nimport xml.etree.ElementTree as ET\nfrom tqdm.notebook import tqdm\n\n# Deep Learning libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\nimport cv2\nfrom ultralytics import YOLO\n\n\n\n# --- Configuration ---\nBASE_DIR = '/kaggle/working/PIE/content'\nANNOTATION_DIR = '/kaggle/working/PIE/annotations/annotations'\n# CLIP_DIR = os.path.join(BASE_DIR, 'PIE_clips') # Not used directly in this version\n\n# Model Hyperparameters\nSEQ_LEN = 15        # Number of past frames to observe\nPRED_LEN = 1        # Predict state at the end of sequence (relative to seq_len)\nINPUT_SIZE_BBOX = 4 # (center_x, center_y, width, height) - normalized\nINPUT_SIZE_POSE = 34  # 17 keypoints * 2 coordinates (from YOLOv8 pose)\nLSTM_HIDDEN_SIZE = 128\nNUM_LSTM_LAYERS = 2\nDROPOUT_RATE = 0.3\nNUM_CLASSES = 2      # 0: not-crossing, 1: crossing\nATTENTION_DIM = 128  # Dimension for the attention mechanism\n\n# Training Hyperparameters\nLEARNING_RATE = 1e-4\nBATCH_SIZE = 64\nNUM_EPOCHS = 20 # Adjust as needed\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\nTRAIN_SETS = ['set06']\nVAL_SETS = ['set01', 'set02'] \n\n# --- Helper Function: Compute IOU ---\ndef compute_iou(boxA, boxB):\n    # boxA and boxB are in [x1, y1, x2, y2] format\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    interW = max(0, xB - xA)\n    interH = max(0, yB - yA)\n    interArea = interW * interH\n    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n    return iou\n\n# --- Data Preprocessing Functions ---\n\ndef parse_annotations(xml_file):\n    \"\"\"Parses a PIE annotation XML file.\"\"\"\n    try:\n        tree = ET.parse(xml_file)\n        root = tree.getroot()\n    except ET.ParseError:\n        print(f\"Error parsing {xml_file}\")\n        return None, None\n\n    img_width = int(root.find('.//original_size/width').text)\n    img_height = int(root.find('.//original_size/height').text)\n\n    ped_tracks = {} # {ped_id: {frame: {'bbox': [xtl,ytl,xbr,ybr], 'cross': label}}}\n\n    for track in root.findall('.//track[@label=\"pedestrian\"]'):\n        for box in track.findall('.//box'):\n            frame = int(box.get('frame'))\n            ped_id = box.find('.//attribute[@name=\"id\"]').text\n            xtl = float(box.get('xtl'))\n            ytl = float(box.get('ytl'))\n            xbr = float(box.get('xbr'))\n            ybr = float(box.get('ybr'))\n            occluded = int(box.get('occluded', 0)) # Handle missing occluded tag\n\n            # Only process non-occluded boxes for simplicity\n            if occluded > 0:\n                continue\n\n            cross_status = box.find('.//attribute[@name=\"cross\"]').text\n            # Map labels: 1 for crossing, 0 otherwise\n            cross_label = 1 if cross_status == 'crossing' else 0\n\n            if ped_id not in ped_tracks:\n                ped_tracks[ped_id] = {}\n\n            # Normalize bounding box: [center_x, center_y, width, height]\n            center_x = ((xtl + xbr) / 2) / img_width\n            center_y = ((ytl + ybr) / 2) / img_height\n            width = (xbr - xtl) / img_width\n            height = (ybr - ytl) / img_height\n\n            # Basic check for valid bbox dimensions\n            if width <= 0 or height <= 0 or not (0 <= center_x <= 1) or not (0 <= center_y <= 1):\n                continue # Skip invalid boxes\n\n            ped_tracks[ped_id][frame] = {\n                'bbox': [center_x, center_y, width, height],\n                'cross': cross_label\n            }\n\n    return ped_tracks, (img_width, img_height)\n\ndef create_sequences(ped_tracks, seq_len, pred_len, set_folder):\n    \"\"\"Creates sequences of features and labels from pedestrian tracks.\n       Now also stores frame numbers and image dimensions for pose extraction.\"\"\"\n    sequences = []\n    for ped_id, frames_data in ped_tracks.items():\n        sorted_frames = sorted(frames_data.keys())\n\n        for i in range(len(sorted_frames) - seq_len - pred_len + 1):\n            seq_frames = sorted_frames[i : i + seq_len]\n            target_frame = sorted_frames[i + seq_len + pred_len - 1]\n\n            # Ensure frames are consecutive\n            is_continuous = all(seq_frames[j+1] - seq_frames[j] == 1 for j in range(len(seq_frames)-1))\n            is_target_continuous = (target_frame - seq_frames[-1] == pred_len)\n\n            if not (is_continuous and is_target_continuous):\n                continue\n\n            # Extract bbox features\n            bbox_seq = [frames_data[f]['bbox'] for f in seq_frames]\n            label = frames_data[target_frame]['cross']\n\n            # For pose extraction, store the frame numbers and image dimensions.\n            # (Assume all frames in the XML share the same image dimensions)\n            # Note: set_folder helps locate images in BASE_DIR.\n            sequence = {\n                'bbox': np.array(bbox_seq, dtype=np.float32),\n                'label': label,\n                'frames': seq_frames,\n                'set_folder': set_folder\n            }\n            # Also store image dimensions from the first frame\n            # (Assuming they are constant within a video)\n            # Here, we pick the first frame to get dims from the parsed annotation.\n            # You may adjust if different frames have different dims.\n            sequence['img_dims'] = None  # will be set outside if needed\n            sequences.append(sequence)\n    return sequences\n\n# --- Dataset Class ---\n\nclass PIEDataset(Dataset):\n    def __init__(self, annotation_dir, set_folders, seq_len, pred_len):\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.sequences = []\n        self.BASE_DIR = BASE_DIR  # Use global BASE_DIR for images\n\n        # Initialize the YOLOv8 pose model (assumed to be in the working directory)\n        self.pose_model = YOLO('yolo8n-pose.pt')  # Ensure the model file is available\n\n        print(f\"Loading data from sets: {set_folders}\")\n        for set_folder in tqdm(set_folders):\n            set_path = os.path.join(annotation_dir, set_folder)\n            if not os.path.isdir(set_path):\n                print(f\"Warning: Annotation directory not found for {set_folder}\")\n                continue\n\n            xml_files = [f for f in os.listdir(set_path) if f.endswith('.xml')]\n            for xml_file in tqdm(xml_files, desc=f\"Processing {set_folder}\", leave=False):\n                file_path = os.path.join(set_path, xml_file)\n                ped_tracks, dims = parse_annotations(file_path)\n                if dims is None:\n                    continue\n                img_width, img_height = dims\n                if ped_tracks:\n                    video_sequences = create_sequences(ped_tracks, seq_len, pred_len, set_folder)\n                    # Set image dimensions for each sequence\n                    for seq in video_sequences:\n                        seq['img_dims'] = (img_width, img_height)\n                    self.sequences.extend(video_sequences)\n\n        print(f\"Loaded {len(self.sequences)} sequences.\")\n        # Basic balancing (undersampling majority class if highly imbalanced)\n        labels = [s['label'] for s in self.sequences]\n        count_0 = labels.count(0)\n        count_1 = labels.count(1)\n        print(f\"Class distribution: 0={count_0}, 1={count_1}\")\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        sequence_data = self.sequences[idx]\n\n        # --- Modality Extraction for BBox ---\n        bbox_features = torch.tensor(sequence_data['bbox'], dtype=torch.float32)\n\n        # --- Pose Estimation using YOLOv8 Pose ---\n        # For each frame in the sequence, load the image and extract keypoints\n        set_folder = sequence_data['set_folder']\n        img_dims = sequence_data['img_dims']  # (img_width, img_height)\n        img_width, img_height = img_dims\n        pose_seq = []\n\n        for frame in sequence_data['frames']:\n            # Assumption: images are stored in BASE_DIR/<set_folder> and named as \"<frame>.jpg\"\n            image_path = os.path.join(self.BASE_DIR, set_folder, f\"{frame}.jpg\")\n            # Load image using OpenCV\n            image = cv2.imread(image_path)\n            if image is None:\n                # If image is not found, use a zero vector\n                pose_seq.append(np.zeros(INPUT_SIZE_POSE, dtype=np.float32))\n                continue\n\n            # Convert BGR to RGB\n            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n            # Run YOLOv8 pose model on the image\n            results = self.pose_model(image_rgb, verbose=False)  # results is a list\n            keypoints_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)  # default if no match is found\n\n            # Convert annotation bbox from normalized (center_x, center_y, w, h) to pixel coordinates\n            ann_bbox = sequence_data['bbox'][sequence_data['frames'].index(frame)]\n            ann_cx, ann_cy, ann_w, ann_h = ann_bbox\n            ann_x1 = (ann_cx - ann_w/2) * img_width\n            ann_y1 = (ann_cy - ann_h/2) * img_height\n            ann_x2 = (ann_cx + ann_w/2) * img_width\n            ann_y2 = (ann_cy + ann_h/2) * img_height\n            ann_box_pixels = [ann_x1, ann_y1, ann_x2, ann_y2]\n\n            # Search for the detection with best IOU with the annotation bbox\n            best_iou = 0.0\n            best_keypoints = None\n            if results and len(results) > 0:\n                # Each result may contain multiple detections.\n                res = results[0]\n                # res.boxes.xyxy gives detections; res.keypoints.data gives keypoints if available.\n                if res.boxes is not None and res.keypoints is not None:\n                    boxes = res.boxes.xyxy.cpu().numpy()  # shape: (n, 4)\n                    kps = res.keypoints.cpu().numpy()       # shape: (n, 17, 3)\n                    for i in range(len(boxes)):\n                        det_box = boxes[i]\n                        iou = compute_iou(ann_box_pixels, det_box)\n                        if iou > best_iou:\n                            best_iou = iou\n                            best_keypoints = kps[i]\n\n            # If a detection with sufficient overlap is found, process its keypoints\n            if best_keypoints is not None and best_iou > 0.3:\n                # Extract x,y coordinates (ignoring confidence) and normalize them\n                norm_keypoints = []\n                for (x, y, conf) in best_keypoints:\n                    norm_keypoints.extend([x / img_width, y / img_height])\n                keypoints_vector = np.array(norm_keypoints, dtype=np.float32)\n                # Ensure it is of length 34\n                if keypoints_vector.shape[0] != INPUT_SIZE_POSE:\n                    keypoints_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n            # Else, keypoints_vector remains zeros\n\n            pose_seq.append(keypoints_vector)\n        pose_features = torch.tensor(np.array(pose_seq), dtype=torch.float32)\n\n        # --- Return combined modalities ---\n        features = {\n            'bbox': bbox_features,\n            'pose': pose_features\n        }\n        label = torch.tensor(sequence_data['label'], dtype=torch.long) # Use long for CrossEntropyLoss\n\n        return features, label\n    \n\n# --- Model Architecture ---\n\nclass Attention(nn.Module):\n    \"\"\" Simple Dot-Product Attention or Learned Attention\"\"\"\n    def __init__(self, hidden_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.attention_net = nn.Sequential(\n            nn.Linear(hidden_dim, attention_dim),\n            nn.Tanh(),\n            nn.Linear(attention_dim, 1)\n        )\n\n    def forward(self, lstm_output):\n        attention_scores = self.attention_net(lstm_output).squeeze(2)\n        attention_weights = torch.softmax(attention_scores, dim=1)\n        context_vector = torch.sum(lstm_output * attention_weights.unsqueeze(2), dim=1)\n        return context_vector, attention_weights\n\nclass MultiStreamAdaptiveLSTM(nn.Module):\n    def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes,\n                 attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n        super(MultiStreamAdaptiveLSTM, self).__init__()\n        self.stream_names = stream_names\n        self.lstms = nn.ModuleDict()\n        self.attentions = nn.ModuleDict()\n\n        for name in self.stream_names:\n            input_size = input_sizes[name]\n            self.lstms[name] = nn.LSTM(input_size, lstm_hidden_size, num_lstm_layers,\n                                       batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0,\n                                       bidirectional=False)\n            self.attentions[name] = Attention(lstm_hidden_size, attention_dim)\n\n        num_streams = len(self.stream_names)\n        combined_feature_dim = lstm_hidden_size * num_streams\n\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc1 = nn.Linear(combined_feature_dim, combined_feature_dim // 2)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(combined_feature_dim // 2, num_classes)\n\n    def forward(self, x):\n        stream_context_vectors = []\n        stream_att_weights = {}\n\n        for name in self.stream_names:\n            lstm_input = x[name]\n            lstm_out, _ = self.lstms[name](lstm_input)\n            context_vector, attention_weights = self.attentions[name](lstm_out)\n            stream_context_vectors.append(context_vector)\n            stream_att_weights[name] = attention_weights\n\n        fused_features = torch.cat(stream_context_vectors, dim=1)\n        x = self.dropout(fused_features)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        logits = self.fc2(x)\n        return logits\n\n# --- Training and Evaluation Functions ---\n\ndef train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n\n    for features, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n        input_features = {name: features[name].to(device) for name in model.stream_names}\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(input_features)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    avg_loss = total_loss / len(dataloader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    return avg_loss, accuracy\n\ndef evaluate_epoch(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_labels = []\n    all_preds = []\n    all_probs = []\n\n    with torch.no_grad():\n        for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n            input_features = {name: features[name].to(device) for name in model.stream_names}\n            labels = labels.to(device)\n            outputs = model(input_features)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            probs = torch.softmax(outputs, dim=1)\n            preds = torch.argmax(probs, dim=1)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n\n    avg_loss = total_loss / len(dataloader)\n    all_probs = np.array(all_probs)\n    all_labels = np.array(all_labels)\n    all_preds = np.array(all_preds)\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', pos_label=1, zero_division=0\n    )\n    if len(np.unique(all_labels)) > 1:\n       auc = roc_auc_score(all_labels, all_probs[:, 1])\n    else:\n       auc = float('nan')\n    metrics = {\n        'loss': avg_loss,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'auc': auc\n    }\n    return metrics\n\n# --- Main Execution ---\n\nif __name__ == '__main__':\n    # Initialize Datasets and Dataloaders\n    train_dataset = PIEDataset(ANNOTATION_DIR, TRAIN_SETS, SEQ_LEN, PRED_LEN)\n    val_dataset = PIEDataset(ANNOTATION_DIR, VAL_SETS, SEQ_LEN, PRED_LEN)\n\n    if len(train_dataset) == 0 or len(val_dataset) == 0:\n         raise ValueError(\"Dataset loading failed or resulted in empty datasets. Check paths and parsing.\")\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n    # Initialize Model, Loss, and Optimizer\n    input_sizes = {'bbox': INPUT_SIZE_BBOX, 'pose': INPUT_SIZE_POSE}\n    model = MultiStreamAdaptiveLSTM(\n        input_sizes=input_sizes,\n        lstm_hidden_size=LSTM_HIDDEN_SIZE,\n        num_lstm_layers=NUM_LSTM_LAYERS,\n        num_classes=NUM_CLASSES,\n        attention_dim=ATTENTION_DIM,\n        dropout_rate=DROPOUT_RATE,\n        stream_names=['bbox', 'pose']\n    ).to(DEVICE)\n\n    print(model)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    # Training Loop\n    best_val_f1 = -1.0\n    print(\"\\n--- Starting Training ---\")\n    for epoch in range(NUM_EPOCHS):\n        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n        val_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}:\")\n        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        print(f\"  Val Loss:   {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n        print(f\"  Val Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n        print(f\"  Val AUC: {val_metrics['auc']:.4f}\")\n\n        if val_metrics['f1'] > best_val_f1:\n            best_val_f1 = val_metrics['f1']\n            torch.save(model.state_dict(), 'best_model.pth')\n            print(f\"  Saved new best model with F1: {best_val_f1:.4f}\")\n        print(\"-\" * 20)\n\n    print(\"--- Training Finished ---\")\n\n    # Final Evaluation on Validation Set\n    print(\"\\n--- Final Evaluation on Validation Set using Best Model ---\")\n    model.load_state_dict(torch.load('best_model.pth', map_location=DEVICE))\n    final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n\n    print(\"Final Performance Metrics:\")\n    print(f\"  Accuracy:  {final_metrics['accuracy']:.4f}\")\n    print(f\"  Precision: {final_metrics['precision']:.4f}\")\n    print(f\"  Recall:    {final_metrics['recall']:.4f}\")\n    print(f\"  F1 Score:  {final_metrics['f1']:.4f}\")\n    print(f\"  AUC:       {final_metrics['auc']:.4f}\")\n    print(f\"  Loss:      {final_metrics['loss']:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T03:24:48.033808Z","iopub.execute_input":"2025-03-26T03:24:48.034194Z","iopub.status.idle":"2025-03-26T03:36:30.102197Z","shell.execute_reply.started":"2025-03-26T03:24:48.034170Z","shell.execute_reply":"2025-03-26T03:36:30.101274Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading data from sets: ['set06']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5006f7abb2fb48cc979f044f3ee50d7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing set06:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Loaded 37730 sequences.\nClass distribution: 0=29385, 1=8345\nLoading data from sets: ['set01', 'set02']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f386662568b64136a878690882752db2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing set01:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing set02:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Loaded 64096 sequences.\nClass distribution: 0=49937, 1=14159\nMultiStreamAdaptiveLSTM(\n  (lstms): ModuleDict(\n    (bbox): LSTM(4, 128, num_layers=2, batch_first=True, dropout=0.3)\n    (pose): LSTM(34, 128, num_layers=2, batch_first=True, dropout=0.3)\n  )\n  (attentions): ModuleDict(\n    (bbox): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=128, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (pose): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=128, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc1): Linear(in_features=256, out_features=128, bias=True)\n  (relu): ReLU()\n  (fc2): Linear(in_features=128, out_features=2, bias=True)\n)\n\n--- Starting Training ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1/20:\n  Train Loss: 0.4775, Train Acc: 0.7856\n  Val Loss:   0.4571, Val Acc: 0.7970\n  Val Precision: 0.5933, Recall: 0.2574, F1: 0.3590\n  Val AUC: 0.7693\n  Saved new best model with F1: 0.3590\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2/20:\n  Train Loss: 0.3813, Train Acc: 0.8488\n  Val Loss:   0.4585, Val Acc: 0.7931\n  Val Precision: 0.5382, Recall: 0.4469, F1: 0.4883\n  Val AUC: 0.7711\n  Saved new best model with F1: 0.4883\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3/20:\n  Train Loss: 0.3593, Train Acc: 0.8548\n  Val Loss:   0.4467, Val Acc: 0.8096\n  Val Precision: 0.6102, Recall: 0.3817, F1: 0.4697\n  Val AUC: 0.7767\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4/20:\n  Train Loss: 0.3453, Train Acc: 0.8557\n  Val Loss:   0.4845, Val Acc: 0.7938\n  Val Precision: 0.5381, Recall: 0.4705, F1: 0.5020\n  Val AUC: 0.7798\n  Saved new best model with F1: 0.5020\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5/20:\n  Train Loss: 0.3399, Train Acc: 0.8577\n  Val Loss:   0.4973, Val Acc: 0.7922\n  Val Precision: 0.5333, Recall: 0.4748, F1: 0.5023\n  Val AUC: 0.7826\n  Saved new best model with F1: 0.5023\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6/20:\n  Train Loss: 0.3354, Train Acc: 0.8579\n  Val Loss:   0.4956, Val Acc: 0.7935\n  Val Precision: 0.5379, Recall: 0.4642, F1: 0.4983\n  Val AUC: 0.7834\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7/20:\n  Train Loss: 0.3322, Train Acc: 0.8588\n  Val Loss:   0.5242, Val Acc: 0.7818\n  Val Precision: 0.5059, Recall: 0.5198, F1: 0.5128\n  Val AUC: 0.7898\n  Saved new best model with F1: 0.5128\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8/20:\n  Train Loss: 0.3299, Train Acc: 0.8594\n  Val Loss:   0.4683, Val Acc: 0.8128\n  Val Precision: 0.6043, Recall: 0.4419, F1: 0.5105\n  Val AUC: 0.7945\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9/20:\n  Train Loss: 0.3280, Train Acc: 0.8605\n  Val Loss:   0.5568, Val Acc: 0.7796\n  Val Precision: 0.5011, Recall: 0.5440, F1: 0.5217\n  Val AUC: 0.7962\n  Saved new best model with F1: 0.5217\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10/20:\n  Train Loss: 0.3245, Train Acc: 0.8605\n  Val Loss:   0.5204, Val Acc: 0.7861\n  Val Precision: 0.5156, Recall: 0.5245, F1: 0.5200\n  Val AUC: 0.7993\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11/20:\n  Train Loss: 0.3224, Train Acc: 0.8610\n  Val Loss:   0.4782, Val Acc: 0.8034\n  Val Precision: 0.5618, Recall: 0.4998, F1: 0.5290\n  Val AUC: 0.8058\n  Saved new best model with F1: 0.5290\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12/20:\n  Train Loss: 0.3198, Train Acc: 0.8617\n  Val Loss:   0.5249, Val Acc: 0.7908\n  Val Precision: 0.5254, Recall: 0.5492, F1: 0.5371\n  Val AUC: 0.8066\n  Saved new best model with F1: 0.5371\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13/20:\n  Train Loss: 0.3181, Train Acc: 0.8612\n  Val Loss:   0.4665, Val Acc: 0.8092\n  Val Precision: 0.5805, Recall: 0.4915, F1: 0.5323\n  Val AUC: 0.8088\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 14/20:\n  Train Loss: 0.3144, Train Acc: 0.8623\n  Val Loss:   0.5440, Val Acc: 0.7903\n  Val Precision: 0.5223, Recall: 0.5954, F1: 0.5564\n  Val AUC: 0.8094\n  Saved new best model with F1: 0.5564\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 15/20:\n  Train Loss: 0.3134, Train Acc: 0.8632\n  Val Loss:   0.4881, Val Acc: 0.8096\n  Val Precision: 0.5745, Recall: 0.5321, F1: 0.5525\n  Val AUC: 0.8136\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 16/20:\n  Train Loss: 0.3129, Train Acc: 0.8628\n  Val Loss:   0.5332, Val Acc: 0.7963\n  Val Precision: 0.5366, Recall: 0.5710, F1: 0.5533\n  Val AUC: 0.8095\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 17/20:\n  Train Loss: 0.3109, Train Acc: 0.8626\n  Val Loss:   0.5822, Val Acc: 0.7991\n  Val Precision: 0.5436, Recall: 0.5645, F1: 0.5538\n  Val AUC: 0.8059\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 18/20:\n  Train Loss: 0.3068, Train Acc: 0.8627\n  Val Loss:   0.4745, Val Acc: 0.8177\n  Val Precision: 0.6060, Recall: 0.4995, F1: 0.5476\n  Val AUC: 0.8059\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 19/20:\n  Train Loss: 0.3060, Train Acc: 0.8636\n  Val Loss:   0.5472, Val Acc: 0.8135\n  Val Precision: 0.5827, Recall: 0.5476, F1: 0.5646\n  Val AUC: 0.7971\n  Saved new best model with F1: 0.5646\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 20/20:\n  Train Loss: 0.3023, Train Acc: 0.8639\n  Val Loss:   0.5913, Val Acc: 0.8045\n  Val Precision: 0.5567, Recall: 0.5652, F1: 0.5609\n  Val AUC: 0.7999\n--------------------\n--- Training Finished ---\n\n--- Final Evaluation on Validation Set using Best Model ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Final Performance Metrics:\n  Accuracy:  0.8135\n  Precision: 0.5827\n  Recall:    0.5476\n  F1 Score:  0.5646\n  AUC:       0.7971\n  Loss:      0.5472\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed \n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport zipfile\nimport random\nimport math\nimport xml.etree.ElementTree as ET\nfrom tqdm.notebook import tqdm\n\n# Deep Learning libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\nimport cv2\nfrom ultralytics import YOLO\n\n\n\n# --- Configuration ---\nBASE_DIR = '/kaggle/working/PIE/content'\nANNOTATION_DIR = '/kaggle/working/PIE/annotations/annotations'\n# CLIP_DIR = os.path.join(BASE_DIR, 'PIE_clips') # Not used directly in this version\n\n# Model Hyperparameters\nSEQ_LEN = 15        # Number of past frames to observe\nPRED_LEN = 1        # Predict state at the end of sequence (relative to seq_len)\nINPUT_SIZE_BBOX = 4 # (center_x, center_y, width, height) - normalized\nINPUT_SIZE_POSE = 34  # 17 keypoints * 2 coordinates (from YOLOv8 pose)\nLSTM_HIDDEN_SIZE = 128\nNUM_LSTM_LAYERS = 2\nDROPOUT_RATE = 0.3\nNUM_CLASSES = 2      # 0: not-crossing, 1: crossing\nATTENTION_DIM = 128  # Dimension for the attention mechanism\n\n# Training Hyperparameters\nLEARNING_RATE = 1e-4\nBATCH_SIZE = 64\nNUM_EPOCHS = 20 # Adjust as needed\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\nTRAIN_SETS = ['set06']\nVAL_SETS = ['set01', 'set02'] \n\n# --- Helper Function: Compute IOU ---\ndef compute_iou(boxA, boxB):\n    # boxA and boxB are in [x1, y1, x2, y2] format\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    interW = max(0, xB - xA)\n    interH = max(0, yB - yA)\n    interArea = interW * interH\n    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n    return iou\n\n# --- Data Preprocessing Functions ---\n\ndef parse_annotations(xml_file):\n    \"\"\"Parses a PIE annotation XML file.\"\"\"\n    try:\n        tree = ET.parse(xml_file)\n        root = tree.getroot()\n    except ET.ParseError:\n        print(f\"Error parsing {xml_file}\")\n        return None, None\n\n    img_width = int(root.find('.//original_size/width').text)\n    img_height = int(root.find('.//original_size/height').text)\n\n    ped_tracks = {} # {ped_id: {frame: {'bbox': [xtl,ytl,xbr,ybr], 'cross': label}}}\n\n    for track in root.findall('.//track[@label=\"pedestrian\"]'):\n        for box in track.findall('.//box'):\n            frame = int(box.get('frame'))\n            ped_id = box.find('.//attribute[@name=\"id\"]').text\n            xtl = float(box.get('xtl'))\n            ytl = float(box.get('ytl'))\n            xbr = float(box.get('xbr'))\n            ybr = float(box.get('ybr'))\n            occluded = int(box.get('occluded', 0)) # Handle missing occluded tag\n\n            # Only process non-occluded boxes for simplicity\n            if occluded > 0:\n                continue\n\n            cross_status = box.find('.//attribute[@name=\"cross\"]').text\n            # Map labels: 1 for crossing, 0 otherwise\n            cross_label = 1 if cross_status == 'crossing' else 0\n\n            if ped_id not in ped_tracks:\n                ped_tracks[ped_id] = {}\n\n            # Normalize bounding box: [center_x, center_y, width, height]\n            center_x = ((xtl + xbr) / 2) / img_width\n            center_y = ((ytl + ybr) / 2) / img_height\n            width = (xbr - xtl) / img_width\n            height = (ybr - ytl) / img_height\n\n            # Basic check for valid bbox dimensions\n            if width <= 0 or height <= 0 or not (0 <= center_x <= 1) or not (0 <= center_y <= 1):\n                continue # Skip invalid boxes\n\n            ped_tracks[ped_id][frame] = {\n                'bbox': [center_x, center_y, width, height],\n                'cross': cross_label\n            }\n\n    return ped_tracks, (img_width, img_height)\n\ndef create_sequences(ped_tracks, seq_len, pred_len, set_folder):\n    \"\"\"Creates sequences of features and labels from pedestrian tracks.\n       Now also stores frame numbers and image dimensions for pose extraction.\"\"\"\n    sequences = []\n    for ped_id, frames_data in ped_tracks.items():\n        sorted_frames = sorted(frames_data.keys())\n\n        for i in range(len(sorted_frames) - seq_len - pred_len + 1):\n            seq_frames = sorted_frames[i : i + seq_len]\n            target_frame = sorted_frames[i + seq_len + pred_len - 1]\n\n            # Ensure frames are consecutive\n            is_continuous = all(seq_frames[j+1] - seq_frames[j] == 1 for j in range(len(seq_frames)-1))\n            is_target_continuous = (target_frame - seq_frames[-1] == pred_len)\n\n            if not (is_continuous and is_target_continuous):\n                continue\n\n            # Extract bbox features\n            bbox_seq = [frames_data[f]['bbox'] for f in seq_frames]\n            label = frames_data[target_frame]['cross']\n\n            # For pose extraction, store the frame numbers and image dimensions.\n            # (Assume all frames in the XML share the same image dimensions)\n            # Note: set_folder helps locate images in BASE_DIR.\n            sequence = {\n                'bbox': np.array(bbox_seq, dtype=np.float32),\n                'label': label,\n                'frames': seq_frames,\n                'set_folder': set_folder\n            }\n            # Also store image dimensions from the first frame\n            # (Assuming they are constant within a video)\n            # Here, we pick the first frame to get dims from the parsed annotation.\n            # You may adjust if different frames have different dims.\n            sequence['img_dims'] = None  # will be set outside if needed\n            sequences.append(sequence)\n    return sequences\n\n# --- Dataset Class ---\n\nclass PIEDataset(Dataset):\n    def __init__(self, annotation_dir, set_folders, seq_len, pred_len):\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.sequences = []\n        self.BASE_DIR = BASE_DIR  # Use global BASE_DIR for images\n\n        # Initialize the YOLOv8 pose model (assumed to be in the working directory)\n        self.pose_model = YOLO('/kaggle/input/yolov11n/pytorch/default/1/yolo11n-pose.pt')  # Ensure the model file is available\n\n        print(f\"Loading data from sets: {set_folders}\")\n        for set_folder in tqdm(set_folders):\n            set_path = os.path.join(annotation_dir, set_folder)\n            if not os.path.isdir(set_path):\n                print(f\"Warning: Annotation directory not found for {set_folder}\")\n                continue\n\n            xml_files = [f for f in os.listdir(set_path) if f.endswith('.xml')]\n            for xml_file in tqdm(xml_files, desc=f\"Processing {set_folder}\", leave=False):\n                file_path = os.path.join(set_path, xml_file)\n                ped_tracks, dims = parse_annotations(file_path)\n                if dims is None:\n                    continue\n                img_width, img_height = dims\n                if ped_tracks:\n                    video_sequences = create_sequences(ped_tracks, seq_len, pred_len, set_folder)\n                    # Set image dimensions for each sequence\n                    for seq in video_sequences:\n                        seq['img_dims'] = (img_width, img_height)\n                    self.sequences.extend(video_sequences)\n\n        print(f\"Loaded {len(self.sequences)} sequences.\")\n        # Basic balancing (undersampling majority class if highly imbalanced)\n        labels = [s['label'] for s in self.sequences]\n        count_0 = labels.count(0)\n        count_1 = labels.count(1)\n        print(f\"Class distribution: 0={count_0}, 1={count_1}\")\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        sequence_data = self.sequences[idx]\n\n        # --- Modality Extraction for BBox ---\n        bbox_features = torch.tensor(sequence_data['bbox'], dtype=torch.float32)\n\n        # --- Pose Estimation using YOLOv8 Pose ---\n        # For each frame in the sequence, load the image and extract keypoints\n        set_folder = sequence_data['set_folder']\n        img_dims = sequence_data['img_dims']  # (img_width, img_height)\n        img_width, img_height = img_dims\n        pose_seq = []\n\n        for frame in sequence_data['frames']:\n            # Assumption: images are stored in BASE_DIR/<set_folder> and named as \"<frame>.jpg\"\n            image_path = os.path.join(self.BASE_DIR, set_folder, f\"{frame}.jpg\")\n            # Load image using OpenCV\n            image = cv2.imread(image_path)\n            if image is None:\n                # If image is not found, use a zero vector\n                pose_seq.append(np.zeros(INPUT_SIZE_POSE, dtype=np.float32))\n                continue\n\n            # Convert BGR to RGB\n            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n            # Run YOLOv8 pose model on the image\n            results = self.pose_model(image_rgb, verbose=False)  # results is a list\n            keypoints_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)  # default if no match is found\n\n            # Convert annotation bbox from normalized (center_x, center_y, w, h) to pixel coordinates\n            ann_bbox = sequence_data['bbox'][sequence_data['frames'].index(frame)]\n            ann_cx, ann_cy, ann_w, ann_h = ann_bbox\n            ann_x1 = (ann_cx - ann_w/2) * img_width\n            ann_y1 = (ann_cy - ann_h/2) * img_height\n            ann_x2 = (ann_cx + ann_w/2) * img_width\n            ann_y2 = (ann_cy + ann_h/2) * img_height\n            ann_box_pixels = [ann_x1, ann_y1, ann_x2, ann_y2]\n\n            # Search for the detection with best IOU with the annotation bbox\n            best_iou = 0.0\n            best_keypoints = None\n            if results and len(results) > 0:\n                # Each result may contain multiple detections.\n                res = results[0]\n                # res.boxes.xyxy gives detections; res.keypoints.data gives keypoints if available.\n                if res.boxes is not None and res.keypoints is not None:\n                    boxes = res.boxes.xyxy.cpu().numpy()  # shape: (n, 4)\n                    kps = res.keypoints.cpu().numpy()       # shape: (n, 17, 3)\n                    for i in range(len(boxes)):\n                        det_box = boxes[i]\n                        iou = compute_iou(ann_box_pixels, det_box)\n                        if iou > best_iou:\n                            best_iou = iou\n                            best_keypoints = kps[i]\n\n            # If a detection with sufficient overlap is found, process its keypoints\n            if best_keypoints is not None and best_iou > 0.3:\n                # Extract x,y coordinates (ignoring confidence) and normalize them\n                norm_keypoints = []\n                for (x, y, conf) in best_keypoints:\n                    norm_keypoints.extend([x / img_width, y / img_height])\n                keypoints_vector = np.array(norm_keypoints, dtype=np.float32)\n                # Ensure it is of length 34\n                if keypoints_vector.shape[0] != INPUT_SIZE_POSE:\n                    keypoints_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n            # Else, keypoints_vector remains zeros\n\n            pose_seq.append(keypoints_vector)\n        pose_features = torch.tensor(np.array(pose_seq), dtype=torch.float32)\n\n        # --- Return combined modalities ---\n        features = {\n            'bbox': bbox_features,\n            'pose': pose_features\n        }\n        label = torch.tensor(sequence_data['label'], dtype=torch.long) # Use long for CrossEntropyLoss\n\n        return features, label\n    \n\n# --- Model Architecture ---\n\nclass Attention(nn.Module):\n    \"\"\" Simple Dot-Product Attention or Learned Attention\"\"\"\n    def __init__(self, hidden_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.attention_net = nn.Sequential(\n            nn.Linear(hidden_dim, attention_dim),\n            nn.Tanh(),\n            nn.Linear(attention_dim, 1)\n        )\n\n    def forward(self, lstm_output):\n        attention_scores = self.attention_net(lstm_output).squeeze(2)\n        attention_weights = torch.softmax(attention_scores, dim=1)\n        context_vector = torch.sum(lstm_output * attention_weights.unsqueeze(2), dim=1)\n        return context_vector, attention_weights\n\nclass MultiStreamAdaptiveLSTM(nn.Module):\n    def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes,\n                 attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n        super(MultiStreamAdaptiveLSTM, self).__init__()\n        self.stream_names = stream_names\n        self.lstms = nn.ModuleDict()\n        self.attentions = nn.ModuleDict()\n\n        for name in self.stream_names:\n            input_size = input_sizes[name]\n            self.lstms[name] = nn.LSTM(input_size, lstm_hidden_size, num_lstm_layers,\n                                       batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0,\n                                       bidirectional=False)\n            self.attentions[name] = Attention(lstm_hidden_size, attention_dim)\n\n        num_streams = len(self.stream_names)\n        combined_feature_dim = lstm_hidden_size * num_streams\n\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc1 = nn.Linear(combined_feature_dim, combined_feature_dim // 2)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(combined_feature_dim // 2, num_classes)\n\n    def forward(self, x):\n        stream_context_vectors = []\n        stream_att_weights = {}\n\n        for name in self.stream_names:\n            lstm_input = x[name]\n            lstm_out, _ = self.lstms[name](lstm_input)\n            context_vector, attention_weights = self.attentions[name](lstm_out)\n            stream_context_vectors.append(context_vector)\n            stream_att_weights[name] = attention_weights\n\n        fused_features = torch.cat(stream_context_vectors, dim=1)\n        x = self.dropout(fused_features)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        logits = self.fc2(x)\n        return logits\n\n# --- Training and Evaluation Functions ---\n\ndef train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n\n    for features, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n        input_features = {name: features[name].to(device) for name in model.stream_names}\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(input_features)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    avg_loss = total_loss / len(dataloader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    return avg_loss, accuracy\n\ndef evaluate_epoch(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_labels = []\n    all_preds = []\n    all_probs = []\n\n    with torch.no_grad():\n        for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n            input_features = {name: features[name].to(device) for name in model.stream_names}\n            labels = labels.to(device)\n            outputs = model(input_features)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            probs = torch.softmax(outputs, dim=1)\n            preds = torch.argmax(probs, dim=1)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n\n    avg_loss = total_loss / len(dataloader)\n    all_probs = np.array(all_probs)\n    all_labels = np.array(all_labels)\n    all_preds = np.array(all_preds)\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', pos_label=1, zero_division=0\n    )\n    if len(np.unique(all_labels)) > 1:\n       auc = roc_auc_score(all_labels, all_probs[:, 1])\n    else:\n       auc = float('nan')\n    metrics = {\n        'loss': avg_loss,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'auc': auc\n    }\n    return metrics\n\n# --- Main Execution ---\n\nif __name__ == '__main__':\n    # Initialize Datasets and Dataloaders\n    train_dataset = PIEDataset(ANNOTATION_DIR, TRAIN_SETS, SEQ_LEN, PRED_LEN)\n    val_dataset = PIEDataset(ANNOTATION_DIR, VAL_SETS, SEQ_LEN, PRED_LEN)\n\n    if len(train_dataset) == 0 or len(val_dataset) == 0:\n         raise ValueError(\"Dataset loading failed or resulted in empty datasets. Check paths and parsing.\")\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n    # Initialize Model, Loss, and Optimizer\n    input_sizes = {'bbox': INPUT_SIZE_BBOX, 'pose': INPUT_SIZE_POSE}\n    model = MultiStreamAdaptiveLSTM(\n        input_sizes=input_sizes,\n        lstm_hidden_size=LSTM_HIDDEN_SIZE,\n        num_lstm_layers=NUM_LSTM_LAYERS,\n        num_classes=NUM_CLASSES,\n        attention_dim=ATTENTION_DIM,\n        dropout_rate=DROPOUT_RATE,\n        stream_names=['bbox', 'pose']\n    ).to(DEVICE)\n\n    print(model)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    # Training Loop\n    best_val_f1 = -1.0\n    print(\"\\n--- Starting Training ---\")\n    for epoch in range(NUM_EPOCHS):\n        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n        val_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}:\")\n        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        print(f\"  Val Loss:   {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n        print(f\"  Val Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n        print(f\"  Val AUC: {val_metrics['auc']:.4f}\")\n\n        if val_metrics['f1'] > best_val_f1:\n            best_val_f1 = val_metrics['f1']\n            torch.save(model.state_dict(), 'best_model.pth')\n            print(f\"  Saved new best model with F1: {best_val_f1:.4f}\")\n        print(\"-\" * 20)\n\n    print(\"--- Training Finished ---\")\n\n    # Final Evaluation on Validation Set\n    print(\"\\n--- Final Evaluation on Validation Set using Best Model ---\")\n    model.load_state_dict(torch.load('best_model.pth', map_location=DEVICE))\n    final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n\n    print(\"Final Performance Metrics:\")\n    print(f\"  Accuracy:  {final_metrics['accuracy']:.4f}\")\n    print(f\"  Precision: {final_metrics['precision']:.4f}\")\n    print(f\"  Recall:    {final_metrics['recall']:.4f}\")\n    print(f\"  F1 Score:  {final_metrics['f1']:.4f}\")\n    print(f\"  AUC:       {final_metrics['auc']:.4f}\")\n    print(f\"  Loss:      {final_metrics['loss']:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T03:47:57.362476Z","iopub.execute_input":"2025-03-26T03:47:57.363224Z","iopub.status.idle":"2025-03-26T03:59:45.205771Z","shell.execute_reply.started":"2025-03-26T03:47:57.363189Z","shell.execute_reply":"2025-03-26T03:59:45.204797Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading data from sets: ['set06']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"561903d525044755b11df142d2993ef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing set06:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Loaded 37730 sequences.\nClass distribution: 0=29385, 1=8345\nLoading data from sets: ['set01', 'set02']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b52983b69914083b4d3c27706a63bf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing set01:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing set02:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Loaded 64096 sequences.\nClass distribution: 0=49937, 1=14159\nMultiStreamAdaptiveLSTM(\n  (lstms): ModuleDict(\n    (bbox): LSTM(4, 128, num_layers=2, batch_first=True, dropout=0.3)\n    (pose): LSTM(34, 128, num_layers=2, batch_first=True, dropout=0.3)\n  )\n  (attentions): ModuleDict(\n    (bbox): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=128, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (pose): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=128, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc1): Linear(in_features=256, out_features=128, bias=True)\n  (relu): ReLU()\n  (fc2): Linear(in_features=128, out_features=2, bias=True)\n)\n\n--- Starting Training ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1/20:\n  Train Loss: 0.4601, Train Acc: 0.7988\n  Val Loss:   0.4474, Val Acc: 0.8026\n  Val Precision: 0.5716, Recall: 0.4244, F1: 0.4871\n  Val AUC: 0.7739\n  Saved new best model with F1: 0.4871\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2/20:\n  Train Loss: 0.3633, Train Acc: 0.8540\n  Val Loss:   0.4628, Val Acc: 0.7994\n  Val Precision: 0.5585, Recall: 0.4396, F1: 0.4919\n  Val AUC: 0.7762\n  Saved new best model with F1: 0.4919\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3/20:\n  Train Loss: 0.3454, Train Acc: 0.8564\n  Val Loss:   0.4774, Val Acc: 0.7976\n  Val Precision: 0.5509, Recall: 0.4529, F1: 0.4971\n  Val AUC: 0.7803\n  Saved new best model with F1: 0.4971\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4/20:\n  Train Loss: 0.3390, Train Acc: 0.8581\n  Val Loss:   0.5129, Val Acc: 0.7811\n  Val Precision: 0.5046, Recall: 0.4983, F1: 0.5014\n  Val AUC: 0.7835\n  Saved new best model with F1: 0.5014\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5/20:\n  Train Loss: 0.3345, Train Acc: 0.8587\n  Val Loss:   0.5069, Val Acc: 0.7863\n  Val Precision: 0.5171, Recall: 0.4952, F1: 0.5059\n  Val AUC: 0.7889\n  Saved new best model with F1: 0.5059\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6/20:\n  Train Loss: 0.3308, Train Acc: 0.8591\n  Val Loss:   0.6003, Val Acc: 0.7464\n  Val Precision: 0.4434, Recall: 0.5801, F1: 0.5026\n  Val AUC: 0.7865\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7/20:\n  Train Loss: 0.3289, Train Acc: 0.8600\n  Val Loss:   0.4790, Val Acc: 0.8006\n  Val Precision: 0.5581, Recall: 0.4684, F1: 0.5093\n  Val AUC: 0.7958\n  Saved new best model with F1: 0.5093\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8/20:\n  Train Loss: 0.3253, Train Acc: 0.8607\n  Val Loss:   0.5000, Val Acc: 0.7894\n  Val Precision: 0.5237, Recall: 0.5154, F1: 0.5195\n  Val AUC: 0.8012\n  Saved new best model with F1: 0.5195\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9/20:\n  Train Loss: 0.3223, Train Acc: 0.8616\n  Val Loss:   0.5185, Val Acc: 0.7826\n  Val Precision: 0.5077, Recall: 0.5252, F1: 0.5164\n  Val AUC: 0.7981\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10/20:\n  Train Loss: 0.3198, Train Acc: 0.8618\n  Val Loss:   0.5435, Val Acc: 0.7760\n  Val Precision: 0.4941, Recall: 0.5806, F1: 0.5339\n  Val AUC: 0.8012\n  Saved new best model with F1: 0.5339\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11/20:\n  Train Loss: 0.3178, Train Acc: 0.8618\n  Val Loss:   0.5513, Val Acc: 0.7765\n  Val Precision: 0.4950, Recall: 0.5685, F1: 0.5292\n  Val AUC: 0.7982\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12/20:\n  Train Loss: 0.3149, Train Acc: 0.8628\n  Val Loss:   0.5799, Val Acc: 0.7787\n  Val Precision: 0.4993, Recall: 0.6060, F1: 0.5475\n  Val AUC: 0.8062\n  Saved new best model with F1: 0.5475\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13/20:\n  Train Loss: 0.3137, Train Acc: 0.8620\n  Val Loss:   0.6152, Val Acc: 0.7775\n  Val Precision: 0.4972, Recall: 0.6296, F1: 0.5556\n  Val AUC: 0.8069\n  Saved new best model with F1: 0.5556\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 14/20:\n  Train Loss: 0.3132, Train Acc: 0.8636\n  Val Loss:   0.4926, Val Acc: 0.7992\n  Val Precision: 0.5461, Recall: 0.5382, F1: 0.5421\n  Val AUC: 0.8155\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89a4165f76354fbdb006e030d0f83e5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d72b2e9bb454649b343ab1f451fed78"}},"metadata":{}},{"name":"stdout","text":"Epoch 15/20:\n  Train Loss: 0.3108, Train Acc: 0.8627\n  Val Loss:   0.5813, Val Acc: 0.7930\n  Val Precision: 0.5282, Recall: 0.5875, F1: 0.5563\n  Val AUC: 0.8098\n  Saved new best model with F1: 0.5563\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"026fd2484f304e21a6cf354a27e2047c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b8a3e91d17f4c7e82f351cada2b41e9"}},"metadata":{}},{"name":"stdout","text":"Epoch 16/20:\n  Train Loss: 0.3119, Train Acc: 0.8629\n  Val Loss:   0.5905, Val Acc: 0.7853\n  Val Precision: 0.5116, Recall: 0.6226, F1: 0.5617\n  Val AUC: 0.8103\n  Saved new best model with F1: 0.5617\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66100cc63a5e4e06a7133ccf477070d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fd82f32f8454c10bf465b3529a7e99a"}},"metadata":{}},{"name":"stdout","text":"Epoch 17/20:\n  Train Loss: 0.3090, Train Acc: 0.8624\n  Val Loss:   0.5778, Val Acc: 0.7940\n  Val Precision: 0.5299, Recall: 0.5965, F1: 0.5613\n  Val AUC: 0.8113\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2d473bc64254be7a61327efe504c9ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c20e13c4dde4926b8d93b89db695d60"}},"metadata":{}},{"name":"stdout","text":"Epoch 18/20:\n  Train Loss: 0.3085, Train Acc: 0.8637\n  Val Loss:   0.5032, Val Acc: 0.8044\n  Val Precision: 0.5580, Recall: 0.5522, F1: 0.5551\n  Val AUC: 0.8134\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2b5ff076cdb47dcb4b03a8932c59bd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56c541fc59b14d639ef34457c175324e"}},"metadata":{}},{"name":"stdout","text":"Epoch 19/20:\n  Train Loss: 0.3063, Train Acc: 0.8632\n  Val Loss:   0.5242, Val Acc: 0.8094\n  Val Precision: 0.5690, Recall: 0.5658, F1: 0.5674\n  Val AUC: 0.8126\n  Saved new best model with F1: 0.5674\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f2e919ec1054ed1a0495a8215e0dca7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a159a4d452b46b981df89a1f0d2d518"}},"metadata":{}},{"name":"stdout","text":"Epoch 20/20:\n  Train Loss: 0.3031, Train Acc: 0.8649\n  Val Loss:   0.5041, Val Acc: 0.8222\n  Val Precision: 0.6189, Recall: 0.5075, F1: 0.5577\n  Val AUC: 0.8066\n--------------------\n--- Training Finished ---\n\n--- Final Evaluation on Validation Set using Best Model ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b728a260e474ee9b3516bf384f07967"}},"metadata":{}},{"name":"stdout","text":"Final Performance Metrics:\n  Accuracy:  0.8094\n  Precision: 0.5690\n  Recall:    0.5658\n  F1 Score:  0.5674\n  AUC:       0.8126\n  Loss:      0.5242\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}