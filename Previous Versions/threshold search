{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807c87e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:28:06.424111Z",
     "iopub.status.busy": "2025-05-07T09:28:06.423851Z",
     "iopub.status.idle": "2025-05-07T09:28:13.373848Z",
     "shell.execute_reply": "2025-05-07T09:28:13.372714Z"
    },
    "papermill": {
     "duration": 6.963382,
     "end_time": "2025-05-07T09:28:13.375450",
     "exception": false,
     "start_time": "2025-05-07T09:28:06.412068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PIE'...\r\n",
      "remote: Enumerating objects: 178, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (93/93), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (74/74), done.\u001b[K\r\n",
      "remote: Total 178 (delta 32), reused 75 (delta 17), pack-reused 85 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (178/178), 144.63 MiB | 36.03 MiB/s, done.\r\n",
      "Resolving deltas: 100% (73/73), done.\r\n",
      "Updating files: 100% (41/41), done.\r\n",
      "unzip:  cannot find or open /content/PIE/annotations/annotations.zip, /content/PIE/annotations/annotations.zip.zip or /content/PIE/annotations/annotations.zip.ZIP.\r\n",
      "unzip:  cannot find or open /content/PIE/annotations/annotations_vehicle.zip, /content/PIE/annotations/annotations_vehicle.zip.zip or /content/PIE/annotations/annotations_vehicle.zip.ZIP.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/aras62/PIE.git\n",
    "!unzip /content/PIE/annotations/annotations.zip -d /content/PIE\n",
    "!unzip /content/PIE/annotations/annotations_vehicle.zip -d /content/PIE\n",
    "# !git clone https://github.com/hustvl/YOLOP.git\n",
    "!mkdir /kaggle/working/PIE/content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b5876e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:28:13.397598Z",
     "iopub.status.busy": "2025-05-07T09:28:13.397250Z",
     "iopub.status.idle": "2025-05-07T09:28:18.656026Z",
     "shell.execute_reply": "2025-05-07T09:28:18.655137Z"
    },
    "papermill": {
     "duration": 5.271263,
     "end_time": "2025-05-07T09:28:18.657715",
     "exception": false,
     "start_time": "2025-05-07T09:28:13.386452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q ultralytics opencv-python-headless "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e5755b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:28:18.679991Z",
     "iopub.status.busy": "2025-05-07T09:28:18.679731Z",
     "iopub.status.idle": "2025-05-07T09:28:24.623149Z",
     "shell.execute_reply": "2025-05-07T09:28:24.622209Z"
    },
    "papermill": {
     "duration": 5.956165,
     "end_time": "2025-05-07T09:28:24.624751",
     "exception": false,
     "start_time": "2025-05-07T09:28:18.668586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import math\n",
    "import zipfile\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c5da9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:28:24.646861Z",
     "iopub.status.busy": "2025-05-07T09:28:24.646486Z",
     "iopub.status.idle": "2025-05-07T09:28:26.160921Z",
     "shell.execute_reply": "2025-05-07T09:28:26.160032Z"
    },
    "papermill": {
     "duration": 1.526708,
     "end_time": "2025-05-07T09:28:26.162380",
     "exception": false,
     "start_time": "2025-05-07T09:28:24.635672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped successfully.\n"
     ]
    }
   ],
   "source": [
    "zip_path = \"/kaggle/working/PIE/annotations/annotations.zip\"\n",
    "extract_to = \"/kaggle/working/PIE/annotations/\"\n",
    "\n",
    "if os.path.exists(extract_to + 'annotations'):\n",
    "    print(\"Exists already. Not unzipping.\")\n",
    "else:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(\"Unzipped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b15f455b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:28:26.192117Z",
     "iopub.status.busy": "2025-05-07T09:28:26.191846Z",
     "iopub.status.idle": "2025-05-07T09:28:26.954261Z",
     "shell.execute_reply": "2025-05-07T09:28:26.953261Z"
    },
    "papermill": {
     "duration": 0.779889,
     "end_time": "2025-05-07T09:28:26.955745",
     "exception": false,
     "start_time": "2025-05-07T09:28:26.175856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped successfully.\n"
     ]
    }
   ],
   "source": [
    "zip_path = \"/kaggle/working/PIE/annotations/annotations_vehicle.zip\"\n",
    "extract_to = \"/kaggle/working/PIE/annotations/\"\n",
    "\n",
    "if os.path.exists(extract_to + 'annotations_vehicle'):\n",
    "    print(\"Exists already. Not unzipping.\")\n",
    "else:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(\"Unzipped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db81312",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:28:26.977322Z",
     "iopub.status.busy": "2025-05-07T09:28:26.977100Z",
     "iopub.status.idle": "2025-05-07T09:28:26.989986Z",
     "shell.execute_reply": "2025-05-07T09:28:26.989222Z"
    },
    "papermill": {
     "duration": 0.024891,
     "end_time": "2025-05-07T09:28:26.991239",
     "exception": false,
     "start_time": "2025-05-07T09:28:26.966348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped successfully.\n"
     ]
    }
   ],
   "source": [
    "zip_path = \"/kaggle/working/PIE/annotations/annotations_attributes.zip\"\n",
    "extract_to = \"/kaggle/working/PIE/annotations/\"\n",
    "\n",
    "if os.path.exists(extract_to + \"annotations_attributes\"):\n",
    "    print(\"Exists already. Not unzipping.\")\n",
    "else:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(\"Unzipped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df180448",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:28:27.012894Z",
     "iopub.status.busy": "2025-05-07T09:28:27.012684Z",
     "iopub.status.idle": "2025-05-07T09:28:27.022304Z",
     "shell.execute_reply": "2025-05-07T09:28:27.021574Z"
    },
    "papermill": {
     "duration": 0.022077,
     "end_time": "2025-05-07T09:28:27.023520",
     "exception": false,
     "start_time": "2025-05-07T09:28:27.001443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------------------------------------------------------\n",
    "# # CELL 1: DATA PREPARATION & BALANCING  (run once before training)\n",
    "# # -----------------------------------------------------------------------------\n",
    "# #  This cell:\n",
    "# #    1. Loads (or regenerates) the PIE database\n",
    "# #    2. Computes per-signal standardisation scalers\n",
    "# #    3. Extracts ALL training sequences for every stream\n",
    "# #    4. Balances the dataset 50 / 50 on the crossing label\n",
    "# #    5. Writes two pickles:\n",
    "# #         - /kaggle/working/balanced_train_data.pkl\n",
    "# #         - /kaggle/working/scalers.pkl\n",
    "# # -----------------------------------------------------------------------------\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "# import time\n",
    "# import pickle\n",
    "# import gc\n",
    "# from pathlib import Path\n",
    "\n",
    "# import cv2                               # used internally by PIE utilities\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# #                                PIE utilities                                 #\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# pie_utilities_path = \"/kaggle/working/PIE/utilities\"\n",
    "# if pie_utilities_path not in sys.path:\n",
    "#     sys.path.insert(0, pie_utilities_path)\n",
    "\n",
    "# try:\n",
    "#     from pie_data import PIE\n",
    "# except ImportError as e:\n",
    "#     print(\n",
    "#         f\"[WARN] Could not import PIE from {pie_utilities_path}. \"\n",
    "#         f\"If the DB cache already exists this is fine.\\n→ {e}\"\n",
    "#     )\n",
    "#     PIE = None\n",
    "\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# #                              configuration                                   #\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# PIE_ROOT_PATH           = \"/kaggle/working/PIE\"\n",
    "# POSE_DATA_DIR           = \"/kaggle/input/pose-data/extracted_poses2\"\n",
    "# PIE_DATABASE_CACHE_PATH = \"/kaggle/input/pie-database/pie_database.pkl\"\n",
    "\n",
    "# TRAIN_SETS_STR = [\"set01\", \"set02\", \"set04\"]\n",
    "\n",
    "# BALANCED_DATA_PKL_PATH  = \"/kaggle/working/balanced_train_data.pkl\"\n",
    "# SCALERS_PKL_PATH        = \"/kaggle/working/scalers.pkl\"\n",
    "\n",
    "# # Streams used throughout the project ----------------------------------------\n",
    "# ALL_POSSIBLE_STREAMS = [\n",
    "#     \"bbox\",\n",
    "#     \"pose\",\n",
    "#     \"ego_speed\",\n",
    "#     \"ego_acc\",\n",
    "#     \"ego_gyro\",\n",
    "#     \"ped_action\",\n",
    "#     \"ped_look\",\n",
    "#     \"ped_occlusion\",\n",
    "#     \"traffic_light\",\n",
    "#     \"static_context\",\n",
    "# ]\n",
    "\n",
    "# # Feature sizes & categorical constants --------------------------------------\n",
    "# SEQ_LEN, PRED_LEN = 30, 1\n",
    "\n",
    "# INPUT_SIZE_BBOX       = 4\n",
    "# INPUT_SIZE_POSE       = 34\n",
    "# INPUT_SIZE_EGO_SPEED  = 1\n",
    "# INPUT_SIZE_EGO_ACC    = 2\n",
    "# INPUT_SIZE_EGO_GYRO   = 1\n",
    "# INPUT_SIZE_PED_ACTION = 1\n",
    "# INPUT_SIZE_PED_LOOK   = 1\n",
    "# INPUT_SIZE_PED_OCC    = 1\n",
    "# INPUT_SIZE_TL_STATE   = 4\n",
    "\n",
    "# NUM_SIGNALIZED_CATS   = 4\n",
    "# NUM_INTERSECTION_CATS = 5\n",
    "# NUM_AGE_CATS          = 4\n",
    "# NUM_GENDER_CATS       = 3\n",
    "# NUM_TRAFFIC_DIR_CATS  = 2\n",
    "\n",
    "# LANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7: 4, 8: 4}\n",
    "# NUM_LANE_CATS   = len(set(LANE_CATEGORIES.values()))\n",
    "\n",
    "# INPUT_SIZE_STATIC = (\n",
    "#     NUM_SIGNALIZED_CATS\n",
    "#     + NUM_INTERSECTION_CATS\n",
    "#     + NUM_AGE_CATS\n",
    "#     + NUM_GENDER_CATS\n",
    "#     + NUM_TRAFFIC_DIR_CATS\n",
    "#     + NUM_LANE_CATS\n",
    "# )  # → 23\n",
    "\n",
    "# TL_STATE_MAP = {\"__undefined__\": 0, \"red\": 1, \"yellow\": 2, \"green\": 3}\n",
    "# NUM_TL_STATES = len(TL_STATE_MAP)\n",
    "\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# #                               helper utils                                   #\n",
    "# # -----------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "# def to_one_hot(index: int, num_classes: int) -> np.ndarray:\n",
    "#     vec = np.zeros(num_classes, dtype=np.float32)\n",
    "#     vec[int(np.clip(index, 0, num_classes - 1))] = 1.0\n",
    "#     return vec\n",
    "\n",
    "\n",
    "# def balance_samples_count(seq_data: dict, label_key: str, seed: int = 42) -> dict:\n",
    "#     \"\"\"Undersample majority class so positive and negative labels are equal.\"\"\"\n",
    "#     labels = [lbl[0] for lbl in seq_data[label_key]]\n",
    "#     n_pos  = int(np.sum(labels))\n",
    "#     n_neg  = len(labels) - n_pos\n",
    "\n",
    "#     if n_pos == n_neg:\n",
    "#         print(\"Dataset already balanced.\")\n",
    "#         return seq_data.copy()\n",
    "\n",
    "#     majority_label    = 0 if n_neg > n_pos else 1\n",
    "#     minority_count    = min(n_pos, n_neg)\n",
    "#     majority_indices  = np.where(np.array(labels) == majority_label)[0]\n",
    "#     minority_indices  = np.where(np.array(labels) != majority_label)[0]\n",
    "\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     keep_majority = rng.choice(majority_indices, size=minority_count, replace=False)\n",
    "#     final_indices = np.concatenate([minority_indices, keep_majority])\n",
    "#     rng.shuffle(final_indices)\n",
    "\n",
    "#     balanced = {}\n",
    "#     for k, v in seq_data.items():\n",
    "#         balanced[k] = [v[i] for i in final_indices]\n",
    "\n",
    "#     print(f\"Balanced: 1s={minority_count} | 0s={minority_count}\")\n",
    "#     return balanced\n",
    "\n",
    "\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# #                                PIEDataset                                    #\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# class PIEDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Lightweight dataset that can generate any subset of the PIE feature streams.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         pie_db: dict,\n",
    "#         set_names: list[str],\n",
    "#         pose_dir: str,\n",
    "#         seq_len: int,\n",
    "#         pred_len: int,\n",
    "#         scalers: dict,\n",
    "#         streams_to_generate: list[str],\n",
    "#     ):\n",
    "#         self.pie_db            = pie_db\n",
    "#         self.set_names         = set_names\n",
    "#         self.pose_dir          = pose_dir\n",
    "#         self.seq_len           = seq_len\n",
    "#         self.pred_len          = pred_len\n",
    "#         self.scalers           = scalers\n",
    "#         self.streams           = streams_to_generate\n",
    "#         self._input_sizes      = self._build_input_size_map()\n",
    "#         self.all_pose_data     = {}\n",
    "#         self.sequences         = []\n",
    "\n",
    "#         if \"pose\" in self.streams:\n",
    "#             self._load_pose_pkls()\n",
    "#         self._enumerate_sequences()\n",
    "\n",
    "#     # ------------------------ internal helpers -------------------------------\n",
    "#     def _build_input_size_map(self) -> dict:\n",
    "#         special = {\n",
    "#             \"TRAFFIC_LIGHT\": \"TL_STATE\",\n",
    "#             \"STATIC_CONTEXT\": \"STATIC\",\n",
    "#             \"EGO_SPEED\": \"EGO_SPEED\",\n",
    "#             \"EGO_ACC\": \"EGO_ACC\",\n",
    "#             \"EGO_GYRO\": \"EGO_GYRO\",\n",
    "#             \"PED_ACTION\": \"PED_ACTION\",\n",
    "#             \"PED_LOOK\": \"PED_LOOK\",\n",
    "#             \"PED_OCCLUSION\": \"PED_OCC\",\n",
    "#         }\n",
    "#         sizes = {}\n",
    "#         for s in ALL_POSSIBLE_STREAMS:\n",
    "#             const = f\"INPUT_SIZE_{special.get(s.upper(), s.upper())}\"\n",
    "#             if s == \"bbox\":\n",
    "#                 const = \"INPUT_SIZE_BBOX\"\n",
    "#             elif s == \"pose\":\n",
    "#                 const = \"INPUT_SIZE_POSE\"\n",
    "#             sizes[s] = globals().get(const, 1)\n",
    "#         return sizes\n",
    "\n",
    "#     def _load_pose_pkls(self):\n",
    "#         print(\"Loading pose PKLs …\")\n",
    "#         for set_id in self.set_names:\n",
    "#             set_dir = Path(self.pose_dir) / set_id\n",
    "#             if not set_dir.is_dir():\n",
    "#                 continue\n",
    "#             self.all_pose_data[set_id] = {}\n",
    "#             for pkl_path in tqdm(set_dir.glob(f\"{set_id}_*_poses.pkl\"), leave=False):\n",
    "#                 try:\n",
    "#                     with open(pkl_path, \"rb\") as fp:\n",
    "#                         loaded = pickle.load(fp)\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"[pose load] {pkl_path}: {e}\")\n",
    "#                     continue\n",
    "\n",
    "#                 if len(loaded) != 1:\n",
    "#                     continue\n",
    "#                 (key, data), *_ = loaded.items()\n",
    "#                 vid = \"_\".join(key.split(\"_\")[1:])\n",
    "#                 if vid in self.pie_db.get(set_id, {}):\n",
    "#                     self.all_pose_data[set_id][vid] = data\n",
    "\n",
    "#     def _enumerate_sequences(self):\n",
    "#         print(\"Enumerating sequences …\")\n",
    "#         for set_id in self.set_names:\n",
    "#             for vid, vdb in self.pie_db.get(set_id, {}).items():\n",
    "#                 for pid, pdb in vdb.get(\"ped_annotations\", {}).items():\n",
    "#                     frames = pdb.get(\"frames\", [])\n",
    "#                     if len(frames) < self.seq_len + self.pred_len:\n",
    "#                         continue\n",
    "#                     frames = sorted(frames)\n",
    "#                     for i in range(len(frames) - self.seq_len - self.pred_len + 1):\n",
    "#                         start = frames[i]\n",
    "#                         obs_end = frames[i + self.seq_len - 1]\n",
    "#                         if obs_end - start != self.seq_len - 1:\n",
    "#                             continue\n",
    "#                         target = frames[i + self.seq_len + self.pred_len - 1]\n",
    "#                         if target - obs_end != self.pred_len:\n",
    "#                             continue\n",
    "#                         self.sequences.append((set_id, vid, pid, start))\n",
    "#         print(f\"Total sequences: {len(self.sequences)}\")\n",
    "\n",
    "#     # ------------------ Dataset API ------------------------------------------\n",
    "#     def __len__(self):\n",
    "#         return len(self.sequences)\n",
    "\n",
    "#     def __getitem__(self, idx: int):\n",
    "#         set_id, vid, pid, start = self.sequences[idx]\n",
    "#         vdb  = self.pie_db[set_id][vid]\n",
    "#         pdb  = vdb[\"ped_annotations\"][pid]\n",
    "#         ego  = vdb.get(\"vehicle_annotations\", {})\n",
    "#         tldb = vdb.get(\"traffic_annotations\", {})\n",
    "\n",
    "#         frame_nums = list(range(start, start + self.seq_len))\n",
    "#         target_f   = start + self.seq_len + self.pred_len - 1\n",
    "\n",
    "#         # label ---------------------------------------------------------------\n",
    "#         label = 0\n",
    "#         if (\n",
    "#             \"frames\" in pdb\n",
    "#             and \"behavior\" in pdb\n",
    "#             and \"cross\" in pdb[\"behavior\"]\n",
    "#             and target_f in pdb[\"frames\"]\n",
    "#         ):\n",
    "#             try:\n",
    "#                 j = pdb[\"frames\"].index(target_f)\n",
    "#                 label = pdb[\"behavior\"][\"cross\"][j]\n",
    "#                 if label == -1:\n",
    "#                     label = 0\n",
    "#             except (ValueError, IndexError):\n",
    "#                 pass\n",
    "\n",
    "#         # static context ------------------------------------------------------\n",
    "#         static_vec = np.zeros(INPUT_SIZE_STATIC, np.float32)\n",
    "#         if \"static_context\" in self.streams:\n",
    "#             attr  = pdb.get(\"attributes\", {})\n",
    "#             sig   = attr.get(\"signalized\", 0)\n",
    "#             intr  = attr.get(\"intersection\", 0)\n",
    "#             age   = attr.get(\"age\", 2)\n",
    "#             gen   = attr.get(\"gender\", 0)\n",
    "#             tdir  = int(attr.get(\"traffic_direction\", 0))\n",
    "#             ln    = attr.get(\"num_lanes\", 2)\n",
    "#             lncat = LANE_CATEGORIES.get(ln, LANE_CATEGORIES[max(LANE_CATEGORIES)])\n",
    "#             static_vec = np.concatenate(\n",
    "#                 [\n",
    "#                     to_one_hot(sig,  NUM_SIGNALIZED_CATS),\n",
    "#                     to_one_hot(intr, NUM_INTERSECTION_CATS),\n",
    "#                     to_one_hot(age,  NUM_AGE_CATS),\n",
    "#                     to_one_hot(gen,  NUM_GENDER_CATS),\n",
    "#                     to_one_hot(tdir, NUM_TRAFFIC_DIR_CATS),\n",
    "#                     to_one_hot(lncat, NUM_LANE_CATS),\n",
    "#                 ]\n",
    "#             ).astype(np.float32)\n",
    "\n",
    "#         # per-frame feature assembly -----------------------------------------\n",
    "#         feats = {s: [] for s in self.streams}\n",
    "\n",
    "#         for fn in frame_nums:\n",
    "#             fidx = -1\n",
    "#             if \"frames\" in pdb:\n",
    "#                 try:\n",
    "#                     fidx = pdb[\"frames\"].index(fn)\n",
    "#                 except ValueError:\n",
    "#                     pass\n",
    "\n",
    "#             ego_f = ego.get(fn, {})\n",
    "\n",
    "#             # bbox ----------------------------------------------------------\n",
    "#             if \"bbox\" in self.streams:\n",
    "#                 bb = np.zeros(INPUT_SIZE_BBOX, np.float32)\n",
    "#                 if (\n",
    "#                     fidx != -1\n",
    "#                     and \"bbox\" in pdb\n",
    "#                     and len(pdb[\"bbox\"]) > fidx\n",
    "#                 ):\n",
    "#                     try:\n",
    "#                         x1, y1, x2, y2 = pdb[\"bbox\"][fidx]\n",
    "#                         w_img = vdb.get(\"width\", 1920)\n",
    "#                         h_img = vdb.get(\"height\", 1080)\n",
    "#                         if w_img > 0 and h_img > 0:\n",
    "#                             cx = ((x1 + x2) / 2) / w_img\n",
    "#                             cy = ((y1 + y2) / 2) / h_img\n",
    "#                             w  = (x2 - x1) / w_img\n",
    "#                             h  = (y2 - y1) / h_img\n",
    "#                             if 0 < w and 0 < h and 0 <= cx <= 1 and 0 <= cy <= 1:\n",
    "#                                 bb = np.array([cx, cy, w, h], np.float32)\n",
    "#                     except Exception:\n",
    "#                         pass\n",
    "#                 feats[\"bbox\"].append(bb)\n",
    "\n",
    "#             # pose ----------------------------------------------------------\n",
    "#             if \"pose\" in self.streams:\n",
    "#                 pvec = np.zeros(INPUT_SIZE_POSE, np.float32)\n",
    "#                 pose_set = self.all_pose_data.get(set_id, {}).get(vid, {})\n",
    "#                 p_loaded = pose_set.get(fn, {}).get(pid)\n",
    "#                 if (\n",
    "#                     isinstance(p_loaded, np.ndarray)\n",
    "#                     and p_loaded.shape == (INPUT_SIZE_POSE,)\n",
    "#                 ):\n",
    "#                     pvec = p_loaded\n",
    "#                 feats[\"pose\"].append(pvec)\n",
    "\n",
    "#             # ego signals ---------------------------------------------------\n",
    "#             if \"ego_speed\" in self.streams:\n",
    "#                 s = ego_f.get(\"OBD_speed\", 0.0) or ego_f.get(\"GPS_speed\", 0.0)\n",
    "#                 s = (s - self.scalers.get(\"ego_speed_mean\", 0.0)) / self.scalers.get(\n",
    "#                     \"ego_speed_std\", 1.0\n",
    "#                 )\n",
    "#                 feats[\"ego_speed\"].append([s])\n",
    "\n",
    "#             if \"ego_acc\" in self.streams:\n",
    "#                 ax = ego_f.get(\"accX\", 0.0)\n",
    "#                 ay = ego_f.get(\"accY\", 0.0)\n",
    "#                 ax = (ax - self.scalers.get(\"accX_mean\", 0.0)) / self.scalers.get(\n",
    "#                     \"accX_std\", 1.0\n",
    "#                 )\n",
    "#                 ay = (ay - self.scalers.get(\"accY_mean\", 0.0)) / self.scalers.get(\n",
    "#                     \"accY_std\", 1.0\n",
    "#                 )\n",
    "#                 feats[\"ego_acc\"].append([ax, ay])\n",
    "\n",
    "#             if \"ego_gyro\" in self.streams:\n",
    "#                 gz = ego_f.get(\"gyroZ\", 0.0)\n",
    "#                 gz = (gz - self.scalers.get(\"gyroZ_mean\", 0.0)) / self.scalers.get(\n",
    "#                     \"gyroZ_std\", 1.0\n",
    "#                 )\n",
    "#                 feats[\"ego_gyro\"].append([gz])\n",
    "\n",
    "#             # pedestrian behaviour -----------------------------------------\n",
    "#             if \"ped_action\" in self.streams:\n",
    "#                 action = (\n",
    "#                     pdb[\"behavior\"][\"action\"][fidx]\n",
    "#                     if fidx != -1\n",
    "#                     and \"behavior\" in pdb\n",
    "#                     and \"action\" in pdb[\"behavior\"]\n",
    "#                     and len(pdb[\"behavior\"][\"action\"]) > fidx\n",
    "#                     else 0\n",
    "#                 )\n",
    "#                 feats[\"ped_action\"].append([float(action)])\n",
    "\n",
    "#             if \"ped_look\" in self.streams:\n",
    "#                 look = (\n",
    "#                     pdb[\"behavior\"][\"look\"][fidx]\n",
    "#                     if fidx != -1\n",
    "#                     and \"behavior\" in pdb\n",
    "#                     and \"look\" in pdb[\"behavior\"]\n",
    "#                     and len(pdb[\"behavior\"][\"look\"]) > fidx\n",
    "#                     else 0\n",
    "#                 )\n",
    "#                 feats[\"ped_look\"].append([float(look)])\n",
    "\n",
    "#             if \"ped_occlusion\" in self.streams:\n",
    "#                 occ = (\n",
    "#                     float(pdb[\"occlusion\"][fidx]) / 2.0\n",
    "#                     if fidx != -1\n",
    "#                     and \"occlusion\" in pdb\n",
    "#                     and len(pdb[\"occlusion\"]) > fidx\n",
    "#                     else 0.0\n",
    "#                 )\n",
    "#                 feats[\"ped_occlusion\"].append([occ])\n",
    "\n",
    "#             # traffic light -------------------------------------------------\n",
    "#             if \"traffic_light\" in self.streams:\n",
    "#                 tl_state = 0\n",
    "#                 for obj in tldb.values():\n",
    "#                     if obj.get(\"obj_class\") != \"traffic_light\":\n",
    "#                         continue\n",
    "#                     if \"frames\" not in obj or \"state\" not in obj:\n",
    "#                         continue\n",
    "#                     try:\n",
    "#                         j = obj[\"frames\"].index(fn)\n",
    "#                         if obj[\"state\"][j] != 0:\n",
    "#                             tl_state = obj[\"state\"][j]\n",
    "#                             break\n",
    "#                     except (ValueError, IndexError):\n",
    "#                         continue\n",
    "#                 feats[\"traffic_light\"].append(to_one_hot(tl_state, NUM_TL_STATES))\n",
    "\n",
    "#             # static context -----------------------------------------------\n",
    "#             if \"static_context\" in self.streams:\n",
    "#                 feats[\"static_context\"].append(static_vec)\n",
    "\n",
    "#         # numpy → torch ------------------------------------------------------\n",
    "#         out = {\n",
    "#             s: torch.tensor(np.asarray(feats[s], np.float32), dtype=torch.float32)\n",
    "#             for s in self.streams\n",
    "#         }\n",
    "#         return out, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# #                       MAIN: build balanced training set\n",
    "# # =============================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"\\n--- DATA PREPARATION ---\")\n",
    "\n",
    "#     # 1) load / regenerate PIE DB -------------------------------------------\n",
    "#     cache = Path(PIE_DATABASE_CACHE_PATH)\n",
    "#     if cache.is_file():\n",
    "#         print(\"Loading PIE database cache …\")\n",
    "#         with cache.open(\"rb\") as fp:\n",
    "#             pie_db = pickle.load(fp)\n",
    "#         print(\"✓ PIE DB loaded.\")\n",
    "#     else:\n",
    "#         if PIE is None:\n",
    "#             raise RuntimeError(\"PIE class unavailable: cannot rebuild database.\")\n",
    "#         print(\"Cache not found – regenerating PIE DB …\")\n",
    "#         pie_db = PIE(data_path=PIE_ROOT_PATH, regen_database=True).generate_database()\n",
    "#         if not pie_db:\n",
    "#             raise RuntimeError(\"PIE DB generation failed.\")\n",
    "#         print(\"✓ PIE DB generated.\")\n",
    "\n",
    "#     # 2) compute scalers -----------------------------------------------------\n",
    "#     print(\"\\nComputing scalers …\")\n",
    "#     spd, accx, accy, gyz = [], [], [], []\n",
    "#     for sid in TRAIN_SETS_STR:\n",
    "#         for vid, vdb in pie_db.get(sid, {}).items():\n",
    "#             for frame, e in vdb.get(\"vehicle_annotations\", {}).items():\n",
    "#                 s  = e.get(\"OBD_speed\", 0.0) or e.get(\"GPS_speed\", 0.0)\n",
    "#                 spd.append(s)\n",
    "#                 accx.append(e.get(\"accX\", 0.0))\n",
    "#                 accy.append(e.get(\"accY\", 0.0))\n",
    "#                 gyz.append(e.get(\"gyroZ\", 0.0))\n",
    "\n",
    "#     scalers = {}\n",
    "#     if spd:\n",
    "#         scalers[\"ego_speed_mean\"] = float(np.mean(spd))\n",
    "#         scalers[\"ego_speed_std\"]  = float(max(np.std(spd), 1e-6))\n",
    "#     if accx:\n",
    "#         scalers[\"accX_mean\"] = float(np.mean(accx))\n",
    "#         scalers[\"accX_std\"]  = float(max(np.std(accx), 1e-6))\n",
    "#         scalers[\"accY_mean\"] = float(np.mean(accy))\n",
    "#         scalers[\"accY_std\"]  = float(max(np.std(accy), 1e-6))\n",
    "#     if gyz:\n",
    "#         scalers[\"gyroZ_mean\"] = float(np.mean(gyz))\n",
    "#         scalers[\"gyroZ_std\"]  = float(max(np.std(gyz), 1e-6))\n",
    "\n",
    "#     print(\"Scalers:\", scalers)\n",
    "\n",
    "#     # 3) extract full training dataset --------------------------------------\n",
    "#     print(\"\\nExtracting training sequences (all streams) …\")\n",
    "#     full_ds = PIEDataset(\n",
    "#         pie_db,\n",
    "#         TRAIN_SETS_STR,\n",
    "#         POSE_DATA_DIR,\n",
    "#         SEQ_LEN,\n",
    "#         PRED_LEN,\n",
    "#         scalers,\n",
    "#         ALL_POSSIBLE_STREAMS,\n",
    "#     )\n",
    "\n",
    "#     train_dict = {s: [] for s in ALL_POSSIBLE_STREAMS}\n",
    "#     train_dict[\"label\"] = []\n",
    "\n",
    "#     for i in tqdm(range(len(full_ds)), desc=\"seq\"):\n",
    "#         feat, lbl = full_ds[i]\n",
    "#         for s in ALL_POSSIBLE_STREAMS:\n",
    "#             train_dict[s].append(feat[s].numpy())\n",
    "#         train_dict[\"label\"].append([lbl.item()])\n",
    "\n",
    "#     print(f\"Raw training samples: {len(train_dict['label'])}\")\n",
    "\n",
    "#     # 4) balance -------------------------------------------------------------\n",
    "#     balanced = balance_samples_count(train_dict, \"label\")\n",
    "#     del train_dict, full_ds\n",
    "#     gc.collect()\n",
    "\n",
    "#     # 5) write pickles -------------------------------------------------------\n",
    "#     print(\"\\nSaving balanced data …\")\n",
    "#     with open(BALANCED_DATA_PKL_PATH, \"wb\") as fp:\n",
    "#         pickle.dump(balanced, fp, pickle.HIGHEST_PROTOCOL)\n",
    "#     print(f\"✓ {BALANCED_DATA_PKL_PATH}\")\n",
    "\n",
    "#     print(\"Saving scalers …\")\n",
    "#     with open(SCALERS_PKL_PATH, \"wb\") as fp:\n",
    "#         pickle.dump(scalers, fp, pickle.HIGHEST_PROTOCOL)\n",
    "#     print(f\"✓ {SCALERS_PKL_PATH}\")\n",
    "\n",
    "#     del pie_db\n",
    "#     gc.collect()\n",
    "\n",
    "#     print(\"\\n--- DATA PREPARATION COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2aaa0c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:28:27.045586Z",
     "iopub.status.busy": "2025-05-07T09:28:27.045377Z",
     "iopub.status.idle": "2025-05-07T09:28:27.058765Z",
     "shell.execute_reply": "2025-05-07T09:28:27.058077Z"
    },
    "papermill": {
     "duration": 0.026285,
     "end_time": "2025-05-07T09:28:27.060044",
     "exception": false,
     "start_time": "2025-05-07T09:28:27.033759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # --- CELL 2: ABLATION STUDY – MODEL TRAINING AND EVALUATION (with Weighted Average Fusion) ---\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "# import gc\n",
    "# import time\n",
    "# import math\n",
    "# import random\n",
    "# import pickle\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import pandas as pd                      # results-summary table\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from tqdm.notebook import tqdm\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.metrics import (\n",
    "#     accuracy_score,\n",
    "#     precision_recall_fscore_support,\n",
    "#     roc_auc_score,\n",
    "#     confusion_matrix,\n",
    "#     ConfusionMatrixDisplay,\n",
    "# )\n",
    "\n",
    "# # --- Add PIE utilities path if necessary (adjust path) ------------------------\n",
    "# pie_utilities_path = \"/kaggle/working/PIE/utilities\"\n",
    "# if pie_utilities_path not in sys.path:\n",
    "#     sys.path.insert(0, pie_utilities_path)\n",
    "\n",
    "# try:\n",
    "#     from pie_data import PIE\n",
    "# except ImportError as e:\n",
    "#     print(f\"Warn: Could not import PIE class: {e}\")\n",
    "#     PIE = None\n",
    "\n",
    "# # --- Configuration ------------------------------------------------------------\n",
    "# PIE_ROOT_PATH = \"/kaggle/working/PIE\"\n",
    "# POSE_DATA_DIR = \"/kaggle/input/pose-data/extracted_poses2\"\n",
    "# PIE_DATABASE_CACHE_PATH = \"/kaggle/input/pie-database/pie_database.pkl\"\n",
    "\n",
    "# # --- Define ALL possible streams (used by Dataset class) ----------------------\n",
    "# ALL_POSSIBLE_STREAMS = [\n",
    "#     \"bbox\",\n",
    "#     \"pose\",\n",
    "#     \"ego_speed\",\n",
    "#     \"ego_acc\",\n",
    "#     \"ego_gyro\",\n",
    "#     \"ped_action\",\n",
    "#     \"ped_look\",\n",
    "#     \"ped_occlusion\",\n",
    "#     \"traffic_light\",\n",
    "#     \"static_context\",\n",
    "# ]\n",
    "\n",
    "# # --- *** CHOOSE ACTIVE STREAMS FOR THIS EXPERIMENT *** ------------------------\n",
    "# ACTIVE_STREAMS = [\n",
    "#     \"bbox\",\n",
    "#     \"pose\",\n",
    "#     \"ego_speed\",\n",
    "#     \"ego_acc\",\n",
    "#     \"ego_gyro\",\n",
    "#     \"ped_action\",\n",
    "#     \"ped_look\",\n",
    "#     \"ped_occlusion\",\n",
    "#     \"traffic_light\",\n",
    "#     \"static_context\",\n",
    "# ]\n",
    "# # ------------------------------------------------------------------------------\n",
    "\n",
    "# print(f\"--- Running Weighted Average Fusion With Active Streams: {ACTIVE_STREAMS} ---\")\n",
    "\n",
    "# # --- Model Hyper-parameters ---------------------------------------------------\n",
    "# SEQ_LEN, PRED_LEN = 30, 1\n",
    "# INPUT_SIZE_BBOX = 4\n",
    "# INPUT_SIZE_POSE = 34\n",
    "# INPUT_SIZE_EGO_SPEED = 1\n",
    "# INPUT_SIZE_EGO_ACC = 2\n",
    "# INPUT_SIZE_EGO_GYRO = 1\n",
    "# INPUT_SIZE_PED_ACTION = 1\n",
    "# INPUT_SIZE_PED_LOOK = 1\n",
    "# INPUT_SIZE_PED_OCC = 1\n",
    "# INPUT_SIZE_TL_STATE = 4\n",
    "# NUM_SIGNALIZED_CATS = 4\n",
    "# NUM_INTERSECTION_CATS = 5\n",
    "# NUM_AGE_CATS = 4\n",
    "# NUM_GENDER_CATS = 3\n",
    "# NUM_TRAFFIC_DIR_CATS = 2\n",
    "# LANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7: 4, 8: 4}\n",
    "# NUM_LANE_CATS = len(set(LANE_CATEGORIES.values()))\n",
    "# INPUT_SIZE_STATIC = (\n",
    "#     NUM_SIGNALIZED_CATS\n",
    "#     + NUM_INTERSECTION_CATS\n",
    "#     + NUM_AGE_CATS\n",
    "#     + NUM_GENDER_CATS\n",
    "#     + NUM_TRAFFIC_DIR_CATS\n",
    "#     + NUM_LANE_CATS\n",
    "# )\n",
    "\n",
    "# LSTM_HIDDEN_SIZE = 256\n",
    "# NUM_LSTM_LAYERS = 2\n",
    "# DROPOUT_RATE = 0.3\n",
    "# NUM_CLASSES = 2\n",
    "# ATTENTION_DIM = 128\n",
    "\n",
    "# # --- Training Hyper-parameters ------------------------------------------------\n",
    "# LEARNING_RATE = 1e-4\n",
    "# BATCH_SIZE = 32\n",
    "# NUM_EPOCHS = 30  \n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# # --- Dataset splits -----------------------------------------------------------\n",
    "# VAL_SETS_STR = [\"set05\", \"set06\"]\n",
    "\n",
    "# # --- Paths for pre-processed data --------------------------------------------\n",
    "# BALANCED_DATA_PKL_PATH = \"/kaggle/working/balanced_train_data.pkl\"\n",
    "# SCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\"\n",
    "\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# #                               Helper classes                                #\n",
    "# # -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# def to_one_hot(index, num_classes):\n",
    "#     vec = np.zeros(num_classes, dtype=np.float32)\n",
    "#     safe_index = int(np.clip(index, 0, num_classes - 1))\n",
    "#     vec[safe_index] = 1.0\n",
    "#     return vec\n",
    "\n",
    "\n",
    "# class PIEDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Dataset that can dynamically enable/disable streams.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         pie_database,\n",
    "#         set_names,\n",
    "#         pose_data_dir,\n",
    "#         seq_len,\n",
    "#         pred_len,\n",
    "#         scalers=None,\n",
    "#         active_streams=None,\n",
    "#     ):\n",
    "#         self.pie_db = pie_database\n",
    "#         self.set_names = set_names\n",
    "#         self.pose_data_dir = pose_data_dir\n",
    "#         self.seq_len = seq_len\n",
    "#         self.pred_len = pred_len\n",
    "#         self.scalers = scalers or {}\n",
    "#         self.active_streams = active_streams or ALL_POSSIBLE_STREAMS\n",
    "#         self.sequences = []\n",
    "#         self.all_pose_data = {}\n",
    "\n",
    "#         self._input_sizes_for_error = self._get_input_sizes_dict()\n",
    "\n",
    "#         if \"pose\" in self.active_streams:\n",
    "#             self._load_pose_data()\n",
    "\n",
    "#         self._generate_sequence_list()\n",
    "#         if not self.sequences:\n",
    "#             raise ValueError(f\"Dataset init failed: No sequences for {self.set_names}\")\n",
    "\n",
    "#     # --------------------------------------------------------------------- #\n",
    "#     #                        internal helper methods                        #\n",
    "#     # --------------------------------------------------------------------- #\n",
    "\n",
    "#     def _get_input_sizes_dict(self):\n",
    "#         \"\"\"\n",
    "#         Build a dict {stream_name: feature_size}.\n",
    "#         \"\"\"\n",
    "#         input_sizes = {}\n",
    "#         special_cases = {\n",
    "#             \"TRAFFIC_LIGHT\": \"TL_STATE\",\n",
    "#             \"STATIC_CONTEXT\": \"STATIC\",\n",
    "#             \"EGO_SPEED\": \"EGO_SPEED\",\n",
    "#             \"EGO_ACC\": \"EGO_ACC\",\n",
    "#             \"EGO_GYRO\": \"EGO_GYRO\",\n",
    "#             \"PED_ACTION\": \"PED_ACTION\",\n",
    "#             \"PED_LOOK\": \"PED_LOOK\",\n",
    "#             \"PED_OCCLUSION\": \"PED_OCC\",\n",
    "#         }\n",
    "\n",
    "#         for stream in ALL_POSSIBLE_STREAMS:\n",
    "#             size_constant_name = f\"INPUT_SIZE_{stream.upper()}\"\n",
    "#             stream_upper_key = stream.upper()\n",
    "#             suffix = special_cases.get(stream_upper_key)\n",
    "\n",
    "#             if suffix:\n",
    "#                 size_constant_name = f\"INPUT_SIZE_{suffix}\"\n",
    "#             elif stream == \"bbox\":\n",
    "#                 size_constant_name = \"INPUT_SIZE_BBOX\"\n",
    "#             elif stream == \"pose\":\n",
    "#                 size_constant_name = \"INPUT_SIZE_POSE\"\n",
    "\n",
    "#             input_sizes[stream] = globals().get(size_constant_name, 1)\n",
    "\n",
    "#         return input_sizes\n",
    "\n",
    "#     def _load_pose_data(self):\n",
    "#         \"\"\"\n",
    "#         Load pose dictionaries once per dataset instance.\n",
    "#         \"\"\"\n",
    "#         sets_loaded_count = 0\n",
    "#         for set_id in self.set_names:\n",
    "#             self.all_pose_data[set_id] = {}\n",
    "#             pose_set_path = os.path.join(self.pose_data_dir, set_id)\n",
    "#             if not os.path.isdir(pose_set_path):\n",
    "#                 continue\n",
    "\n",
    "#             pkl_files_in_set = [\n",
    "#                 f\n",
    "#                 for f in os.listdir(pose_set_path)\n",
    "#                 if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")\n",
    "#             ]\n",
    "#             if not pkl_files_in_set:\n",
    "#                 continue\n",
    "\n",
    "#             loaded_video_count = 0\n",
    "#             for pkl_filename in pkl_files_in_set:\n",
    "#                 pkl_file_path = os.path.join(pose_set_path, pkl_filename)\n",
    "#                 try:\n",
    "#                     with open(pkl_file_path, \"rb\") as f:\n",
    "#                         loaded_pkl_content = pickle.load(f)\n",
    "#                 except FileNotFoundError:\n",
    "#                     continue\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error loading pose PKL {pkl_file_path}: {e}\")\n",
    "#                     continue\n",
    "\n",
    "#                 # Every pose-PKL contains a single key (video), by convention\n",
    "#                 if len(loaded_pkl_content) != 1:\n",
    "#                     continue\n",
    "\n",
    "#                 unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n",
    "#                 video_id = \"_\".join(unique_video_key.split(\"_\")[1:])\n",
    "#                 if video_id in self.pie_db.get(set_id, {}):\n",
    "#                     self.all_pose_data[set_id][video_id] = video_data\n",
    "#                     loaded_video_count += 1\n",
    "\n",
    "#             if loaded_video_count > 0:\n",
    "#                 sets_loaded_count += 1\n",
    "\n",
    "#     def _generate_sequence_list(self):\n",
    "#         \"\"\"\n",
    "#         Enumerate every sliding window that satisfies length + prediction horizon.\n",
    "#         \"\"\"\n",
    "#         sequence_count = 0\n",
    "#         for set_id in self.set_names:\n",
    "#             if set_id not in self.pie_db:\n",
    "#                 continue\n",
    "#             for video_id, video_data in self.pie_db[set_id].items():\n",
    "#                 if \"ped_annotations\" not in video_data:\n",
    "#                     continue\n",
    "#                 for ped_id, ped_data in video_data[\"ped_annotations\"].items():\n",
    "#                     frames = ped_data.get(\"frames\", [])\n",
    "#                     if len(frames) < self.seq_len + self.pred_len:\n",
    "#                         continue\n",
    "\n",
    "#                     frames_sorted = sorted(frames)\n",
    "#                     for i in range(len(frames_sorted) - self.seq_len - self.pred_len + 1):\n",
    "#                         start_f = frames_sorted[i]\n",
    "#                         obs_end_f = frames_sorted[i + self.seq_len - 1]\n",
    "\n",
    "#                         # consecutiveness check\n",
    "#                         if obs_end_f - start_f != self.seq_len - 1:\n",
    "#                             continue\n",
    "\n",
    "#                         target_idx = i + self.seq_len + self.pred_len - 1\n",
    "#                         if target_idx >= len(frames_sorted):\n",
    "#                             continue\n",
    "\n",
    "#                         target_f = frames_sorted[target_idx]\n",
    "#                         if target_f - obs_end_f != self.pred_len:\n",
    "#                             continue\n",
    "\n",
    "#                         self.sequences.append((set_id, video_id, ped_id, start_f))\n",
    "#                         sequence_count += 1\n",
    "\n",
    "#         print(f\"Dataset initialized with {sequence_count} sequences for sets {self.set_names}.\")\n",
    "\n",
    "#     # --------------------------------------------------------------------- #\n",
    "#     #                              overrides                                #\n",
    "#     # --------------------------------------------------------------------- #\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sequences)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Returns:\n",
    "#             features_dict   {stream_name: Tensor(seq_len, feat_dim)}\n",
    "#             label_tensor    Tensor([])\n",
    "#         \"\"\"\n",
    "#         set_id, video_id, ped_id, start_frame = self.sequences[idx]\n",
    "#         frame_nums = list(range(start_frame, start_frame + self.seq_len))\n",
    "#         target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n",
    "\n",
    "#         # convenient aliases\n",
    "#         video_db = self.pie_db.get(set_id, {}).get(video_id, {})\n",
    "#         ped_db = video_db.get(\"ped_annotations\", {}).get(ped_id, {})\n",
    "#         ego_db = video_db.get(\"vehicle_annotations\", {})\n",
    "#         traffic_db = video_db.get(\"traffic_annotations\", {})\n",
    "#         ped_attributes = ped_db.get(\"attributes\", {})\n",
    "\n",
    "#         feature_sequences = {s: [] for s in self.active_streams}\n",
    "#         label = 0\n",
    "\n",
    "#         if (\n",
    "#             \"frames\" in ped_db\n",
    "#             and \"behavior\" in ped_db\n",
    "#             and \"cross\" in ped_db[\"behavior\"]\n",
    "#         ):\n",
    "#             try:\n",
    "#                 target_idx = ped_db[\"frames\"].index(target_frame_num)\n",
    "#                 label = ped_db[\"behavior\"][\"cross\"][target_idx]\n",
    "#                 if label == -1:\n",
    "#                     label = 0\n",
    "#             except (ValueError, IndexError):\n",
    "#                 pass\n",
    "\n",
    "#         # --- static context ------------------------------------------------\n",
    "#         static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n",
    "#         if \"static_context\" in self.active_streams:\n",
    "#             sig_idx = ped_attributes.get(\"signalized\", 0)\n",
    "#             int_idx = ped_attributes.get(\"intersection\", 0)\n",
    "#             age_idx = ped_attributes.get(\"age\", 2)\n",
    "#             gen_idx = ped_attributes.get(\"gender\", 0)\n",
    "#             td_idx = int(ped_attributes.get(\"traffic_direction\", 0))\n",
    "#             nl_val = ped_attributes.get(\"num_lanes\", 2)\n",
    "#             nl_cat_idx = LANE_CATEGORIES.get(\n",
    "#                 nl_val, LANE_CATEGORIES[max(LANE_CATEGORIES.keys())]\n",
    "#             )\n",
    "\n",
    "#             static_vec = np.concatenate(\n",
    "#                 [\n",
    "#                     to_one_hot(sig_idx, NUM_SIGNALIZED_CATS),\n",
    "#                     to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n",
    "#                     to_one_hot(age_idx, NUM_AGE_CATS),\n",
    "#                     to_one_hot(gen_idx, NUM_GENDER_CATS),\n",
    "#                     to_one_hot(td_idx, NUM_TRAFFIC_DIR_CATS),\n",
    "#                     to_one_hot(nl_cat_idx, NUM_LANE_CATS),\n",
    "#                 ]\n",
    "#             )\n",
    "\n",
    "#             if static_vec.shape[0] != INPUT_SIZE_STATIC:\n",
    "#                 static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n",
    "\n",
    "#         # -------------------------------------------------------------------\n",
    "#         #                    per-frame feature extraction                    #\n",
    "#         # -------------------------------------------------------------------\n",
    "#         for frame_num in frame_nums:\n",
    "#             frame_db_idx = -1\n",
    "#             if \"frames\" in ped_db:\n",
    "#                 try:\n",
    "#                     frame_db_idx = ped_db[\"frames\"].index(frame_num)\n",
    "#                 except ValueError:\n",
    "#                     pass\n",
    "\n",
    "#             ego_frame_data = ego_db.get(frame_num, {})\n",
    "\n",
    "#             # ---------- bbox ------------------------------------------------\n",
    "#             if \"bbox\" in self.active_streams:\n",
    "#                 bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32)\n",
    "#                 if (\n",
    "#                     frame_db_idx != -1\n",
    "#                     and \"bbox\" in ped_db\n",
    "#                     and len(ped_db[\"bbox\"]) > frame_db_idx\n",
    "#                 ):\n",
    "#                     try:\n",
    "#                         x1, y1, x2, y2 = ped_db[\"bbox\"][frame_db_idx]\n",
    "#                         img_w = video_db.get(\"width\", 1920)\n",
    "#                         img_h = video_db.get(\"height\", 1080)\n",
    "#                         if img_w > 0 and img_h > 0:\n",
    "#                             cx = ((x1 + x2) / 2) / img_w\n",
    "#                             cy = ((y1 + y2) / 2) / img_h\n",
    "#                             w = (x2 - x1) / img_w\n",
    "#                             h = (y2 - y1) / img_h\n",
    "#                             if 0 < w and 0 < h and 0 <= cx <= 1 and 0 <= cy <= 1:\n",
    "#                                 bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n",
    "#                     except Exception:\n",
    "#                         pass\n",
    "\n",
    "#                 feature_sequences[\"bbox\"].append(bbox_norm)\n",
    "\n",
    "#             # ---------- pose -----------------------------------------------\n",
    "#             if \"pose\" in self.active_streams:\n",
    "#                 pose_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n",
    "#                 vid_pose = self.all_pose_data.get(set_id, {}).get(video_id, {})\n",
    "#                 frame_pose = vid_pose.get(frame_num, {})\n",
    "#                 loaded_pose = frame_pose.get(ped_id)\n",
    "\n",
    "#                 if (\n",
    "#                     loaded_pose is not None\n",
    "#                     and isinstance(loaded_pose, np.ndarray)\n",
    "#                     and loaded_pose.shape == (INPUT_SIZE_POSE,)\n",
    "#                 ):\n",
    "#                     pose_vector = loaded_pose\n",
    "\n",
    "#                 feature_sequences[\"pose\"].append(pose_vector)\n",
    "\n",
    "#             # ---------- ego-speed ------------------------------------------\n",
    "#             if \"ego_speed\" in self.active_streams:\n",
    "#                 speed = ego_frame_data.get(\"OBD_speed\", 0.0)\n",
    "#                 if speed == 0.0:\n",
    "#                     speed = ego_frame_data.get(\"GPS_speed\", 0.0)\n",
    "\n",
    "#                 speed_scaled = (\n",
    "#                     speed - self.scalers.get(\"ego_speed_mean\", 0.0)\n",
    "#                 ) / self.scalers.get(\"ego_speed_std\", 1.0)\n",
    "#                 feature_sequences[\"ego_speed\"].append([speed_scaled])\n",
    "\n",
    "#             # ---------- ego-acc --------------------------------------------\n",
    "#             if \"ego_acc\" in self.active_streams:\n",
    "#                 acc_x = ego_frame_data.get(\"accX\", 0.0)\n",
    "#                 acc_y = ego_frame_data.get(\"accY\", 0.0)\n",
    "#                 acc_x_scaled = (\n",
    "#                     acc_x - self.scalers.get(\"accX_mean\", 0.0)\n",
    "#                 ) / self.scalers.get(\"accX_std\", 1.0)\n",
    "#                 acc_y_scaled = (\n",
    "#                     acc_y - self.scalers.get(\"accY_mean\", 0.0)\n",
    "#                 ) / self.scalers.get(\"accY_std\", 1.0)\n",
    "#                 feature_sequences[\"ego_acc\"].append([acc_x_scaled, acc_y_scaled])\n",
    "\n",
    "#             # ---------- ego-gyro -------------------------------------------\n",
    "#             if \"ego_gyro\" in self.active_streams:\n",
    "#                 gyro_z = ego_frame_data.get(\"gyroZ\", 0.0)\n",
    "#                 gyro_z_scaled = (\n",
    "#                     gyro_z - self.scalers.get(\"gyroZ_mean\", 0.0)\n",
    "#                 ) / self.scalers.get(\"gyroZ_std\", 1.0)\n",
    "#                 feature_sequences[\"ego_gyro\"].append([gyro_z_scaled])\n",
    "\n",
    "#             # ---------- ped_action -----------------------------------------\n",
    "#             if \"ped_action\" in self.active_streams:\n",
    "#                 action = 0\n",
    "#                 if (\n",
    "#                     frame_db_idx != -1\n",
    "#                     and \"behavior\" in ped_db\n",
    "#                     and \"action\" in ped_db[\"behavior\"]\n",
    "#                     and len(ped_db[\"behavior\"][\"action\"]) > frame_db_idx\n",
    "#                 ):\n",
    "#                     action = ped_db[\"behavior\"][\"action\"][frame_db_idx]\n",
    "#                 feature_sequences[\"ped_action\"].append([float(action)])\n",
    "\n",
    "#             # ---------- ped_look -------------------------------------------\n",
    "#             if \"ped_look\" in self.active_streams:\n",
    "#                 look = 0\n",
    "#                 if (\n",
    "#                     frame_db_idx != -1\n",
    "#                     and \"behavior\" in ped_db\n",
    "#                     and \"look\" in ped_db[\"behavior\"]\n",
    "#                     and len(ped_db[\"behavior\"][\"look\"]) > frame_db_idx\n",
    "#                 ):\n",
    "#                     look = ped_db[\"behavior\"][\"look\"][frame_db_idx]\n",
    "#                 feature_sequences[\"ped_look\"].append([float(look)])\n",
    "\n",
    "#             # ---------- ped_occlusion --------------------------------------\n",
    "#             if \"ped_occlusion\" in self.active_streams:\n",
    "#                 occ = 0.0\n",
    "#                 if (\n",
    "#                     frame_db_idx != -1\n",
    "#                     and \"occlusion\" in ped_db\n",
    "#                     and len(ped_db[\"occlusion\"]) > frame_db_idx\n",
    "#                 ):\n",
    "#                     occ_val = ped_db[\"occlusion\"][frame_db_idx]\n",
    "#                     occ = float(occ_val) / 2.0\n",
    "#                 feature_sequences[\"ped_occlusion\"].append([occ])\n",
    "\n",
    "#             # ---------- traffic_light --------------------------------------\n",
    "#             if \"traffic_light\" in self.active_streams:\n",
    "#                 state_int = 0\n",
    "#                 for obj_id, obj_data in traffic_db.items():\n",
    "#                     if (\n",
    "#                         obj_data.get(\"obj_class\") == \"traffic_light\"\n",
    "#                         and \"frames\" in obj_data\n",
    "#                         and \"state\" in obj_data\n",
    "#                     ):\n",
    "#                         try:\n",
    "#                             tl_idx = obj_data[\"frames\"].index(frame_num)\n",
    "#                             state_val = obj_data[\"state\"][tl_idx]\n",
    "#                             if state_val != 0:\n",
    "#                                 state_int = state_val\n",
    "#                                 break\n",
    "#                         except (ValueError, IndexError):\n",
    "#                             continue\n",
    "#                 feature_sequences[\"traffic_light\"].append(\n",
    "#                     to_one_hot(state_int, INPUT_SIZE_TL_STATE)\n",
    "#                 )\n",
    "\n",
    "#             # ---------- static_context (per-frame replicate) ---------------\n",
    "#             if \"static_context\" in self.active_streams:\n",
    "#                 feature_sequences[\"static_context\"].append(static_vec)\n",
    "\n",
    "#         # -------------------------------------------------------------------\n",
    "#         # convert to tensors / safe fallback\n",
    "#         # -------------------------------------------------------------------\n",
    "#         features = {}\n",
    "#         try:\n",
    "#             for name in self.active_streams:\n",
    "#                 features[name] = torch.tensor(\n",
    "#                     np.asarray(feature_sequences[name], dtype=np.float32),\n",
    "#                     dtype=torch.float32,\n",
    "#                 )\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error converting features idx {idx}: {e}. Returning zeros.\")\n",
    "#             features = {\n",
    "#                 name: torch.zeros(\n",
    "#                     (self.seq_len, self._input_sizes_for_error.get(name, 1)),\n",
    "#                     dtype=torch.float32,\n",
    "#                 )\n",
    "#                 for name in self.active_streams\n",
    "#             }\n",
    "\n",
    "#         return features, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# class BalancedDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Memory-based balanced dataset generated by the prep notebook cell.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, data_dict, active_streams, label_key=\"label\"):\n",
    "#         self.active_streams = active_streams\n",
    "#         self.label_key = label_key\n",
    "\n",
    "#         if self.label_key not in data_dict or not data_dict[self.label_key]:\n",
    "#             raise ValueError(f\"Label key '{self.label_key}' missing/empty.\")\n",
    "\n",
    "#         self.num_samples = len(data_dict[self.label_key])\n",
    "#         if self.num_samples == 0:\n",
    "#             print(\"Warning: BalancedDataset initialized with zero samples.\")\n",
    "\n",
    "#         # convert every requested stream to tensor\n",
    "#         self.features = {}\n",
    "#         for stream in self.active_streams:\n",
    "#             if stream in data_dict and data_dict[stream]:\n",
    "#                 try:\n",
    "#                     self.features[stream] = torch.tensor(\n",
    "#                         np.asarray(data_dict[stream]), dtype=torch.float32\n",
    "#                     )\n",
    "#                 except ValueError as e:\n",
    "#                     raise ValueError(f\"Error converting stream '{stream}': {e}\")\n",
    "#             else:\n",
    "#                 raise KeyError(f\"Stream '{stream}' missing or empty in data.\")\n",
    "\n",
    "#         try:\n",
    "#             self.labels = torch.tensor(\n",
    "#                 [lbl[0] for lbl in data_dict[self.label_key]], dtype=torch.long\n",
    "#             )\n",
    "#         except (IndexError, TypeError) as e:\n",
    "#             raise ValueError(f\"Error converting labels: {e}\")\n",
    "\n",
    "#         for stream in self.active_streams:\n",
    "#             if len(self.features[stream]) != self.num_samples:\n",
    "#                 raise ValueError(\n",
    "#                     f\"Len mismatch: '{stream}' ({len(self.features[stream])}) vs labels ({self.num_samples})\"\n",
    "#                 )\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_samples\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         feature_dict = {s: self.features[s][idx] for s in self.active_streams}\n",
    "#         label = self.labels[idx]\n",
    "#         return feature_dict, label\n",
    "\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, hidden_dim, attention_dim):\n",
    "#         super().__init__()\n",
    "#         self.attention_net = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, attention_dim),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(attention_dim, 1),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, lstm_output):\n",
    "#         att_scores = self.attention_net(lstm_output).squeeze(2)\n",
    "#         att_weights = torch.softmax(att_scores, dim=1)\n",
    "#         context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1)\n",
    "#         return context_vector, att_weights\n",
    "\n",
    "\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# #                  ***  MODEL WITH WEIGHTED AVERAGE FUSION  ***                #\n",
    "# # -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# class MultiStreamWeightedAvgLSTM(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         input_sizes,\n",
    "#         lstm_hidden_size,\n",
    "#         num_lstm_layers,\n",
    "#         num_classes,\n",
    "#         attention_dim,\n",
    "#         dropout_rate,\n",
    "#         stream_names=None,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "\n",
    "#         if not stream_names:\n",
    "#             raise ValueError(\"stream_names cannot be empty.\")\n",
    "#         self.stream_names = stream_names\n",
    "#         self.num_active_streams = len(stream_names)\n",
    "#         self.lstm_output_dim = lstm_hidden_size * 2  # Bi-LSTM doubles hidden\n",
    "\n",
    "#         self.lstms = nn.ModuleDict()\n",
    "#         self.attentions = nn.ModuleDict()\n",
    "\n",
    "#         print(f\"Initializing Weighted-Avg model with streams: {self.stream_names}\")\n",
    "\n",
    "#         for name in self.stream_names:\n",
    "#             if name not in input_sizes:\n",
    "#                 raise KeyError(f\"Input size for stream '{name}' not provided.\")\n",
    "\n",
    "#             in_size = input_sizes[name]\n",
    "#             print(f\"  – Adding stream '{name}' (input {in_size})\")\n",
    "\n",
    "#             self.lstms[name] = nn.LSTM(\n",
    "#                 in_size,\n",
    "#                 lstm_hidden_size,\n",
    "#                 num_lstm_layers,\n",
    "#                 batch_first=True,\n",
    "#                 dropout=dropout_rate if num_lstm_layers > 1 else 0,\n",
    "#                 bidirectional=True,\n",
    "#             )\n",
    "#             self.attentions[name] = Attention(self.lstm_output_dim, attention_dim)\n",
    "\n",
    "#         # learnable fusion weights (one per stream)\n",
    "#         self.fusion_weights = nn.Parameter(torch.ones(self.num_active_streams))\n",
    "\n",
    "#         # classification head\n",
    "#         fused_dim = self.lstm_output_dim\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         inter_dim = max(num_classes * 4, fused_dim // 2)\n",
    "#         self.fc1 = nn.Linear(fused_dim, inter_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(inter_dim, num_classes)\n",
    "\n",
    "#     # --------------------------------------------------------------------- #\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # gather context vectors ------------------------------------------------\n",
    "#         ctx_vecs = []\n",
    "#         for name in self.stream_names:\n",
    "#             if name not in x:\n",
    "#                 # gracefully handle a missing stream during inference\n",
    "#                 zero_ctx = torch.zeros(\n",
    "#                     x[next(iter(x))].shape[0],\n",
    "#                     self.lstm_output_dim,\n",
    "#                     device=x[next(iter(x))].device,\n",
    "#                 )\n",
    "#                 ctx_vecs.append(zero_ctx)\n",
    "#                 continue\n",
    "\n",
    "#             lstm_out, _ = self.lstms[name](x[name])\n",
    "#             context_vector, _ = self.attentions[name](lstm_out)\n",
    "#             ctx_vecs.append(context_vector)\n",
    "\n",
    "#         if len(ctx_vecs) != self.num_active_streams:\n",
    "#             raise RuntimeError(\n",
    "#                 f\"context_vectors({len(ctx_vecs)}) != num_streams({self.num_active_streams})\"\n",
    "#             )\n",
    "\n",
    "#         # weighted average fusion ---------------------------------------------\n",
    "#         stacked = torch.stack(ctx_vecs, dim=1)  # (B, N, D)\n",
    "#         weights = torch.softmax(self.fusion_weights, dim=0).view(1, -1, 1)\n",
    "#         fused = torch.sum(stacked * weights, dim=1)\n",
    "\n",
    "#         # classification head --------------------------------------------------\n",
    "#         out = self.dropout(fused)\n",
    "#         out = self.relu(self.fc1(out))\n",
    "#         out = self.dropout(out)\n",
    "#         logits = self.fc2(out)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# #                       Training / evaluation helpers                          #\n",
    "# # -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     all_preds, all_labels = [], []\n",
    "#     active = model.stream_names\n",
    "\n",
    "#     for feats, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "#         inputs = {n: feats[n].to(device) for n in active if n in feats}\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         all_preds.extend(torch.argmax(outputs, 1).cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#     return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n",
    "\n",
    "\n",
    "# def evaluate_epoch(model, dataloader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     all_labels, all_preds, all_probs = [], [], []\n",
    "#     active = model.stream_names\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for feats, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "#             inputs = {n: feats[n].to(device) for n in active if n in feats}\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#             probs = torch.softmax(outputs, 1)\n",
    "#             preds = torch.argmax(probs, 1)\n",
    "\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "#     avg_loss = total_loss / max(1, len(dataloader))\n",
    "#     all_probs = np.asarray(all_probs)\n",
    "#     all_labels = np.asarray(all_labels)\n",
    "#     all_preds = np.asarray(all_preds)\n",
    "\n",
    "#     acc = accuracy_score(all_labels, all_preds)\n",
    "#     prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "#         all_labels, all_preds, average=\"binary\", pos_label=1, zero_division=0\n",
    "#     )\n",
    "#     auc = (\n",
    "#         roc_auc_score(all_labels, all_probs[:, 1])\n",
    "#         if len(np.unique(all_labels)) > 1\n",
    "#         else float(\"nan\")\n",
    "#     )\n",
    "\n",
    "#     return {\n",
    "#         \"loss\": avg_loss,\n",
    "#         \"accuracy\": acc,\n",
    "#         \"precision\": prec,\n",
    "#         \"recall\": rec,\n",
    "#         \"f1\": f1,\n",
    "#         \"auc\": auc,\n",
    "#     }\n",
    "\n",
    "\n",
    "# def get_predictions_and_labels(model, dataloader, device):\n",
    "#     model.eval()\n",
    "#     labels_all, preds_all = [], []\n",
    "#     active = model.stream_names\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for feats, labels in tqdm(dataloader, desc=\"Generating CM Data\", leave=False):\n",
    "#             inputs = {n: feats[n].to(device) for n in active if n in feats}\n",
    "#             outputs = model(inputs)\n",
    "#             preds = torch.argmax(outputs, 1)\n",
    "#             labels_all.extend(labels.cpu().numpy())\n",
    "#             preds_all.extend(preds.cpu().numpy())\n",
    "\n",
    "#     return np.asarray(labels_all), np.asarray(preds_all)\n",
    "\n",
    "\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# #                            Main execution block                              #\n",
    "# # -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"--- Running Model Training/Evaluation with Weighted Fusion ---\")\n",
    "#     print(f\"Active Streams: {ACTIVE_STREAMS}\")\n",
    "\n",
    "#     # ------------------ load balanced data & scalers -------------------------\n",
    "#     print(\"\\nLoading balanced training data …\")\n",
    "#     try:\n",
    "#         with open(BALANCED_DATA_PKL_PATH, \"rb\") as f:\n",
    "#             balanced_train_data_dict = pickle.load(f)\n",
    "#         with open(SCALERS_PKL_PATH, \"rb\") as f:\n",
    "#             scalers = pickle.load(f)\n",
    "#         print(\"   ✓ pre-processed data loaded.\")\n",
    "#     except FileNotFoundError as e:\n",
    "#         print(f\"ERROR: {e}.  Run the preprocessing cell first.\")\n",
    "#         sys.exit(1)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading pre-processed data: {e}\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "#     # -------------------------- load PIE database ----------------------------\n",
    "#     print(\"\\nLoading PIE database cache for validation …\")\n",
    "#     if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n",
    "#         raise FileNotFoundError(\"PIE database cache not found.\")\n",
    "\n",
    "#     try:\n",
    "#         with open(PIE_DATABASE_CACHE_PATH, \"rb\") as f:\n",
    "#             pie_database = pickle.load(f)\n",
    "#     except Exception as e:\n",
    "#         raise RuntimeError(f\"Failed to load PIE database: {e}\")\n",
    "#     print(\"   ✓ PIE database loaded.\")\n",
    "\n",
    "#     # ------------------- create datasets / dataloaders -----------------------\n",
    "#     print(\"\\nCreating Datasets and DataLoaders …\")\n",
    "#     try:\n",
    "#         train_dataset = BalancedDataset(\n",
    "#             balanced_train_data_dict, ACTIVE_STREAMS, label_key=\"label\"\n",
    "#         )\n",
    "#         del balanced_train_data_dict\n",
    "\n",
    "#         val_dataset = PIEDataset(\n",
    "#             pie_database,\n",
    "#             VAL_SETS_STR,\n",
    "#             POSE_DATA_DIR,\n",
    "#             SEQ_LEN,\n",
    "#             PRED_LEN,\n",
    "#             scalers,\n",
    "#             ALL_POSSIBLE_STREAMS,  # provide all streams for val\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error creating datasets: {e}\")\n",
    "#         raise\n",
    "\n",
    "#     if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "#         raise ValueError(\"One of the datasets is empty!\")\n",
    "\n",
    "#     train_loader = DataLoader(\n",
    "#         train_dataset,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=True,\n",
    "#         num_workers=2,\n",
    "#         pin_memory=True,\n",
    "#     )\n",
    "#     val_loader = DataLoader(\n",
    "#         val_dataset,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=False,\n",
    "#         num_workers=2,\n",
    "#         pin_memory=True,\n",
    "#     )\n",
    "#     print(\"   ✓ DataLoaders ready.\")\n",
    "#     del pie_database\n",
    "#     gc.collect()\n",
    "\n",
    "#     # ------------------------- initialise the model --------------------------\n",
    "#     print(\"\\nInitialising model …\")\n",
    "#     current_input_sizes = {}\n",
    "#     SPECIAL = {\n",
    "#         \"TRAFFIC_LIGHT\": \"TL_STATE\",\n",
    "#         \"STATIC_CONTEXT\": \"STATIC\",\n",
    "#         \"EGO_SPEED\": \"EGO_SPEED\",\n",
    "#         \"EGO_ACC\": \"EGO_ACC\",\n",
    "#         \"EGO_GYRO\": \"EGO_GYRO\",\n",
    "#         \"PED_ACTION\": \"PED_ACTION\",\n",
    "#         \"PED_LOOK\": \"PED_LOOK\",\n",
    "#         \"PED_OCCLUSION\": \"PED_OCC\",\n",
    "#     }\n",
    "\n",
    "#     for s in ACTIVE_STREAMS:\n",
    "#         name = f\"INPUT_SIZE_{SPECIAL.get(s.upper(), s.upper())}\"\n",
    "#         if s == \"bbox\":\n",
    "#             name = \"INPUT_SIZE_BBOX\"\n",
    "#         elif s == \"pose\":\n",
    "#             name = \"INPUT_SIZE_POSE\"\n",
    "\n",
    "#         if name not in globals():\n",
    "#             raise ValueError(f\"Input-size constant {name} not found.\")\n",
    "\n",
    "#         current_input_sizes[s] = globals()[name]\n",
    "\n",
    "#     model = MultiStreamWeightedAvgLSTM(\n",
    "#         current_input_sizes,\n",
    "#         LSTM_HIDDEN_SIZE,\n",
    "#         NUM_LSTM_LAYERS,\n",
    "#         NUM_CLASSES,\n",
    "#         ATTENTION_DIM,\n",
    "#         DROPOUT_RATE,\n",
    "#         stream_names=ACTIVE_STREAMS,\n",
    "#     ).to(DEVICE)\n",
    "\n",
    "#     print(\"\\n--- Model architecture ---\")\n",
    "#     print(model)\n",
    "#     total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "#     print(f\"Trainable parameters: {total_params:,}\")\n",
    "#     print(\"-\" * 30)\n",
    "\n",
    "#     # ---------------- loss / optimiser --------------------------------------\n",
    "#     print(\"\\nCalculating class weights …\")\n",
    "#     train_labels = train_dataset.labels.tolist()\n",
    "#     n0, n1 = train_labels.count(0), train_labels.count(1)\n",
    "#     total = len(train_labels)\n",
    "#     if total == 0:\n",
    "#         w0, w1 = 1.0, 1.0\n",
    "#     elif n0 == 0:\n",
    "#         w0, w1 = 0.0, 1.0\n",
    "#     elif n1 == 0:\n",
    "#         w0, w1 = 1.0, 0.0\n",
    "#     else:\n",
    "#         w0, w1 = total / (2.0 * n0), total / (2.0 * n1)\n",
    "\n",
    "#     class_weights = torch.tensor([w0, w1], dtype=torch.float32).to(DEVICE)\n",
    "#     print(f\"Loss weights → 0: {w0:.2f}, 1: {w1:.2f}\")\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "#     best_val_f1 = -1.0\n",
    "#     history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": [], \"val_f1\": []}\n",
    "#     best_model_path = \"\"\n",
    "\n",
    "#     # --------------------------- training loop -------------------------------\n",
    "#     print(\"\\n--- Starting training ---\")\n",
    "#     for epoch in range(NUM_EPOCHS):\n",
    "#         t0 = time.time()\n",
    "\n",
    "#         train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "#         metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "#         history[\"train_loss\"].append(train_loss)\n",
    "#         history[\"val_loss\"].append(metrics[\"loss\"])\n",
    "#         history[\"train_acc\"].append(train_acc)\n",
    "#         history[\"val_acc\"].append(metrics[\"accuracy\"])\n",
    "#         history[\"val_f1\"].append(metrics[\"f1\"])\n",
    "\n",
    "#         print(f\"\\nEpoch {epoch + 1:02d}/{NUM_EPOCHS} – {time.time() - t0:.1f}s\")\n",
    "#         print(f\"  train loss {train_loss:.4f} | acc {train_acc:.4f}\")\n",
    "#         print(f\"  val   loss {metrics['loss']:.4f} | acc {metrics['accuracy']:.4f}\")\n",
    "#         print(\n",
    "#             f\"           prec {metrics['precision']:.4f} | rec {metrics['recall']:.4f} | f1 {metrics['f1']:.4f} | auc {metrics['auc']:.4f}\"\n",
    "#         )\n",
    "\n",
    "#         if metrics[\"f1\"] > best_val_f1:\n",
    "#             best_val_f1 = metrics[\"f1\"]\n",
    "#             best_model_path = f\"best_model_weighted_{'_'.join(sorted(ACTIVE_STREAMS))}_ep{epoch + 1}.pth\"\n",
    "#             torch.save(model.state_dict(), best_model_path)\n",
    "#             print(f\"  ✓ new best model saved → {best_model_path} (F1 {best_val_f1:.4f})\")\n",
    "\n",
    "#     print(\"\\n--- Training finished ---\")\n",
    "\n",
    "#     # --------------------------- plots ---------------------------------------\n",
    "#     print(\"\\nPlotting training curves …\")\n",
    "#     fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "#     ax[0].plot(range(1, NUM_EPOCHS + 1), history[\"train_loss\"], label=\"Train\")\n",
    "#     ax[0].plot(range(1, NUM_EPOCHS + 1), history[\"val_loss\"], label=\"Val\")\n",
    "#     ax[0].set_xlabel(\"Epoch\")\n",
    "#     ax[0].set_ylabel(\"Loss\")\n",
    "#     ax[0].set_title(\"Loss curve\")\n",
    "#     ax[0].legend()\n",
    "#     ax[0].grid(True)\n",
    "\n",
    "#     ax[1].plot(range(1, NUM_EPOCHS + 1), history[\"train_acc\"], label=\"Train Acc\")\n",
    "#     ax[1].plot(range(1, NUM_EPOCHS + 1), history[\"val_acc\"], label=\"Val Acc\")\n",
    "#     ax[1].plot(range(1, NUM_EPOCHS + 1), history[\"val_f1\"], \"--\", label=\"Val F1\")\n",
    "#     ax[1].set_xlabel(\"Epoch\")\n",
    "#     ax[1].set_ylabel(\"Metric\")\n",
    "#     ax[1].set_title(\"Accuracy & F1\")\n",
    "#     ax[1].legend()\n",
    "#     ax[1].grid(True)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # ------------------- final evaluation (best model) -----------------------\n",
    "#     print(\"\\n--- Final Evaluation on Validation set ---\")\n",
    "#     if best_model_path and os.path.exists(best_model_path):\n",
    "#         try:\n",
    "#             model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "#             print(f\"Loaded best model: {best_model_path}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Warning: could not load best model ({e}).  Using last epoch params.\")\n",
    "#     else:\n",
    "#         print(\"Warning: best model not found, using last epoch parameters.\")\n",
    "\n",
    "#     final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n",
    "#     y_true, y_pred = get_predictions_and_labels(model, val_loader, DEVICE)\n",
    "#     cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "#     print(\"\\n--- Final metrics (Weighted Avg Fusion) ---\")\n",
    "#     for k, v in final_metrics.items():\n",
    "#         print(f\"{k:<10}: {v:.4f}\")\n",
    "\n",
    "#     print(f\"(Best validation F1 during training: {best_val_f1:.4f})\")\n",
    "\n",
    "#     ConfusionMatrixDisplay(cm, display_labels=[\"Not Crossing\", \"Crossing\"]).plot(\n",
    "#         cmap=plt.cm.Blues\n",
    "#     )\n",
    "#     plt.title(\"Confusion Matrix\")\n",
    "#     plt.show()\n",
    "\n",
    "#     # ------------------- inspect learned fusion weights ----------------------\n",
    "#     if hasattr(model, \"fusion_weights\"):\n",
    "#         w = torch.softmax(model.fusion_weights, 0).detach().cpu().numpy()\n",
    "#         print(\"\\n--- Learned fusion weights ---\")\n",
    "#         for stream, weight in zip(model.stream_names, w):\n",
    "#             print(f\"{stream:<15}: {weight:.4f}\")\n",
    "#         print(\"-\" * 30)\n",
    "\n",
    "#     print(\"\\n--- Script complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78603ca1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:28:27.081599Z",
     "iopub.status.busy": "2025-05-07T09:28:27.081393Z",
     "iopub.status.idle": "2025-05-07T09:28:27.090858Z",
     "shell.execute_reply": "2025-05-07T09:28:27.090086Z"
    },
    "papermill": {
     "duration": 0.021908,
     "end_time": "2025-05-07T09:28:27.092109",
     "exception": false,
     "start_time": "2025-05-07T09:28:27.070201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # --- CELL 1: DATA PREPARATION & BALANCING (Including YOLOP – Run Once) ---\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from tqdm.notebook import tqdm\n",
    "# import random\n",
    "# import pickle\n",
    "# import time\n",
    "# import sys\n",
    "# import gc\n",
    "# import cv2  # required for PIE class\n",
    "\n",
    "# # --- Add PIE utilities path ---\n",
    "# pie_utilities_path = '/kaggle/working/PIE/utilities'\n",
    "# if pie_utilities_path not in sys.path:\n",
    "#     sys.path.insert(0, pie_utilities_path)\n",
    "# try:\n",
    "#     from pie_data import PIE\n",
    "# except ImportError as e:\n",
    "#     print(f\"Warn: Could not import PIE class: {e}.  Database must exist.\")\n",
    "#     PIE = None\n",
    "\n",
    "# # --- Configuration ---\n",
    "# PIE_ROOT_PATH = '/kaggle/working/PIE'\n",
    "# POSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2'\n",
    "# PIE_DATABASE_CACHE_PATH = '/kaggle/input/pie-database/pie_database.pkl'\n",
    "# YOLOP_FEATURE_DIR = '/kaggle/input/yolop-data/yolop features'  # setXX/setYY_vidZZ_yolop_features.pkl\n",
    "\n",
    "# # --- Streams ---\n",
    "# ALL_POSSIBLE_STREAMS = [\n",
    "#     'bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro',\n",
    "#     'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light',\n",
    "#     'static_context', 'yolop'\n",
    "# ]\n",
    "# print(f\"Data will be prepared for streams: {ALL_POSSIBLE_STREAMS}\")\n",
    "\n",
    "# # --- Sizes & constants ---\n",
    "# SEQ_LEN = 30\n",
    "# PRED_LEN = 1\n",
    "# INPUT_SIZE_BBOX = 4\n",
    "# INPUT_SIZE_POSE = 34\n",
    "# INPUT_SIZE_EGO_SPEED = 1\n",
    "# INPUT_SIZE_EGO_ACC = 2\n",
    "# INPUT_SIZE_EGO_GYRO = 1\n",
    "# INPUT_SIZE_PED_ACTION = 1\n",
    "# INPUT_SIZE_PED_LOOK = 1\n",
    "# INPUT_SIZE_PED_OCC = 1\n",
    "# INPUT_SIZE_TL_STATE = 4\n",
    "\n",
    "# NUM_SIGNALIZED_CATS = 4\n",
    "# NUM_INTERSECTION_CATS = 5\n",
    "# NUM_AGE_CATS = 4\n",
    "# NUM_GENDER_CATS = 3\n",
    "# NUM_TRAFFIC_DIR_CATS = 2\n",
    "# LANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7: 4, 8: 4}\n",
    "# NUM_LANE_CATS = len(set(LANE_CATEGORIES.values()))\n",
    "# INPUT_SIZE_STATIC = (\n",
    "#     NUM_SIGNALIZED_CATS + NUM_INTERSECTION_CATS + NUM_AGE_CATS +\n",
    "#     NUM_GENDER_CATS + NUM_TRAFFIC_DIR_CATS + NUM_LANE_CATS\n",
    "# )\n",
    "\n",
    "# # YOLOP feature size\n",
    "# GRID_SIZE = 3\n",
    "# YOLOP_DRIVABLE_FEATURES_DIM = GRID_SIZE * GRID_SIZE\n",
    "# YOLOP_LANE_FEATURES_DIM = GRID_SIZE * GRID_SIZE\n",
    "# YOLOP_OBJECT_FEATURES_DIM = 2\n",
    "# INPUT_SIZE_YOLOP = (\n",
    "#     YOLOP_DRIVABLE_FEATURES_DIM + YOLOP_LANE_FEATURES_DIM + YOLOP_OBJECT_FEATURES_DIM\n",
    "# )\n",
    "\n",
    "# # --- Dataset splits ---\n",
    "# TRAIN_SETS_STR = ['set01', 'set02', 'set04']\n",
    "\n",
    "# # --- Label/state maps ---\n",
    "# TL_STATE_MAP = {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}\n",
    "# NUM_TL_STATES = len(TL_STATE_MAP)\n",
    "\n",
    "# # --- Output paths ---\n",
    "# BALANCED_DATA_PKL_PATH = \"/kaggle/working/balanced_train_data_with_yolop.pkl\"\n",
    "# SCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\"\n",
    "\n",
    "# # --- Helpers ---------------------------------------------------------------\n",
    "# def to_one_hot(index, num_classes):\n",
    "#     vec = np.zeros(num_classes, dtype=np.float32)\n",
    "#     safe_index = int(np.clip(index, 0, num_classes - 1))\n",
    "#     vec[safe_index] = 1.0\n",
    "#     return vec\n",
    "\n",
    "\n",
    "# def balance_samples_count(seq_data, label_type, random_seed=42):\n",
    "#     print('---------------------------------------------------------')\n",
    "#     print(f\"Balancing samples based on '{label_type}' key\")\n",
    "\n",
    "#     if label_type not in seq_data:\n",
    "#         raise KeyError(f\"Label type '{label_type}' not found.\")\n",
    "\n",
    "#     try:\n",
    "#         gt_labels = [lbl[0] for lbl in seq_data[label_type]]\n",
    "#     except (IndexError, TypeError):\n",
    "#         raise ValueError(\n",
    "#             f\"Labels under '{label_type}' not in expected format [[label_val]].\"\n",
    "#         )\n",
    "\n",
    "#     if not all(l in [0, 1] for l in gt_labels):\n",
    "#         print(\"Warning: labels contain values other than 0 or 1.\")\n",
    "\n",
    "#     num_pos = np.count_nonzero(gt_labels)\n",
    "#     num_neg = len(gt_labels) - num_pos\n",
    "#     new_seq_data = {}\n",
    "\n",
    "#     if num_neg == num_pos:\n",
    "#         print(\"Samples already balanced.\")\n",
    "#         return seq_data.copy()\n",
    "\n",
    "#     majority_label = 0 if num_neg > num_pos else 1\n",
    "#     minority_count = min(num_neg, num_pos)\n",
    "#     print(\n",
    "#         f\"Unbalanced: Positive (1): {num_pos} | Negative (0): {num_neg}\\n\"\n",
    "#         f\"Undersampling majority class ({majority_label}) to {minority_count}\"\n",
    "#     )\n",
    "\n",
    "#     majority_idx = np.where(np.array(gt_labels) == majority_label)[0]\n",
    "#     minority_idx = np.where(np.array(gt_labels) != majority_label)[0]\n",
    "#     np.random.seed(random_seed)\n",
    "#     keep_majority = np.random.choice(majority_idx, minority_count, replace=False)\n",
    "#     final_idx = np.concatenate((minority_idx, keep_majority))\n",
    "#     np.random.shuffle(final_idx)\n",
    "\n",
    "#     for k, v_list in seq_data.items():\n",
    "#         if isinstance(v_list, list) and len(v_list) == len(gt_labels):\n",
    "#             try:\n",
    "#                 if v_list and isinstance(v_list[0], np.ndarray):\n",
    "#                     v_array = np.array(v_list)\n",
    "#                     new_seq_data[k] = list(v_array[final_idx])\n",
    "#                 else:\n",
    "#                     new_seq_data[k] = [v_list[i] for i in final_idx]\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing key '{k}': {e}.  Skipped.\")\n",
    "#                 new_seq_data[k] = []\n",
    "#         else:\n",
    "#             new_seq_data[k] = v_list\n",
    "\n",
    "#     new_gt = [lbl[0] for lbl in new_seq_data[label_type]]\n",
    "#     final_pos = np.count_nonzero(new_gt)\n",
    "#     final_neg = len(new_gt) - final_pos\n",
    "#     print(f\"Balanced:   Positive (1): {final_pos} | Negative (0): {final_neg}\")\n",
    "#     print('---------------------------------------------------------')\n",
    "#     return new_seq_data\n",
    "\n",
    "\n",
    "# # --- Dataset class ---------------------------------------------------------\n",
    "# class PIEDataset(Dataset):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         pie_database,\n",
    "#         set_names,\n",
    "#         pose_data_dir,\n",
    "#         yolop_data_dir,\n",
    "#         seq_len,\n",
    "#         pred_len,\n",
    "#         scalers=None,\n",
    "#         streams_to_generate=None,\n",
    "#     ):\n",
    "#         self.pie_db = pie_database\n",
    "#         self.set_names = set_names\n",
    "#         self.pose_data_dir = pose_data_dir\n",
    "#         self.yolop_data_dir = yolop_data_dir\n",
    "#         self.seq_len = seq_len\n",
    "#         self.pred_len = pred_len\n",
    "#         self.scalers = scalers or {}\n",
    "#         self.streams_to_generate = streams_to_generate or ALL_POSSIBLE_STREAMS\n",
    "#         self.sequences = []\n",
    "#         self.all_pose_data = {}\n",
    "#         self.all_yolop_data = {}\n",
    "#         self._input_sizes_for_error = self._get_input_sizes_dict()\n",
    "\n",
    "#         if 'pose' in self.streams_to_generate:\n",
    "#             self._load_pose_data()\n",
    "#         if 'yolop' in self.streams_to_generate:\n",
    "#             self._load_yolop_data()\n",
    "\n",
    "#         self._generate_sequence_list()\n",
    "\n",
    "#         if not self.sequences:\n",
    "#             raise ValueError(f\"No sequences found for sets {self.set_names}\")\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     def _get_input_sizes_dict(self):\n",
    "#         input_sizes = {}\n",
    "#         special = {\n",
    "#             'TRAFFIC_LIGHT': 'TL_STATE',\n",
    "#             'STATIC_CONTEXT': 'STATIC',\n",
    "#             'EGO_SPEED': 'EGO_SPEED',\n",
    "#             'EGO_ACC': 'EGO_ACC',\n",
    "#             'EGO_GYRO': 'EGO_GYRO',\n",
    "#             'PED_ACTION': 'PED_ACTION',\n",
    "#             'PED_LOOK': 'PED_LOOK',\n",
    "#             'PED_OCCLUSION': 'PED_OCC',\n",
    "#             'YOLOP': 'YOLOP',\n",
    "#         }\n",
    "#         for stream in ALL_POSSIBLE_STREAMS:\n",
    "#             size_name = f'INPUT_SIZE_{stream.upper()}'\n",
    "#             key = stream.upper()\n",
    "#             suffix = special.get(key)\n",
    "#             if suffix:\n",
    "#                 size_name = f'INPUT_SIZE_{suffix}'\n",
    "#             elif stream == 'bbox':\n",
    "#                 size_name = 'INPUT_SIZE_BBOX'\n",
    "#             elif stream == 'pose':\n",
    "#                 size_name = 'INPUT_SIZE_POSE'\n",
    "\n",
    "#             input_sizes[stream] = globals().get(size_name, 1)\n",
    "\n",
    "#         return input_sizes\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     def _load_pose_data(self):\n",
    "#         for set_id in self.set_names:\n",
    "#             self.all_pose_data[set_id] = {}\n",
    "#             set_path = os.path.join(self.pose_data_dir, set_id)\n",
    "#             if not os.path.isdir(set_path):\n",
    "#                 continue\n",
    "#             pkl_files = [\n",
    "#                 f for f in os.listdir(set_path)\n",
    "#                 if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")\n",
    "#             ]\n",
    "#             for pkl_name in pkl_files:\n",
    "#                 pkl_path = os.path.join(set_path, pkl_name)\n",
    "#                 try:\n",
    "#                     with open(pkl_path, 'rb') as f:\n",
    "#                         content = pickle.load(f)\n",
    "#                     if len(content) != 1:\n",
    "#                         continue\n",
    "#                     uniq_key, video_data = list(content.items())[0]\n",
    "#                     vid = \"_\".join(uniq_key.split('_')[1:])\n",
    "#                     if vid in self.pie_db.get(set_id, {}):\n",
    "#                         self.all_pose_data[set_id][vid] = video_data\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Pose load error {pkl_path}: {e}\")\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "    \n",
    "\n",
    "#     def _load_yolop_data(self): # New method to load YOLOP features\n",
    "#         print(f\"\\nLoading YOLOP data for sets: {self.set_names} from {self.yolop_data_dir}\")\n",
    "#         sets_loaded_count = 0\n",
    "#         # Initialize dictionaries for all sets requested by this dataset instance\n",
    "#         for set_id in self.set_names:\n",
    "#             self.all_yolop_data[set_id] = {}\n",
    "\n",
    "#         if not os.path.isdir(self.yolop_data_dir):\n",
    "#             print(f\"Error: YOLOP feature directory not found: {self.yolop_data_dir}\")\n",
    "#             return\n",
    "\n",
    "#         try:\n",
    "#             all_pkl_files = [f for f in os.listdir(self.yolop_data_dir)\n",
    "#                              if f.endswith(\"_yolop_features.pkl\")]\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error listing files in YOLOP directory {self.yolop_data_dir}: {e}\")\n",
    "#             return\n",
    "\n",
    "#         if not all_pkl_files:\n",
    "#             print(f\"Warning: No '*_yolop_features.pkl' files found directly in {self.yolop_data_dir}\")\n",
    "\n",
    "#         # --- Initialize count BEFORE the loop ---\n",
    "#         loaded_file_count = 0\n",
    "#         files_for_needed_sets = 0\n",
    "#         # ---\n",
    "\n",
    "#         for pkl_filename in tqdm(all_pkl_files, desc=\"Loading YOLOP PKLs\"):\n",
    "#             try:\n",
    "#                 parts = pkl_filename.replace(\"_yolop_features.pkl\", \"\").split('_')\n",
    "#                 set_id_from_file = parts[0]\n",
    "#                 video_id = \"_\".join(parts[1:])\n",
    "#             except IndexError:\n",
    "#                 print(f\"Warning: Could not parse set/video ID from filename: {pkl_filename}\")\n",
    "#                 continue\n",
    "\n",
    "#             if set_id_from_file in self.set_names:\n",
    "#                 files_for_needed_sets += 1\n",
    "#                 pkl_file_path = os.path.join(self.yolop_data_dir, pkl_filename)\n",
    "#                 try:\n",
    "#                     with open(pkl_file_path, 'rb') as f:\n",
    "#                         loaded_pkl_content = pickle.load(f)\n",
    "\n",
    "#                     if len(loaded_pkl_content) != 1:\n",
    "#                         print(f\"Warn: PKL {pkl_filename} format issue (expected 1 key). Skip.\")\n",
    "#                         continue\n",
    "\n",
    "#                     unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n",
    "#                     expected_key = f\"{set_id_from_file}_{video_id}\"\n",
    "#                     if unique_video_key != expected_key:\n",
    "#                          print(f\"Warn: Key mismatch in {pkl_filename}. Expected '{expected_key}', found '{unique_video_key}'. Trying to use found key.\")\n",
    "\n",
    "#                     self.all_yolop_data[set_id_from_file][video_id] = video_data\n",
    "#                     loaded_file_count += 1 # Increment only on success\n",
    "\n",
    "#                 except FileNotFoundError:\n",
    "#                      print(f\"Warn: YOLOP feature file not found during loading: {pkl_file_path}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error loading YOLOP PKL {pkl_file_path}: {e}\")\n",
    "\n",
    "#         print(f\"Finished loading YOLOP data. Found {loaded_file_count} relevant files out of {files_for_needed_sets} expected for sets {self.set_names}.\")\n",
    "#         if loaded_file_count > 0:\n",
    "#              actual_sets_loaded = sum(1 for s in self.set_names if s in self.all_yolop_data and self.all_yolop_data[s])\n",
    "#              print(f\"Data loaded for {actual_sets_loaded} sets.\")\n",
    "#     # ------------------------------------------------------------------\n",
    "#     def _generate_sequence_list(self):\n",
    "#         for set_id in tqdm(self.set_names, desc=\"Generating sequences\"):\n",
    "#             if set_id not in self.pie_db:\n",
    "#                 continue\n",
    "#             for video_id, video_data in self.pie_db[set_id].items():\n",
    "#                 ped_ann = video_data.get('ped_annotations', {})\n",
    "#                 for ped_id, ped_data in ped_ann.items():\n",
    "#                     if 'frames' not in ped_data or len(ped_data['frames']) < self.seq_len + self.pred_len:\n",
    "#                         continue\n",
    "#                     frames_sorted = sorted(ped_data['frames'])\n",
    "#                     for i in range(len(frames_sorted) - self.seq_len - self.pred_len + 1):\n",
    "#                         start_f = frames_sorted[i]\n",
    "#                         end_obs = frames_sorted[i + self.seq_len - 1]\n",
    "#                         if end_obs - start_f != self.seq_len - 1:\n",
    "#                             continue\n",
    "#                         target_idx = i + self.seq_len + self.pred_len - 1\n",
    "#                         target_f = frames_sorted[target_idx]\n",
    "#                         if target_f - end_obs != self.pred_len:\n",
    "#                             continue\n",
    "#                         self.sequences.append((set_id, video_id, ped_id, start_f))\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     def __len__(self):\n",
    "#         return len(self.sequences)\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     def __getitem__(self, idx):\n",
    "#         set_id, video_id, ped_id, start_frame = self.sequences[idx]\n",
    "#         frame_nums = list(range(start_frame, start_frame + self.seq_len))\n",
    "#         target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n",
    "\n",
    "#         video_db = self.pie_db.get(set_id, {}).get(video_id, {})\n",
    "#         ped_db = video_db.get('ped_annotations', {}).get(ped_id, {})\n",
    "#         ego_db = video_db.get('vehicle_annotations', {})\n",
    "#         traffic_db = video_db.get('traffic_annotations', {})\n",
    "#         ped_attr = ped_db.get('attributes', {})\n",
    "\n",
    "#         feat_seq = {s: [] for s in self.streams_to_generate}\n",
    "#         label = 0\n",
    "\n",
    "#         if 'frames' in ped_db and 'behavior' in ped_db and 'cross' in ped_db['behavior']:\n",
    "#             try:\n",
    "#                 idx_target = ped_db['frames'].index(target_frame_num)\n",
    "#                 label = ped_db['behavior']['cross'][idx_target]\n",
    "#                 if label == -1:\n",
    "#                     label = 0\n",
    "#             except (ValueError, IndexError):\n",
    "#                 pass\n",
    "\n",
    "#         static_vec = None\n",
    "#         if 'static_context' in self.streams_to_generate:\n",
    "#             sig_idx = ped_attr.get('signalized', 0)\n",
    "#             int_idx = ped_attr.get('intersection', 0)\n",
    "#             age_idx = ped_attr.get('age', 2)\n",
    "#             gen_idx = ped_attr.get('gender', 0)\n",
    "#             td_idx = int(ped_attr.get('traffic_direction', 0))\n",
    "#             nl_val = ped_attr.get('num_lanes', 2)\n",
    "#             nl_idx = LANE_CATEGORIES.get(nl_val, LANE_CATEGORIES[max(LANE_CATEGORIES)])\n",
    "#             static_vec = np.concatenate([\n",
    "#                 to_one_hot(sig_idx, NUM_SIGNALIZED_CATS),\n",
    "#                 to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n",
    "#                 to_one_hot(age_idx, NUM_AGE_CATS),\n",
    "#                 to_one_hot(gen_idx, NUM_GENDER_CATS),\n",
    "#                 to_one_hot(td_idx, NUM_TRAFFIC_DIR_CATS),\n",
    "#                 to_one_hot(nl_idx, NUM_LANE_CATS),\n",
    "#             ]).astype(np.float32)\n",
    "#             if static_vec.shape[0] != INPUT_SIZE_STATIC:\n",
    "#                 static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n",
    "\n",
    "#         for frame in frame_nums:\n",
    "#             frame_idx = -1\n",
    "#             if 'frames' in ped_db:\n",
    "#                 try:\n",
    "#                     frame_idx = ped_db['frames'].index(frame)\n",
    "#                 except ValueError:\n",
    "#                     pass\n",
    "\n",
    "#             ego = ego_db.get(frame, {})\n",
    "\n",
    "#             # ------------------------------ bbox\n",
    "#             if 'bbox' in self.streams_to_generate:\n",
    "#                 bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32)\n",
    "#                 if frame_idx != -1 and 'bbox' in ped_db and len(ped_db['bbox']) > frame_idx:\n",
    "#                     try:\n",
    "#                         x1, y1, x2, y2 = ped_db['bbox'][frame_idx]\n",
    "#                         iw = video_db.get('width', 1920)\n",
    "#                         ih = video_db.get('height', 1080)\n",
    "#                         if iw > 0 and ih > 0:\n",
    "#                             cx = ((x1 + x2) / 2) / iw\n",
    "#                             cy = ((y1 + y2) / 2) / ih\n",
    "#                             w = (x2 - x1) / iw\n",
    "#                             h = (y2 - y1) / ih\n",
    "#                             if 0 <= cx <= 1 and 0 <= cy <= 1 and w > 0 and h > 0:\n",
    "#                                 bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n",
    "#                     except Exception:\n",
    "#                         pass\n",
    "#                 feat_seq['bbox'].append(bbox_norm)\n",
    "\n",
    "#             # ------------------------------ pose\n",
    "#             if 'pose' in self.streams_to_generate:\n",
    "#                 pose_vec = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n",
    "#                 pose_vid = self.all_pose_data.get(set_id, {}).get(video_id, {})\n",
    "#                 pose_frame = pose_vid.get(frame, {})\n",
    "#                 loaded_pose = pose_frame.get(ped_id)\n",
    "#                 if (\n",
    "#                     loaded_pose is not None\n",
    "#                     and isinstance(loaded_pose, np.ndarray)\n",
    "#                     and loaded_pose.shape == (INPUT_SIZE_POSE,)\n",
    "#                 ):\n",
    "#                     pose_vec = loaded_pose\n",
    "#                 feat_seq['pose'].append(pose_vec)\n",
    "\n",
    "#             # ------------------------------ ego_speed\n",
    "#             if 'ego_speed' in self.streams_to_generate:\n",
    "#                 speed = ego.get('OBD_speed', 0.0) or ego.get('GPS_speed', 0.0)\n",
    "#                 sp_scaled = (\n",
    "#                     speed - self.scalers.get('ego_speed_mean', 0.0)\n",
    "#                 ) / self.scalers.get('ego_speed_std', 1.0)\n",
    "#                 feat_seq['ego_speed'].append([sp_scaled])\n",
    "\n",
    "#             # ------------------------------ ego_acc\n",
    "#             if 'ego_acc' in self.streams_to_generate:\n",
    "#                 accX = ego.get('accX', 0.0)\n",
    "#                 accY = ego.get('accY', 0.0)\n",
    "#                 accX_s = (accX - self.scalers.get('accX_mean', 0.0)) / self.scalers.get('accX_std', 1.0)\n",
    "#                 accY_s = (accY - self.scalers.get('accY_mean', 0.0)) / self.scalers.get('accY_std', 1.0)\n",
    "#                 feat_seq['ego_acc'].append([accX_s, accY_s])\n",
    "\n",
    "#             # ------------------------------ ego_gyro\n",
    "#             if 'ego_gyro' in self.streams_to_generate:\n",
    "#                 gyroZ = ego.get('gyroZ', 0.0)\n",
    "#                 gyroZ_s = (gyroZ - self.scalers.get('gyroZ_mean', 0.0)) / self.scalers.get('gyroZ_std', 1.0)\n",
    "#                 feat_seq['ego_gyro'].append([gyroZ_s])\n",
    "\n",
    "#             # ------------------------------ ped_action\n",
    "#             if 'ped_action' in self.streams_to_generate:\n",
    "#                 action = 0\n",
    "#                 if (\n",
    "#                     frame_idx != -1\n",
    "#                     and 'behavior' in ped_db\n",
    "#                     and 'action' in ped_db['behavior']\n",
    "#                     and len(ped_db['behavior']['action']) > frame_idx\n",
    "#                 ):\n",
    "#                     action = ped_db['behavior']['action'][frame_idx]\n",
    "#                 feat_seq['ped_action'].append([float(action)])\n",
    "\n",
    "#             # ------------------------------ ped_look\n",
    "#             if 'ped_look' in self.streams_to_generate:\n",
    "#                 look = 0\n",
    "#                 if (\n",
    "#                     frame_idx != -1\n",
    "#                     and 'behavior' in ped_db\n",
    "#                     and 'look' in ped_db['behavior']\n",
    "#                     and len(ped_db['behavior']['look']) > frame_idx\n",
    "#                 ):\n",
    "#                     look = ped_db['behavior']['look'][frame_idx]\n",
    "#                 feat_seq['ped_look'].append([float(look)])\n",
    "\n",
    "#             # ------------------------------ ped_occlusion\n",
    "#             if 'ped_occlusion' in self.streams_to_generate:\n",
    "#                 occ = 0.0\n",
    "#                 if (\n",
    "#                     frame_idx != -1\n",
    "#                     and 'occlusion' in ped_db\n",
    "#                     and len(ped_db['occlusion']) > frame_idx\n",
    "#                 ):\n",
    "#                     occ_val = ped_db['occlusion'][frame_idx]\n",
    "#                     occ = float(occ_val) / 2.0\n",
    "#                 feat_seq['ped_occlusion'].append([occ])\n",
    "\n",
    "#             # ------------------------------ traffic_light\n",
    "#             if 'traffic_light' in self.streams_to_generate:\n",
    "#                 state_int = 0\n",
    "#                 for obj_id, obj in traffic_db.items():\n",
    "#                     if (\n",
    "#                         obj.get('obj_class') == 'traffic_light'\n",
    "#                         and 'frames' in obj and 'state' in obj\n",
    "#                     ):\n",
    "#                         try:\n",
    "#                             tl_idx = obj['frames'].index(frame)\n",
    "#                             state_val = obj['state'][tl_idx]\n",
    "#                             if state_val != 0:\n",
    "#                                 state_int = state_val\n",
    "#                                 break\n",
    "#                         except (ValueError, IndexError):\n",
    "#                             continue\n",
    "#                 feat_seq['traffic_light'].append(to_one_hot(state_int, NUM_TL_STATES))\n",
    "\n",
    "#             # ------------------------------ static_context\n",
    "#             if 'static_context' in self.streams_to_generate:\n",
    "#                 feat_seq['static_context'].append(\n",
    "#                     static_vec if static_vec is not None else np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n",
    "#                 )\n",
    "\n",
    "#             # ------------------------------ YOLOP\n",
    "#             if 'yolop' in self.streams_to_generate:\n",
    "#                 yolop_vec = np.zeros(INPUT_SIZE_YOLOP, dtype=np.float32)\n",
    "#                 if set_id in self.all_yolop_data and video_id in self.all_yolop_data[set_id]:\n",
    "#                     frame_yolo = self.all_yolop_data[set_id][video_id].get(frame, {})\n",
    "#                     loaded_yolo = frame_yolo.get(ped_id)\n",
    "#                     if (\n",
    "#                         loaded_yolo is not None\n",
    "#                         and isinstance(loaded_yolo, np.ndarray)\n",
    "#                         and loaded_yolo.shape == (INPUT_SIZE_YOLOP,)\n",
    "#                     ):\n",
    "#                         yolop_vec = loaded_yolo\n",
    "#                 feat_seq['yolop'].append(yolop_vec)\n",
    "\n",
    "#         features = {}\n",
    "#         try:\n",
    "#             for s in self.streams_to_generate:\n",
    "#                 features[s] = torch.tensor(np.array(feat_seq[s], dtype=np.float32))\n",
    "#         except Exception as e:\n",
    "#             print(f\"Tensor conversion error idx {idx}: {e}\")\n",
    "#             features = {\n",
    "#                 name: torch.zeros(\n",
    "#                     (self.seq_len, self._input_sizes_for_error.get(name, 1)),\n",
    "#                     dtype=torch.float32,\n",
    "#                 )\n",
    "#                 for name in self.streams_to_generate\n",
    "#             }\n",
    "\n",
    "#         return features, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# # --- Main data-preparation execution ---------------------------------------\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     print(\"--- Running Data Preparation ---\")\n",
    "\n",
    "#     # Load or create PIE database\n",
    "#     if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n",
    "#         if PIE is None:\n",
    "#             raise ImportError(\"PIE class not imported.\")\n",
    "#         print(\"Generating PIE database cache …\")\n",
    "#         pie_intf = PIE(data_path=PIE_ROOT_PATH, regen_database=True)\n",
    "#         pie_database = pie_intf.generate_database()\n",
    "#         if not pie_database:\n",
    "#             raise RuntimeError(\"Failed to generate PIE database.\")\n",
    "#     else:\n",
    "#         print(\"Loading PIE database cache …\")\n",
    "#         with open(PIE_DATABASE_CACHE_PATH, 'rb') as f:\n",
    "#             pie_database = pickle.load(f)\n",
    "\n",
    "#     # Standardization scalers\n",
    "#     print(\"Calculating standardization parameters …\")\n",
    "#     all_sp, all_ax, all_ay, all_gz = [], [], [], []\n",
    "#     for set_id in TRAIN_SETS_STR:\n",
    "#         for vid, vdata in pie_database.get(set_id, {}).items():\n",
    "#             for _, ego in vdata.get('vehicle_annotations', {}).items():\n",
    "#                 speed = ego.get('OBD_speed', 0.0) or ego.get('GPS_speed', 0.0)\n",
    "#                 all_sp.append(speed)\n",
    "#                 all_ax.append(ego.get('accX', 0.0))\n",
    "#                 all_ay.append(ego.get('accY', 0.0))\n",
    "#                 all_gz.append(ego.get('gyroZ', 0.0))\n",
    "\n",
    "#     scalers = {}\n",
    "#     if all_sp:\n",
    "#         scalers['ego_speed_mean'] = np.mean(all_sp)\n",
    "#         scalers['ego_speed_std'] = max(np.std(all_sp), 1e-6)\n",
    "#     if all_ax:\n",
    "#         scalers['accX_mean'] = np.mean(all_ax)\n",
    "#         scalers['accX_std'] = max(np.std(all_ax), 1e-6)\n",
    "#         scalers['accY_mean'] = np.mean(all_ay)\n",
    "#         scalers['accY_std'] = max(np.std(all_ay), 1e-6)\n",
    "#     if all_gz:\n",
    "#         scalers['gyroZ_mean'] = np.mean(all_gz)\n",
    "#         scalers['gyroZ_std'] = max(np.std(all_gz), 1e-6)\n",
    "\n",
    "#     print(\"Initializing full training dataset …\")\n",
    "#     full_train_ds = PIEDataset(\n",
    "#         pie_database,\n",
    "#         TRAIN_SETS_STR,\n",
    "#         POSE_DATA_DIR,\n",
    "#         YOLOP_FEATURE_DIR,\n",
    "#         SEQ_LEN,\n",
    "#         PRED_LEN,\n",
    "#         scalers,\n",
    "#         ALL_POSSIBLE_STREAMS,\n",
    "#     )\n",
    "\n",
    "#     # Extract all features for balancing\n",
    "#     print(\"Extracting data for balancing …\")\n",
    "#     data_dict = {s: [] for s in ALL_POSSIBLE_STREAMS}\n",
    "#     data_dict['label'] = []\n",
    "#     for i in tqdm(range(len(full_train_ds)), desc=\"Extracting\"):\n",
    "#         feats, lbl = full_train_ds[i]\n",
    "#         for s in ALL_POSSIBLE_STREAMS:\n",
    "#             data_dict[s].append(feats[s].numpy())\n",
    "#         data_dict['label'].append([lbl.item()])\n",
    "\n",
    "#     balanced_dict = balance_samples_count(data_dict, 'label')\n",
    "\n",
    "#     # Save outputs\n",
    "#     print(\"Saving balanced data …\")\n",
    "#     with open(BALANCED_DATA_PKL_PATH, 'wb') as f:\n",
    "#         pickle.dump(balanced_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "#     with open(SCALERS_PKL_PATH, 'wb') as f:\n",
    "#         pickle.dump(scalers, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#     del pie_database, full_train_ds, balanced_dict\n",
    "#     gc.collect()\n",
    "#     print(\"--- Data preparation complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f37ad47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:28:27.117818Z",
     "iopub.status.busy": "2025-05-07T09:28:27.117572Z",
     "iopub.status.idle": "2025-05-07T09:28:27.138068Z",
     "shell.execute_reply": "2025-05-07T09:28:27.136409Z"
    },
    "papermill": {
     "duration": 0.037266,
     "end_time": "2025-05-07T09:28:27.139512",
     "exception": false,
     "start_time": "2025-05-07T09:28:27.102246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # --- IMPORTS ---\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader, Subset # Import Subset\n",
    "# import xml.etree.ElementTree as ET\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from tqdm.notebook import tqdm\n",
    "# import random\n",
    "# import math\n",
    "# import zipfile\n",
    "# import cv2\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import pickle\n",
    "# import time\n",
    "# import sys\n",
    "# import gc\n",
    "\n",
    "# # --- Add PIE utilities path if necessary (adjust path) ---\n",
    "# pie_utilities_path = '/kaggle/working/PIE/utilities'\n",
    "# if pie_utilities_path not in sys.path:\n",
    "#     sys.path.insert(0, pie_utilities_path)\n",
    "# PIE = None # Declare PIE before try block\n",
    "# try:\n",
    "#     from pie_data import PIE\n",
    "# except ImportError as e:\n",
    "#     print(f\"Warning: Could not import PIE class from {pie_utilities_path}. Database must already exist. Error: {e}\")\n",
    "#     # PIE remains None, handled later if needed\n",
    "\n",
    "# # --- Configuration ---\n",
    "# PIE_ROOT_PATH = '/kaggle/working/PIE'\n",
    "# VIDEO_INPUT_DIR = '/kaggle/input'\n",
    "# POSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2'\n",
    "# PIE_DATABASE_CACHE_PATH = '/kaggle/input/pie-database/pie_database.pkl'\n",
    "# YOLOP_FEATURE_DIR = '/kaggle/input/yolop-data/yolop features'\n",
    "\n",
    "# # --- Define ALL possible streams (used by Dataset class) ---\n",
    "# ALL_POSSIBLE_STREAMS = [\n",
    "#     'bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro',\n",
    "#     'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context',\n",
    "#     'yolop'\n",
    "# ]\n",
    "# print(f\"All possible streams: {ALL_POSSIBLE_STREAMS}\")\n",
    "\n",
    "# # --- *** CHOOSE ACTIVE STREAMS FOR THIS EXPERIMENT RUN *** ---\n",
    "# ACTIVE_STREAMS = [\n",
    "#     'bbox',\n",
    "#     'ped_action',\n",
    "#     'ped_look',\n",
    "#     'ego_speed',\n",
    "#     'ego_acc',\n",
    "#     'yolop'\n",
    "# ]\n",
    "# print(f\"--- Running Experiment With Active Streams: {ACTIVE_STREAMS} ---\")\n",
    "# # --- *** END ACTIVE STREAM SELECTION *** ---\n",
    "\n",
    "# # --- Model Hyperparameters ---\n",
    "# SEQ_LEN = 30\n",
    "# PRED_LEN = 1\n",
    "# # --- Input Sizes ---\n",
    "# INPUT_SIZE_BBOX = 4\n",
    "# INPUT_SIZE_POSE = 34\n",
    "# INPUT_SIZE_EGO_SPEED = 1\n",
    "# INPUT_SIZE_EGO_ACC = 2\n",
    "# INPUT_SIZE_EGO_GYRO = 1\n",
    "# INPUT_SIZE_PED_ACTION = 1\n",
    "# INPUT_SIZE_PED_LOOK = 1\n",
    "# INPUT_SIZE_PED_OCC = 1\n",
    "# INPUT_SIZE_TL_STATE = 4\n",
    "# NUM_SIGNALIZED_CATS = 4\n",
    "# NUM_INTERSECTION_CATS = 5\n",
    "# NUM_AGE_CATS = 4\n",
    "# NUM_GENDER_CATS = 3\n",
    "# NUM_TRAFFIC_DIR_CATS = 2\n",
    "# LANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7:4, 8:4}\n",
    "# NUM_LANE_CATS = len(set(LANE_CATEGORIES.values()))\n",
    "# INPUT_SIZE_STATIC = NUM_SIGNALIZED_CATS + NUM_INTERSECTION_CATS + NUM_AGE_CATS + NUM_GENDER_CATS + NUM_TRAFFIC_DIR_CATS + NUM_LANE_CATS\n",
    "# GRID_SIZE = 3\n",
    "# INPUT_SIZE_YOLOP = GRID_SIZE**2 * 2 + 2 # 20\n",
    "\n",
    "# LSTM_HIDDEN_SIZE = 256\n",
    "# NUM_LSTM_LAYERS = 2\n",
    "# DROPOUT_RATE = 0.3\n",
    "# NUM_CLASSES = 2\n",
    "# ATTENTION_DIM = 128\n",
    "\n",
    "# # --- Training Hyperparameters ---\n",
    "# LEARNING_RATE = 1e-4\n",
    "# BATCH_SIZE = 32\n",
    "# NUM_EPOCHS = 15\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# # --- Dataset Splits ---\n",
    "# TRAIN_SETS_STR = ['set01', 'set02', 'set04']\n",
    "# VAL_SETS_STR = ['set05','set06']\n",
    "\n",
    "# # --- Mappings ---\n",
    "# TL_STATE_MAP = {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}\n",
    "# NUM_TL_STATES = len(TL_STATE_MAP)\n",
    "# SIGNALIZED_MAP = {'n/a': 0, 'C': 1, 'S': 2, 'CS': 3}\n",
    "# INTERSECTION_MAP = {'midblock': 0, 'T': 1, 'T-left': 2, 'T-right': 3, 'four-way': 4}\n",
    "# AGE_MAP = {'child': 0, 'young': 1, 'adult': 2, 'senior': 3}\n",
    "# GENDER_MAP = {'n/a': 0, 'female': 1, 'male': 2}\n",
    "# TRAFFIC_DIR_MAP = {'OW': 0, 'TW': 1}\n",
    "\n",
    "# # --- Output Files (for intermediate balanced data/scalers) ---\n",
    "# BALANCED_DATA_PKL_PATH = \"/kaggle/working/balanced_train_data_with_yolop.pkl\"\n",
    "# SCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\"\n",
    "\n",
    "# # --- Helper: One-Hot Encoding ---\n",
    "# def to_one_hot(index, num_classes):\n",
    "#     vec = np.zeros(num_classes, dtype=np.float32)\n",
    "#     safe_index = int(np.clip(index, 0, num_classes - 1))\n",
    "#     vec[safe_index] = 1.0\n",
    "#     return vec\n",
    "\n",
    "# # --- Balancing Function ---\n",
    "# def balance_samples_count(seq_data, label_type, random_seed=42):\n",
    "#     print('---------------------------------------------------------')\n",
    "#     print(f\"Balancing samples based on '{label_type}' key\")\n",
    "#     if label_type not in seq_data: raise KeyError(f\"Label type '{label_type}' not found.\")\n",
    "\n",
    "#     gt_labels = [] # Initialize before try block\n",
    "#     try:\n",
    "#         gt_labels = [lbl[0] for lbl in seq_data[label_type]]\n",
    "#     except (IndexError, TypeError) as e:\n",
    "#         raise ValueError(f\"Labels under '{label_type}' not in expected format [[label_val]]. Original error: {e}\") from e\n",
    "\n",
    "#     if not gt_labels: # Check if gt_labels was successfully populated\n",
    "#         print(f\"Warning: No valid labels found under key '{label_type}'. Cannot balance.\")\n",
    "#         return seq_data.copy() # Return original data if labels are missing/malformed\n",
    "\n",
    "#     if not all(isinstance(l, (int, float)) and l in [0, 1] for l in gt_labels):\n",
    "#         print(f\"Warning: Labels for balancing contain values other than 0 or 1 or are not numeric.\")\n",
    "#         # Decide how to handle non 0/1 labels if necessary, here we proceed assuming they exist\n",
    "\n",
    "#     num_pos_samples = np.count_nonzero(np.array(gt_labels)); num_neg_samples = len(gt_labels) - num_pos_samples\n",
    "#     new_seq_data = {}\n",
    "#     if num_neg_samples == num_pos_samples:\n",
    "#         print('Samples already balanced.'); return seq_data.copy()\n",
    "#     else:\n",
    "#         print(f'Unbalanced: Positive (1): {num_pos_samples} | Negative (0): {num_neg_samples}')\n",
    "#         majority_label = 0 if num_neg_samples > num_pos_samples else 1\n",
    "#         minority_count = min(num_neg_samples, num_pos_samples); print(f\"Undersampling majority class ({majority_label}) to match count ({minority_count}).\")\n",
    "#         majority_indices = np.where(np.array(gt_labels) == majority_label)[0]; minority_indices = np.where(np.array(gt_labels) != majority_label)[0]\n",
    "#         np.random.seed(random_seed); keep_majority_indices = np.random.choice(majority_indices, size=minority_count, replace=False)\n",
    "#         final_indices = np.concatenate((minority_indices, keep_majority_indices)); np.random.shuffle(final_indices)\n",
    "\n",
    "#         for k, v_list in seq_data.items():\n",
    "#             if isinstance(v_list, list) and len(v_list) == len(gt_labels):\n",
    "#                  try:\n",
    "#                      # Check if list contains numpy arrays before converting the whole list\n",
    "#                      if v_list and isinstance(v_list[0], np.ndarray):\n",
    "#                          # Ensure all elements are arrays of compatible shape if needed,\n",
    "#                          # or handle potential errors during conversion\n",
    "#                          try:\n",
    "#                              v_array = np.array(v_list) # This might fail if arrays have different shapes\n",
    "#                              new_seq_data[k] = list(v_array[final_indices])\n",
    "#                          except ValueError as ve:\n",
    "#                              print(f\"Warning: Could not convert list for key '{k}' to single NumPy array due to varying shapes/types. Processing element-wise. Error: {ve}\")\n",
    "#                              # Fallback to list comprehension if conversion fails\n",
    "#                              new_seq_data[k] = [v_list[i] for i in final_indices]\n",
    "#                      else:\n",
    "#                          # Simple list comprehension for non-array lists\n",
    "#                          new_seq_data[k] = [v_list[i] for i in final_indices]\n",
    "#                  except Exception as e:\n",
    "#                      # Catch any other unexpected errors during processing\n",
    "#                      print(f\"Error processing key '{k}' during balancing: {e}. Assigning empty list.\")\n",
    "#                      new_seq_data[k] = []\n",
    "#             else:\n",
    "#                  # Keep lists that don't match the label length (e.g., metadata)\n",
    "#                  print(f\"Warn: Skipping key '{k}' in balancing (length mismatch or not a list).\")\n",
    "#                  new_seq_data[k] = v_list # Keep original\n",
    "\n",
    "#         if label_type in new_seq_data:\n",
    "#             # Recalculate counts from the balanced data\n",
    "#             new_gt_labels = []\n",
    "#             try:\n",
    "#                 new_gt_labels = [lbl[0] for lbl in new_seq_data[label_type]]\n",
    "#                 final_pos = np.count_nonzero(np.array(new_gt_labels)); final_neg = len(new_gt_labels) - final_pos;\n",
    "#                 print(f'Balanced:   Positive (1): {final_pos} | Negative (0): {final_neg}')\n",
    "#             except (IndexError, TypeError, ValueError) as e:\n",
    "#                  print(f\"Error verifying balanced labels for key '{label_type}': {e}\")\n",
    "#         else:\n",
    "#             print(\"Error: Label key lost during balancing process.\")\n",
    "#         print('---------------------------------------------------------')\n",
    "#         return new_seq_data\n",
    "\n",
    "# # --- Dataset Class ---\n",
    "# class PIEDataset(Dataset):\n",
    "#     def __init__(self, pie_database, set_names, pose_data_dir, yolop_data_dir, seq_len, pred_len, scalers=None, streams_to_generate=None):\n",
    "#         self.pie_db = pie_database; self.set_names = set_names; self.pose_data_dir = pose_data_dir; self.yolop_data_dir = yolop_data_dir\n",
    "#         self.seq_len = seq_len; self.pred_len = pred_len; self.scalers = scalers or {};\n",
    "#         self.streams_to_generate = streams_to_generate or ALL_POSSIBLE_STREAMS\n",
    "#         self.sequences = []; self.all_pose_data = {}; self.all_yolop_data = {}\n",
    "#         self._input_sizes_for_error = self._get_input_sizes_dict()\n",
    "#         if 'pose' in self.streams_to_generate: self._load_pose_data()\n",
    "#         if 'yolop' in self.streams_to_generate: self._load_yolop_data()\n",
    "#         self._generate_sequence_list()\n",
    "#         if not self.sequences: raise ValueError(f\"Dataset init failed: No sequences generated for sets {self.set_names}\")\n",
    "\n",
    "#     def _get_input_sizes_dict(self):\n",
    "#         input_sizes = {}; special_cases = {'TRAFFIC_LIGHT': 'TL_STATE', 'STATIC_CONTEXT': 'STATIC', 'EGO_SPEED': 'EGO_SPEED', 'EGO_ACC': 'EGO_ACC', 'EGO_GYRO': 'EGO_GYRO', 'PED_ACTION': 'PED_ACTION', 'PED_LOOK': 'PED_LOOK', 'PED_OCCLUSION': 'PED_OCC', 'YOLOP':'YOLOP'}\n",
    "#         for stream in ALL_POSSIBLE_STREAMS:\n",
    "#             size_constant_name = f'INPUT_SIZE_{stream.upper()}'; stream_upper_key = stream.upper(); suffix = special_cases.get(stream_upper_key)\n",
    "#             if suffix: size_constant_name = f'INPUT_SIZE_{suffix}'\n",
    "#             elif stream == 'bbox': size_constant_name = 'INPUT_SIZE_BBOX'\n",
    "#             elif stream == 'pose': size_constant_name = 'INPUT_SIZE_POSE'\n",
    "#             if size_constant_name in globals(): input_sizes[stream] = globals()[size_constant_name]\n",
    "#             else: input_sizes[stream] = 1 # Default size if constant not found\n",
    "#         return input_sizes\n",
    "\n",
    "#     def _load_pose_data(self):\n",
    "#         print(f\"\\nLoading pose data for sets: {self.set_names} from {self.pose_data_dir}\"); sets_loaded_count = 0\n",
    "#         for set_id in self.set_names:\n",
    "#             self.all_pose_data[set_id] = {}; pose_set_path = os.path.join(self.pose_data_dir, set_id)\n",
    "#             if not os.path.isdir(pose_set_path):\n",
    "#                 # print(f\"Warning: Pose directory not found for set {set_id}: {pose_set_path}\") # Optional warning\n",
    "#                 continue\n",
    "#             pkl_files_in_set = [f for f in os.listdir(pose_set_path) if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")];\n",
    "#             if not pkl_files_in_set:\n",
    "#                 # print(f\"Warning: No pose pkl files found in {pose_set_path}\") # Optional warning\n",
    "#                 continue;\n",
    "#             loaded_video_count = 0\n",
    "#             for pkl_filename in pkl_files_in_set:\n",
    "#                 pkl_file_path = os.path.join(pose_set_path, pkl_filename);\n",
    "#                 try:\n",
    "#                     with open(pkl_file_path, 'rb') as f: loaded_pkl_content = pickle.load(f)\n",
    "#                     if not isinstance(loaded_pkl_content, dict) or len(loaded_pkl_content) != 1:\n",
    "#                          print(f\"Warning: Unexpected format in pose PKL {pkl_file_path}. Skipping.\")\n",
    "#                          continue\n",
    "#                     unique_video_key, video_data = list(loaded_pkl_content.items())[0]; video_id = \"_\".join(unique_video_key.split('_')[1:])\n",
    "#                     # Check if this video is actually in our PIE database for this set\n",
    "#                     if set_id in self.pie_db and video_id in self.pie_db[set_id]:\n",
    "#                          self.all_pose_data[set_id][video_id] = video_data; loaded_video_count += 1\n",
    "#                     # else: # Optional: Warn if pose data exists but video isn't in DB\n",
    "#                     #    print(f\"Debug: Pose data found for {set_id}/{video_id} but video not in main pie_db. Skipping.\")\n",
    "#                 except FileNotFoundError:\n",
    "#                     # This specific error is less likely now due to os.listdir, but keep for robustness\n",
    "#                     print(f\"Warning: Pose file not found during loading (should not happen after listdir): {pkl_file_path}\")\n",
    "#                 except pickle.UnpicklingError as pe:\n",
    "#                      print(f\"Error unpickling pose file {pkl_file_path}: {pe}. Skipping.\")\n",
    "#                 except Exception as e:\n",
    "#                      print(f\"Error loading or processing pose PKL {pkl_file_path}: {e}\")\n",
    "#             if loaded_video_count > 0: sets_loaded_count += 1\n",
    "#         print(f\"Finished loading pose data for {sets_loaded_count} relevant sets.\")\n",
    "\n",
    "#     def _load_yolop_data(self):\n",
    "#         print(f\"\\nLoading YOLOP data for sets: {self.set_names} from {self.yolop_data_dir}\"); sets_loaded_count = 0\n",
    "#         for set_id in self.set_names: self.all_yolop_data[set_id] = {} # Initialize dict for each set\n",
    "\n",
    "#         if not os.path.isdir(self.yolop_data_dir):\n",
    "#             print(f\"Error: YOLOP feature directory not found: {self.yolop_data_dir}\"); return\n",
    "\n",
    "#         all_pkl_files = [] # Initialize before try block\n",
    "#         try:\n",
    "#             all_pkl_files = [f for f in os.listdir(self.yolop_data_dir) if f.endswith(\"_yolop_features.pkl\")]\n",
    "#         except FileNotFoundError: # More specific than general Exception\n",
    "#              print(f\"Error: YOLOP feature directory not found when listing files: {self.yolop_data_dir}\"); return\n",
    "#         except PermissionError:\n",
    "#              print(f\"Error: Permission denied when listing files in YOLOP directory {self.yolop_data_dir}\"); return\n",
    "#         except Exception as e:\n",
    "#              print(f\"Error listing files in YOLOP directory {self.yolop_data_dir}: {e}\"); return\n",
    "\n",
    "#         if not all_pkl_files: print(f\"Warning: No '*_yolop_features.pkl' files found directly in {self.yolop_data_dir}\")\n",
    "\n",
    "#         loaded_file_count = 0; files_for_needed_sets = 0\n",
    "#         for pkl_filename in tqdm(all_pkl_files, desc=\"Loading YOLOP PKLs\"):\n",
    "#             set_id_from_file = None # Initialize before try\n",
    "#             video_id = None # Initialize before try\n",
    "#             try:\n",
    "#                 parts = pkl_filename.replace(\"_yolop_features.pkl\", \"\").split('_');\n",
    "#                 if len(parts) < 2: # Need at least setX_videoY\n",
    "#                      raise IndexError(\"Filename does not contain enough parts for set and video ID.\")\n",
    "#                 set_id_from_file = parts[0]; video_id = \"_\".join(parts[1:])\n",
    "#             except IndexError as e: # Catch specific error\n",
    "#                 print(f\"Warning: Could not parse set/video ID from filename '{pkl_filename}': {e}. Skipping.\"); continue\n",
    "\n",
    "#             if set_id_from_file in self.set_names:\n",
    "#                 files_for_needed_sets += 1; pkl_file_path = os.path.join(self.yolop_data_dir, pkl_filename)\n",
    "#                 try:\n",
    "#                     with open(pkl_file_path, 'rb') as f: loaded_pkl_content = pickle.load(f)\n",
    "#                     # Basic validation of loaded content structure\n",
    "#                     if not isinstance(loaded_pkl_content, dict) or len(loaded_pkl_content) != 1:\n",
    "#                         print(f\"Warn: PKL {pkl_filename} has unexpected format (expected dict with 1 item). Skip.\"); continue\n",
    "#                     unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n",
    "#                     # Key doesn't strictly need to match video_id if format is {internal_key: data}\n",
    "#                     # Ensure the set_id exists in the outer dictionary\n",
    "#                     if set_id_from_file not in self.all_yolop_data:\n",
    "#                         self.all_yolop_data[set_id_from_file] = {}\n",
    "#                     self.all_yolop_data[set_id_from_file][video_id] = video_data; loaded_file_count += 1\n",
    "#                 except FileNotFoundError:\n",
    "#                     print(f\"Warn: YOLOP feature file not found during loading: {pkl_file_path}\")\n",
    "#                 except pickle.UnpicklingError as pe:\n",
    "#                      print(f\"Error unpickling YOLOP file {pkl_file_path}: {pe}. Skipping.\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error loading or processing YOLOP PKL {pkl_file_path}: {e}\")\n",
    "\n",
    "#         print(f\"Finished loading YOLOP data. Found {loaded_file_count} relevant files out of {files_for_needed_sets} possible for sets {self.set_names}.\")\n",
    "#         if loaded_file_count > 0:\n",
    "#             actual_sets_loaded = sum(1 for s in self.set_names if s in self.all_yolop_data and self.all_yolop_data[s])\n",
    "#             print(f\"Data successfully loaded for {actual_sets_loaded} sets.\")\n",
    "\n",
    "#     def _generate_sequence_list(self):\n",
    "#         sequence_count = 0; ped_count = 0\n",
    "#         for set_id in tqdm(self.set_names, desc=f\"Generating Sequences for {self.set_names}\"):\n",
    "#             if set_id not in self.pie_db: continue\n",
    "#             for video_id, video_data in self.pie_db[set_id].items():\n",
    "#                 if 'ped_annotations' not in video_data: continue\n",
    "#                 for ped_id, ped_data in video_data['ped_annotations'].items():\n",
    "#                     ped_count += 1;\n",
    "#                     if 'frames' not in ped_data or not isinstance(ped_data['frames'], list) or len(ped_data['frames']) < self.seq_len + self.pred_len: continue\n",
    "#                     # Ensure frames are sorted and integers\n",
    "#                     try:\n",
    "#                         sorted_frames = sorted([int(f) for f in ped_data['frames']])\n",
    "#                     except (ValueError, TypeError):\n",
    "#                         print(f\"Warning: Non-integer or unsortable frame numbers for {set_id}/{video_id}/{ped_id}. Skipping ped.\")\n",
    "#                         continue\n",
    "\n",
    "#                     for i in range(len(sorted_frames) - self.seq_len - self.pred_len + 1):\n",
    "#                         start_frame = sorted_frames[i]; end_frame_observe = sorted_frames[i + self.seq_len - 1]\n",
    "#                         # Check for frame continuity in observation window\n",
    "#                         if end_frame_observe - start_frame != self.seq_len - 1: continue\n",
    "\n",
    "#                         target_frame_actual_idx = i + self.seq_len + self.pred_len - 1\n",
    "#                         # Check index bounds (already implicitly handled by range end, but explicit check is fine)\n",
    "#                         if target_frame_actual_idx >= len(sorted_frames): continue\n",
    "#                         target_frame = sorted_frames[target_frame_actual_idx]\n",
    "\n",
    "#                         # Check for frame continuity up to target frame\n",
    "#                         if target_frame - end_frame_observe != self.pred_len: continue\n",
    "\n",
    "#                         self.sequences.append((set_id, video_id, ped_id, start_frame)); sequence_count += 1\n",
    "#         print(f\"Dataset initialized with {sequence_count} sequences from {ped_count} pedestrians for sets {self.set_names}.\")\n",
    "\n",
    "#     def __len__(self): return len(self.sequences)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         set_id, video_id, ped_id, start_frame = self.sequences[idx];\n",
    "#         frame_nums = list(range(start_frame, start_frame + self.seq_len));\n",
    "#         target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n",
    "\n",
    "#         # Safely access nested dictionary structure\n",
    "#         video_db = self.pie_db.get(set_id, {}).get(video_id, {});\n",
    "#         ped_db = video_db.get('ped_annotations', {}).get(ped_id, {});\n",
    "#         ego_db = video_db.get('vehicle_annotations', {});\n",
    "#         traffic_db = video_db.get('traffic_annotations', {});\n",
    "#         ped_attributes = ped_db.get('attributes', {})\n",
    "\n",
    "#         feature_sequences = {stream: [] for stream in self.streams_to_generate};\n",
    "#         label = 0 # Default label\n",
    "\n",
    "#         # --- Safely Extract Label ---\n",
    "#         target_frame_db_idx = -1 # Initialize before try\n",
    "#         if 'frames' in ped_db and isinstance(ped_db['frames'], list) and \\\n",
    "#            'behavior' in ped_db and isinstance(ped_db['behavior'], dict) and \\\n",
    "#            'cross' in ped_db['behavior'] and isinstance(ped_db['behavior']['cross'], list):\n",
    "#             try:\n",
    "#                 # Ensure target_frame_num is comparable to elements in ped_db['frames']\n",
    "#                 target_frame_num_comp = int(target_frame_num) # Or str if frames are strings\n",
    "#                 target_frame_db_idx = ped_db['frames'].index(target_frame_num_comp)\n",
    "#                 if target_frame_db_idx < len(ped_db['behavior']['cross']):\n",
    "#                     label_val = ped_db['behavior']['cross'][target_frame_db_idx];\n",
    "#                     # Handle potential non-integer labels or placeholder -1\n",
    "#                     if isinstance(label_val, (int, float)) and label_val == 1:\n",
    "#                         label = 1\n",
    "#                     else:\n",
    "#                         label = 0 # Treat -1 or other values as 0 (not crossing)\n",
    "#                 # else: label remains 0 if index out of bounds for 'cross' list\n",
    "#             except (ValueError, TypeError, IndexError) as e:\n",
    "#                  # ValueError if target_frame_num not in list\n",
    "#                  # TypeError if list contains non-comparable types\n",
    "#                  # IndexError if list is empty or other issues (less likely here)\n",
    "#                  # print(f\"Debug: Label not found for idx {idx}, frame {target_frame_num}. Error: {e}\") # Optional debug\n",
    "#                  label = 0 # Keep default label if lookup fails\n",
    "\n",
    "#         # --- Extract Static Context ---\n",
    "#         static_vec = None # Initialize before conditional block\n",
    "#         if 'static_context' in self.streams_to_generate:\n",
    "#             # Use .get with defaults for safety\n",
    "#             sig_idx = SIGNALIZED_MAP.get(ped_attributes.get('signalized', 'n/a'), 0)\n",
    "#             int_idx = INTERSECTION_MAP.get(ped_attributes.get('intersection', 'midblock'), 0)\n",
    "#             age_idx = AGE_MAP.get(ped_attributes.get('age', 'adult'), 2)\n",
    "#             gen_idx = GENDER_MAP.get(ped_attributes.get('gender', 'n/a'), 0)\n",
    "#             td_val = ped_attributes.get('traffic_direction', 'OW')\n",
    "#             td_idx = TRAFFIC_DIR_MAP.get(td_val, 0)\n",
    "#             nl_val = ped_attributes.get('num_lanes', 2)\n",
    "#             # Ensure nl_val is usable as a key or default\n",
    "#             nl_cat_idx = LANE_CATEGORIES.get(int(nl_val), LANE_CATEGORIES[max(LANE_CATEGORIES.keys())]) if isinstance(nl_val, (int, str)) and str(nl_val).isdigit() else LANE_CATEGORIES[max(LANE_CATEGORIES.keys())]\n",
    "\n",
    "#             try:\n",
    "#                 static_features_list = [\n",
    "#                     to_one_hot(sig_idx, NUM_SIGNALIZED_CATS), to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n",
    "#                     to_one_hot(age_idx, NUM_AGE_CATS), to_one_hot(gen_idx, NUM_GENDER_CATS),\n",
    "#                     to_one_hot(td_idx, NUM_TRAFFIC_DIR_CATS), to_one_hot(nl_cat_idx, NUM_LANE_CATS)\n",
    "#                 ]\n",
    "#                 static_vec = np.concatenate(static_features_list)\n",
    "#                 if static_vec.shape[0] != INPUT_SIZE_STATIC:\n",
    "#                     print(f\"Warning: Static vector size mismatch for idx {idx}. Expected {INPUT_SIZE_STATIC}, got {static_vec.shape[0]}. Using zeros.\")\n",
    "#                     static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n",
    "#             except Exception as e:\n",
    "#                  print(f\"Error creating static context vector for idx {idx}: {e}. Using zeros.\")\n",
    "#                  static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n",
    "\n",
    "\n",
    "#         # --- Extract Sequential Features ---\n",
    "#         for frame_num in frame_nums:\n",
    "#             frame_db_idx = -1 # Default index if frame not found for ped\n",
    "#             if 'frames' in ped_db and isinstance(ped_db['frames'], list):\n",
    "#                  try:\n",
    "#                      frame_num_comp = int(frame_num) # Or str if frames are strings\n",
    "#                      frame_db_idx = ped_db['frames'].index(frame_num_comp)\n",
    "#                  except (ValueError, TypeError):\n",
    "#                      pass # Keep frame_db_idx as -1 if frame not found or type mismatch\n",
    "\n",
    "#             ego_frame_data = ego_db.get(frame_num, {}) # Use .get for safety\n",
    "\n",
    "#             # --- Bbox ---\n",
    "#             if 'bbox' in self.streams_to_generate:\n",
    "#                 bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32); # Default\n",
    "#                 if frame_db_idx != -1 and 'bbox' in ped_db and isinstance(ped_db['bbox'], list) and len(ped_db['bbox']) > frame_db_idx:\n",
    "#                      try:\n",
    "#                          bbox_coords = ped_db['bbox'][frame_db_idx]\n",
    "#                          # Check if bbox_coords is a valid list/tuple of 4 numbers\n",
    "#                          if isinstance(bbox_coords, (list, tuple)) and len(bbox_coords) == 4 and all(isinstance(n, (int, float)) for n in bbox_coords):\n",
    "#                              x1, y1, x2, y2 = bbox_coords\n",
    "#                              img_w = video_db.get('width', 1920); img_h = video_db.get('height', 1080)\n",
    "#                              # Ensure width/height are valid numbers > 0\n",
    "#                              if isinstance(img_w, (int, float)) and img_w > 0 and isinstance(img_h, (int, float)) and img_h > 0:\n",
    "#                                  cx = ((x1 + x2) / 2) / img_w; cy = ((y1 + y2) / 2) / img_h;\n",
    "#                                  w = (x2 - x1) / img_w; h = (y2 - y1) / img_h;\n",
    "#                                  # Basic validation of normalized coordinates\n",
    "#                                  if w>0 and h>0 and 0<=cx<=1 and 0<=cy<=1:\n",
    "#                                      bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n",
    "#                                  # else: print(f\"Debug: Invalid normalized bbox values idx {idx} frame {frame_num}\") # Optional debug\n",
    "#                          # else: print(f\"Debug: Invalid bbox data format idx {idx} frame {frame_num}\") # Optional debug\n",
    "#                      except (TypeError, ValueError, ZeroDivisionError) as e: # Catch specific errors\n",
    "#                           # TypeError if coords are not numbers, ValueError if unpacking fails (less likely with check)\n",
    "#                           # ZeroDivisionError if img_w/h is 0\n",
    "#                           print(f\"Warn: Error processing bbox idx {idx} frame {frame_num}: {e}. Using zeros.\")\n",
    "#                           bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32) # Ensure default on error\n",
    "#                      except Exception as e: # Catch any other unexpected error\n",
    "#                           print(f\"Warn: Unexpected error processing bbox idx {idx} frame {frame_num}: {e}. Using zeros.\")\n",
    "#                           bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32)\n",
    "#                 feature_sequences['bbox'].append(bbox_norm)\n",
    "\n",
    "#             # --- Pose ---\n",
    "#             if 'pose' in self.streams_to_generate:\n",
    "#                 pose_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32); # Default\n",
    "#                 # Safely access nested pose data structure\n",
    "#                 vid_pose_data = self.all_pose_data.get(set_id, {}).get(video_id, {});\n",
    "#                 frame_pose_data = vid_pose_data.get(frame_num, {});\n",
    "#                 loaded_pose = frame_pose_data.get(ped_id)\n",
    "#                 if loaded_pose is not None and isinstance(loaded_pose, np.ndarray) and loaded_pose.shape == (INPUT_SIZE_POSE,):\n",
    "#                     pose_vector = loaded_pose # Use loaded data if valid\n",
    "#                 feature_sequences['pose'].append(pose_vector)\n",
    "\n",
    "#             # --- Ego Speed ---\n",
    "#             if 'ego_speed' in self.streams_to_generate:\n",
    "#                 speed = 0.0 # Default\n",
    "#                 try:\n",
    "#                     obd_speed = ego_frame_data.get('OBD_speed')\n",
    "#                     gps_speed = ego_frame_data.get('GPS_speed')\n",
    "#                     # Prioritize OBD, fallback to GPS, ensure numeric\n",
    "#                     if isinstance(obd_speed, (int, float)) and obd_speed > 0: # Often 0 means unavailable\n",
    "#                         speed = float(obd_speed)\n",
    "#                     elif isinstance(gps_speed, (int, float)):\n",
    "#                         speed = float(gps_speed)\n",
    "#                     # Apply scaling safely\n",
    "#                     mean = self.scalers.get('ego_speed_mean', 0.0)\n",
    "#                     std = self.scalers.get('ego_speed_std', 1.0)\n",
    "#                     speed_scaled = (speed - mean) / std if std != 0 else 0.0\n",
    "#                 except Exception as e:\n",
    "#                      print(f\"Warn: Error processing ego_speed idx {idx} frame {frame_num}: {e}. Using 0.\")\n",
    "#                      speed_scaled = 0.0\n",
    "#                 feature_sequences['ego_speed'].append([speed_scaled])\n",
    "\n",
    "#             # --- Ego Acc ---\n",
    "#             if 'ego_acc' in self.streams_to_generate:\n",
    "#                 accX_scaled, accY_scaled = 0.0, 0.0 # Defaults\n",
    "#                 try:\n",
    "#                     accX = float(ego_frame_data.get('accX', 0.0)) # Ensure float\n",
    "#                     accY = float(ego_frame_data.get('accY', 0.0)) # Ensure float\n",
    "#                     accX_mean = self.scalers.get('accX_mean', 0.0); accX_std = self.scalers.get('accX_std', 1.0)\n",
    "#                     accY_mean = self.scalers.get('accY_mean', 0.0); accY_std = self.scalers.get('accY_std', 1.0)\n",
    "#                     accX_scaled = (accX - accX_mean) / accX_std if accX_std != 0 else 0.0\n",
    "#                     accY_scaled = (accY - accY_mean) / accY_std if accY_std != 0 else 0.0\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Warn: Error processing ego_acc idx {idx} frame {frame_num}: {e}. Using 0,0.\")\n",
    "#                     accX_scaled, accY_scaled = 0.0, 0.0\n",
    "#                 feature_sequences['ego_acc'].append([accX_scaled, accY_scaled])\n",
    "\n",
    "#             # --- Ego Gyro ---\n",
    "#             if 'ego_gyro' in self.streams_to_generate:\n",
    "#                 gyroZ_scaled = 0.0 # Default\n",
    "#                 try:\n",
    "#                     gyroZ = float(ego_frame_data.get('gyroZ', 0.0)) # Ensure float\n",
    "#                     mean = self.scalers.get('gyroZ_mean', 0.0); std = self.scalers.get('gyroZ_std', 1.0)\n",
    "#                     gyroZ_scaled = (gyroZ - mean) / std if std != 0 else 0.0\n",
    "#                 except Exception as e:\n",
    "#                      print(f\"Warn: Error processing ego_gyro idx {idx} frame {frame_num}: {e}. Using 0.\")\n",
    "#                      gyroZ_scaled = 0.0\n",
    "#                 feature_sequences['ego_gyro'].append([gyroZ_scaled])\n",
    "\n",
    "#             # --- Ped Action ---\n",
    "#             if 'ped_action' in self.streams_to_generate:\n",
    "#                 action = 0.0 # Default\n",
    "#                 if frame_db_idx != -1 and 'behavior' in ped_db and isinstance(ped_db['behavior'], dict) and \\\n",
    "#                    'action' in ped_db['behavior'] and isinstance(ped_db['behavior']['action'], list) and \\\n",
    "#                    len(ped_db['behavior']['action']) > frame_db_idx:\n",
    "#                    try:\n",
    "#                        action_val = ped_db['behavior']['action'][frame_db_idx]\n",
    "#                        action = float(action_val) # Convert to float, handle potential errors\n",
    "#                    except (ValueError, TypeError) as e:\n",
    "#                        print(f\"Warn: Invalid ped_action value idx {idx} frame {frame_num}: {action_val}. Error: {e}. Using 0.\")\n",
    "#                        action = 0.0\n",
    "#                 feature_sequences['ped_action'].append([action])\n",
    "\n",
    "#             # --- Ped Look ---\n",
    "#             if 'ped_look' in self.streams_to_generate:\n",
    "#                 look = 0.0 # Default\n",
    "#                 if frame_db_idx != -1 and 'behavior' in ped_db and isinstance(ped_db['behavior'], dict) and \\\n",
    "#                    'look' in ped_db['behavior'] and isinstance(ped_db['behavior']['look'], list) and \\\n",
    "#                    len(ped_db['behavior']['look']) > frame_db_idx:\n",
    "#                    try:\n",
    "#                        look_val = ped_db['behavior']['look'][frame_db_idx]\n",
    "#                        look = float(look_val) # Convert to float\n",
    "#                    except (ValueError, TypeError) as e:\n",
    "#                        print(f\"Warn: Invalid ped_look value idx {idx} frame {frame_num}: {look_val}. Error: {e}. Using 0.\")\n",
    "#                        look = 0.0\n",
    "#                 feature_sequences['ped_look'].append([look])\n",
    "\n",
    "#             # --- Ped Occlusion ---\n",
    "#             if 'ped_occlusion' in self.streams_to_generate:\n",
    "#                 occ = 0.0; # Default (no occlusion)\n",
    "#                 if frame_db_idx != -1 and 'occlusion' in ped_db and isinstance(ped_db['occlusion'], list) and \\\n",
    "#                    len(ped_db['occlusion']) > frame_db_idx:\n",
    "#                    try:\n",
    "#                        occ_val = ped_db['occlusion'][frame_db_idx];\n",
    "#                        # Assuming occlusion is 0, 1, 2 -> map to 0.0, 0.5, 1.0\n",
    "#                        occ = float(occ_val) / 2.0 if isinstance(occ_val, (int, float)) else 0.0\n",
    "#                    except (ValueError, TypeError) as e:\n",
    "#                         print(f\"Warn: Invalid ped_occlusion value idx {idx} frame {frame_num}: {occ_val}. Error: {e}. Using 0.\")\n",
    "#                         occ = 0.0\n",
    "#                 feature_sequences['ped_occlusion'].append([occ])\n",
    "\n",
    "#             # --- Traffic Light ---\n",
    "#             if 'traffic_light' in self.streams_to_generate:\n",
    "#                 state_int = 0 # Default state (__undefined__)\n",
    "#                 if isinstance(traffic_db, dict): # Check traffic_db is a dict\n",
    "#                     for obj_id, obj_data in traffic_db.items():\n",
    "#                          if isinstance(obj_data, dict) and obj_data.get('obj_class') == 'traffic_light' and \\\n",
    "#                             'frames' in obj_data and isinstance(obj_data['frames'], list) and \\\n",
    "#                             'state' in obj_data and isinstance(obj_data['state'], list):\n",
    "#                               try:\n",
    "#                                   frame_num_comp = int(frame_num) # Ensure comparable type\n",
    "#                                   tl_frame_idx = obj_data['frames'].index(frame_num_comp);\n",
    "#                                   if tl_frame_idx < len(obj_data['state']):\n",
    "#                                       state_val = obj_data['state'][tl_frame_idx];\n",
    "#                                       # Use the state if it's defined (non-zero)\n",
    "#                                       if state_val in TL_STATE_MAP and state_val != 0:\n",
    "#                                           state_int = TL_STATE_MAP[state_val] if isinstance(state_val, str) else int(state_val) # Handle both str/int keys if needed\n",
    "#                                           break # Found relevant TL state for this frame\n",
    "#                               except (ValueError, TypeError, IndexError):\n",
    "#                                   continue # Frame not found for this TL or other error, check next TL\n",
    "#                 feature_sequences['traffic_light'].append(to_one_hot(state_int, NUM_TL_STATES))\n",
    "\n",
    "#             # --- Static Context (Append per frame) ---\n",
    "#             if 'static_context' in self.streams_to_generate:\n",
    "#                 # Use the static_vec calculated once outside the loop\n",
    "#                 feature_sequences['static_context'].append(static_vec if static_vec is not None else np.zeros(INPUT_SIZE_STATIC, dtype=np.float32))\n",
    "\n",
    "#             # --- YOLOP Feature Extraction Logic ---\n",
    "#             if 'yolop' in self.streams_to_generate:\n",
    "#                 yolop_vector = np.zeros(INPUT_SIZE_YOLOP, dtype=np.float32) # Default zeros\n",
    "#                 # Safely access nested yolop data\n",
    "#                 if set_id in self.all_yolop_data and video_id in self.all_yolop_data[set_id]:\n",
    "#                     frame_yolop_data = self.all_yolop_data[set_id][video_id].get(frame_num, {})\n",
    "#                     loaded_yolop = frame_yolop_data.get(ped_id)\n",
    "#                     # Validate loaded data\n",
    "#                     if loaded_yolop is not None and isinstance(loaded_yolop, np.ndarray) and loaded_yolop.shape == (INPUT_SIZE_YOLOP,):\n",
    "#                         yolop_vector = loaded_yolop\n",
    "#                 feature_sequences['yolop'].append(yolop_vector)\n",
    "\n",
    "#         # --- Convert sequences to Tensors ---\n",
    "#         features = {};\n",
    "#         try:\n",
    "#             for stream_name in self.streams_to_generate:\n",
    "#                  if stream_name in feature_sequences:\n",
    "#                       # Convert list of numpy arrays/scalars to a single numpy array before tensor conversion\n",
    "#                       np_array_feature = np.array(feature_sequences[stream_name], dtype=np.float32)\n",
    "#                       # Check shape consistency (optional but good)\n",
    "#                       expected_shape = (self.seq_len, self._input_sizes_for_error.get(stream_name, 1))\n",
    "#                       if np_array_feature.shape != expected_shape:\n",
    "#                            # Handle shape mismatch: print warning, pad/truncate, or raise error\n",
    "#                            # Here, we'll print a warning and try to use it anyway if the first dim matches\n",
    "#                            print(f\"Warning: Feature shape mismatch for '{stream_name}' idx {idx}. Expected {expected_shape}, got {np_array_feature.shape}. Trying to proceed.\")\n",
    "#                            if np_array_feature.shape[0] != self.seq_len:\n",
    "#                                # If sequence length is wrong, this is critical, create zeros\n",
    "#                                print(f\"Error: Critical shape mismatch (sequence length) for '{stream_name}' idx {idx}. Using zeros.\")\n",
    "#                                features[stream_name] = torch.zeros(expected_shape, dtype=torch.float32)\n",
    "#                            else:\n",
    "#                                # If only feature dim mismatches, maybe adaptable? Or force zeros?\n",
    "#                                print(f\"Warning: Feature dimension mismatch for '{stream_name}' idx {idx}. Using zeros.\")\n",
    "#                                features[stream_name] = torch.zeros(expected_shape, dtype=torch.float32)\n",
    "#                                # Or try reshaping if possible? features[stream_name] = torch.tensor(np_array_feature.reshape(expected_shape), dtype=torch.float32)\n",
    "#                       else:\n",
    "#                            features[stream_name] = torch.tensor(np_array_feature, dtype=torch.float32)\n",
    "#                  else:\n",
    "#                       # This case should not happen if initialized correctly, but handle defensively\n",
    "#                       print(f\"Warning: Stream '{stream_name}' requested but not found in feature_sequences for idx {idx}. Adding zeros.\")\n",
    "#                       expected_shape = (self.seq_len, self._input_sizes_for_error.get(stream_name, 1))\n",
    "#                       features[stream_name] = torch.zeros(expected_shape, dtype=torch.float32)\n",
    "\n",
    "#         except (ValueError, TypeError) as e: # Catch specific numpy/tensor conversion errors\n",
    "#              print(f\"Error converting features to tensor for idx {idx}: {e}. Returning dummy zeros.\")\n",
    "#              # Ensure dummy features have correct shapes based on _input_sizes_for_error\n",
    "#              features = { name: torch.zeros((self.seq_len, self._input_sizes_for_error.get(name, 1)), dtype=torch.float32)\n",
    "#                           for name in self.streams_to_generate }\n",
    "#              label = 0 # Return default label with dummy features\n",
    "\n",
    "#         return features, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# # --- Wrapper Dataset for Balanced Data ---\n",
    "# class BalancedDataset(Dataset):\n",
    "#     def __init__(self, data_dict, active_streams, label_key='label'):\n",
    "#         self.active_streams = active_streams\n",
    "#         self.label_key = label_key\n",
    "#         if self.label_key not in data_dict or not data_dict[self.label_key]:\n",
    "#              raise ValueError(f\"Label key '{self.label_key}' missing or empty in provided data_dict.\")\n",
    "\n",
    "#         self.num_samples = len(data_dict[self.label_key])\n",
    "#         if self.num_samples == 0:\n",
    "#             print(\"Warning: BalancedDataset initialized with zero samples.\")\n",
    "#             # Initialize features/labels as empty tensors to avoid errors later if len=0\n",
    "#             self.features = {stream: torch.empty(0) for stream in self.active_streams}\n",
    "#             self.labels = torch.empty(0, dtype=torch.long)\n",
    "#             return # Exit early if no samples\n",
    "\n",
    "#         self.features = {}\n",
    "#         for stream in self.active_streams:\n",
    "#              if stream in data_dict and data_dict[stream] is not None: # Check stream exists and is not None\n",
    "#                  try:\n",
    "#                      # Convert list of numpy arrays (expected from balancing) to a single tensor\n",
    "#                      stream_data_np = np.array(data_dict[stream])\n",
    "#                      self.features[stream] = torch.tensor(stream_data_np, dtype=torch.float32)\n",
    "#                  except ValueError as e:\n",
    "#                       # Handle cases where np.array fails (e.g., inconsistent shapes within the list)\n",
    "#                       raise ValueError(f\"Error converting balanced data for stream '{stream}' to tensor. Inconsistent shapes? Error: {e}\") from e\n",
    "#                  except TypeError as e:\n",
    "#                      raise TypeError(f\"Error converting balanced data for stream '{stream}' to tensor. Invalid data types? Error: {e}\") from e\n",
    "#                  except Exception as e: # Catch other unexpected errors\n",
    "#                       raise RuntimeError(f\"Unexpected error creating tensor for balanced stream '{stream}': {e}\") from e\n",
    "\n",
    "#                  # Check length consistency after successful tensor creation\n",
    "#                  if len(self.features[stream]) != self.num_samples:\n",
    "#                      raise ValueError(f\"Length mismatch after tensor creation: Stream '{stream}' ({len(self.features[stream])}) vs Labels ({self.num_samples})\")\n",
    "\n",
    "#              else:\n",
    "#                   # If an active stream is missing or None in the balanced data, it's an error\n",
    "#                   raise KeyError(f\"Active stream '{stream}' requested but missing or None in the balanced data dictionary.\")\n",
    "\n",
    "#         try:\n",
    "#             # Ensure labels are correctly extracted (assuming list of lists like [[0], [1]])\n",
    "#             self.labels = torch.tensor([lbl[0] for lbl in data_dict[self.label_key]], dtype=torch.long)\n",
    "#             if len(self.labels) != self.num_samples:\n",
    "#                  raise ValueError(f\"Length mismatch after label processing: Labels ({len(self.labels)}) vs Expected ({self.num_samples})\")\n",
    "#         except (IndexError, TypeError) as e:\n",
    "#              raise ValueError(f\"Error processing labels from balanced data. Expected list of single-element lists (e.g., [[0], [1]]). Error: {e}\") from e\n",
    "#         except Exception as e:\n",
    "#              raise RuntimeError(f\"Unexpected error creating label tensor from balanced data: {e}\") from e\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_samples\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if idx >= self.num_samples:\n",
    "#              raise IndexError(\"Index out of bounds\")\n",
    "#         # Return only features for streams active in this run\n",
    "#         try:\n",
    "#             feature_dict = {stream: self.features[stream][idx] for stream in self.active_streams if stream in self.features}\n",
    "#         except KeyError as e:\n",
    "#              raise KeyError(f\"Active stream '{e}' not found in self.features during __getitem__ for index {idx}. This should not happen if init was successful.\") from e\n",
    "#         label = self.labels[idx]\n",
    "#         return feature_dict, label\n",
    "\n",
    "\n",
    "# # --- Model Architecture ---\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, hidden_dim, attention_dim):\n",
    "#         super(Attention, self).__init__()\n",
    "#         # Basic validation\n",
    "#         if not isinstance(hidden_dim, int) or hidden_dim <= 0:\n",
    "#             raise ValueError(\"hidden_dim must be a positive integer\")\n",
    "#         if not isinstance(attention_dim, int) or attention_dim <= 0:\n",
    "#             raise ValueError(\"attention_dim must be a positive integer\")\n",
    "\n",
    "#         self.attention_net = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, attention_dim),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(attention_dim, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, lstm_output):\n",
    "#         # lstm_output shape: (batch, seq_len, hidden_dim * num_directions)\n",
    "#         if lstm_output.ndim != 3:\n",
    "#              raise ValueError(f\"Expected lstm_output to have 3 dimensions (batch, seq, hidden), got {lstm_output.ndim}\")\n",
    "#         try:\n",
    "#             att_scores = self.attention_net(lstm_output).squeeze(2) # (batch, seq_len)\n",
    "#             if att_scores.shape[0] != lstm_output.shape[0] or att_scores.shape[1] != lstm_output.shape[1]:\n",
    "#                  raise RuntimeError(\"Attention scores shape mismatch after squeeze.\")\n",
    "#             att_weights = torch.softmax(att_scores, dim=1) # (batch, seq_len)\n",
    "#             # Weighted sum: (batch, seq_len, hidden) * (batch, seq_len, 1) -> sum along seq_len dim\n",
    "#             context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1) # (batch, hidden)\n",
    "#         except Exception as e:\n",
    "#              print(f\"Error during Attention forward pass: {e}\")\n",
    "#              # Depending on desired robustness, could return zeros or re-raise\n",
    "#              raise RuntimeError(\"Failed in Attention forward pass\") from e\n",
    "#         return context_vector, att_weights\n",
    "\n",
    "# class MultiStreamAdaptiveLSTM(nn.Module):\n",
    "#     def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes, attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n",
    "#         super(MultiStreamAdaptiveLSTM, self).__init__()\n",
    "#         if not stream_names:\n",
    "#             raise ValueError(\"stream_names cannot be empty.\")\n",
    "#         if not isinstance(input_sizes, dict):\n",
    "#             raise TypeError(\"input_sizes must be a dictionary.\")\n",
    "#         self.stream_names = stream_names\n",
    "#         self.lstms = nn.ModuleDict()\n",
    "#         self.attentions = nn.ModuleDict()\n",
    "#         print(f\"Initializing model with streams: {self.stream_names}\")\n",
    "\n",
    "#         num_active_streams = 0 # Count streams successfully added\n",
    "#         for name in self.stream_names:\n",
    "#             if name not in input_sizes:\n",
    "#                 # Option 1: Raise error if a requested stream is missing config\n",
    "#                 raise KeyError(f\"Input size for stream '{name}' not provided in input_sizes dictionary.\")\n",
    "#                 # Option 2: Warn and skip (might lead to downstream errors if logic expects it)\n",
    "#                 # print(f\"Warning: Input size for stream '{name}' not provided. Skipping this stream.\")\n",
    "#                 # continue\n",
    "\n",
    "#             current_input_size = input_sizes[name]\n",
    "#             if not isinstance(current_input_size, int) or current_input_size <= 0:\n",
    "#                  raise ValueError(f\"Invalid input size ({current_input_size}) for stream '{name}'. Must be a positive integer.\")\n",
    "\n",
    "#             print(f\"  - Adding stream '{name}' with input size {current_input_size}\")\n",
    "#             try:\n",
    "#                 self.lstms[name] = nn.LSTM(\n",
    "#                     current_input_size, lstm_hidden_size, num_lstm_layers,\n",
    "#                     batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0,\n",
    "#                     bidirectional=True # Assuming bidirectional based on Attention hidden_dim * 2\n",
    "#                 )\n",
    "#                 self.attentions[name] = Attention(lstm_hidden_size * 2 , attention_dim) # *2 for bidirectional\n",
    "#                 num_active_streams += 1\n",
    "#             except Exception as e:\n",
    "#                  print(f\"Error initializing LSTM/Attention for stream '{name}': {e}\")\n",
    "#                  raise # Re-raise to stop model creation if a part fails\n",
    "\n",
    "#         if num_active_streams == 0:\n",
    "#              raise ValueError(\"No streams were successfully initialized in the model.\")\n",
    "\n",
    "#         combined_feature_dim = lstm_hidden_size * 2 * num_active_streams # *2 for bidirectional\n",
    "#         print(f\"  Combined feature dimension: {combined_feature_dim}\")\n",
    "\n",
    "#         # Classifier layers\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         # Ensure intermediate_dim is reasonable\n",
    "#         intermediate_dim = max(num_classes * 2, combined_feature_dim // 2, 16) # Added min size\n",
    "#         self.fc1 = nn.Linear(combined_feature_dim, intermediate_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(intermediate_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if not isinstance(x, dict):\n",
    "#              raise TypeError(f\"Input 'x' must be a dictionary of stream tensors, got {type(x)}\")\n",
    "\n",
    "#         stream_context_vectors = []\n",
    "#         stream_att_weights = {} # Optional: if needed for analysis\n",
    "\n",
    "#         processed_streams = 0\n",
    "#         for name in self.stream_names: # Iterate through streams model was configured for\n",
    "#             if name not in x:\n",
    "#                 # This indicates a problem upstream (DataLoader/Dataset didn't provide expected data)\n",
    "#                 raise KeyError(f\"Input dictionary 'x' is missing expected stream '{name}' during forward pass.\")\n",
    "#                 # Alternative (more robust, less strict): warn and skip\n",
    "#                 # print(f\"Warning: Stream '{name}' expected but not found in input batch. Skipping.\")\n",
    "#                 # continue\n",
    "\n",
    "#             stream_input = x[name]\n",
    "#             # Basic validation of input tensor for this stream\n",
    "#             if not isinstance(stream_input, torch.Tensor):\n",
    "#                  raise TypeError(f\"Input for stream '{name}' must be a torch.Tensor, got {type(stream_input)}\")\n",
    "#             if stream_input.ndim != 3: # Expect (batch, seq_len, feature_dim)\n",
    "#                  raise ValueError(f\"Input tensor for stream '{name}' has incorrect dimensions ({stream_input.ndim}). Expected 3 (batch, seq, feature).\")\n",
    "\n",
    "#             try:\n",
    "#                 lstm_out, _ = self.lstms[name](stream_input) # (batch, seq_len, hidden*2)\n",
    "#                 context_vector, attention_weights = self.attentions[name](lstm_out) # (batch, hidden*2)\n",
    "#                 stream_context_vectors.append(context_vector)\n",
    "#                 # stream_att_weights[name] = attention_weights # Optional storage\n",
    "#                 processed_streams += 1\n",
    "#             except Exception as e:\n",
    "#                  print(f\"Error processing stream '{name}' in forward pass: {e}\")\n",
    "#                  raise RuntimeError(f\"Failed during LSTM/Attention for stream '{name}'\") from e\n",
    "\n",
    "#         # Check if any streams were actually processed (especially important if skipping was allowed)\n",
    "#         if not stream_context_vectors:\n",
    "#             raise RuntimeError(\"No stream outputs generated during forward pass. Input might be missing all expected streams.\")\n",
    "#         if processed_streams != len(self.stream_names):\n",
    "#              print(f\"Warning: Only processed {processed_streams}/{len(self.stream_names)} expected streams.\") # Should not happen if KeyErrors are raised\n",
    "\n",
    "#         try:\n",
    "#             fused_features = torch.cat(stream_context_vectors, dim=1) # Concatenate along feature dimension\n",
    "#             # Check shape after concat: (batch, sum_of_context_vector_dims)\n",
    "#             # Expected dim = num_active_streams * lstm_hidden_size * 2\n",
    "#             expected_fused_dim = processed_streams * self.lstms[self.stream_names[0]].hidden_size * 2 # Get hidden size from one LSTM\n",
    "#             if fused_features.shape[1] != expected_fused_dim:\n",
    "#                  raise RuntimeError(f\"Fused feature dimension mismatch. Expected {expected_fused_dim}, got {fused_features.shape[1]}\")\n",
    "\n",
    "#             # Pass through classifier\n",
    "#             out = self.dropout(fused_features)\n",
    "#             out = self.relu(self.fc1(out))\n",
    "#             out = self.dropout(out)\n",
    "#             logits = self.fc2(out)\n",
    "#         except Exception as e:\n",
    "#              print(f\"Error during final classification layers: {e}\")\n",
    "#              raise RuntimeError(\"Failed in classifier part of forward pass\") from e\n",
    "\n",
    "#         return logits\n",
    "\n",
    "# # --- Training and Evaluation Functions ---\n",
    "# def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     active_streams = model.stream_names # Get streams the model expects\n",
    "\n",
    "#     batch_num = 0\n",
    "#     for features, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "#         batch_num += 1\n",
    "#         # Ensure features is a dict and move only expected streams to device\n",
    "#         if not isinstance(features, dict):\n",
    "#             print(f\"Error: Expected features to be a dict in batch {batch_num}, got {type(features)}. Skipping batch.\")\n",
    "#             continue\n",
    "#         input_features = {}\n",
    "#         try:\n",
    "#             for name in active_streams:\n",
    "#                 if name not in features:\n",
    "#                     raise KeyError(f\"Required stream '{name}' missing from batch {batch_num}.\")\n",
    "#                 input_features[name] = features[name].to(device)\n",
    "#             labels = labels.to(device)\n",
    "#         except KeyError as e:\n",
    "#             print(f\"Error preparing batch {batch_num}: {e}. Skipping.\")\n",
    "#             continue\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error moving batch {batch_num} to device {device}: {e}. Skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         try:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(input_features) # Shape: (batch_size, num_classes)\n",
    "#             # Ensure labels have the correct shape for CrossEntropyLoss (batch_size)\n",
    "#             if labels.ndim != 1:\n",
    "#                  labels = labels.squeeze() # Attempt to fix if shape is (batch_size, 1)\n",
    "#                  if labels.ndim != 1:\n",
    "#                       raise ValueError(f\"Labels have incorrect shape {labels.shape} for CrossEntropyLoss in batch {batch_num}.\")\n",
    "\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             # Optional: Gradient clipping\n",
    "#             # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "#             with torch.no_grad(): # Ensure preds calculation doesn't track gradients\n",
    "#                  preds = torch.argmax(outputs, dim=1)\n",
    "#                  all_preds.extend(preds.cpu().numpy())\n",
    "#                  all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error during training step for batch {batch_num}: {e}\")\n",
    "#             # Decide whether to skip batch or re-raise\n",
    "#             # Skipping might hide persistent issues\n",
    "#             # raise e # Option: Stop training on error\n",
    "\n",
    "#     avg_loss = total_loss / max(1, len(dataloader)) # Avoid division by zero if dataloader is empty\n",
    "#     accuracy = 0.0\n",
    "#     if all_labels: # Calculate accuracy only if some batches were processed\n",
    "#        try:\n",
    "#            accuracy = accuracy_score(all_labels, all_preds)\n",
    "#        except ValueError as e:\n",
    "#            print(f\"Error calculating training accuracy: {e}. Labels: {np.unique(all_labels)}, Preds: {np.unique(all_preds)}\")\n",
    "\n",
    "#     return avg_loss, accuracy\n",
    "\n",
    "# def evaluate_epoch(model, dataloader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     all_labels = []\n",
    "#     all_preds = []\n",
    "#     all_probs = []\n",
    "#     active_streams = model.stream_names\n",
    "\n",
    "#     batch_num = 0\n",
    "#     with torch.no_grad():\n",
    "#         for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "#             batch_num += 1\n",
    "#             if not isinstance(features, dict):\n",
    "#                 print(f\"Error: Expected features to be a dict in eval batch {batch_num}, got {type(features)}. Skipping.\")\n",
    "#                 continue\n",
    "#             input_features = {}\n",
    "#             try:\n",
    "#                 for name in active_streams:\n",
    "#                     if name not in features:\n",
    "#                         raise KeyError(f\"Required stream '{name}' missing from eval batch {batch_num}.\")\n",
    "#                     input_features[name] = features[name].to(device)\n",
    "#                 labels = labels.to(device)\n",
    "#             except KeyError as e:\n",
    "#                 print(f\"Error preparing eval batch {batch_num}: {e}. Skipping.\")\n",
    "#                 continue\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error moving eval batch {batch_num} to device {device}: {e}. Skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#             try:\n",
    "#                 outputs = model(input_features)\n",
    "#                 # Ensure labels have the correct shape\n",
    "#                 if labels.ndim != 1:\n",
    "#                      labels = labels.squeeze()\n",
    "#                      if labels.ndim != 1:\n",
    "#                           raise ValueError(f\"Labels have incorrect shape {labels.shape} for loss calculation in eval batch {batch_num}.\")\n",
    "\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 total_loss += loss.item()\n",
    "\n",
    "#                 probs = torch.softmax(outputs, dim=1)\n",
    "#                 preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "#                 all_labels.extend(labels.cpu().numpy())\n",
    "#                 all_preds.extend(preds.cpu().numpy())\n",
    "#                 all_probs.extend(probs.cpu().numpy()) # Store probabilities for AUC\n",
    "\n",
    "#             except Exception as e:\n",
    "#                  print(f\"Error during evaluation step for batch {batch_num}: {e}\")\n",
    "#                  # Continue evaluation if one batch fails? Or stop?\n",
    "#                  # continue\n",
    "\n",
    "#     avg_loss = total_loss / max(1, len(dataloader))\n",
    "#     # Initialize metrics with default values\n",
    "#     accuracy, precision, recall, f1, auc = 0.0, 0.0, 0.0, 0.0, float('nan')\n",
    "\n",
    "#     if not all_labels: # Check if any data was processed\n",
    "#         print(\"Warning: No labels collected during evaluation. Returning zero/NaN metrics.\")\n",
    "#         return {'loss': avg_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}\n",
    "\n",
    "#     try:\n",
    "#         all_probs = np.array(all_probs)\n",
    "#         all_labels = np.array(all_labels)\n",
    "#         all_preds = np.array(all_preds)\n",
    "\n",
    "#         accuracy = accuracy_score(all_labels, all_preds)\n",
    "#         precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', pos_label=1, zero_division=0)\n",
    "\n",
    "#         # Calculate AUC only if both classes are present in true labels\n",
    "#         if len(np.unique(all_labels)) > 1:\n",
    "#             # Ensure probabilities correspond to the positive class (class 1)\n",
    "#             if all_probs.shape[1] == 2:\n",
    "#                  auc = roc_auc_score(all_labels, all_probs[:, 1])\n",
    "#             else:\n",
    "#                  print(f\"Warning: Probability array has unexpected shape {all_probs.shape}. Cannot calculate AUC.\")\n",
    "#                  auc = float('nan')\n",
    "#         else:\n",
    "#             print(\"Warning: Only one class present in ground truth labels during evaluation. AUC is not defined.\")\n",
    "#             auc = float('nan') # AUC is not defined for single-class data\n",
    "\n",
    "#     except ValueError as e:\n",
    "#         print(f\"Error calculating evaluation metrics: {e}\")\n",
    "#         # This might happen if shapes mismatch or other sklearn issues\n",
    "#     except Exception as e:\n",
    "#         print(f\"Unexpected error calculating evaluation metrics: {e}\")\n",
    "\n",
    "\n",
    "#     return {'loss': avg_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}\n",
    "\n",
    "# def get_predictions_and_labels(model, dataloader, device):\n",
    "#     model.eval()\n",
    "#     all_labels = []\n",
    "#     all_preds = []\n",
    "#     active_streams = model.stream_names\n",
    "\n",
    "#     batch_num = 0\n",
    "#     with torch.no_grad():\n",
    "#         for features, labels in tqdm(dataloader, desc=\"Generating CM Data\", leave=False):\n",
    "#              batch_num += 1\n",
    "#              if not isinstance(features, dict):\n",
    "#                  print(f\"Error: Expected features to be a dict in CM batch {batch_num}, got {type(features)}. Skipping.\")\n",
    "#                  continue\n",
    "#              input_features = {}\n",
    "#              try:\n",
    "#                  for name in active_streams:\n",
    "#                      if name not in features:\n",
    "#                          raise KeyError(f\"Required stream '{name}' missing from CM batch {batch_num}.\")\n",
    "#                      input_features[name] = features[name].to(device)\n",
    "#                  labels = labels.to(device)\n",
    "#              except KeyError as e:\n",
    "#                  print(f\"Error preparing CM batch {batch_num}: {e}. Skipping.\")\n",
    "#                  continue\n",
    "#              except Exception as e:\n",
    "#                  print(f\"Error moving CM batch {batch_num} to device {device}: {e}. Skipping.\")\n",
    "#                  continue\n",
    "\n",
    "#              try:\n",
    "#                  outputs = model(input_features)\n",
    "#                  preds = torch.argmax(outputs, dim=1)\n",
    "#                  all_labels.extend(labels.cpu().numpy())\n",
    "#                  all_preds.extend(preds.cpu().numpy())\n",
    "#              except Exception as e:\n",
    "#                   print(f\"Error during prediction for CM data batch {batch_num}: {e}\")\n",
    "#                   # continue\n",
    "\n",
    "#     return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "# # --- Main Execution Block ---\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     # --- Step 1: Data Preparation ---\n",
    "#     run_data_prep = not (os.path.exists(BALANCED_DATA_PKL_PATH) and os.path.exists(SCALERS_PKL_PATH))\n",
    "\n",
    "#     # Declare variables used in both branches before the conditional block\n",
    "#     pie_database = None\n",
    "#     scalers = {}\n",
    "#     balanced_train_data_dict = None\n",
    "\n",
    "#     if run_data_prep:\n",
    "#         print(\"--- Running Data Preparation ---\")\n",
    "#         # --- Generate/Load PIE Database ---\n",
    "#         print(f\"Checking for PIE database cache at: {PIE_DATABASE_CACHE_PATH}\")\n",
    "#         if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n",
    "#             if PIE is None: # Check if import failed earlier\n",
    "#                  raise ImportError(\"PIE class could not be imported, and database cache does not exist. Cannot generate database.\")\n",
    "#             print(\"PIE database cache not found. Generating...\");\n",
    "#             try:\n",
    "#                  # Assuming PIE class needs initialization parameters like data_path\n",
    "#                  pie_dataset_interface = PIE(data_path=PIE_ROOT_PATH, regen_database=True) # Adjust params as needed\n",
    "#                  pie_database = pie_dataset_interface.generate_database()\n",
    "#                  if not pie_database: raise RuntimeError(\"PIE database generation returned empty.\")\n",
    "#                  print(\"PIE database generated successfully.\")\n",
    "#                  # Optional: Save the generated database\n",
    "#                  # try:\n",
    "#                  #     with open(PIE_DATABASE_CACHE_PATH.replace('.pkl', '_newly_generated.pkl'), 'wb') as f: # Save with different name\n",
    "#                  #         pickle.dump(pie_database, f, pickle.HIGHEST_PROTOCOL)\n",
    "#                  #     print(f\"Saved newly generated database.\")\n",
    "#                  # except Exception as e:\n",
    "#                  #     print(f\"Error saving newly generated PIE database: {e}\")\n",
    "\n",
    "#             except Exception as e:\n",
    "#                  raise RuntimeError(f\"Failed to initialize or run PIE database generation: {e}\") from e\n",
    "#         else:\n",
    "#             print(\"Loading PIE database from cache...\")\n",
    "#             try:\n",
    "#                 with open(PIE_DATABASE_CACHE_PATH, 'rb') as f: pie_database = pickle.load(f)\n",
    "#                 print(\"PIE database loaded successfully.\")\n",
    "#                 if not isinstance(pie_database, dict) or not pie_database: # Basic validation\n",
    "#                      raise ValueError(\"Loaded PIE database is not a valid dictionary or is empty.\")\n",
    "#             except FileNotFoundError:\n",
    "#                  raise FileNotFoundError(f\"PIE database cache file not found at {PIE_DATABASE_CACHE_PATH} despite os.path.exists being true initially.\")\n",
    "#             except pickle.UnpicklingError as pe:\n",
    "#                  raise RuntimeError(f\"Failed to unpickle PIE database from {PIE_DATABASE_CACHE_PATH}: {pe}\") from pe\n",
    "#             except Exception as e:\n",
    "#                  raise RuntimeError(f\"Failed to load PIE database from cache: {e}\") from e\n",
    "\n",
    "#         if pie_database is None: # Should have been caught by exceptions, but double check\n",
    "#              raise RuntimeError(\"PIE Database is None after generation/loading attempt.\")\n",
    "\n",
    "#         # --- Calculate Standardization Parameters ---\n",
    "#         print(\"\\nCalculating standardization parameters from training set...\")\n",
    "#         all_train_ego_speeds = []; all_train_accX = []; all_train_accY = []; all_train_gyroZ = []\n",
    "#         # Check if pie_database is a dictionary before iterating\n",
    "#         if isinstance(pie_database, dict):\n",
    "#             for set_id in TRAIN_SETS_STR:\n",
    "#                  if set_id in pie_database and isinstance(pie_database[set_id], dict):\n",
    "#                      for video_id, video_data in pie_database[set_id].items():\n",
    "#                           if isinstance(video_data, dict) and 'vehicle_annotations' in video_data and isinstance(video_data['vehicle_annotations'], dict):\n",
    "#                                for frame_num, ego_frame_data in video_data['vehicle_annotations'].items():\n",
    "#                                    if isinstance(ego_frame_data, dict): # Check ego data is a dict\n",
    "#                                         try: # Safely get and convert values\n",
    "#                                              obd_speed = ego_frame_data.get('OBD_speed')\n",
    "#                                              gps_speed = ego_frame_data.get('GPS_speed')\n",
    "#                                              speed = 0.0\n",
    "#                                              if isinstance(obd_speed, (int, float)) and obd_speed > 0:\n",
    "#                                                   speed = float(obd_speed)\n",
    "#                                              elif isinstance(gps_speed, (int, float)):\n",
    "#                                                   speed = float(gps_speed)\n",
    "#                                              all_train_ego_speeds.append(speed)\n",
    "\n",
    "#                                              accX = float(ego_frame_data.get('accX', 0.0))\n",
    "#                                              accY = float(ego_frame_data.get('accY', 0.0))\n",
    "#                                              gyroZ = float(ego_frame_data.get('gyroZ', 0.0))\n",
    "#                                              all_train_accX.append(accX); all_train_accY.append(accY); all_train_gyroZ.append(gyroZ)\n",
    "#                                         except (TypeError, ValueError) as e:\n",
    "#                                              print(f\"Warning: Invalid data type for ego stats in {set_id}/{video_id}/frame {frame_num}. Skipping frame. Error: {e}\")\n",
    "#         else:\n",
    "#              print(\"Warning: pie_database is not a dictionary. Cannot calculate scalers.\")\n",
    "\n",
    "\n",
    "#         scalers = {} # Initialize scalers dict\n",
    "#         try:\n",
    "#             if all_train_ego_speeds:\n",
    "#                  mean_speed = np.mean(all_train_ego_speeds); std_speed = np.std(all_train_ego_speeds)\n",
    "#                  scalers['ego_speed_mean'] = mean_speed; scalers['ego_speed_std'] = std_speed if std_speed > 1e-6 else 1.0;\n",
    "#                  print(f\"  Ego Speed: Mean={scalers['ego_speed_mean']:.2f}, Std={scalers['ego_speed_std']:.2f}\")\n",
    "#             if all_train_accX: # Assume accY exists if accX does\n",
    "#                  mean_accX = np.mean(all_train_accX); std_accX = np.std(all_train_accX)\n",
    "#                  mean_accY = np.mean(all_train_accY); std_accY = np.std(all_train_accY)\n",
    "#                  scalers['accX_mean'] = mean_accX; scalers['accX_std'] = std_accX if std_accX > 1e-6 else 1.0;\n",
    "#                  scalers['accY_mean'] = mean_accY; scalers['accY_std'] = std_accY if std_accY > 1e-6 else 1.0;\n",
    "#                  print(f\"  Ego AccX: Mean={scalers['accX_mean']:.2f}, Std={scalers['accX_std']:.2f}\"); print(f\"  Ego AccY: Mean={scalers['accY_mean']:.2f}, Std={scalers['accY_std']:.2f}\")\n",
    "#             if all_train_gyroZ:\n",
    "#                  mean_gyroZ = np.mean(all_train_gyroZ); std_gyroZ = np.std(all_train_gyroZ)\n",
    "#                  scalers['gyroZ_mean'] = mean_gyroZ; scalers['gyroZ_std'] = std_gyroZ if std_gyroZ > 1e-6 else 1.0;\n",
    "#                  print(f\"  Ego GyroZ: Mean={scalers['gyroZ_mean']:.2f}, Std={scalers['gyroZ_std']:.2f}\")\n",
    "#         except Exception as e:\n",
    "#              print(f\"Error calculating scaler statistics: {e}\") # Catch potential numpy errors\n",
    "#         print(\"Standardization parameters calculation finished.\")\n",
    "\n",
    "#         # --- Initialize FULL Training Dataset ---\n",
    "#         print(\"\\nInitializing full training dataset (for extraction/balancing)...\")\n",
    "#         full_train_dataset = None # Initialize before try block\n",
    "#         try:\n",
    "#             full_train_dataset = PIEDataset(pie_database, TRAIN_SETS_STR, POSE_DATA_DIR, YOLOP_FEATURE_DIR, SEQ_LEN, PRED_LEN, scalers, ALL_POSSIBLE_STREAMS)\n",
    "#             if len(full_train_dataset) == 0:\n",
    "#                  # This isn't necessarily an error, could be no valid sequences found\n",
    "#                  print(\"Warning: Full Train Dataset initialization resulted in 0 sequences.\")\n",
    "#                  # Depending on requirements, could raise ValueError here or allow proceeding\n",
    "#                  # raise ValueError(\"Full Train Dataset loading resulted in 0 sequences.\")\n",
    "#         except Exception as e:\n",
    "#              print(f\"Error initializing full training dataset: {e}\")\n",
    "#              raise # Re-raise critical error\n",
    "\n",
    "\n",
    "#         # --- Prepare and Balance Training Data ---\n",
    "#         print(\"\\nExtracting ALL stream data from training set for balancing...\")\n",
    "#         training_data_dict = {stream: [] for stream in ALL_POSSIBLE_STREAMS}; training_data_dict['label'] = []\n",
    "#         # Proceed only if full_train_dataset was created and has items\n",
    "#         if full_train_dataset and len(full_train_dataset) > 0:\n",
    "#             num_extraction_errors = 0\n",
    "#             for i in tqdm(range(len(full_train_dataset)), desc=\"Extracting data\"):\n",
    "#                  try:\n",
    "#                      features, label = full_train_dataset[i] # __getitem__ might raise errors\n",
    "#                      # Validate features dictionary\n",
    "#                      if not isinstance(features, dict):\n",
    "#                           raise TypeError(f\"__getitem__ returned features of type {type(features)}, expected dict.\")\n",
    "#                      for stream_name in ALL_POSSIBLE_STREAMS:\n",
    "#                          if stream_name in features and features[stream_name] is not None:\n",
    "#                               # Convert tensor back to numpy for storage in dict (if not already numpy)\n",
    "#                               feature_data = features[stream_name]\n",
    "#                               if isinstance(feature_data, torch.Tensor):\n",
    "#                                    training_data_dict[stream_name].append(feature_data.cpu().numpy())\n",
    "#                               elif isinstance(feature_data, np.ndarray):\n",
    "#                                    training_data_dict[stream_name].append(feature_data)\n",
    "#                               else:\n",
    "#                                    raise TypeError(f\"Feature data for stream '{stream_name}' is type {type(feature_data)}, expected Tensor or ndarray.\")\n",
    "#                          else:\n",
    "#                               # If a stream is missing from __getitem__ output, record it and add zeros\n",
    "#                               print(f\"Warn: Stream '{stream_name}' missing from __getitem__ output for idx {i}. Appending zeros.\")\n",
    "#                               # Determine expected feature size for the zeros array\n",
    "#                               expected_size = full_train_dataset._input_sizes_for_error.get(stream_name, 1)\n",
    "#                               training_data_dict[stream_name].append(np.zeros((SEQ_LEN, expected_size), dtype=np.float32))\n",
    "#                      # Append label\n",
    "#                      training_data_dict['label'].append([label.item()]) # Assuming label is a 0-dim tensor\n",
    "\n",
    "#                  except Exception as e:\n",
    "#                      num_extraction_errors += 1\n",
    "#                      print(f\"Error extracting data for index {i}: {e}. Skipping item.\")\n",
    "#                      # Optionally add placeholder data or just skip\n",
    "#                      if num_extraction_errors > len(full_train_dataset) * 0.1: # Stop if too many errors\n",
    "#                           print(\"Error: Excessive errors during data extraction. Aborting.\")\n",
    "#                           raise RuntimeError(\"Too many errors during data extraction.\") from e\n",
    "\n",
    "#             print(f\"Data extraction finished. Original training samples considered: {len(full_train_dataset)}, Errors encountered: {num_extraction_errors}\")\n",
    "#             print(f\"Samples successfully extracted: {len(training_data_dict['label'])}\")\n",
    "\n",
    "#             if not training_data_dict['label']: # Check if any data was actually extracted\n",
    "#                  print(\"Error: No data was successfully extracted. Cannot proceed with balancing.\")\n",
    "#                  # Handle this case: maybe exit or raise an error\n",
    "#                  sys.exit(\"Exiting due to lack of extracted data.\")\n",
    "#             else:\n",
    "#                  # --- Balance Data ---\n",
    "#                  label_key_for_balancing = 'label'\n",
    "#                  try:\n",
    "#                      balanced_train_data_dict = balance_samples_count(training_data_dict, label_type=label_key_for_balancing)\n",
    "#                  except Exception as e:\n",
    "#                       print(f\"Error during sample balancing: {e}\")\n",
    "#                       raise # Balancing is critical, re-raise\n",
    "\n",
    "#         else:\n",
    "#              print(\"Skipping data extraction and balancing because full_train_dataset is empty or None.\")\n",
    "#              balanced_train_data_dict = {} # Ensure it's an empty dict if skipped\n",
    "\n",
    "#         # --- Cleanup ---\n",
    "#         del training_data_dict # Free memory\n",
    "#         if full_train_dataset: del full_train_dataset\n",
    "\n",
    "\n",
    "#         # --- Save Balanced Data and Scalers ---\n",
    "#         if balanced_train_data_dict: # Only save if balancing happened and produced data\n",
    "#             print(f\"\\nSaving balanced training data to: {BALANCED_DATA_PKL_PATH}\")\n",
    "#             try:\n",
    "#                 with open(BALANCED_DATA_PKL_PATH, 'wb') as f: pickle.dump(balanced_train_data_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "#                 print(\" -> Balanced data saved.\")\n",
    "#             except pickle.PicklingError as pe: print(f\"  Error pickling balanced data: {pe}\")\n",
    "#             except Exception as e: print(f\"  Error saving balanced data: {e}\")\n",
    "#         else:\n",
    "#              print(\"\\nSkipping saving balanced data (was not generated or is empty).\")\n",
    "\n",
    "#         if scalers: # Only save if scalers were calculated\n",
    "#             print(f\"\\nSaving scalers to: {SCALERS_PKL_PATH}\")\n",
    "#             try:\n",
    "#                 with open(SCALERS_PKL_PATH, 'wb') as f: pickle.dump(scalers, f, pickle.HIGHEST_PROTOCOL)\n",
    "#                 print(\" -> Scalers saved.\")\n",
    "#             except pickle.PicklingError as pe: print(f\"  Error pickling scalers: {pe}\")\n",
    "#             except Exception as e: print(f\"  Error saving scalers: {e}\")\n",
    "#         else:\n",
    "#              print(\"\\nSkipping saving scalers (were not calculated or are empty).\")\n",
    "\n",
    "#         # Delete PIE db only if it was loaded/generated in this block\n",
    "#         if 'pie_database' in locals() and pie_database is not None: del pie_database\n",
    "#         gc.collect()\n",
    "#         print(\"\\n--- Data Preparation and Balancing Completed ---\")\n",
    "\n",
    "#     else: # Data prep files should exist, load them\n",
    "#         print(\"--- Skipping Data Preparation: Loading Pre-Saved Files ---\")\n",
    "#         print(f\"\\nLoading balanced training data from: {BALANCED_DATA_PKL_PATH}\")\n",
    "#         print(f\"Loading scalers from: {SCALERS_PKL_PATH}\")\n",
    "#         try:\n",
    "#             if not os.path.exists(BALANCED_DATA_PKL_PATH):\n",
    "#                  raise FileNotFoundError(f\"Balanced data file not found: {BALANCED_DATA_PKL_PATH}\")\n",
    "#             with open(BALANCED_DATA_PKL_PATH, 'rb') as f: balanced_train_data_dict = pickle.load(f)\n",
    "\n",
    "#             if not os.path.exists(SCALERS_PKL_PATH):\n",
    "#                  raise FileNotFoundError(f\"Scalers file not found: {SCALERS_PKL_PATH}\")\n",
    "#             with open(SCALERS_PKL_PATH, 'rb') as f: scalers = pickle.load(f)\n",
    "\n",
    "#             # Basic validation of loaded data\n",
    "#             if not isinstance(balanced_train_data_dict, dict):\n",
    "#                  raise TypeError(f\"Loaded balanced data is not a dictionary (type: {type(balanced_train_data_dict)}).\")\n",
    "#             if not isinstance(scalers, dict):\n",
    "#                  raise TypeError(f\"Loaded scalers data is not a dictionary (type: {type(scalers)}).\")\n",
    "\n",
    "#             print(\" -> Pre-processed data loaded successfully.\")\n",
    "\n",
    "#         except FileNotFoundError as e:\n",
    "#              print(f\"ERROR: Required pre-processed file not found: {e}. Cannot continue without running the data preparation phase.\")\n",
    "#              sys.exit(1) # Exit script\n",
    "#         except pickle.UnpicklingError as pe:\n",
    "#              print(f\"ERROR: Failed to unpickle pre-processed data: {pe}. Files might be corrupted. Re-run prep phase.\")\n",
    "#              sys.exit(1)\n",
    "#         except Exception as e:\n",
    "#              print(f\"ERROR: Unexpected error loading pre-processed data: {e}\")\n",
    "#              sys.exit(1)\n",
    "\n",
    "#         # Ensure loaded variables are not None before proceeding\n",
    "#         if balanced_train_data_dict is None or scalers is None:\n",
    "#              print(\"ERROR: Loading pre-processed data resulted in None variables. Cannot continue.\")\n",
    "#              sys.exit(1)\n",
    "\n",
    "\n",
    "#     # --- Step 2: Model Training and Evaluation ---\n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"--- Running Model Training and Evaluation ---\")\n",
    "#     print(f\"Active Streams for this run: {ACTIVE_STREAMS}\")\n",
    "#     print(\"-\" * 70)\n",
    "\n",
    "#     # --- Load Full PIE Database (needed for Validation Dataset) ---\n",
    "#     # This needs to happen regardless of whether data prep ran or not\n",
    "#     print(\"\\nLoading PIE database cache for Validation Dataset...\")\n",
    "#     if pie_database is None: # Check if it needs loading (wasn't loaded/kept from prep phase)\n",
    "#         if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n",
    "#              raise FileNotFoundError(f\"PIE database cache not found at {PIE_DATABASE_CACHE_PATH}. Required for Validation Dataset.\")\n",
    "#         try:\n",
    "#             with open(PIE_DATABASE_CACHE_PATH, 'rb') as f: pie_database = pickle.load(f)\n",
    "#             print(\" -> PIE database loaded successfully for validation.\")\n",
    "#             if not isinstance(pie_database, dict) or not pie_database: # Basic validation\n",
    "#                  raise ValueError(\"Loaded PIE database for validation is not a valid dictionary or is empty.\")\n",
    "#         except FileNotFoundError: # Should be caught by os.path.exists, but defensive check\n",
    "#              raise FileNotFoundError(f\"PIE database cache file not found at {PIE_DATABASE_CACHE_PATH} when loading for validation.\")\n",
    "#         except pickle.UnpicklingError as pe:\n",
    "#              raise RuntimeError(f\"Failed to unpickle PIE database for validation from {PIE_DATABASE_CACHE_PATH}: {pe}\") from pe\n",
    "#         except Exception as e:\n",
    "#              raise RuntimeError(f\"Failed to load PIE database for validation: {e}\") from e\n",
    "\n",
    "#     if pie_database is None: # Final check\n",
    "#          raise RuntimeError(\"PIE Database is still None before creating Validation Dataset.\")\n",
    "\n",
    "\n",
    "#     # --- Create Datasets and DataLoaders ---\n",
    "#     print(\"\\nCreating Datasets and DataLoaders...\")\n",
    "#     train_dataset, val_dataset = None, None # Initialize before try\n",
    "#     train_loader, val_loader = None, None # Initialize before try\n",
    "#     try:\n",
    "#         # Ensure balanced data dict is valid before creating dataset\n",
    "#         if not balanced_train_data_dict or 'label' not in balanced_train_data_dict or not balanced_train_data_dict['label']:\n",
    "#              raise ValueError(\"Balanced training data dictionary is invalid or missing labels. Cannot create training dataset.\")\n",
    "#         train_dataset = BalancedDataset(balanced_train_data_dict, ACTIVE_STREAMS, label_key='label')\n",
    "\n",
    "#         # Free memory associated with the large dictionary now\n",
    "#         del balanced_train_data_dict\n",
    "#         gc.collect()\n",
    "\n",
    "#         # Validation dataset needs the PIE database and scalers\n",
    "#         if not pie_database: raise ValueError(\"PIE database is None, cannot create validation dataset.\")\n",
    "#         if not scalers: print(\"Warning: Scalers dictionary is empty when creating validation dataset.\") # Non-fatal?\n",
    "\n",
    "#         val_dataset = PIEDataset(pie_database, VAL_SETS_STR, POSE_DATA_DIR, YOLOP_FEATURE_DIR, SEQ_LEN, PRED_LEN, scalers, ALL_POSSIBLE_STREAMS)\n",
    "\n",
    "#         if len(train_dataset) == 0: print(\"Warning: Training dataset is empty after initialization.\") # Potentially problematic\n",
    "#         if len(val_dataset) == 0: print(\"Warning: Validation dataset is empty after initialization.\") # Potentially problematic\n",
    "#         # Decide if empty datasets should halt execution\n",
    "#         if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "#             raise ValueError(\"Cannot proceed with empty Training or Validation dataset.\")\n",
    "\n",
    "#         # Create DataLoaders\n",
    "#         # Consider adding persistent_workers=True if num_workers > 0 for efficiency\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=min(2, os.cpu_count()), pin_memory=True, drop_last=True) # drop_last can help with batch norm stability\n",
    "#         val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=min(2, os.cpu_count()), pin_memory=True)\n",
    "#         print(f\"DataLoaders created. Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "\n",
    "#     except (ValueError, KeyError, TypeError, RuntimeError, Exception) as e: # Catch specific and general errors\n",
    "#         print(f\"FATAL ERROR creating Datasets or DataLoaders: {e}\")\n",
    "#         # Potentially print more context here if needed\n",
    "#         raise # Re-raise to stop execution\n",
    "\n",
    "#     # Free memory again - PIE database might be large\n",
    "#     del pie_database\n",
    "#     gc.collect()\n",
    "\n",
    "#     # --- Initialize Model ---\n",
    "#     print(\"\\nInitializing model...\")\n",
    "#     model = None # Initialize before try block\n",
    "#     try:\n",
    "#         current_input_sizes = {}\n",
    "#         for stream in ACTIVE_STREAMS: # Based on ACTIVE_STREAMS for this run\n",
    "#             size_constant_name = f'INPUT_SIZE_{stream.upper()}'\n",
    "#             special_cases = {'TRAFFIC_LIGHT': 'TL_STATE','STATIC_CONTEXT': 'STATIC','EGO_SPEED': 'EGO_SPEED','EGO_ACC': 'EGO_ACC','EGO_GYRO': 'EGO_GYRO','PED_ACTION': 'PED_ACTION','PED_LOOK': 'PED_LOOK','PED_OCCLUSION': 'PED_OCC', 'YOLOP':'YOLOP'}\n",
    "#             stream_upper_key = stream.upper(); suffix = special_cases.get(stream_upper_key)\n",
    "#             if suffix: size_constant_name = f'INPUT_SIZE_{suffix}'\n",
    "#             elif stream == 'bbox': size_constant_name = 'INPUT_SIZE_BBOX'\n",
    "#             elif stream == 'pose': size_constant_name = 'INPUT_SIZE_POSE'\n",
    "#             # Add other stream-specific mappings if necessary\n",
    "\n",
    "#             if size_constant_name in globals():\n",
    "#                  input_size = globals()[size_constant_name]\n",
    "#                  if not isinstance(input_size, int) or input_size <= 0:\n",
    "#                       raise ValueError(f\"Invalid input size ({input_size}) retrieved for constant {size_constant_name} (stream {stream}).\")\n",
    "#                  current_input_sizes[stream] = input_size\n",
    "#             else:\n",
    "#                  raise ValueError(f\"Input size constant '{size_constant_name}' not found in globals for active stream '{stream}'\")\n",
    "\n",
    "#         # Ensure all active streams have a size\n",
    "#         if len(current_input_sizes) != len(ACTIVE_STREAMS):\n",
    "#              missing_streams = set(ACTIVE_STREAMS) - set(current_input_sizes.keys())\n",
    "#              raise ValueError(f\"Input sizes could not be determined for all active streams. Missing: {missing_streams}\")\n",
    "\n",
    "#         model = MultiStreamAdaptiveLSTM(\n",
    "#             input_sizes=current_input_sizes,\n",
    "#             lstm_hidden_size=LSTM_HIDDEN_SIZE,\n",
    "#             num_lstm_layers=NUM_LSTM_LAYERS,\n",
    "#             num_classes=NUM_CLASSES,\n",
    "#             attention_dim=ATTENTION_DIM,\n",
    "#             dropout_rate=DROPOUT_RATE,\n",
    "#             stream_names=ACTIVE_STREAMS # Pass only active streams the model should use\n",
    "#         ).to(DEVICE)\n",
    "\n",
    "#         print(\"\\n--- Model Architecture ---\"); print(model);\n",
    "#         num_params = sum(p.numel() for p in model.parameters() if p.requires_grad);\n",
    "#         print(f\"Total Trainable Parameters: {num_params:,}\"); print(\"-\" * 30)\n",
    "\n",
    "#     except (KeyError, ValueError, TypeError, RuntimeError, Exception) as e:\n",
    "#          print(f\"FATAL ERROR initializing model: {e}\")\n",
    "#          raise # Re-raise to stop execution\n",
    "\n",
    "#     # --- Class Weighting & Optimizer ---\n",
    "#     print(\"\\nCalculating Class Weights for Loss Function...\")\n",
    "#     class_weights = torch.tensor([1.0, 1.0], dtype=torch.float32).to(DEVICE) # Default weights\n",
    "#     try:\n",
    "#         # Ensure train_dataset exists and has labels\n",
    "#         if train_dataset and hasattr(train_dataset, 'labels') and len(train_dataset.labels) > 0:\n",
    "#             balanced_train_labels_list = train_dataset.labels.cpu().numpy() # Get labels from balanced dataset instance\n",
    "#             count_0 = np.count_nonzero(balanced_train_labels_list == 0);\n",
    "#             count_1 = np.count_nonzero(balanced_train_labels_list == 1);\n",
    "#             total = len(balanced_train_labels_list)\n",
    "\n",
    "#             if total > 0 and count_0 > 0 and count_1 > 0: # Ensure counts are valid\n",
    "#                 weight_0 = total / (2.0 * count_0); weight_1 = total / (2.0 * count_1)\n",
    "#                 class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float32).to(DEVICE)\n",
    "#                 print(f\"Using Calculated Class Weights for Loss: 0={weight_0:.2f}, 1={weight_1:.2f}\")\n",
    "#             elif total > 0:\n",
    "#                  print(f\"Warning: Only one class present in balanced training labels (0: {count_0}, 1: {count_1}). Using default weights [1.0, 1.0].\")\n",
    "#             else:\n",
    "#                  print(\"Warning: Training dataset has no labels. Using default weights [1.0, 1.0].\")\n",
    "#         else:\n",
    "#             print(\"Warning: Could not access labels from training dataset. Using default weights [1.0, 1.0].\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#          print(f\"Error calculating class weights: {e}. Using default weights [1.0, 1.0].\")\n",
    "\n",
    "\n",
    "#     # Initialize criterion and optimizer only if model exists\n",
    "#     criterion = None\n",
    "#     optimizer = None\n",
    "#     if model:\n",
    "#          try:\n",
    "#              criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "#              optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "#          except Exception as e:\n",
    "#               print(f\"FATAL ERROR creating criterion or optimizer: {e}\")\n",
    "#               raise # Stop execution\n",
    "#     else:\n",
    "#          raise RuntimeError(\"Model was not initialized. Cannot create criterion/optimizer.\")\n",
    "\n",
    "\n",
    "#     best_val_f1 = -1.0; train_losses, val_losses = [], []; train_accs, val_accs = [], []; val_f1s = []\n",
    "#     best_epoch_path = \"\" # Initialize before loop\n",
    "\n",
    "\n",
    "#     # --- Training Loop ---\n",
    "#     print(\"\\n--- Starting Training ---\")\n",
    "#     if not train_loader or not val_loader:\n",
    "#          raise RuntimeError(\"DataLoaders are not available. Cannot start training.\")\n",
    "\n",
    "#     for epoch in range(NUM_EPOCHS):\n",
    "#         epoch_start_time = time.time()\n",
    "#         try:\n",
    "#             train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "#             val_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n",
    "#             epoch_duration = time.time() - epoch_start_time\n",
    "\n",
    "#             # Store metrics\n",
    "#             train_losses.append(train_loss); val_losses.append(val_metrics['loss'])\n",
    "#             train_accs.append(train_acc); val_accs.append(val_metrics['accuracy'])\n",
    "#             val_f1s.append(val_metrics['f1'])\n",
    "\n",
    "#             print(f\"--- Epoch {epoch+1}/{NUM_EPOCHS} ({epoch_duration:.2f} sec) ---\")\n",
    "#             print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "#             print(f\"  Val Loss:   {val_metrics.get('loss', float('nan')):.4f}, Val Acc:  {val_metrics.get('accuracy', float('nan')):.4f}\") # Use .get for safety\n",
    "#             print(f\"  Val Prec:   {val_metrics.get('precision', float('nan')):.4f}, Recall: {val_metrics.get('recall', float('nan')):.4f}, F1: {val_metrics.get('f1', float('nan')):.4f}\")\n",
    "#             print(f\"  Val AUC:    {val_metrics.get('auc', float('nan')):.4f}\") # AUC might be nan\n",
    "\n",
    "#             # Save best model based on validation F1\n",
    "#             current_f1 = val_metrics.get('f1', -1.0) # Default to -1 if F1 is missing/NaN\n",
    "#             if not np.isnan(current_f1) and current_f1 > best_val_f1:\n",
    "#                 best_val_f1 = current_f1\n",
    "#                 # Ensure active streams string is filesystem-safe\n",
    "#                 safe_streams_str = \"_\".join(sorted(ACTIVE_STREAMS)).replace('/', '_').replace('\\\\', '_')\n",
    "#                 current_best_epoch_path = f'best_model_{safe_streams_str}_ep{epoch+1}.pth'\n",
    "#                 try:\n",
    "#                     torch.save(model.state_dict(), current_best_epoch_path)\n",
    "#                     # Clean up previous best model file if it exists? Optional.\n",
    "#                     # if best_epoch_path and os.path.exists(best_epoch_path):\n",
    "#                     #     os.remove(best_epoch_path)\n",
    "#                     best_epoch_path = current_best_epoch_path # Store path to the new best model\n",
    "#                     print(f\"  >> Saved new best model to {best_epoch_path} (F1: {best_val_f1:.4f})\")\n",
    "#                 except Exception as e:\n",
    "#                      print(f\"  >> Error saving model checkpoint {current_best_epoch_path}: {e}\")\n",
    "#             print(\"-\" * 30)\n",
    "\n",
    "#         except KeyboardInterrupt:\n",
    "#              print(\"\\nTraining interrupted by user.\")\n",
    "#              break # Exit training loop\n",
    "#         except Exception as e:\n",
    "#              print(f\"\\n--- ERROR DURING EPOCH {epoch+1} ---\")\n",
    "#              print(f\"Error: {e}\")\n",
    "#              # Option: break the loop, or try to continue?\n",
    "#              # For stability, breaking might be safer if the error is persistent (e.g., OOM)\n",
    "#              print(\"Stopping training due to error.\")\n",
    "#              import traceback\n",
    "#              traceback.print_exc() # Print stack trace for debugging\n",
    "#              break\n",
    "\n",
    "#     print(\"--- Training Finished ---\")\n",
    "\n",
    "#     # --- Plotting ---\n",
    "#     print(\"\\n--- Plotting Training History ---\")\n",
    "#     try:\n",
    "#         # Ensure metrics lists are not empty before plotting\n",
    "#         if train_losses and val_losses and train_accs and val_accs and val_f1s:\n",
    "#             fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "#             epochs_range = range(1, len(train_losses) + 1) # Use actual number of completed epochs\n",
    "\n",
    "#             axes[0].plot(epochs_range, train_losses, label='Train Loss')\n",
    "#             axes[0].plot(epochs_range, val_losses, label='Val Loss')\n",
    "#             axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].set_title('Loss Curve'); axes[0].legend(); axes[0].grid(True)\n",
    "\n",
    "#             axes[1].plot(epochs_range, train_accs, label='Train Accuracy')\n",
    "#             axes[1].plot(epochs_range, val_accs, label='Val Accuracy')\n",
    "#             axes[1].plot(epochs_range, val_f1s, label='Val F1-Score', linestyle='--')\n",
    "#             axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Metric'); axes[1].set_title('Accuracy & F1-Score Curve'); axes[1].legend(); axes[1].grid(True)\n",
    "\n",
    "#             plt.tight_layout(); plt.show()\n",
    "#         else:\n",
    "#              print(\"Skipping plotting - insufficient metric data collected.\")\n",
    "#     except Exception as e:\n",
    "#          print(f\"Error generating plots: {e}\")\n",
    "\n",
    "\n",
    "#     # --- Final Evaluation ---\n",
    "#     print(\"\\n--- Final Evaluation on Validation Set using Best Model ---\")\n",
    "#     final_metrics = {} # Initialize before try\n",
    "#     true_labels, pred_labels = np.array([]), np.array([]) # Initialize before try\n",
    "#     try:\n",
    "#         # Check if a best model was saved and exists\n",
    "#         if best_epoch_path and os.path.exists(best_epoch_path):\n",
    "#             print(f\"Loading best saved model '{best_epoch_path}'\")\n",
    "#             try:\n",
    "#                 # Load state dict onto the correct device\n",
    "#                 state_dict = torch.load(best_epoch_path, map_location=DEVICE)\n",
    "#                 model.load_state_dict(state_dict)\n",
    "#                 print(\" -> Best model loaded successfully.\")\n",
    "#             except FileNotFoundError:\n",
    "#                  print(f\"Warn: Best model file '{best_epoch_path}' not found despite check. Evaluating final model state.\")\n",
    "#             except Exception as e:\n",
    "#                  print(f\"Warn: Error loading best model state_dict from {best_epoch_path}: {e}. Evaluating final model state.\")\n",
    "#         else:\n",
    "#              print(f\"Warning: No best model saved or found at '{best_epoch_path}'. Evaluating final model state from training end.\")\n",
    "\n",
    "#         # Perform final evaluation\n",
    "#         if model and val_loader and criterion:\n",
    "#              final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n",
    "#              true_labels, pred_labels = get_predictions_and_labels(model, val_loader, DEVICE)\n",
    "#         else:\n",
    "#              print(\"Error: Model, validation loader, or criterion not available for final evaluation.\")\n",
    "#              # Set final metrics to NaN/empty if evaluation cannot run\n",
    "#              final_metrics = {'accuracy': float('nan'), 'precision': float('nan'), 'recall': float('nan'), 'f1': float('nan'), 'auc': float('nan'), 'loss': float('nan')}\n",
    "#              true_labels, pred_labels = np.array([]), np.array([])\n",
    "\n",
    "\n",
    "#         # Display results\n",
    "#         print(\"\\n--- Final Performance Metrics ---\")\n",
    "#         print(f\"  Streams:   {', '.join(ACTIVE_STREAMS)}\")\n",
    "#         print(f\"  Accuracy:  {final_metrics.get('accuracy', float('nan')):.4f}\")\n",
    "#         print(f\"  Precision: {final_metrics.get('precision', float('nan')):.4f}\")\n",
    "#         print(f\"  Recall:    {final_metrics.get('recall', float('nan')):.4f}\")\n",
    "#         print(f\"  F1 Score:  {final_metrics.get('f1', float('nan')):.4f}\")\n",
    "#         print(f\"  AUC:       {final_metrics.get('auc', float('nan')):.4f}\")\n",
    "#         print(f\"  Loss:      {final_metrics.get('loss', float('nan')):.4f}\")\n",
    "#         if best_epoch_path:\n",
    "#              print(f\"  (Best Validation F1 during training: {best_val_f1:.4f} - from model '{os.path.basename(best_epoch_path)}')\")\n",
    "#         else:\n",
    "#              print(f\"  (Best Validation F1 during training: {best_val_f1:.4f} - model file not saved/found)\")\n",
    "\n",
    "\n",
    "#         # Plot Confusion Matrix\n",
    "#         if true_labels.size > 0 and pred_labels.size > 0: # Ensure labels were generated\n",
    "#              print(\"\\n--- Confusion Matrix ---\")\n",
    "#              try:\n",
    "#                  cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1]) # Explicitly define labels\n",
    "#                  labels_display = ['Not Crossing', 'Crossing']\n",
    "#                  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_display);\n",
    "#                  disp.plot(cmap=plt.cm.Blues);\n",
    "#                  plt.title(f'CM ({\", \".join(ACTIVE_STREAMS)})');\n",
    "#                  plt.show()\n",
    "#              except ValueError as e:\n",
    "#                   print(f\"Error generating confusion matrix: {e}\")\n",
    "#                   print(f\"True labels unique: {np.unique(true_labels)}, Pred labels unique: {np.unique(pred_labels)}\")\n",
    "#              except Exception as e:\n",
    "#                   print(f\"Unexpected error generating confusion matrix display: {e}\")\n",
    "#         else:\n",
    "#              print(\"Skipping confusion matrix - no prediction data available.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#          print(f\"\\n--- ERROR DURING FINAL EVALUATION ---\")\n",
    "#          print(f\"Error: {e}\")\n",
    "#          import traceback\n",
    "#          traceback.print_exc()\n",
    "\n",
    "\n",
    "#     print(\"\\n--- Script Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84cef572",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:28:27.161925Z",
     "iopub.status.busy": "2025-05-07T09:28:27.161711Z",
     "iopub.status.idle": "2025-05-07T09:28:27.174296Z",
     "shell.execute_reply": "2025-05-07T09:28:27.173594Z"
    },
    "papermill": {
     "duration": 0.025387,
     "end_time": "2025-05-07T09:28:27.175507",
     "exception": false,
     "start_time": "2025-05-07T09:28:27.150120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------------------------------------------------------\n",
    "# # CELL 1: DATA PREPARATION & BALANCING (Including YOLOP - Run Once)\n",
    "# # -----------------------------------------------------------------------------\n",
    "# #  This cell:\n",
    "# #    1. Loads (or regenerates) the PIE database\n",
    "# #    2. Computes per-signal standardisation scalers\n",
    "# #    3. Extracts ALL training sequences for every stream (incl. YOLOP)\n",
    "# #    4. Balances the dataset 50 / 50 on the crossing label\n",
    "# #    5. Writes two pickles:\n",
    "# #         - /kaggle/working/balanced_train_data_with_yolop.pkl\n",
    "# #         - /kaggle/working/scalers.pkl\n",
    "# # -----------------------------------------------------------------------------\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "# import time\n",
    "# import pickle\n",
    "# import gc\n",
    "# from pathlib import Path\n",
    "# import random # For balance_samples_count seed\n",
    "\n",
    "# import cv2                               # used internally by PIE utilities\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# #                                PIE utilities                                 #\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# pie_utilities_path = \"/kaggle/working/PIE/utilities\"\n",
    "# if pie_utilities_path not in sys.path:\n",
    "#     sys.path.insert(0, pie_utilities_path)\n",
    "\n",
    "# try:\n",
    "#     from pie_data import PIE\n",
    "# except ImportError as e:\n",
    "#     print(\n",
    "#         f\"[WARN] Could not import PIE from {pie_utilities_path}. \"\n",
    "#         f\"If the DB cache already exists this is fine.\\n→ {e}\"\n",
    "#     )\n",
    "#     PIE = None\n",
    "\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# #                              configuration                                   #\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# PIE_ROOT_PATH           = \"/kaggle/working/PIE\"\n",
    "# POSE_DATA_DIR           = \"/kaggle/input/pose-data/extracted_poses2\"\n",
    "# YOLOP_FEATURE_DIR       = '/kaggle/input/yolop-data/yolop features'  # <<< YOLOP DIR\n",
    "# PIE_DATABASE_CACHE_PATH = \"/kaggle/input/pie-database/pie_database.pkl\"\n",
    "\n",
    "# TRAIN_SETS_STR = [\"set01\", \"set02\", \"set04\"]\n",
    "\n",
    "# # --- Updated Output Paths ---\n",
    "# BALANCED_DATA_PKL_PATH  = \"/kaggle/working/balanced_train_data_with_yolop.pkl\" # <<< UPDATED\n",
    "# SCALERS_PKL_PATH        = \"/kaggle/working/scalers.pkl\"\n",
    "\n",
    "# # Streams used throughout the project ----------------------------------------\n",
    "# ALL_POSSIBLE_STREAMS = [\n",
    "#     \"bbox\",\n",
    "#     \"pose\",\n",
    "#     \"ego_speed\",\n",
    "#     \"ego_acc\",\n",
    "#     \"ego_gyro\",\n",
    "#     \"ped_action\",\n",
    "#     \"ped_look\",\n",
    "#     \"ped_occlusion\",\n",
    "#     \"traffic_light\",\n",
    "#     \"static_context\",\n",
    "#     \"yolop\" # <<< ADDED YOLOP\n",
    "# ]\n",
    "# print(f\"Data will be prepared for ALL streams: {ALL_POSSIBLE_STREAMS}\")\n",
    "\n",
    "\n",
    "# # Feature sizes & categorical constants --------------------------------------\n",
    "# SEQ_LEN, PRED_LEN = 30, 1\n",
    "\n",
    "# INPUT_SIZE_BBOX       = 4\n",
    "# INPUT_SIZE_POSE       = 34\n",
    "# INPUT_SIZE_EGO_SPEED  = 1\n",
    "# INPUT_SIZE_EGO_ACC    = 2\n",
    "# INPUT_SIZE_EGO_GYRO   = 1\n",
    "# INPUT_SIZE_PED_ACTION = 1\n",
    "# INPUT_SIZE_PED_LOOK   = 1\n",
    "# INPUT_SIZE_PED_OCC    = 1\n",
    "# INPUT_SIZE_TL_STATE   = 4\n",
    "\n",
    "# NUM_SIGNALIZED_CATS   = 4\n",
    "# NUM_INTERSECTION_CATS = 5\n",
    "# NUM_AGE_CATS          = 4\n",
    "# NUM_GENDER_CATS       = 3\n",
    "# NUM_TRAFFIC_DIR_CATS  = 2\n",
    "\n",
    "# LANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7: 4, 8: 4}\n",
    "# NUM_LANE_CATS   = len(set(LANE_CATEGORIES.values()))\n",
    "\n",
    "# INPUT_SIZE_STATIC = (\n",
    "#     NUM_SIGNALIZED_CATS\n",
    "#     + NUM_INTERSECTION_CATS\n",
    "#     + NUM_AGE_CATS\n",
    "#     + NUM_GENDER_CATS\n",
    "#     + NUM_TRAFFIC_DIR_CATS\n",
    "#     + NUM_LANE_CATS\n",
    "# )  # → 23\n",
    "\n",
    "# # --- YOLOP Size Calculation ---\n",
    "# GRID_SIZE = 3\n",
    "# YOLOP_DRIVABLE_FEATURES_DIM = GRID_SIZE * GRID_SIZE\n",
    "# YOLOP_LANE_FEATURES_DIM = GRID_SIZE * GRID_SIZE\n",
    "# YOLOP_OBJECT_FEATURES_DIM = 2 # Example: 2 object features (e.g. closest car x,y)\n",
    "# INPUT_SIZE_YOLOP = (\n",
    "#     YOLOP_DRIVABLE_FEATURES_DIM + YOLOP_LANE_FEATURES_DIM + YOLOP_OBJECT_FEATURES_DIM\n",
    "# ) # -> 9 + 9 + 2 = 20\n",
    "# # ---\n",
    "\n",
    "# TL_STATE_MAP = {\"__undefined__\": 0, \"red\": 1, \"yellow\": 2, \"green\": 3}\n",
    "# NUM_TL_STATES = len(TL_STATE_MAP)\n",
    "\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# #                               helper utils                                   #\n",
    "# # -----------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "# def to_one_hot(index: int, num_classes: int) -> np.ndarray:\n",
    "#     vec = np.zeros(num_classes, dtype=np.float32)\n",
    "#     safe_idx = int(np.clip(index, 0, num_classes - 1)) # Ensure index is within bounds\n",
    "#     vec[safe_idx] = 1.0\n",
    "#     return vec\n",
    "\n",
    "# # Using the more robust balance_samples_count from the first \"previous code\"\n",
    "# def balance_samples_count(seq_data: dict, label_key: str, seed: int = 42) -> dict:\n",
    "#     print('---------------------------------------------------------')\n",
    "#     print(f\"Balancing samples based on '{label_key}' key\")\n",
    "\n",
    "#     if label_key not in seq_data or not seq_data[label_key]:\n",
    "#         raise KeyError(f\"Label type '{label_key}' not found or is empty in seq_data.\")\n",
    "\n",
    "#     try:\n",
    "#         # Assuming labels are like [[0], [1], [0]]\n",
    "#         gt_labels = [lbl[0] for lbl in seq_data[label_key]]\n",
    "#     except (IndexError, TypeError) as e:\n",
    "#         raise ValueError(\n",
    "#             f\"Labels under '{label_key}' not in expected format [[label_val]]. Error: {e}\"\n",
    "#         ) from e\n",
    "\n",
    "#     if not all(isinstance(l, (int, float)) and l in [0, 1] for l in gt_labels):\n",
    "#         print(f\"Warning: Labels for balancing contain values other than 0 or 1 or are not numeric: {set(gt_labels)}\")\n",
    "\n",
    "#     num_pos = np.count_nonzero(np.array(gt_labels)) # Count 1s\n",
    "#     num_neg = len(gt_labels) - num_pos # Count 0s\n",
    "#     new_seq_data = {}\n",
    "\n",
    "#     if num_neg == num_pos:\n",
    "#         print(\"Samples already balanced.\")\n",
    "#         return seq_data.copy()\n",
    "\n",
    "#     print(f\"Unbalanced: Positive (1): {num_pos} | Negative (0): {num_neg}\")\n",
    "#     majority_label = 0 if num_neg > num_pos else 1\n",
    "#     minority_count = min(num_neg, num_pos)\n",
    "#     print(f\"Undersampling majority class ({majority_label}) to match count ({minority_count}).\")\n",
    "\n",
    "#     majority_indices = np.where(np.array(gt_labels) == majority_label)[0]\n",
    "#     minority_indices = np.where(np.array(gt_labels) != majority_label)[0]\n",
    "\n",
    "#     np.random.seed(seed) # Use the provided seed for reproducibility\n",
    "#     keep_majority_indices = np.random.choice(majority_indices, size=minority_count, replace=False)\n",
    "#     final_indices = np.concatenate((minority_indices, keep_majority_indices))\n",
    "#     np.random.shuffle(final_indices) # Shuffle the combined indices\n",
    "\n",
    "#     for k, v_list in seq_data.items():\n",
    "#         if isinstance(v_list, list) and len(v_list) == len(gt_labels):\n",
    "#             try:\n",
    "#                 # Check if list contains numpy arrays before converting the whole list\n",
    "#                 if v_list and isinstance(v_list[0], np.ndarray):\n",
    "#                     try:\n",
    "#                         v_array = np.array(v_list, dtype=object) # Use dtype=object if shapes vary\n",
    "#                         new_seq_data[k] = list(v_array[final_indices])\n",
    "#                     except ValueError as ve: # If shapes are truly incompatible for np.array\n",
    "#                         print(f\"Warning: Could not convert list for key '{k}' to single NumPy array due to varying shapes/types. Processing element-wise. Error: {ve}\")\n",
    "#                         new_seq_data[k] = [v_list[i] for i in final_indices] # Fallback\n",
    "#                 else: # Simple list comprehension for non-array lists\n",
    "#                     new_seq_data[k] = [v_list[i] for i in final_indices]\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing key '{k}' during balancing: {e}. Assigning empty list.\")\n",
    "#                 new_seq_data[k] = [] # Or handle as appropriate\n",
    "#         else: # Keep lists that don't match the label length (e.g., metadata)\n",
    "#             # print(f\"Warn: Skipping key '{k}' in balancing (length mismatch or not a list).\") # Can be verbose\n",
    "#             new_seq_data[k] = v_list # Keep original\n",
    "\n",
    "#     if label_key in new_seq_data and new_seq_data[label_key]: # Check if label key exists and is not empty\n",
    "#         new_gt_labels = [lbl[0] for lbl in new_seq_data[label_key]]\n",
    "#         final_pos = np.count_nonzero(np.array(new_gt_labels))\n",
    "#         final_neg = len(new_gt_labels) - final_pos\n",
    "#         print(f'Balanced:   Positive (1): {final_pos} | Negative (0): {final_neg}')\n",
    "#     else:\n",
    "#         print(f\"Warning: Label key '{label_key}' missing or empty after balancing.\")\n",
    "#     print('---------------------------------------------------------')\n",
    "#     return new_seq_data\n",
    "\n",
    "\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# #                                PIEDataset                                    #\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# class PIEDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Lightweight dataset that can generate any subset of the PIE feature streams,\n",
    "#     including YOLOP.\n",
    "#     \"\"\"\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         pie_db: dict,\n",
    "#         set_names: list[str],\n",
    "#         pose_dir: str,\n",
    "#         yolop_dir: str, # <<< ADDED YOLOP DIR PARAM\n",
    "#         seq_len: int,\n",
    "#         pred_len: int,\n",
    "#         scalers: dict,\n",
    "#         streams_to_generate: list[str], # Streams to actually process and return\n",
    "#     ):\n",
    "#         self.pie_db            = pie_db\n",
    "#         self.set_names         = set_names\n",
    "#         self.pose_dir          = pose_dir\n",
    "#         self.yolop_dir         = yolop_dir # <<< STORE YOLOP DIR\n",
    "#         self.seq_len           = seq_len\n",
    "#         self.pred_len          = pred_len\n",
    "#         self.scalers           = scalers or {} # Ensure scalers is a dict\n",
    "#         self.streams           = streams_to_generate or ALL_POSSIBLE_STREAMS\n",
    "#         self._input_sizes      = self._build_input_size_map() # Map ALL streams to sizes\n",
    "#         self.all_pose_data     = {}\n",
    "#         self.all_yolop_data    = {} # <<< ADDED YOLOP DATA STORAGE\n",
    "\n",
    "#         # Load data only if the stream is in the requested streams_to_generate\n",
    "#         if \"pose\" in self.streams:\n",
    "#             self._load_pose_pkls()\n",
    "#         if \"yolop\" in self.streams:\n",
    "#              self._load_yolop_data()\n",
    "\n",
    "#         self._enumerate_sequences()\n",
    "#         if not self.sequences: # If no sequences found after enumeration\n",
    "#              raise ValueError(f\"PIEDataset initialization failed: No valid sequences found for sets {self.set_names} \"\n",
    "#                               f\"with seq_len={self.seq_len} and pred_len={self.pred_len}.\")\n",
    "\n",
    "#     # ------------------------ internal helpers -------------------------------\n",
    "#     def _build_input_size_map(self) -> dict:\n",
    "#         \"\"\" Builds a map of stream_name to its feature dimension for ALL streams. \"\"\"\n",
    "#         special = {\n",
    "#             \"TRAFFIC_LIGHT\": \"TL_STATE\",\n",
    "#             \"STATIC_CONTEXT\": \"STATIC\",\n",
    "#             \"EGO_SPEED\": \"EGO_SPEED\",\n",
    "#             \"EGO_ACC\": \"EGO_ACC\",\n",
    "#             \"EGO_GYRO\": \"EGO_GYRO\",\n",
    "#             \"PED_ACTION\": \"PED_ACTION\",\n",
    "#             \"PED_LOOK\": \"PED_LOOK\",\n",
    "#             \"PED_OCCLUSION\": \"PED_OCC\",\n",
    "#             \"YOLOP\": \"YOLOP\", # <<< ADDED YOLOP\n",
    "#         }\n",
    "#         sizes = {}\n",
    "#         for s in ALL_POSSIBLE_STREAMS: # Iterate through all defined streams\n",
    "#             const_suffix = s.upper()\n",
    "#             if const_suffix in special:\n",
    "#                 const_suffix = special[const_suffix] # Use mapped suffix if special case\n",
    "\n",
    "#             const_name = f\"INPUT_SIZE_{const_suffix}\"\n",
    "\n",
    "#             # Explicit overrides for common streams if not perfectly mapped by special\n",
    "#             if s == \"bbox\": const_name = \"INPUT_SIZE_BBOX\"\n",
    "#             elif s == \"pose\": const_name = \"INPUT_SIZE_POSE\"\n",
    "#             # YOLOP is handled by the special dict\n",
    "\n",
    "#             sizes[s] = globals().get(const_name, 1) # Default to 1 if constant not found\n",
    "#         return sizes\n",
    "\n",
    "#     def _load_pose_pkls(self):\n",
    "#         print(\"Loading pose PKLs …\")\n",
    "#         loaded_any_pose = False\n",
    "#         for set_id in self.set_names:\n",
    "#             set_dir = Path(self.pose_dir) / set_id\n",
    "#             if not set_dir.is_dir():\n",
    "#                 # print(f\"[Info] Pose directory not found for set {set_id}: {set_dir}\")\n",
    "#                 continue\n",
    "#             self.all_pose_data[set_id] = {}\n",
    "#             pkl_files = list(set_dir.glob(f\"{set_id}_*_poses.pkl\"))\n",
    "#             if not pkl_files:\n",
    "#                 # print(f\"[Info] No pose PKL files found in {set_dir}\")\n",
    "#                 continue\n",
    "\n",
    "#             for pkl_path in tqdm(pkl_files, desc=f\"Pose PKLs {set_id}\", leave=False):\n",
    "#                 try:\n",
    "#                     with open(pkl_path, \"rb\") as fp:\n",
    "#                         loaded = pickle.load(fp)\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"[WARN] Pose load error {pkl_path}: {e}\")\n",
    "#                     continue\n",
    "\n",
    "#                 if not isinstance(loaded, dict) or len(loaded) != 1: # Check format\n",
    "#                     # print(f\"[WARN] Unexpected format in pose PKL {pkl_path}. Skipping.\")\n",
    "#                     continue\n",
    "\n",
    "#                 (key, data), *_ = loaded.items() # Get the single item\n",
    "#                 # Robustly parse video ID from the key (e.g., \"set01_vid01\" -> \"vid01\")\n",
    "#                 parts = key.split('_')\n",
    "#                 if len(parts) < 2: continue # Needs at least setX_vidY\n",
    "#                 vid = \"_\".join(parts[1:])\n",
    "\n",
    "#                 # Check if this video ID exists in the main PIE database for this set\n",
    "#                 if set_id in self.pie_db and vid in self.pie_db[set_id]:\n",
    "#                     self.all_pose_data[set_id][vid] = data\n",
    "#                     loaded_any_pose = True\n",
    "#         if not loaded_any_pose:\n",
    "#             print(\"[WARN] No pose data was successfully loaded.\")\n",
    "\n",
    "\n",
    "#     # --- ADDED _load_yolop_data METHOD (adapted from previous prompt's version) ---\n",
    "#     def _load_yolop_data(self):\n",
    "#         print(f\"Loading YOLOP data for sets: {self.set_names} from {self.yolop_dir}\")\n",
    "#         loaded_any_yolop = False\n",
    "#         # Initialize dictionaries for all sets requested by this dataset instance\n",
    "#         for set_id in self.set_names:\n",
    "#             self.all_yolop_data[set_id] = {} # Ensure dict exists for each set\n",
    "\n",
    "#         yolop_path_obj = Path(self.yolop_dir)\n",
    "#         if not yolop_path_obj.is_dir():\n",
    "#             print(f\"[ERROR] YOLOP feature directory not found: {self.yolop_dir}\")\n",
    "#             return\n",
    "\n",
    "#         try:\n",
    "#             # Look for files directly in the directory: e.g. set01_vid01_yolop_features.pkl\n",
    "#             all_pkl_files = list(yolop_path_obj.glob(\"*_yolop_features.pkl\"))\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERROR] Error listing files in YOLOP directory {self.yolop_dir}: {e}\")\n",
    "#             return\n",
    "\n",
    "#         if not all_pkl_files:\n",
    "#             print(f\"[WARN] No '*_yolop_features.pkl' files found directly in {self.yolop_dir}.\")\n",
    "#             return\n",
    "\n",
    "#         loaded_file_count = 0\n",
    "#         files_for_needed_sets = 0\n",
    "\n",
    "#         for pkl_file_path in tqdm(all_pkl_files, desc=\"Loading YOLOP PKLs\", leave=False):\n",
    "#             pkl_filename = pkl_file_path.name\n",
    "#             try:\n",
    "#                 # Robust parsing of set/video ID from filename\n",
    "#                 # e.g., \"set01_vid02_yolop_features.pkl\" -> set_id=\"set01\", video_id=\"vid02\"\n",
    "#                 parts = pkl_filename.replace(\"_yolop_features.pkl\", \"\").split('_')\n",
    "#                 if len(parts) < 2: # Needs at least setX_videoY\n",
    "#                      raise IndexError(\"Filename does not contain enough parts for set and video ID.\")\n",
    "#                 set_id_from_file = parts[0]\n",
    "#                 video_id = \"_\".join(parts[1:]) # Handles video IDs with underscores\n",
    "#             except IndexError as e:\n",
    "#                 print(f\"[WARN] Could not parse set/video ID from YOLOP filename '{pkl_filename}': {e}. Skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#             # Process only if the set is one of the sets this PIEDataset instance is for\n",
    "#             if set_id_from_file in self.set_names:\n",
    "#                 files_for_needed_sets += 1\n",
    "#                 try:\n",
    "#                     with open(pkl_file_path, 'rb') as f:\n",
    "#                         loaded_pkl_content = pickle.load(f)\n",
    "\n",
    "#                     # YOLOP PKL format: { \"setXX_videoYY\": {frame_num: {ped_id: yolop_vector, ...}, ...} }\n",
    "#                     # Or: { \"setXX_videoYY\": {ped_id: {frame_num: yolop_vector, ...}, ...} } - Check your structure\n",
    "#                     # The provided previous code implies the former: loaded_pkl_content has 1 key\n",
    "#                     if not isinstance(loaded_pkl_content, dict) or len(loaded_pkl_content) != 1:\n",
    "#                         print(f\"[WARN] YOLOP PKL {pkl_filename} has unexpected format (expected dict with 1 video key). Skip.\")\n",
    "#                         continue\n",
    "\n",
    "#                     # The single key in the PKL should be like \"setID_videoID\"\n",
    "#                     unique_video_key_in_pkl, per_video_yolop_data = list(loaded_pkl_content.items())[0]\n",
    "\n",
    "#                     # Store it under the parsed set_id_from_file and video_id\n",
    "#                     # Ensure the set_id sub-dictionary exists\n",
    "#                     if set_id_from_file not in self.all_yolop_data:\n",
    "#                         self.all_yolop_data[set_id_from_file] = {}\n",
    "\n",
    "#                     # Now, 'per_video_yolop_data' should be the dict: {frame_num: {ped_id: features}}\n",
    "#                     self.all_yolop_data[set_id_from_file][video_id] = per_video_yolop_data\n",
    "#                     loaded_file_count += 1\n",
    "#                     loaded_any_yolop = True\n",
    "\n",
    "#                 except FileNotFoundError: # Should not happen with Path.glob\n",
    "#                      print(f\"[WARN] YOLOP feature file not found during loading (unexpected): {pkl_file_path}\")\n",
    "#                 except pickle.UnpicklingError as pe:\n",
    "#                      print(f\"[ERROR] Error unpickling YOLOP file {pkl_file_path}: {pe}. Skipping.\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"[ERROR] Error loading or processing YOLOP PKL {pkl_file_path}: {e}\")\n",
    "\n",
    "#         print(f\"Finished loading YOLOP data. Found {loaded_file_count} relevant files out of {files_for_needed_sets} expected for sets {self.set_names}.\")\n",
    "#         if not loaded_any_yolop and files_for_needed_sets > 0 :\n",
    "#              print(\"[WARN] No YOLOP data was successfully loaded for the required sets.\")\n",
    "#     # --- END YOLOP LOAD ---\n",
    "\n",
    "\n",
    "#     def _enumerate_sequences(self):\n",
    "#         print(\"Enumerating sequences …\")\n",
    "#         self.sequences = [] # Reset sequences\n",
    "#         for set_id in self.set_names:\n",
    "#             if set_id not in self.pie_db: # Skip if set is not in the loaded PIE database\n",
    "#                 # print(f\"[Info] Set {set_id} not found in PIE database. Skipping sequence enumeration for this set.\")\n",
    "#                 continue\n",
    "\n",
    "#             for vid, vdb in self.pie_db.get(set_id, {}).items():\n",
    "#                 ped_annotations = vdb.get(\"ped_annotations\", {})\n",
    "#                 if not isinstance(ped_annotations, dict): continue # Ensure it's a dict\n",
    "\n",
    "#                 for pid, pdb in ped_annotations.items():\n",
    "#                     if not isinstance(pdb, dict): continue # Ensure ped data is a dict\n",
    "#                     frames = pdb.get(\"frames\", [])\n",
    "\n",
    "#                     if not isinstance(frames, list) or len(frames) < self.seq_len + self.pred_len:\n",
    "#                         continue # Not enough frames for a sequence\n",
    "\n",
    "#                     try:\n",
    "#                         # Ensure frames are sortable (e.g., integers)\n",
    "#                         frames_sorted = sorted([int(f) for f in frames])\n",
    "#                     except (ValueError, TypeError):\n",
    "#                          # print(f\"[WARN] Non-integer or unsortable frame numbers for {set_id}/{vid}/{pid}. Skipping ped.\")\n",
    "#                          continue # Skip this pedestrian if frames are invalid\n",
    "\n",
    "#                     for i in range(len(frames_sorted) - self.seq_len - self.pred_len + 1):\n",
    "#                         start_frame = frames_sorted[i]\n",
    "#                         obs_end_frame = frames_sorted[i + self.seq_len - 1]\n",
    "\n",
    "#                         # Check sequence continuity for observation window\n",
    "#                         if obs_end_frame - start_frame != self.seq_len - 1:\n",
    "#                             continue\n",
    "\n",
    "#                         target_frame_actual_idx = i + self.seq_len + self.pred_len - 1\n",
    "#                         # Bounds check (already implicitly handled by range end, but explicit check is fine)\n",
    "#                         if target_frame_actual_idx >= len(frames_sorted):\n",
    "#                             continue\n",
    "#                         target_frame = frames_sorted[target_frame_actual_idx]\n",
    "\n",
    "#                         # Check sequence continuity for prediction horizon\n",
    "#                         if target_frame - obs_end_frame != self.pred_len:\n",
    "#                             continue\n",
    "\n",
    "#                         self.sequences.append((set_id, vid, pid, start_frame))\n",
    "#         print(f\"Total sequences enumerated: {len(self.sequences)}\")\n",
    "\n",
    "#     # ------------------ Dataset API ------------------------------------------\n",
    "#     def __len__(self):\n",
    "#         return len(self.sequences)\n",
    "\n",
    "#     def __getitem__(self, idx: int):\n",
    "#         set_id, vid, pid, start_fnum = self.sequences[idx]\n",
    "\n",
    "#         # Safe dictionary access\n",
    "#         vdb  = self.pie_db.get(set_id, {}).get(vid, {})\n",
    "#         pdb  = vdb.get(\"ped_annotations\", {}).get(pid, {})\n",
    "#         ego_db  = vdb.get(\"vehicle_annotations\", {})\n",
    "#         tldb = vdb.get(\"traffic_annotations\", {})\n",
    "\n",
    "#         frame_nums = list(range(start_fnum, start_fnum + self.seq_len))\n",
    "#         target_f_label   = start_fnum + self.seq_len + self.pred_len - 1\n",
    "\n",
    "#         # label ---------------------------------------------------------------\n",
    "#         label = 0 # Default to not crossing\n",
    "#         if isinstance(pdb, dict) and \"frames\" in pdb and isinstance(pdb[\"frames\"], list) and \\\n",
    "#            \"behavior\" in pdb and isinstance(pdb[\"behavior\"], dict) and \\\n",
    "#            \"cross\" in pdb[\"behavior\"] and isinstance(pdb[\"behavior\"][\"cross\"], list):\n",
    "#             try:\n",
    "#                 target_f_comp = int(target_f_label) # Ensure comparable type\n",
    "#                 label_idx = pdb[\"frames\"].index(target_f_comp)\n",
    "#                 if label_idx < len(pdb[\"behavior\"][\"cross\"]): # Check bounds for 'cross' list\n",
    "#                      label_val = pdb[\"behavior\"][\"cross\"][label_idx]\n",
    "#                      # Convert -1 or other non-1 values to 0\n",
    "#                      if isinstance(label_val, (int, float)) and label_val == 1:\n",
    "#                          label = 1\n",
    "#             except (ValueError, TypeError, IndexError): pass # Keep default label if lookup fails\n",
    "\n",
    "#         # static context (calculated once per item) ---------------------------\n",
    "#         static_vec = None\n",
    "#         if \"static_context\" in self.streams: # Check if this stream is requested\n",
    "#             static_vec = np.zeros(INPUT_SIZE_STATIC, np.float32) # Default\n",
    "#             ped_attr  = pdb.get(\"attributes\", {}) if isinstance(pdb, dict) else {}\n",
    "#             if isinstance(ped_attr, dict):\n",
    "#                 try:\n",
    "#                     sig   = ped_attr.get(\"signalized\", 0)\n",
    "#                     intr  = ped_attr.get(\"intersection\", 0)\n",
    "#                     age   = ped_attr.get(\"age\", 2) # Default to adult\n",
    "#                     gen   = ped_attr.get(\"gender\", 0) # Default to n/a\n",
    "#                     tdir  = int(ped_attr.get(\"traffic_direction\", 0)) # Ensure int\n",
    "#                     ln_val    = ped_attr.get(\"num_lanes\", 2) # Default to 2 lanes\n",
    "#                     # Handle potential non-integer lane values for category mapping\n",
    "#                     lncat = LANE_CATEGORIES.get(int(ln_val), LANE_CATEGORIES[max(LANE_CATEGORIES.keys())]) \\\n",
    "#                             if isinstance(ln_val, (int, str)) and str(ln_val).isdigit() \\\n",
    "#                             else LANE_CATEGORIES[max(LANE_CATEGORIES.keys())]\n",
    "\n",
    "#                     static_components = [\n",
    "#                         to_one_hot(sig,  NUM_SIGNALIZED_CATS),\n",
    "#                         to_one_hot(intr, NUM_INTERSECTION_CATS),\n",
    "#                         to_one_hot(age,  NUM_AGE_CATS),\n",
    "#                         to_one_hot(gen,  NUM_GENDER_CATS),\n",
    "#                         to_one_hot(tdir, NUM_TRAFFIC_DIR_CATS),\n",
    "#                         to_one_hot(lncat, NUM_LANE_CATS),\n",
    "#                     ]\n",
    "#                     static_vec_candidate = np.concatenate(static_components).astype(np.float32)\n",
    "#                     if static_vec_candidate.shape[0] == INPUT_SIZE_STATIC:\n",
    "#                          static_vec = static_vec_candidate\n",
    "#                     # else: print(f\"[WARN] Static vec shape mismatch idx {idx}\") # Already default to zeros\n",
    "#                 except Exception as e:\n",
    "#                      # print(f\"[WARN] Error processing static context idx {idx}: {e}\")\n",
    "#                      pass # static_vec remains zeros\n",
    "\n",
    "\n",
    "#         # per-frame feature assembly -----------------------------------------\n",
    "#         # Initialize features for streams requested by this Dataset instance\n",
    "#         feats_sequences = {s: [] for s in self.streams}\n",
    "\n",
    "#         for fn_current in frame_nums:\n",
    "#             frame_db_idx = -1 # Index of current frame in pedestrian's frame list\n",
    "#             if isinstance(pdb, dict) and \"frames\" in pdb and isinstance(pdb[\"frames\"], list):\n",
    "#                 try:\n",
    "#                     fn_comp = int(fn_current) # Ensure comparable type\n",
    "#                     frame_db_idx = pdb[\"frames\"].index(fn_comp)\n",
    "#                 except (ValueError, TypeError): pass # frame_db_idx remains -1\n",
    "\n",
    "#             ego_frame_data = ego_db.get(fn_current, {}) if isinstance(ego_db, dict) else {}\n",
    "#             if not isinstance(ego_frame_data, dict): ego_frame_data = {} # Ensure it's a dict\n",
    "\n",
    "#             # --- Extract features only for streams in self.streams ---\n",
    "\n",
    "#             if \"bbox\" in self.streams:\n",
    "#                 bb_norm = np.zeros(INPUT_SIZE_BBOX, np.float32)\n",
    "#                 if frame_db_idx != -1 and isinstance(pdb, dict) and \"bbox\" in pdb and \\\n",
    "#                    isinstance(pdb[\"bbox\"], list) and len(pdb[\"bbox\"]) > frame_db_idx:\n",
    "#                     try:\n",
    "#                         coords = pdb[\"bbox\"][frame_db_idx]\n",
    "#                         if isinstance(coords, (list, tuple)) and len(coords) == 4:\n",
    "#                             x1, y1, x2, y2 = coords\n",
    "#                             w_img = vdb.get(\"width\", 1920); h_img = vdb.get(\"height\", 1080)\n",
    "#                             if isinstance(w_img, (int,float)) and w_img > 0 and isinstance(h_img,(int,float)) and h_img > 0:\n",
    "#                                 cx = ((x1 + x2) / 2) / w_img; cy = ((y1 + y2) / 2) / h_img\n",
    "#                                 w  = (x2 - x1) / w_img;     h  = (y2 - y1) / h_img\n",
    "#                                 if w > 0 and h > 0 and 0 <= cx <= 1 and 0 <= cy <= 1:\n",
    "#                                     bb_norm = np.array([cx, cy, w, h], np.float32)\n",
    "#                     except Exception: pass # bb_norm remains zeros\n",
    "#                 feats_sequences[\"bbox\"].append(bb_norm)\n",
    "\n",
    "#             if \"pose\" in self.streams:\n",
    "#                 pvec = np.zeros(INPUT_SIZE_POSE, np.float32)\n",
    "#                 pose_set_for_video = self.all_pose_data.get(set_id, {}).get(vid, {})\n",
    "#                 if isinstance(pose_set_for_video, dict):\n",
    "#                     pose_for_frame = pose_set_for_video.get(fn_current, {})\n",
    "#                     if isinstance(pose_for_frame, dict):\n",
    "#                         p_loaded = pose_for_frame.get(pid)\n",
    "#                         if isinstance(p_loaded, np.ndarray) and p_loaded.shape == (INPUT_SIZE_POSE,):\n",
    "#                             pvec = p_loaded\n",
    "#                 feats_sequences[\"pose\"].append(pvec)\n",
    "\n",
    "#             if \"ego_speed\" in self.streams:\n",
    "#                 s_scaled = 0.0\n",
    "#                 try:\n",
    "#                     s_val = ego_frame_data.get(\"OBD_speed\", 0.0) or ego_frame_data.get(\"GPS_speed\", 0.0)\n",
    "#                     s_val = float(s_val)\n",
    "#                     mean = self.scalers.get(\"ego_speed_mean\", 0.0); std = self.scalers.get(\"ego_speed_std\", 1.0)\n",
    "#                     s_scaled = (s_val - mean) / std if std != 0 else 0.0\n",
    "#                 except Exception: pass # s_scaled remains 0.0\n",
    "#                 feats_sequences[\"ego_speed\"].append([s_scaled])\n",
    "\n",
    "#             if \"ego_acc\" in self.streams:\n",
    "#                 ax_s, ay_s = 0.0, 0.0\n",
    "#                 try:\n",
    "#                     ax = float(ego_frame_data.get(\"accX\", 0.0))\n",
    "#                     ay = float(ego_frame_data.get(\"accY\", 0.0))\n",
    "#                     ax_m, ax_std = self.scalers.get(\"accX_mean\", 0.0), self.scalers.get(\"accX_std\", 1.0)\n",
    "#                     ay_m, ay_std = self.scalers.get(\"accY_mean\", 0.0), self.scalers.get(\"accY_std\", 1.0)\n",
    "#                     ax_s = (ax - ax_m) / ax_std if ax_std != 0 else 0.0\n",
    "#                     ay_s = (ay - ay_m) / ay_std if ay_std != 0 else 0.0\n",
    "#                 except Exception: pass # ax_s, ay_s remain 0.0\n",
    "#                 feats_sequences[\"ego_acc\"].append([ax_s, ay_s])\n",
    "\n",
    "#             if \"ego_gyro\" in self.streams:\n",
    "#                 gz_s = 0.0\n",
    "#                 try:\n",
    "#                     gz = float(ego_frame_data.get(\"gyroZ\", 0.0))\n",
    "#                     mean, std = self.scalers.get(\"gyroZ_mean\", 0.0), self.scalers.get(\"gyroZ_std\", 1.0)\n",
    "#                     gz_s = (gz - mean) / std if std != 0 else 0.0\n",
    "#                 except Exception: pass # gz_s remains 0.0\n",
    "#                 feats_sequences[\"ego_gyro\"].append([gz_s])\n",
    "\n",
    "#             if \"ped_action\" in self.streams:\n",
    "#                 action = 0.0\n",
    "#                 if frame_db_idx != -1 and isinstance(pdb, dict) and \"behavior\" in pdb and \\\n",
    "#                    isinstance(pdb[\"behavior\"], dict) and \"action\" in pdb[\"behavior\"] and \\\n",
    "#                    isinstance(pdb[\"behavior\"][\"action\"], list) and len(pdb[\"behavior\"][\"action\"]) > frame_db_idx:\n",
    "#                     try: action = float(pdb[\"behavior\"][\"action\"][frame_db_idx])\n",
    "#                     except (TypeError, ValueError): pass\n",
    "#                 feats_sequences[\"ped_action\"].append([action])\n",
    "\n",
    "#             if \"ped_look\" in self.streams:\n",
    "#                 look = 0.0\n",
    "#                 if frame_db_idx != -1 and isinstance(pdb, dict) and \"behavior\" in pdb and \\\n",
    "#                    isinstance(pdb[\"behavior\"], dict) and \"look\" in pdb[\"behavior\"] and \\\n",
    "#                    isinstance(pdb[\"behavior\"][\"look\"], list) and len(pdb[\"behavior\"][\"look\"]) > frame_db_idx:\n",
    "#                     try: look = float(pdb[\"behavior\"][\"look\"][frame_db_idx])\n",
    "#                     except (TypeError, ValueError): pass\n",
    "#                 feats_sequences[\"ped_look\"].append([look])\n",
    "\n",
    "#             if \"ped_occlusion\" in self.streams:\n",
    "#                 occ = 0.0\n",
    "#                 if frame_db_idx != -1 and isinstance(pdb, dict) and \"occlusion\" in pdb and \\\n",
    "#                    isinstance(pdb[\"occlusion\"], list) and len(pdb[\"occlusion\"]) > frame_db_idx:\n",
    "#                     try:\n",
    "#                         occ_val = pdb[\"occlusion\"][frame_db_idx]\n",
    "#                         occ = float(occ_val) / 2.0 if isinstance(occ_val, (int,float)) else 0.0\n",
    "#                     except (TypeError, ValueError): pass\n",
    "#                 feats_sequences[\"ped_occlusion\"].append([occ])\n",
    "\n",
    "#             if \"traffic_light\" in self.streams:\n",
    "#                 tl_state_int = 0 # Default to '__undefined__'\n",
    "#                 if isinstance(tldb, dict):\n",
    "#                     for obj_data in tldb.values():\n",
    "#                         if isinstance(obj_data, dict) and obj_data.get(\"obj_class\") == \"traffic_light\" and \\\n",
    "#                            \"frames\" in obj_data and isinstance(obj_data[\"frames\"], list) and \\\n",
    "#                            \"state\" in obj_data and isinstance(obj_data[\"state\"], list):\n",
    "#                             try:\n",
    "#                                 fn_comp = int(fn_current)\n",
    "#                                 tl_idx = obj_data[\"frames\"].index(fn_comp)\n",
    "#                                 if tl_idx < len(obj_data[\"state\"]):\n",
    "#                                     state_val = obj_data[\"state\"][tl_idx]\n",
    "#                                     # Use state if defined and non-zero (0 is often undefined for TL)\n",
    "#                                     if state_val in TL_STATE_MAP and state_val != 0:\n",
    "#                                          tl_state_int = TL_STATE_MAP[state_val] if isinstance(state_val, str) else int(state_val)\n",
    "#                                          break # Found relevant TL\n",
    "#                             except (ValueError, TypeError, IndexError): continue\n",
    "#                 feats_sequences[\"traffic_light\"].append(to_one_hot(tl_state_int, NUM_TL_STATES))\n",
    "\n",
    "#             if \"static_context\" in self.streams:\n",
    "#                 # Append the single static_vec calculated outside the loop for each frame\n",
    "#                 feats_sequences[\"static_context\"].append(static_vec if static_vec is not None else np.zeros(INPUT_SIZE_STATIC, np.float32))\n",
    "\n",
    "#             # <<< YOLOP Feature Extraction (only if 'yolop' is in self.streams) >>>\n",
    "#             if 'yolop' in self.streams:\n",
    "#                 yolop_vec = np.zeros(INPUT_SIZE_YOLOP, dtype=np.float32) # Default\n",
    "#                 # Safely access nested YOLOP data: self.all_yolop_data[set_id][video_id][frame_num][ped_id]\n",
    "#                 yolop_set_data = self.all_yolop_data.get(set_id, {})\n",
    "#                 if isinstance(yolop_set_data, dict):\n",
    "#                      yolop_vid_data = yolop_set_data.get(vid, {}) # vid from sequence tuple\n",
    "#                      if isinstance(yolop_vid_data, dict):\n",
    "#                           # fn_current is the current frame number in the loop\n",
    "#                           frame_yolo_data_for_peds = yolop_vid_data.get(fn_current, {})\n",
    "#                           if isinstance(frame_yolo_data_for_peds, dict):\n",
    "#                                loaded_yolo_for_ped = frame_yolo_data_for_peds.get(pid) # pid from sequence tuple\n",
    "#                                # Validate loaded data type and shape\n",
    "#                                if (\n",
    "#                                    loaded_yolo_for_ped is not None and\n",
    "#                                    isinstance(loaded_yolo_for_ped, np.ndarray) and\n",
    "#                                    loaded_yolo_for_ped.shape == (INPUT_SIZE_YOLOP,)\n",
    "#                                ):\n",
    "#                                    yolop_vec = loaded_yolo_for_ped # Use loaded data if valid\n",
    "#                 feats_sequences['yolop'].append(yolop_vec)\n",
    "#             # <<< END YOLOP >>>\n",
    "\n",
    "#         # numpy → torch (Convert collected sequences to tensors)\n",
    "#         output_features = {}\n",
    "#         try:\n",
    "#             for s_name in self.streams: # Iterate through streams requested for this instance\n",
    "#                 # Convert list of numpy arrays/scalars to a single numpy array\n",
    "#                 np_feature_array = np.asarray(feats_sequences[s_name], dtype=np.float32)\n",
    "\n",
    "#                 # Validate shape before tensor conversion\n",
    "#                 expected_feat_dim = self._input_sizes.get(s_name, 1)\n",
    "#                 if np_feature_array.shape == (self.seq_len, expected_feat_dim):\n",
    "#                     output_features[s_name] = torch.tensor(np_feature_array, dtype=torch.float32)\n",
    "#                 else: # Handle shape mismatch, e.g., if a frame had missing data for a scalar\n",
    "#                     # print(f\"[WARN] Feature shape mismatch for '{s_name}' idx {idx}. Expected {(self.seq_len, expected_feat_dim)}, got {np_feature_array.shape}. Using zeros.\")\n",
    "#                     output_features[s_name] = torch.zeros((self.seq_len, expected_feat_dim), dtype=torch.float32)\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERROR] Tensor conversion error for item idx {idx}: {e}. Returning zeros for all requested streams.\")\n",
    "#             # Fallback: return zeros for all requested streams with correct shapes\n",
    "#             output_features = {\n",
    "#                 s_name_err: torch.zeros(\n",
    "#                     (self.seq_len, self._input_sizes.get(s_name_err, 1)),\n",
    "#                     dtype=torch.float32,\n",
    "#                 )\n",
    "#                 for s_name_err in self.streams\n",
    "#             }\n",
    "#             label = 0 # Reset label to default if features are dummy\n",
    "\n",
    "#         return output_features, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# #                       MAIN: build balanced training set\n",
    "# # =============================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"\\n--- DATA PREPARATION (with YOLOP) ---\")\n",
    "\n",
    "#     # 1) load / regenerate PIE DB -------------------------------------------\n",
    "#     cache = Path(PIE_DATABASE_CACHE_PATH)\n",
    "#     pie_db = None # Initialize\n",
    "#     if cache.is_file():\n",
    "#         print(\"Loading PIE database cache …\")\n",
    "#         try:\n",
    "#             with cache.open(\"rb\") as fp:\n",
    "#                 pie_db = pickle.load(fp)\n",
    "#             if not isinstance(pie_db, dict) or not pie_db:\n",
    "#                 print(\"[WARN] Loaded PIE DB cache is invalid or empty. Will attempt to regenerate.\")\n",
    "#                 pie_db = None # Force regeneration if invalid\n",
    "#             else:\n",
    "#                 print(\"✓ PIE DB loaded from cache.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERROR] Failed to load PIE DB cache: {e}. Will attempt to regenerate.\")\n",
    "#             pie_db = None\n",
    "\n",
    "#     if pie_db is None: # If cache loading failed or cache didn't exist\n",
    "#         if PIE is None:\n",
    "#             raise RuntimeError(\"PIE class unavailable: cannot rebuild database.\")\n",
    "#         print(\"Regenerating PIE DB (this might take a while) …\")\n",
    "#         try:\n",
    "#             pie_interface = PIE(data_path=PIE_ROOT_PATH, regen_database=True)\n",
    "#             pie_db = pie_interface.generate_database()\n",
    "#             if not pie_db: # Check if generation was successful\n",
    "#                 raise RuntimeError(\"PIE DB generation returned an empty database.\")\n",
    "#             print(\"✓ PIE DB generated.\")\n",
    "#             # Optional: Save the newly generated DB if you want to update the cache\n",
    "#             # try:\n",
    "#             #     with open(PIE_DATABASE_CACHE_PATH, \"wb\") as fp: # Overwrite cache\n",
    "#             #         pickle.dump(pie_db, fp, pickle.HIGHEST_PROTOCOL)\n",
    "#             #     print(f\"✓ Newly generated PIE DB saved to cache: {PIE_DATABASE_CACHE_PATH}\")\n",
    "#             # except Exception as e:\n",
    "#             #     print(f\"[WARN] Could not save newly generated PIE DB to cache: {e}\")\n",
    "#         except Exception as e:\n",
    "#             raise RuntimeError(f\"PIE DB generation failed: {e}\")\n",
    "\n",
    "\n",
    "#     # 2) compute scalers -----------------------------------------------------\n",
    "#     print(\"\\nComputing scalers from training sets …\")\n",
    "#     spd, accx, accy, gyz = [], [], [], []\n",
    "#     if isinstance(pie_db, dict): # Proceed only if pie_db is valid\n",
    "#         for sid in TRAIN_SETS_STR:\n",
    "#             set_data = pie_db.get(sid, {})\n",
    "#             if not isinstance(set_data, dict): continue # Skip if set data is not a dict\n",
    "#             for vid, vdb_data in set_data.items():\n",
    "#                 vehicle_annots = vdb_data.get(\"vehicle_annotations\", {})\n",
    "#                 if not isinstance(vehicle_annots, dict): continue # Skip if not dict\n",
    "#                 for frame_num, ego_frame_data in vehicle_annots.items():\n",
    "#                     if not isinstance(ego_frame_data, dict): continue # Skip if not dict\n",
    "#                     try:\n",
    "#                         s_val  = ego_frame_data.get(\"OBD_speed\", 0.0) or ego_frame_data.get(\"GPS_speed\", 0.0)\n",
    "#                         spd.append(float(s_val))\n",
    "#                         accx.append(float(ego_frame_data.get(\"accX\", 0.0)))\n",
    "#                         accy.append(float(ego_frame_data.get(\"accY\", 0.0)))\n",
    "#                         gyz.append(float(ego_frame_data.get(\"gyroZ\", 0.0)))\n",
    "#                     except (TypeError, ValueError) as data_err:\n",
    "#                         # print(f\"[WARN] Invalid ego data in {sid}/{vid}/frame {frame_num}: {data_err}. Skipping frame for scalers.\")\n",
    "#                         pass # Skip this frame's data for scaler calculation\n",
    "#     else:\n",
    "#         print(\"[ERROR] Cannot compute scalers: PIE Database is invalid or empty.\")\n",
    "\n",
    "#     scalers = {}\n",
    "#     try:\n",
    "#         if spd:\n",
    "#             scalers[\"ego_speed_mean\"] = float(np.mean(spd))\n",
    "#             scalers[\"ego_speed_std\"]  = float(max(np.std(spd), 1e-6)) # Avoid division by zero\n",
    "#         if accx: # Assume accy also exists if accx does\n",
    "#             scalers[\"accX_mean\"] = float(np.mean(accx))\n",
    "#             scalers[\"accX_std\"]  = float(max(np.std(accx), 1e-6))\n",
    "#             scalers[\"accY_mean\"] = float(np.mean(accy))\n",
    "#             scalers[\"accY_std\"]  = float(max(np.std(accy), 1e-6))\n",
    "#         if gyz:\n",
    "#             scalers[\"gyroZ_mean\"] = float(np.mean(gyz))\n",
    "#             scalers[\"gyroZ_std\"]  = float(max(np.std(gyz), 1e-6))\n",
    "#         print(\"Scalers computed:\", scalers if scalers else \"No scaler data generated.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] Failed to compute scaler statistics: {e}\")\n",
    "\n",
    "\n",
    "#     # 3) extract full training dataset (for ALL streams) ---------------------\n",
    "#     print(\"\\nExtracting training sequences (for ALL defined streams including YOLOP) …\")\n",
    "#     full_ds = None # Initialize\n",
    "#     try:\n",
    "#         full_ds = PIEDataset(\n",
    "#             pie_db,\n",
    "#             TRAIN_SETS_STR,\n",
    "#             POSE_DATA_DIR,\n",
    "#             YOLOP_FEATURE_DIR, # <<< Pass YOLOP directory\n",
    "#             SEQ_LEN,\n",
    "#             PRED_LEN,\n",
    "#             scalers,\n",
    "#             ALL_POSSIBLE_STREAMS, # <<< Generate ALL streams defined in config\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(f\"[FATAL ERROR] Failed to initialize PIEDataset for training data extraction: {e}\")\n",
    "#         raise # Cannot proceed if dataset instantiation fails\n",
    "\n",
    "#     # Initialize dictionary to hold extracted data for all streams\n",
    "#     train_data_to_balance = {s: [] for s in ALL_POSSIBLE_STREAMS}\n",
    "#     train_data_to_balance[\"label\"] = [] # Add label key\n",
    "#     extraction_errors = 0\n",
    "\n",
    "#     if len(full_ds) > 0: # Proceed only if sequences were enumerated\n",
    "#         for i in tqdm(range(len(full_ds)), desc=\"Extracting sequences for balancing\"):\n",
    "#             try:\n",
    "#                 features_dict, label_tensor = full_ds[i] # __getitem__ returns dict of ALL streams\n",
    "#                 # Validate types\n",
    "#                 if not isinstance(features_dict, dict) or not isinstance(label_tensor, torch.Tensor):\n",
    "#                     raise TypeError(f\"PIEDataset __getitem__ returned unexpected types: features={type(features_dict)}, label={type(label_tensor)}\")\n",
    "\n",
    "#                 # Store each stream's data (convert tensor to numpy)\n",
    "#                 for stream_name in ALL_POSSIBLE_STREAMS: # Iterate through ALL streams\n",
    "#                     if stream_name in features_dict and isinstance(features_dict[stream_name], torch.Tensor):\n",
    "#                          train_data_to_balance[stream_name].append(features_dict[stream_name].cpu().numpy())\n",
    "#                     else:\n",
    "#                          # This case should ideally not happen if __getitem__ is robust and returns all streams (even if as zeros)\n",
    "#                          print(f\"[WARN] Stream '{stream_name}' missing or not a tensor from PIEDataset for idx {i}. Appending zeros.\")\n",
    "#                          expected_size_for_stream = full_ds._input_sizes.get(stream_name, 1)\n",
    "#                          train_data_to_balance[stream_name].append(np.zeros((SEQ_LEN, expected_size_for_stream), dtype=np.float32))\n",
    "\n",
    "#                 train_data_to_balance[\"label\"].append([label_tensor.item()]) # Store label as list of single-item list\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 extraction_errors += 1\n",
    "#                 print(f\"[ERROR] Failed extracting data for index {i}: {e}\")\n",
    "#                 if extraction_errors > len(full_ds) * 0.05: # Stop if >5% of items fail\n",
    "#                      print(\"[FATAL] Too many errors during data extraction. Aborting.\")\n",
    "#                      raise RuntimeError(\"Excessive errors during data extraction for balancing.\") from e\n",
    "\n",
    "#         print(f\"Raw training samples extracted: {len(train_data_to_balance['label'])} (Errors during extraction: {extraction_errors})\")\n",
    "#     else:\n",
    "#         print(\"[WARN] PIEDataset for training is empty. No data extracted for balancing.\")\n",
    "\n",
    "\n",
    "#     # 4) balance -------------------------------------------------------------\n",
    "#     balanced_data_dict = {} # Initialize\n",
    "#     if train_data_to_balance[\"label\"]: # Only balance if data was successfully extracted\n",
    "#         print(\"\\nBalancing extracted training data ...\")\n",
    "#         try:\n",
    "#             balanced_data_dict = balance_samples_count(train_data_to_balance, \"label\")\n",
    "#         except Exception as e:\n",
    "#              print(f\"[ERROR] Balancing step failed: {e}\")\n",
    "#              # Decide how to proceed - maybe use unbalanced data? For now, treat as critical.\n",
    "#              raise RuntimeError(\"Dataset balancing process failed.\") from e\n",
    "#     else:\n",
    "#         print(\"Skipping balancing as no data was extracted or labels are missing.\")\n",
    "\n",
    "#     # Clean up memory from the large un-balanced dictionary and dataset object\n",
    "#     del train_data_to_balance, full_ds\n",
    "#     gc.collect()\n",
    "\n",
    "#     # 5) write pickles -------------------------------------------------------\n",
    "#     if balanced_data_dict: # Only save if balancing produced a result\n",
    "#         print(f\"\\nSaving balanced data (including YOLOP features) to: {BALANCED_DATA_PKL_PATH}\")\n",
    "#         try:\n",
    "#             with open(BALANCED_DATA_PKL_PATH, \"wb\") as fp:\n",
    "#                 pickle.dump(balanced_data_dict, fp, pickle.HIGHEST_PROTOCOL)\n",
    "#             print(f\"✓ Balanced data saved.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERROR] Failed to save balanced data pickle: {e}\")\n",
    "#     else:\n",
    "#         print(f\"Skipping saving balanced data (path: {BALANCED_DATA_PKL_PATH}) as it's empty.\")\n",
    "\n",
    "#     if scalers: # Only save if scalers were computed\n",
    "#         print(f\"Saving scalers to: {SCALERS_PKL_PATH}\")\n",
    "#         try:\n",
    "#             with open(SCALERS_PKL_PATH, \"wb\") as fp:\n",
    "#                 pickle.dump(scalers, fp, pickle.HIGHEST_PROTOCOL)\n",
    "#             print(f\"✓ Scalers saved.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERROR] Failed to save scalers pickle: {e}\")\n",
    "#     else:\n",
    "#         print(f\"Skipping saving scalers (path: {SCALERS_PKL_PATH}) as they are empty.\")\n",
    "\n",
    "#     # Final cleanup\n",
    "#     if 'pie_db' in locals() and pie_db is not None: del pie_db\n",
    "#     if 'balanced_data_dict' in locals() and balanced_data_dict is not None : del balanced_data_dict\n",
    "#     gc.collect()\n",
    "\n",
    "#     print(\"\\n--- DATA PREPARATION (with YOLOP) COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4531706c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:28:27.198214Z",
     "iopub.status.busy": "2025-05-07T09:28:27.197914Z",
     "iopub.status.idle": "2025-05-07T09:28:27.218024Z",
     "shell.execute_reply": "2025-05-07T09:28:27.217344Z"
    },
    "papermill": {
     "duration": 0.03344,
     "end_time": "2025-05-07T09:28:27.219256",
     "exception": false,
     "start_time": "2025-05-07T09:28:27.185816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # --- CELL 2: ABLATION STUDY – MODEL TRAINING AND EVALUATION (with Weighted Average Fusion and YOLOP) ---\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "# import gc\n",
    "# import time\n",
    "# import math\n",
    "# import random\n",
    "# import pickle\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import pandas as pd                      # results-summary table\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from tqdm.notebook import tqdm\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from pathlib import Path # Added for Path operations\n",
    "# from sklearn.metrics import (\n",
    "#     accuracy_score,\n",
    "#     precision_recall_fscore_support,\n",
    "#     roc_auc_score,\n",
    "#     confusion_matrix,\n",
    "#     ConfusionMatrixDisplay,\n",
    "# )\n",
    "\n",
    "# # --- Add PIE utilities path if necessary (adjust path) ------------------------\n",
    "# pie_utilities_path = \"/kaggle/working/PIE/utilities\"\n",
    "# if pie_utilities_path not in sys.path:\n",
    "#     sys.path.insert(0, pie_utilities_path)\n",
    "\n",
    "# try:\n",
    "#     from pie_data import PIE\n",
    "# except ImportError as e:\n",
    "#     print(f\"Warn: Could not import PIE class: {e}\")\n",
    "#     PIE = None\n",
    "\n",
    "# # --- Configuration ------------------------------------------------------------\n",
    "# PIE_ROOT_PATH = \"/kaggle/working/PIE\"\n",
    "# POSE_DATA_DIR = \"/kaggle/input/pose-data/extracted_poses2\"\n",
    "# YOLOP_FEATURE_DIR = '/kaggle/input/yolop-data/yolop features'  # <<< YOLOP DIR\n",
    "# PIE_DATABASE_CACHE_PATH = \"/kaggle/input/pie-database/pie_database.pkl\"\n",
    "\n",
    "# # --- Define ALL possible streams (used by Dataset class) ----------------------\n",
    "# ALL_POSSIBLE_STREAMS = [\n",
    "#     \"bbox\",\n",
    "#     \"pose\",\n",
    "#     \"ego_speed\",\n",
    "#     \"ego_acc\",\n",
    "#     \"ego_gyro\",\n",
    "#     \"ped_action\",\n",
    "#     \"ped_look\",\n",
    "#     \"ped_occlusion\",\n",
    "#     \"traffic_light\",\n",
    "#     \"static_context\",\n",
    "#     \"yolop\" # <<< ADDED YOLOP\n",
    "# ]\n",
    "\n",
    "# # --- *** CHOOSE ACTIVE STREAMS FOR THIS EXPERIMENT *** ------------------------\n",
    "# # <<< --- YOU CAN ADD 'yolop' to this list if you want to include it in the fusion model --- >>>\n",
    "# ACTIVE_STREAMS = [\n",
    "#     \"bbox\",\n",
    "#     \"pose\",\n",
    "#     # \"ego_speed\",\n",
    "#     # \"ego_acc\",\n",
    "#     # \"ego_gyro\",\n",
    "#     \"ped_action\",\n",
    "#     \"ped_look\",\n",
    "#     # \"ped_occlusion\",\n",
    "#     # \"traffic_light\",\n",
    "#     \"static_context\",\n",
    "#     \"yolop\" # <<< EXAMPLE: YOLOP ADDED TO ACTIVE STREAMS FOR THIS RUN\n",
    "# ]\n",
    "# # ------------------------------------------------------------------------------\n",
    "\n",
    "# print(f\"--- Running Weighted Average Fusion With Active Streams: {ACTIVE_STREAMS} ---\")\n",
    "\n",
    "# # --- Model Hyper-parameters ---------------------------------------------------\n",
    "# SEQ_LEN, PRED_LEN = 30, 1\n",
    "# INPUT_SIZE_BBOX = 4\n",
    "# INPUT_SIZE_POSE = 34\n",
    "# INPUT_SIZE_EGO_SPEED = 1\n",
    "# INPUT_SIZE_EGO_ACC = 2\n",
    "# INPUT_SIZE_EGO_GYRO = 1\n",
    "# INPUT_SIZE_PED_ACTION = 1\n",
    "# INPUT_SIZE_PED_LOOK = 1\n",
    "# INPUT_SIZE_PED_OCC = 1\n",
    "# INPUT_SIZE_TL_STATE = 4 # Corresponds to NUM_TL_STATES\n",
    "# NUM_SIGNALIZED_CATS = 4\n",
    "# NUM_INTERSECTION_CATS = 5\n",
    "# NUM_AGE_CATS = 4\n",
    "# NUM_GENDER_CATS = 3\n",
    "# NUM_TRAFFIC_DIR_CATS = 2\n",
    "# LANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7: 4, 8: 4}\n",
    "# NUM_LANE_CATS = len(set(LANE_CATEGORIES.values()))\n",
    "# INPUT_SIZE_STATIC = (\n",
    "#     NUM_SIGNALIZED_CATS\n",
    "#     + NUM_INTERSECTION_CATS\n",
    "#     + NUM_AGE_CATS\n",
    "#     + NUM_GENDER_CATS\n",
    "#     + NUM_TRAFFIC_DIR_CATS\n",
    "#     + NUM_LANE_CATS\n",
    "# )\n",
    "\n",
    "# # --- YOLOP Size Calculation (ensure this matches Cell 1) ---\n",
    "# GRID_SIZE = 3\n",
    "# YOLOP_DRIVABLE_FEATURES_DIM = GRID_SIZE * GRID_SIZE\n",
    "# YOLOP_LANE_FEATURES_DIM = GRID_SIZE * GRID_SIZE\n",
    "# YOLOP_OBJECT_FEATURES_DIM = 2\n",
    "# INPUT_SIZE_YOLOP = (\n",
    "#     YOLOP_DRIVABLE_FEATURES_DIM + YOLOP_LANE_FEATURES_DIM + YOLOP_OBJECT_FEATURES_DIM\n",
    "# )\n",
    "# # ---\n",
    "\n",
    "# LSTM_HIDDEN_SIZE = 256\n",
    "# NUM_LSTM_LAYERS = 2\n",
    "# DROPOUT_RATE = 0.3\n",
    "# NUM_CLASSES = 2\n",
    "# ATTENTION_DIM = 128\n",
    "\n",
    "# # --- Training Hyper-parameters ------------------------------------------------\n",
    "# LEARNING_RATE = 1e-4\n",
    "# BATCH_SIZE = 32\n",
    "# NUM_EPOCHS = 30\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# # --- Dataset splits -----------------------------------------------------------\n",
    "# VAL_SETS_STR = [\"set05\", \"set06\"]\n",
    "\n",
    "# # --- Paths for pre-processed data (ensure this matches Cell 1 output) --------\n",
    "# BALANCED_DATA_PKL_PATH = \"/kaggle/working/balanced_train_data_with_yolop.pkl\" # <<< UPDATED\n",
    "# SCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\"\n",
    "\n",
    "# # Mappings needed by PIEDataset helper (if used, like TL_STATE_MAP)\n",
    "# TL_STATE_MAP = {\"__undefined__\": 0, \"red\": 1, \"yellow\": 2, \"green\": 3}\n",
    "# NUM_TL_STATES = len(TL_STATE_MAP) # INPUT_SIZE_TL_STATE should be this value\n",
    "\n",
    "# # -----------------------------------------------------------------------------#\n",
    "# #                               Helper classes                                #\n",
    "# # -----------------------------------------------------------------------------\n",
    "\n",
    "# def to_one_hot(index, num_classes): # Ensure this matches Cell 1\n",
    "#     vec = np.zeros(num_classes, dtype=np.float32)\n",
    "#     safe_index = int(np.clip(index, 0, num_classes - 1))\n",
    "#     vec[safe_index] = 1.0\n",
    "#     return vec\n",
    "\n",
    "\n",
    "# class PIEDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Dataset for Validation. Generates features for ALL_POSSIBLE_STREAMS.\n",
    "#     The training loop/model will select which of these to use.\n",
    "#     Includes YOLOP handling.\n",
    "#     \"\"\"\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         pie_database,\n",
    "#         set_names,\n",
    "#         pose_data_dir,\n",
    "#         yolop_data_dir, # <<< ADDED YOLOP DIR\n",
    "#         seq_len,\n",
    "#         pred_len,\n",
    "#         scalers=None,\n",
    "#         # active_streams param is not used here; __getitem__ generates all, model selects\n",
    "#     ):\n",
    "#         self.pie_db = pie_database\n",
    "#         self.set_names = set_names\n",
    "#         self.pose_data_dir = pose_data_dir\n",
    "#         self.yolop_data_dir = yolop_data_dir # <<< STORE YOLOP DIR\n",
    "#         self.seq_len = seq_len\n",
    "#         self.pred_len = pred_len\n",
    "#         self.scalers = scalers or {}\n",
    "#         self.streams_to_generate_in_item = ALL_POSSIBLE_STREAMS # __getitem__ will create data for all these\n",
    "#         self.sequences = []\n",
    "#         self.all_pose_data = {}\n",
    "#         self.all_yolop_data = {} # <<< ADDED YOLOP STORAGE\n",
    "\n",
    "#         self._input_sizes_map = self._get_input_sizes_dict() # For all streams\n",
    "\n",
    "#         # Load data only if the stream is possible (and dirs exist)\n",
    "#         if \"pose\" in self.streams_to_generate_in_item and self.pose_data_dir:\n",
    "#             self._load_pose_data()\n",
    "#         if \"yolop\" in self.streams_to_generate_in_item and self.yolop_data_dir:\n",
    "#              self._load_yolop_data()\n",
    "\n",
    "#         self._generate_sequence_list()\n",
    "#         if not self.sequences:\n",
    "#             raise ValueError(f\"Validation Dataset init failed: No sequences found for sets {self.set_names}\")\n",
    "\n",
    "#     def _get_input_sizes_dict(self):\n",
    "#         \"\"\" Build a dict {stream_name: feature_size} for ALL_POSSIBLE_STREAMS. \"\"\"\n",
    "#         input_sizes = {}\n",
    "#         special_cases = {\n",
    "#             \"TRAFFIC_LIGHT\": \"TL_STATE\", \"STATIC_CONTEXT\": \"STATIC\", \"EGO_SPEED\": \"EGO_SPEED\",\n",
    "#             \"EGO_ACC\": \"EGO_ACC\", \"EGO_GYRO\": \"EGO_GYRO\", \"PED_ACTION\": \"PED_ACTION\",\n",
    "#             \"PED_LOOK\": \"PED_LOOK\", \"PED_OCCLUSION\": \"PED_OCC\", \"YOLOP\": \"YOLOP\",\n",
    "#         }\n",
    "#         for stream in ALL_POSSIBLE_STREAMS:\n",
    "#             const_suffix = stream.upper()\n",
    "#             if const_suffix in special_cases: const_suffix = special_cases[const_suffix]\n",
    "#             size_constant_name = f\"INPUT_SIZE_{const_suffix}\"\n",
    "#             if stream == \"bbox\": size_constant_name = \"INPUT_SIZE_BBOX\"\n",
    "#             elif stream == \"pose\": size_constant_name = \"INPUT_SIZE_POSE\"\n",
    "#             input_sizes[stream] = globals().get(size_constant_name, 1)\n",
    "#         return input_sizes\n",
    "\n",
    "#     def _load_pose_data(self):\n",
    "#         print(\"Loading Pose data for Validation sets...\")\n",
    "#         loaded_any = False\n",
    "#         for set_id in self.set_names:\n",
    "#             self.all_pose_data[set_id] = {}\n",
    "#             pose_set_path = Path(self.pose_data_dir) / set_id\n",
    "#             if not pose_set_path.is_dir(): continue\n",
    "#             pkl_files = list(pose_set_path.glob(f\"{set_id}_*_poses.pkl\"))\n",
    "#             if not pkl_files: continue\n",
    "#             loaded_video_count = 0\n",
    "#             for pkl_file_path in tqdm(pkl_files, desc=f\"Val Pose {set_id}\", leave=False):\n",
    "#                 try:\n",
    "#                     with open(pkl_file_path, \"rb\") as f: loaded_pkl_content = pickle.load(f)\n",
    "#                     if not isinstance(loaded_pkl_content, dict) or len(loaded_pkl_content) != 1: continue\n",
    "#                     unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n",
    "#                     parts = unique_video_key.split('_');\n",
    "#                     if len(parts) < 2: continue\n",
    "#                     video_id = \"_\".join(parts[1:])\n",
    "#                     if set_id in self.pie_db and video_id in self.pie_db.get(set_id, {}):\n",
    "#                         self.all_pose_data[set_id][video_id] = video_data\n",
    "#                         loaded_any = True; loaded_video_count +=1\n",
    "#                 except Exception as e: print(f\"Err load val pose {pkl_file_path}: {e}\")\n",
    "#             if loaded_video_count > 0 : print(f\"  Loaded {loaded_video_count} pose videos for {set_id}\")\n",
    "#         if not loaded_any: print(\"WARN: No pose data loaded for validation.\")\n",
    "\n",
    "\n",
    "#     def _load_yolop_data(self): # <<< YOLOP Loading for Validation Dataset\n",
    "#         print(f\"Loading YOLOP data for Validation sets: {self.set_names} from {self.yolop_data_dir}\")\n",
    "#         loaded_any_yolop = False\n",
    "#         for set_id in self.set_names: self.all_yolop_data[set_id] = {}\n",
    "\n",
    "#         yolop_path_obj = Path(self.yolop_data_dir)\n",
    "#         if not yolop_path_obj.is_dir():\n",
    "#             print(f\"[ERROR] YOLOP dir not found for validation: {self.yolop_data_dir}\")\n",
    "#             return\n",
    "#         try: all_pkl_files = list(yolop_path_obj.glob(\"*_yolop_features.pkl\"))\n",
    "#         except Exception as e: print(f\"Err list YOLOP files for val: {e}\"); return\n",
    "#         if not all_pkl_files: print(f\"WARN: No YOLOP pkls in {self.yolop_data_dir} for val.\"); return\n",
    "\n",
    "#         loaded_file_count = 0; files_for_needed_sets = 0\n",
    "#         for pkl_file_path in tqdm(all_pkl_files, desc=\"Val YOLOP PKLs\", leave=False):\n",
    "#             pkl_filename = pkl_file_path.name\n",
    "#             try:\n",
    "#                 parts = pkl_filename.replace(\"_yolop_features.pkl\", \"\").split('_')\n",
    "#                 if len(parts) < 2: raise IndexError(\"Filename format err\")\n",
    "#                 set_id_from_file = parts[0]; video_id = \"_\".join(parts[1:])\n",
    "#             except IndexError: continue\n",
    "\n",
    "#             if set_id_from_file in self.set_names:\n",
    "#                 files_for_needed_sets += 1\n",
    "#                 try:\n",
    "#                     with open(pkl_file_path, 'rb') as f: loaded_pkl_content = pickle.load(f)\n",
    "#                     if not isinstance(loaded_pkl_content, dict) or len(loaded_pkl_content) != 1: continue\n",
    "#                     _, per_video_yolop_data = list(loaded_pkl_content.items())[0]\n",
    "#                     if set_id_from_file not in self.all_yolop_data: self.all_yolop_data[set_id_from_file] = {}\n",
    "#                     self.all_yolop_data[set_id_from_file][video_id] = per_video_yolop_data\n",
    "#                     loaded_file_count += 1; loaded_any_yolop = True\n",
    "#                 except Exception as e: print(f\"Err load val YOLOP {pkl_file_path}: {e}\")\n",
    "#         print(f\"Val YOLOP: {loaded_file_count}/{files_for_needed_sets} files loaded.\")\n",
    "#         if not loaded_any_yolop and files_for_needed_sets > 0: print(\"WARN: No validation YOLOP data loaded.\")\n",
    "\n",
    "\n",
    "#     def _generate_sequence_list(self):\n",
    "#         print(f\"Generating sequences for Validation sets: {self.set_names}\")\n",
    "#         sequence_count = 0; self.sequences = []\n",
    "#         for set_id in self.set_names:\n",
    "#             if set_id not in self.pie_db: continue\n",
    "#             for video_id, video_data in self.pie_db.get(set_id, {}).items():\n",
    "#                 ped_ann = video_data.get(\"ped_annotations\", {});\n",
    "#                 if not isinstance(ped_ann, dict): continue\n",
    "#                 for ped_id, ped_data in ped_ann.items():\n",
    "#                     if not isinstance(ped_data, dict): continue\n",
    "#                     frames = ped_data.get(\"frames\", [])\n",
    "#                     if not isinstance(frames,list) or len(frames) < self.seq_len + self.pred_len: continue\n",
    "#                     try: frames_sorted = sorted([int(f) for f in frames])\n",
    "#                     except (ValueError,TypeError): continue\n",
    "#                     for i in range(len(frames_sorted) - self.seq_len - self.pred_len + 1):\n",
    "#                         start_f = frames_sorted[i]; obs_end_f = frames_sorted[i + self.seq_len - 1]\n",
    "#                         if obs_end_f - start_f != self.seq_len - 1: continue\n",
    "#                         target_idx = i + self.seq_len + self.pred_len - 1\n",
    "#                         if target_idx >= len(frames_sorted): continue\n",
    "#                         target_f = frames_sorted[target_idx]\n",
    "#                         if target_f - obs_end_f != self.pred_len: continue\n",
    "#                         self.sequences.append((set_id, video_id, ped_id, start_f)); sequence_count += 1\n",
    "#         print(f\"Validation dataset sequences: {sequence_count}.\")\n",
    "\n",
    "#     def __len__(self): return len(self.sequences)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         set_id, video_id, ped_id, start_frame = self.sequences[idx]\n",
    "#         frame_nums = list(range(start_frame, start_frame + self.seq_len))\n",
    "#         target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n",
    "\n",
    "#         video_db = self.pie_db.get(set_id, {}).get(video_id, {})\n",
    "#         ped_db = video_db.get(\"ped_annotations\", {}).get(ped_id, {})\n",
    "#         ego_db = video_db.get(\"vehicle_annotations\", {})\n",
    "#         traffic_db = video_db.get(\"traffic_annotations\", {})\n",
    "#         ped_attributes = ped_db.get(\"attributes\", {}) if isinstance(ped_db, dict) else {}\n",
    "\n",
    "#         # Initialize for ALL streams this dataset is configured to generate\n",
    "#         feature_sequences = {s: [] for s in self.streams_to_generate_in_item}\n",
    "#         label = 0\n",
    "\n",
    "#         if isinstance(ped_db, dict) and \"frames\" in ped_db and \"behavior\" in ped_db and \"cross\" in ped_db[\"behavior\"]:\n",
    "#             try:\n",
    "#                 target_idx_label = ped_db[\"frames\"].index(int(target_frame_num))\n",
    "#                 if target_idx_label < len(ped_db[\"behavior\"][\"cross\"]):\n",
    "#                     label_val = ped_db[\"behavior\"][\"cross\"][target_idx_label]\n",
    "#                     label = 1 if isinstance(label_val, (int,float)) and label_val == 1 else 0\n",
    "#             except (ValueError, IndexError, TypeError): pass\n",
    "\n",
    "#         static_vec = None\n",
    "#         if \"static_context\" in self.streams_to_generate_in_item:\n",
    "#             static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n",
    "#             if isinstance(ped_attributes, dict):\n",
    "#                 try:\n",
    "#                     sig = ped_attributes.get(\"signalized\",0); intr = ped_attributes.get(\"intersection\",0)\n",
    "#                     age = ped_attributes.get(\"age\",2); gen = ped_attributes.get(\"gender\",0)\n",
    "#                     tdir = int(ped_attributes.get(\"traffic_direction\",0))\n",
    "#                     ln_val = ped_attributes.get(\"num_lanes\",2)\n",
    "#                     lncat = LANE_CATEGORIES.get(int(ln_val), LANE_CATEGORIES[max(LANE_CATEGORIES.keys())]) \\\n",
    "#                             if isinstance(ln_val,(int,str)) and str(ln_val).isdigit() else LANE_CATEGORIES[max(LANE_CATEGORIES.keys())]\n",
    "#                     s_vec = np.concatenate([ to_one_hot(sig,NUM_SIGNALIZED_CATS), to_one_hot(intr,NUM_INTERSECTION_CATS),\n",
    "#                                            to_one_hot(age,NUM_AGE_CATS), to_one_hot(gen,NUM_GENDER_CATS),\n",
    "#                                            to_one_hot(tdir,NUM_TRAFFIC_DIR_CATS), to_one_hot(lncat,NUM_LANE_CATS) ])\n",
    "#                     if s_vec.shape[0] == INPUT_SIZE_STATIC: static_vec = s_vec.astype(np.float32)\n",
    "#                 except Exception: pass # static_vec remains zeros\n",
    "\n",
    "#         for frame_num_current in frame_nums:\n",
    "#             frame_db_idx = -1\n",
    "#             if isinstance(ped_db, dict) and \"frames\" in ped_db and isinstance(ped_db[\"frames\"],list):\n",
    "#                 try: frame_db_idx = ped_db[\"frames\"].index(int(frame_num_current))\n",
    "#                 except (ValueError,TypeError): pass\n",
    "#             ego_f_data = ego_db.get(frame_num_current, {}) if isinstance(ego_db,dict) else {}\n",
    "#             if not isinstance(ego_f_data, dict): ego_f_data = {}\n",
    "\n",
    "#             # --- BBOX ---\n",
    "#             if \"bbox\" in self.streams_to_generate_in_item:\n",
    "#                 bb_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32)\n",
    "#                 if frame_db_idx != -1 and isinstance(ped_db,dict) and \"bbox\" in ped_db and isinstance(ped_db[\"bbox\"],list) and len(ped_db[\"bbox\"]) > frame_db_idx:\n",
    "#                     try:\n",
    "#                         coords = ped_db[\"bbox\"][frame_db_idx]\n",
    "#                         if isinstance(coords,(list,tuple)) and len(coords)==4:\n",
    "#                             x1,y1,x2,y2=coords; iw=video_db.get(\"width\",1920); ih=video_db.get(\"height\",1080)\n",
    "#                             if isinstance(iw,(int,float)) and iw>0 and isinstance(ih,(int,float)) and ih>0:\n",
    "#                                 cx=((x1+x2)/2)/iw; cy=((y1+y2)/2)/ih; w=(x2-x1)/iw; h=(y2-y1)/ih\n",
    "#                                 if w>0 and h>0 and 0<=cx<=1 and 0<=cy<=1: bb_norm=np.array([cx,cy,w,h],dtype=np.float32)\n",
    "#                     except Exception: pass\n",
    "#                 feature_sequences[\"bbox\"].append(bb_norm)\n",
    "#             # --- POSE ---\n",
    "#             if \"pose\" in self.streams_to_generate_in_item:\n",
    "#                 p_vec = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n",
    "#                 v_pose = self.all_pose_data.get(set_id,{}).get(video_id,{})\n",
    "#                 if isinstance(v_pose,dict):\n",
    "#                     f_pose = v_pose.get(frame_num_current,{})\n",
    "#                     if isinstance(f_pose,dict):\n",
    "#                         l_pose = f_pose.get(ped_id)\n",
    "#                         if isinstance(l_pose, np.ndarray) and l_pose.shape==(INPUT_SIZE_POSE,): p_vec=l_pose\n",
    "#                 feature_sequences[\"pose\"].append(p_vec)\n",
    "#             # --- EGO SPEED ---\n",
    "#             if \"ego_speed\" in self.streams_to_generate_in_item:\n",
    "#                 sp_s = 0.0\n",
    "#                 try:\n",
    "#                     sp = float(ego_f_data.get(\"OBD_speed\",0.0) or ego_f_data.get(\"GPS_speed\",0.0))\n",
    "#                     m,s = self.scalers.get(\"ego_speed_mean\",0.0), self.scalers.get(\"ego_speed_std\",1.0)\n",
    "#                     sp_s = (sp-m)/s if s!=0 else 0.0\n",
    "#                 except Exception: pass\n",
    "#                 feature_sequences[\"ego_speed\"].append([sp_s])\n",
    "#             # --- EGO ACC ---\n",
    "#             if \"ego_acc\" in self.streams_to_generate_in_item:\n",
    "#                 ax_s,ay_s = 0.0,0.0\n",
    "#                 try:\n",
    "#                     ax,ay = float(ego_f_data.get(\"accX\",0.0)), float(ego_f_data.get(\"accY\",0.0))\n",
    "#                     xm,xs = self.scalers.get(\"accX_mean\",0.0), self.scalers.get(\"accX_std\",1.0)\n",
    "#                     ym,ys = self.scalers.get(\"accY_mean\",0.0), self.scalers.get(\"accY_std\",1.0)\n",
    "#                     ax_s = (ax-xm)/xs if xs!=0 else 0.0; ay_s = (ay-ym)/ys if ys!=0 else 0.0\n",
    "#                 except Exception: pass\n",
    "#                 feature_sequences[\"ego_acc\"].append([ax_s,ay_s])\n",
    "#             # --- EGO GYRO ---\n",
    "#             if \"ego_gyro\" in self.streams_to_generate_in_item:\n",
    "#                 gz_s = 0.0\n",
    "#                 try:\n",
    "#                     gz = float(ego_f_data.get(\"gyroZ\",0.0))\n",
    "#                     m,s = self.scalers.get(\"gyroZ_mean\",0.0), self.scalers.get(\"gyroZ_std\",1.0)\n",
    "#                     gz_s = (gz-m)/s if s!=0 else 0.0\n",
    "#                 except Exception: pass\n",
    "#                 feature_sequences[\"ego_gyro\"].append([gz_s])\n",
    "#             # --- PED ACTION ---\n",
    "#             if \"ped_action\" in self.streams_to_generate_in_item:\n",
    "#                 act = 0.0\n",
    "#                 if frame_db_idx!=-1 and isinstance(ped_db,dict) and \"behavior\" in ped_db and isinstance(ped_db[\"behavior\"],dict) and \\\n",
    "#                    \"action\" in ped_db[\"behavior\"] and isinstance(ped_db[\"behavior\"][\"action\"],list) and len(ped_db[\"behavior\"][\"action\"])>frame_db_idx:\n",
    "#                    try: act = float(ped_db[\"behavior\"][\"action\"][frame_db_idx])\n",
    "#                    except (TypeError,ValueError): pass\n",
    "#                 feature_sequences[\"ped_action\"].append([act])\n",
    "#             # --- PED LOOK ---\n",
    "#             if \"ped_look\" in self.streams_to_generate_in_item:\n",
    "#                 lk = 0.0\n",
    "#                 if frame_db_idx!=-1 and isinstance(ped_db,dict) and \"behavior\" in ped_db and isinstance(ped_db[\"behavior\"],dict) and \\\n",
    "#                    \"look\" in ped_db[\"behavior\"] and isinstance(ped_db[\"behavior\"][\"look\"],list) and len(ped_db[\"behavior\"][\"look\"])>frame_db_idx:\n",
    "#                    try: lk = float(ped_db[\"behavior\"][\"look\"][frame_db_idx])\n",
    "#                    except (TypeError,ValueError): pass\n",
    "#                 feature_sequences[\"ped_look\"].append([lk])\n",
    "#             # --- PED OCCLUSION ---\n",
    "#             if \"ped_occlusion\" in self.streams_to_generate_in_item:\n",
    "#                 occ = 0.0\n",
    "#                 if frame_db_idx!=-1 and isinstance(ped_db,dict) and \"occlusion\" in ped_db and isinstance(ped_db[\"occlusion\"],list) and len(ped_db[\"occlusion\"])>frame_db_idx:\n",
    "#                     try:\n",
    "#                         ov = ped_db[\"occlusion\"][frame_db_idx]\n",
    "#                         occ = float(ov)/2.0 if isinstance(ov,(int,float)) else 0.0\n",
    "#                     except (TypeError,ValueError): pass\n",
    "#                 feature_sequences[\"ped_occlusion\"].append([occ])\n",
    "#             # --- TRAFFIC LIGHT ---\n",
    "#             if \"traffic_light\" in self.streams_to_generate_in_item:\n",
    "#                 tl_s_int = 0\n",
    "#                 if isinstance(traffic_db,dict):\n",
    "#                     for od in traffic_db.values():\n",
    "#                         if isinstance(od,dict) and od.get(\"obj_class\")==\"traffic_light\" and \"frames\" in od and isinstance(od[\"frames\"],list) and \\\n",
    "#                            \"state\" in od and isinstance(od[\"state\"],list):\n",
    "#                             try:\n",
    "#                                 tl_i = od[\"frames\"].index(int(frame_num_current))\n",
    "#                                 if tl_i < len(od[\"state\"]):\n",
    "#                                     sv = od[\"state\"][tl_i]\n",
    "#                                     if sv in TL_STATE_MAP and sv != 0:\n",
    "#                                         tl_s_int = TL_STATE_MAP[sv] if isinstance(sv,str) else int(sv); break\n",
    "#                             except (ValueError,IndexError,TypeError): continue\n",
    "#                 feature_sequences[\"traffic_light\"].append(to_one_hot(tl_s_int, NUM_TL_STATES))\n",
    "#             # --- STATIC CONTEXT ---\n",
    "#             if \"static_context\" in self.streams_to_generate_in_item:\n",
    "#                 feature_sequences[\"static_context\"].append(static_vec if static_vec is not None else np.zeros(INPUT_SIZE_STATIC,dtype=np.float32))\n",
    "#             # --- YOLOP ---\n",
    "#             if 'yolop' in self.streams_to_generate_in_item:\n",
    "#                 y_vec = np.zeros(INPUT_SIZE_YOLOP, dtype=np.float32)\n",
    "#                 y_set = self.all_yolop_data.get(set_id,{})\n",
    "#                 if isinstance(y_set,dict):\n",
    "#                     y_vid = y_set.get(video_id,{})\n",
    "#                     if isinstance(y_vid,dict):\n",
    "#                         y_frame_peds = y_vid.get(frame_num_current,{})\n",
    "#                         if isinstance(y_frame_peds,dict):\n",
    "#                             l_yolo = y_frame_peds.get(ped_id)\n",
    "#                             if isinstance(l_yolo,np.ndarray) and l_yolo.shape==(INPUT_SIZE_YOLOP,): y_vec=l_yolo\n",
    "#                 feature_sequences['yolop'].append(y_vec)\n",
    "\n",
    "#         features_out = {}\n",
    "#         try:\n",
    "#             for name in self.streams_to_generate_in_item: # Convert all generated streams\n",
    "#                 np_arr = np.asarray(feature_sequences[name], dtype=np.float32)\n",
    "#                 expected_dim = self._input_sizes_map.get(name,1)\n",
    "#                 if np_arr.shape == (self.seq_len, expected_dim):\n",
    "#                     features_out[name] = torch.tensor(np_arr, dtype=torch.float32)\n",
    "#                 else:\n",
    "#                     features_out[name] = torch.zeros((self.seq_len, expected_dim), dtype=torch.float32)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Err convert val feats idx {idx}: {e}. Using zeros.\")\n",
    "#             features_out = { name: torch.zeros((self.seq_len, self._input_sizes_map.get(name,1)), dtype=torch.float32)\n",
    "#                            for name in self.streams_to_generate_in_item }\n",
    "#             label = 0\n",
    "#         return features_out, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# class BalancedDataset(Dataset):\n",
    "#     \"\"\" Memory-based balanced dataset. Loads only ACTIVE_STREAMS. \"\"\"\n",
    "#     def __init__(self, data_dict, active_streams, label_key=\"label\"):\n",
    "#         self.active_streams = active_streams # Streams for THIS run\n",
    "#         self.label_key = label_key\n",
    "\n",
    "#         if self.label_key not in data_dict or not data_dict[self.label_key]:\n",
    "#             raise ValueError(f\"Label key '{self.label_key}' missing/empty in loaded balanced data.\")\n",
    "#         self.num_samples = len(data_dict[self.label_key])\n",
    "#         if self.num_samples == 0: print(\"Warning: BalancedDataset initialized with zero samples.\")\n",
    "\n",
    "#         self.features = {}\n",
    "#         for stream in self.active_streams: # Only load active streams\n",
    "#             if stream in data_dict and data_dict[stream] is not None and len(data_dict[stream]) == self.num_samples :\n",
    "#                 try:\n",
    "#                     self.features[stream] = torch.tensor(np.asarray(data_dict[stream]), dtype=torch.float32)\n",
    "#                 except Exception as e: raise ValueError(f\"Error converting balanced stream '{stream}': {e}\") from e\n",
    "#             elif stream not in data_dict: raise KeyError(f\"Active stream '{stream}' missing from balanced data_dict.\")\n",
    "#             elif not data_dict[stream]: raise ValueError(f\"Active stream '{stream}' is empty in balanced data.\")\n",
    "#             else: raise ValueError(f\"Len mismatch for active stream '{stream}' ({len(data_dict[stream])}) vs labels ({self.num_samples}).\")\n",
    "\n",
    "#         try:\n",
    "#             self.labels = torch.tensor([lbl[0] for lbl in data_dict[self.label_key]], dtype=torch.long)\n",
    "#             if len(self.labels) != self.num_samples: raise ValueError(\"Label length mismatch.\")\n",
    "#         except Exception as e: raise ValueError(f\"Error converting labels from balanced data: {e}\") from e\n",
    "\n",
    "#     def __len__(self): return self.num_samples\n",
    "#     def __getitem__(self, idx):\n",
    "#         if idx >= self.num_samples: raise IndexError(\"Index out of bounds\")\n",
    "#         feat_dict = {s: self.features[s][idx] for s in self.active_streams if s in self.features}\n",
    "#         # It's an error if an active stream isn't in self.features by now\n",
    "#         if len(feat_dict) != len(self.active_streams):\n",
    "#             missing = set(self.active_streams) - set(feat_dict.keys())\n",
    "#             raise RuntimeError(f\"BalancedDataset __getitem__ failed: active streams {missing} not found in loaded features.\")\n",
    "#         return feat_dict, self.labels[idx]\n",
    "\n",
    "\n",
    "# class Attention(nn.Module): # Standard Attention\n",
    "#     def __init__(self, hidden_dim, attention_dim):\n",
    "#         super().__init__()\n",
    "#         self.attention_net = nn.Sequential( nn.Linear(hidden_dim, attention_dim),\n",
    "#                                             nn.Tanh(), nn.Linear(attention_dim, 1) )\n",
    "#     def forward(self, lstm_output):\n",
    "#         att_scores = self.attention_net(lstm_output).squeeze(2)\n",
    "#         att_weights = torch.softmax(att_scores, dim=1)\n",
    "#         context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1)\n",
    "#         return context_vector, att_weights\n",
    "\n",
    "\n",
    "# class MultiStreamWeightedAvgLSTM(nn.Module): # Weighted Fusion Model\n",
    "#     def __init__( self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes,\n",
    "#                   attention_dim, dropout_rate, stream_names=None ):\n",
    "#         super().__init__()\n",
    "#         if not stream_names: raise ValueError(\"stream_names cannot be empty.\")\n",
    "#         self.stream_names = sorted(stream_names) # Crucial: ensure consistent order for weights\n",
    "#         self.num_active_streams = len(self.stream_names)\n",
    "#         if self.num_active_streams == 0: raise ValueError(\"Model init with zero active streams.\")\n",
    "#         self.lstm_output_dim = lstm_hidden_size * 2\n",
    "\n",
    "#         self.lstms = nn.ModuleDict()\n",
    "#         self.attentions = nn.ModuleDict()\n",
    "#         print(f\"Initializing Weighted-Avg model with streams: {self.stream_names}\")\n",
    "#         for name in self.stream_names:\n",
    "#             if name not in input_sizes: raise KeyError(f\"Input size for stream '{name}' not provided.\")\n",
    "#             in_size = input_sizes[name]\n",
    "#             if not isinstance(in_size,int) or in_size<=0: raise ValueError(f\"Invalid size {in_size} for {name}\")\n",
    "#             print(f\"  – Stream '{name}' (input {in_size})\")\n",
    "#             self.lstms[name] = nn.LSTM( in_size, lstm_hidden_size, num_lstm_layers,\n",
    "#                                         batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0,\n",
    "#                                         bidirectional=True )\n",
    "#             self.attentions[name] = Attention(self.lstm_output_dim, attention_dim)\n",
    "\n",
    "#         self.fusion_weights = nn.Parameter(torch.ones(self.num_active_streams))\n",
    "#         fused_dim = self.lstm_output_dim\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         inter_dim = max(num_classes * 2, fused_dim // 2, 16)\n",
    "#         self.fc1 = nn.Linear(fused_dim, inter_dim); self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(inter_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x): # x: dict {'stream_name': tensor}\n",
    "#         if not isinstance(x,dict): raise TypeError(f\"Model input must be dict, got {type(x)}\")\n",
    "#         ctx_vecs = []\n",
    "#         for name in self.stream_names: # Iterate in the sorted order\n",
    "#             if name not in x: raise KeyError(f\"Input batch missing expected stream '{name}'.\")\n",
    "#             stream_in = x[name]\n",
    "#             if not isinstance(stream_in, torch.Tensor) or stream_in.ndim != 3:\n",
    "#                 raise ValueError(f\"Stream '{name}' input invalid: shape {stream_in.shape if isinstance(stream_in, torch.Tensor) else type(stream_in)}\")\n",
    "#             try:\n",
    "#                 lstm_out, _ = self.lstms[name](stream_in)\n",
    "#                 ctx_vec, _ = self.attentions[name](lstm_out)\n",
    "#                 ctx_vecs.append(ctx_vec)\n",
    "#             except Exception as e: raise RuntimeError(f\"Error in LSTM/Att for stream '{name}': {e}\") from e\n",
    "\n",
    "#         if len(ctx_vecs) != self.num_active_streams:\n",
    "#             raise RuntimeError(f\"Ctx vecs ({len(ctx_vecs)}) != num_streams ({self.num_active_streams})\")\n",
    "#         try:\n",
    "#             stacked = torch.stack(ctx_vecs, dim=1) # (B, NumStreams, Hidden*2)\n",
    "#             weights = torch.softmax(self.fusion_weights, dim=0).view(1, -1, 1) # (1, NumStreams, 1)\n",
    "#             fused = torch.sum(stacked * weights, dim=1) # (B, Hidden*2)\n",
    "#         except Exception as e: raise RuntimeError(f\"Error in fusion: {e}\") from e\n",
    "#         try:\n",
    "#             out = self.dropout(fused); out = self.relu(self.fc1(out))\n",
    "#             out = self.dropout(out); logits = self.fc2(out)\n",
    "#         except Exception as e: raise RuntimeError(f\"Error in classifier: {e}\") from e\n",
    "#         return logits\n",
    "\n",
    "# # Training/Evaluation helpers (standard, robust versions)\n",
    "# def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "#     model.train(); total_loss=0.0; all_preds,all_labels=[],[]\n",
    "#     active_model_streams = model.stream_names\n",
    "#     for batch_idx, (features, labels) in enumerate(tqdm(dataloader, desc=\"Training\", leave=False)):\n",
    "#         if not isinstance(features,dict): print(f\"Train batch {batch_idx} feats not dict. Skip.\"); continue\n",
    "#         inputs={}; missing_stream=False\n",
    "#         for name in active_model_streams:\n",
    "#             if name not in features: print(f\"Train batch {batch_idx} missing stream '{name}'. Skip batch.\"); missing_stream=True; break\n",
    "#             inputs[name]=features[name].to(device)\n",
    "#         if missing_stream: continue\n",
    "#         labels=labels.to(device)\n",
    "#         try:\n",
    "#             optimizer.zero_grad(); outputs=model(inputs)\n",
    "#             if labels.ndim!=1: labels=labels.squeeze()\n",
    "#             if labels.ndim!=1: raise ValueError(f\"Train labels shape {labels.shape} invalid.\")\n",
    "#             loss=criterion(outputs,labels); loss.backward(); optimizer.step()\n",
    "#             total_loss+=loss.item()\n",
    "#             with torch.no_grad(): preds=torch.argmax(outputs,1); all_preds.extend(preds.cpu().numpy()); all_labels.extend(labels.cpu().numpy())\n",
    "#         except Exception as e: print(f\"Err train step batch {batch_idx}: {e}\"); # break # Option to stop\n",
    "#     avg_loss = total_loss/max(1,len(dataloader)); acc=0.0\n",
    "#     if all_labels:\n",
    "#         try: acc=accuracy_score(all_labels,all_preds)\n",
    "#         except Exception as e: print(f\"Err calc train acc: {e}\")\n",
    "#     return avg_loss, acc\n",
    "\n",
    "# def evaluate_epoch(model, dataloader, criterion, device):\n",
    "#     model.eval(); total_loss=0.0; all_labels,all_preds,all_probs=[],[],[]\n",
    "#     active_model_streams = model.stream_names\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, (features, labels) in enumerate(tqdm(dataloader, desc=\"Evaluating\", leave=False)):\n",
    "#             if not isinstance(features,dict): print(f\"Eval batch {batch_idx} feats not dict. Skip.\"); continue\n",
    "#             inputs={}; missing_stream=False\n",
    "#             for name in active_model_streams: # Model expects these streams\n",
    "#                 if name not in features: # Validation PIEDataset should provide ALL streams\n",
    "#                     print(f\"Eval batch {batch_idx} missing stream '{name}' FROM DATASET (expected from val PIEDataset). Skip batch.\"); missing_stream=True; break\n",
    "#                 inputs[name]=features[name].to(device)\n",
    "#             if missing_stream: continue\n",
    "#             labels=labels.to(device)\n",
    "#             try:\n",
    "#                 outputs=model(inputs)\n",
    "#                 if labels.ndim!=1: labels=labels.squeeze()\n",
    "#                 if labels.ndim!=1: raise ValueError(f\"Eval labels shape {labels.shape} invalid.\")\n",
    "#                 loss=criterion(outputs,labels); total_loss+=loss.item()\n",
    "#                 probs_batch=torch.softmax(outputs,1); preds_batch=torch.argmax(probs_batch,1)\n",
    "#                 all_labels.extend(labels.cpu().numpy()); all_preds.extend(preds_batch.cpu().numpy()); all_probs.extend(probs_batch.cpu().numpy())\n",
    "#             except Exception as e: print(f\"Err eval step batch {batch_idx}: {e}\"); # continue # Option\n",
    "#     avg_loss = total_loss/max(1,len(dataloader)); acc,prec,rec,f1,auc=0.0,0.0,0.0,0.0,float('nan')\n",
    "#     if not all_labels: print(\"WARN: No labels collected during eval.\")\n",
    "#     else:\n",
    "#         try:\n",
    "#             all_probs_np=np.asarray(all_probs); all_labels_np=np.asarray(all_labels); all_preds_np=np.asarray(all_preds)\n",
    "#             acc=accuracy_score(all_labels_np,all_preds_np)\n",
    "#             prec,rec,f1,_=precision_recall_fscore_support(all_labels_np,all_preds_np,average=\"binary\",pos_label=1,zero_division=0)\n",
    "#             if len(np.unique(all_labels_np))>1 and all_probs_np.shape[1]==2: auc=roc_auc_score(all_labels_np,all_probs_np[:,1])\n",
    "#         except Exception as e: print(f\"Err calc eval metrics: {e}\")\n",
    "#     return {\"loss\":avg_loss,\"accuracy\":acc,\"precision\":prec,\"recall\":rec,\"f1\":f1,\"auc\":auc}\n",
    "\n",
    "# def get_predictions_and_labels(model, dataloader, device): # For CM\n",
    "#     model.eval(); lbls_all,prs_all=[],[]; active_model_streams=model.stream_names\n",
    "#     with torch.no_grad():\n",
    "#         for features,labels in tqdm(dataloader,desc=\"CM Data\",leave=False):\n",
    "#             if not isinstance(features,dict): continue\n",
    "#             inputs={}; missing=False\n",
    "#             for name in active_model_streams:\n",
    "#                 if name not in features: missing=True; break\n",
    "#                 inputs[name]=features[name].to(device)\n",
    "#             if missing: continue\n",
    "#             try: outputs=model(inputs); prs=torch.argmax(outputs,1); lbls_all.extend(labels.cpu().numpy()); prs_all.extend(prs.cpu().numpy())\n",
    "#             except Exception as e: print(f\"Err CM pred: {e}\")\n",
    "#     return np.asarray(lbls_all),np.asarray(prs_all)\n",
    "\n",
    "\n",
    "# # --- Main Execution ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(f\"--- Training/Eval with Weighted Fusion & YOLOP ---\")\n",
    "#     print(f\"ACTIVE_STREAMS for this run: {ACTIVE_STREAMS}\")\n",
    "\n",
    "#     print(f\"\\nLoading balanced data from: {BALANCED_DATA_PKL_PATH}\")\n",
    "#     print(f\"Loading scalers from: {SCALERS_PKL_PATH}\")\n",
    "#     try:\n",
    "#         if not Path(BALANCED_DATA_PKL_PATH).is_file(): raise FileNotFoundError(f\"File not found: {BALANCED_DATA_PKL_PATH}\")\n",
    "#         with open(BALANCED_DATA_PKL_PATH,\"rb\") as f: balanced_data = pickle.load(f)\n",
    "#         if not Path(SCALERS_PKL_PATH).is_file(): raise FileNotFoundError(f\"File not found: {SCALERS_PKL_PATH}\")\n",
    "#         with open(SCALERS_PKL_PATH,\"rb\") as f: scalers = pickle.load(f)\n",
    "#         if not isinstance(balanced_data,dict) or not balanced_data: raise TypeError(\"Balanced data invalid.\")\n",
    "#         if not isinstance(scalers,dict): raise TypeError(\"Scalers invalid.\")\n",
    "#         print(\"   ✓ Pre-processed data loaded.\")\n",
    "#     except Exception as e: print(f\"FATAL: Error loading pre-processed data: {e}. Ensure Cell 1 ran successfully.\"); sys.exit(1)\n",
    "\n",
    "#     print(\"\\nLoading PIE DB for Validation...\")\n",
    "#     pie_db_val = None\n",
    "#     if not Path(PIE_DATABASE_CACHE_PATH).is_file(): raise FileNotFoundError(f\"PIE DB cache not found: {PIE_DATABASE_CACHE_PATH}\")\n",
    "#     try:\n",
    "#         with open(PIE_DATABASE_CACHE_PATH,\"rb\") as f: pie_db_val=pickle.load(f)\n",
    "#         if not isinstance(pie_db_val,dict) or not pie_db_val: raise ValueError(\"Loaded PIE DB for val invalid.\")\n",
    "#         print(\"   ✓ PIE DB for validation loaded.\")\n",
    "#     except Exception as e: raise RuntimeError(f\"Failed to load PIE DB for val: {e}\")\n",
    "\n",
    "#     print(\"\\nCreating Datasets & DataLoaders...\")\n",
    "#     train_ds, val_ds, train_dl, val_dl = None,None,None,None\n",
    "#     try:\n",
    "#         train_ds = BalancedDataset(balanced_data, ACTIVE_STREAMS, label_key=\"label\")\n",
    "#         print(f\"   ✓ Training dataset ({len(train_ds)} samples). Streams: {train_ds.active_streams}\")\n",
    "#         del balanced_data; gc.collect()\n",
    "#         val_ds = PIEDataset(pie_db_val, VAL_SETS_STR, POSE_DATA_DIR, YOLOP_FEATURE_DIR, SEQ_LEN, PRED_LEN, scalers)\n",
    "#         print(f\"   ✓ Validation dataset ({len(val_ds)} samples). Generates: {val_ds.streams_to_generate_in_item}\")\n",
    "#         if len(train_ds)==0 or len(val_ds)==0: raise ValueError(\"A dataset is empty!\")\n",
    "#         num_w = min(os.cpu_count(), 2) # Safer default for workers\n",
    "#         train_dl = DataLoader(train_ds,BATCH_SIZE,shuffle=True,num_workers=num_w,pin_memory=True,drop_last=True)\n",
    "#         val_dl = DataLoader(val_ds,BATCH_SIZE,shuffle=False,num_workers=num_w,pin_memory=True)\n",
    "#         print(f\"   ✓ DataLoaders ready (Train batches: {len(train_dl)}, Val batches: {len(val_dl)}).\")\n",
    "#     except Exception as e: print(f\"FATAL: Error creating Datasets/DataLoaders: {e}\"); raise\n",
    "#     del pie_db_val; gc.collect()\n",
    "\n",
    "#     print(\"\\nInitializing model...\")\n",
    "#     model_instance = None\n",
    "#     try:\n",
    "#         current_stream_input_sizes = {}\n",
    "#         MAP_SPECIAL_TO_CONST_SUFFIX = { \"TRAFFIC_LIGHT\":\"TL_STATE\", \"STATIC_CONTEXT\":\"STATIC\", \"EGO_SPEED\":\"EGO_SPEED\",\n",
    "#                                         \"EGO_ACC\":\"EGO_ACC\", \"EGO_GYRO\":\"EGO_GYRO\", \"PED_ACTION\":\"PED_ACTION\",\n",
    "#                                         \"PED_LOOK\":\"PED_LOOK\", \"PED_OCCLUSION\":\"PED_OCC\", \"YOLOP\":\"YOLOP\" }\n",
    "#         for s_name in ACTIVE_STREAMS: # Iterate through streams chosen for THIS run\n",
    "#             s_upper = s_name.upper()\n",
    "#             suffix = MAP_SPECIAL_TO_CONST_SUFFIX.get(s_upper, s_upper)\n",
    "#             const = f\"INPUT_SIZE_{suffix}\"\n",
    "#             if s_name==\"bbox\": const=\"INPUT_SIZE_BBOX\"\n",
    "#             elif s_name==\"pose\": const=\"INPUT_SIZE_POSE\"\n",
    "#             if const not in globals(): raise ValueError(f\"Constant {const} for stream {s_name} not found.\")\n",
    "#             size_val = globals()[const]\n",
    "#             if not isinstance(size_val,int) or size_val<=0: raise ValueError(f\"Invalid size {size_val} for {const}\")\n",
    "#             current_stream_input_sizes[s_name] = size_val\n",
    "#         if len(current_stream_input_sizes) != len(ACTIVE_STREAMS):\n",
    "#             raise ValueError(\"Size mapping failed for some active streams.\")\n",
    "\n",
    "#         model_instance = MultiStreamWeightedAvgLSTM( current_stream_input_sizes, LSTM_HIDDEN_SIZE, NUM_LSTM_LAYERS,\n",
    "#                                              NUM_CLASSES, ATTENTION_DIM, DROPOUT_RATE,\n",
    "#                                              stream_names=ACTIVE_STREAMS ).to(DEVICE) # Pass active streams to model\n",
    "#         print(\"\\n--- Model Architecture (Weighted Avg Fusion) ---\"); print(model_instance)\n",
    "#         print(f\"Trainable params: {sum(p.numel() for p in model_instance.parameters() if p.requires_grad):,}\")\n",
    "#         print(\"-\" * 30)\n",
    "#     except Exception as e: print(f\"FATAL: Error initializing model: {e}\"); raise\n",
    "\n",
    "#     print(\"\\nCalculating class weights...\")\n",
    "#     cls_weights = torch.tensor([1.0,1.0],dtype=torch.float32).to(DEVICE)\n",
    "#     try:\n",
    "#         if train_ds and hasattr(train_ds,'labels') and len(train_ds.labels)>0:\n",
    "#             lbl_list = train_ds.labels.cpu().numpy(); n0=np.count_nonzero(lbl_list==0); n1=np.count_nonzero(lbl_list==1); tot=len(lbl_list)\n",
    "#             if tot>0 and n0>0 and n1>0: w0=tot/(2.0*n0); w1=tot/(2.0*n1); cls_weights=torch.tensor([w0,w1],dtype=torch.float32).to(DEVICE)\n",
    "#             elif tot>0: print(f\"WARN: Only one class in train labels (0:{n0},1:{n1}). Default weights.\")\n",
    "#             else: print(\"WARN: No labels in train_ds. Default weights.\")\n",
    "#         else: print(\"WARN: Cannot access train_ds labels. Default weights.\")\n",
    "#     except Exception as e: print(f\"Err calc weights: {e}. Default weights.\")\n",
    "#     print(f\"Loss weights → 0: {cls_weights[0]:.2f}, 1: {cls_weights[1]:.2f}\")\n",
    "#     crit = nn.CrossEntropyLoss(weight=cls_weights); opt = optim.Adam(model_instance.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "#     best_f1 = -1.0; hist={\"train_loss\":[],\"val_loss\":[],\"train_acc\":[],\"val_acc\":[],\"val_f1\":[]}; best_path=\"\"\n",
    "#     print(\"\\n--- Starting Training ---\")\n",
    "#     if not train_dl or not val_dl or not model_instance: raise RuntimeError(\"Setup incomplete for training.\")\n",
    "#     for ep in range(NUM_EPOCHS):\n",
    "#         t_start=time.time(); tr_loss,tr_acc=0.0,0.0; ep_metrics={}; ep_fail=False\n",
    "#         try:\n",
    "#             tr_loss,tr_acc = train_epoch(model_instance,train_dl,opt,crit,DEVICE)\n",
    "#             ep_metrics = evaluate_epoch(model_instance,val_dl,crit,DEVICE)\n",
    "#         except Exception as e_ep: print(f\"\\nERR EPOCH {ep+1}: {e_ep}\"); import traceback; traceback.print_exc(); ep_fail=True; ep_metrics={\"loss\":float('nan'),\"accuracy\":float('nan'),\"precision\":float('nan'),\"recall\":float('nan'),\"f1\":float('nan'),\"auc\":float('nan')}\n",
    "#         hist[\"train_loss\"].append(tr_loss if not ep_fail else float('nan')); hist[\"val_loss\"].append(ep_metrics.get(\"loss\",float('nan')))\n",
    "#         hist[\"train_acc\"].append(tr_acc if not ep_fail else float('nan')); hist[\"val_acc\"].append(ep_metrics.get(\"accuracy\",float('nan')))\n",
    "#         hist[\"val_f1\"].append(ep_metrics.get(\"f1\",float('nan')))\n",
    "#         print(f\"\\nEpoch {ep+1:02d}/{NUM_EPOCHS} – {time.time()-t_start:.1f}s\")\n",
    "#         if ep_fail: print(\"  *** EPOCH FAILED ***\")\n",
    "#         else:\n",
    "#             print(f\"  Train Loss: {tr_loss:.4f} | Train Acc: {tr_acc:.4f}\")\n",
    "#             print(f\"  Val Loss:   {ep_metrics.get('loss',-1):.4f} | Val Acc:  {ep_metrics.get('accuracy',-1):.4f}\")\n",
    "#             print(f\"  Val Prec:   {ep_metrics.get('precision',-1):.4f} | Val Rec:  {ep_metrics.get('recall',-1):.4f} | Val F1:   {ep_metrics.get('f1',-1):.4f} | Val AUC:  {ep_metrics.get('auc',float('nan')):.4f}\")\n",
    "#         curr_f1 = ep_metrics.get('f1',-1.0)\n",
    "#         if not ep_fail and not math.isnan(curr_f1) and curr_f1 > best_f1:\n",
    "#             best_f1=curr_f1; safe_s_str=\"_\".join(sorted(ACTIVE_STREAMS)).replace('/','_').replace('\\\\','_')\n",
    "#             best_path=f\"best_model_w_avg_{safe_s_str}_ep{ep+1}.pth\"\n",
    "#             try: torch.save(model_instance.state_dict(),best_path); print(f\"  ✓ New best model saved → {best_path} (F1 {best_f1:.4f})\")\n",
    "#             except Exception as e_save: print(f\"  ERR saving model {best_path}: {e_save}\")\n",
    "#     print(\"\\n--- Training Finished ---\")\n",
    "\n",
    "#     print(\"\\nPlotting training curves...\")\n",
    "#     v_eps=[i+1 for i,f1 in enumerate(hist[\"val_f1\"]) if not math.isnan(f1)]; v_tr_l=[l for l,f1 in zip(hist[\"train_loss\"],hist[\"val_f1\"]) if not math.isnan(f1)]\n",
    "#     v_v_l=[l for l,f1 in zip(hist[\"val_loss\"],hist[\"val_f1\"]) if not math.isnan(f1)]; v_tr_a=[a for a,f1 in zip(hist[\"train_acc\"],hist[\"val_f1\"]) if not math.isnan(f1)]\n",
    "#     v_v_a=[a for a,f1 in zip(hist[\"val_acc\"],hist[\"val_f1\"]) if not math.isnan(f1)]; v_v_f1=[f1 for f1 in hist[\"val_f1\"] if not math.isnan(f1)]\n",
    "#     if not v_eps: print(\"Skip plot: no valid epoch data.\")\n",
    "#     else:\n",
    "#         try:\n",
    "#             fig,ax=plt.subplots(1,2,figsize=(12,5)); ax[0].plot(v_eps,v_tr_l,label=\"Train\"); ax[0].plot(v_eps,v_v_l,label=\"Val\")\n",
    "#             ax[0].set_xlabel(\"Epoch\"); ax[0].set_ylabel(\"Loss\"); ax[0].set_title(\"Loss\"); ax[0].legend(); ax[0].grid(True)\n",
    "#             ax[1].plot(v_eps,v_tr_a,label=\"Train Acc\"); ax[1].plot(v_eps,v_v_a,label=\"Val Acc\"); ax[1].plot(v_eps,v_v_f1,\"--\",label=\"Val F1\")\n",
    "#             ax[1].set_xlabel(\"Epoch\"); ax[1].set_ylabel(\"Metric\"); ax[1].set_title(\"Acc & F1\"); ax[1].legend(); ax[1].grid(True)\n",
    "#             plt.tight_layout(); plt.show()\n",
    "#         except Exception as e_plot: print(f\"ERR plot: {e_plot}\")\n",
    "\n",
    "#     print(\"\\n--- Final Evaluation (Best Model) ---\")\n",
    "#     final_eval_metrics={}; yt,yp=np.array([]),np.array([])\n",
    "#     if best_path and Path(best_path).is_file():\n",
    "#         print(f\"Loading best model: {best_path}\")\n",
    "#         try: model_instance.to(DEVICE); model_instance.load_state_dict(torch.load(best_path,map_location=DEVICE)); print(\"✓ Best model loaded.\")\n",
    "#         except Exception as e_load_best: print(f\"WARN: Cannot load best model {best_path} ({e_load_best}). Using final state.\")\n",
    "#     else: print(\"WARN: Best model path not found. Using final state.\")\n",
    "#     if model_instance and val_dl and crit:\n",
    "#         try: final_eval_metrics=evaluate_epoch(model_instance,val_dl,crit,DEVICE); yt,yp=get_predictions_and_labels(model_instance,val_dl,DEVICE)\n",
    "#         except Exception as e_final_eval: print(f\"ERR final eval: {e_final_eval}\"); final_eval_metrics={\"loss\":float('nan'),\"accuracy\":float('nan'),\"precision\":float('nan'),\"recall\":float('nan'),\"f1\":float('nan'),\"auc\":float('nan')}\n",
    "#     else: print(\"ERR: Model/val_dl/crit not ready for final eval.\")\n",
    "#     print(\"\\n--- Final Metrics (Weighted Avg Fusion) ---\"); print(f\"  Streams: {', '.join(ACTIVE_STREAMS)}\")\n",
    "#     for k_met,v_met in final_eval_metrics.items(): print(f\"  {k_met:<10}: {v_met:.4f}\")\n",
    "#     print(f\"(Best Val F1 during train: {best_f1:.4f} from model {Path(best_path).name if best_path else 'N/A'})\")\n",
    "#     if yt.size>0 and yp.size>0:\n",
    "#         print(\"\\n--- Confusion Matrix ---\")\n",
    "#         try: cm=confusion_matrix(yt,yp,labels=[0,1]); ConfusionMatrixDisplay(cm,display_labels=[\"Not Cross\",\"Cross\"]).plot(cmap=plt.cm.Blues); plt.title(f\"CM ({', '.join(ACTIVE_STREAMS)})\"); plt.show()\n",
    "#         except Exception as e_cm: print(f\"ERR CM: {e_cm}. True unique: {np.unique(yt)}, Pred unique: {np.unique(yp)}\")\n",
    "#     else: print(\"Skip CM: no eval data.\")\n",
    "\n",
    "#     if hasattr(model_instance,\"fusion_weights\") and isinstance(model_instance.fusion_weights,nn.Parameter):\n",
    "#         try:\n",
    "#             with torch.no_grad(): w_sm=torch.softmax(model_instance.fusion_weights,dim=0).cpu().numpy()\n",
    "#             print(\"\\n--- Learned Fusion Weights (softmax) ---\")\n",
    "#             for s_n,w_val in zip(model_instance.stream_names,w_sm): print(f\"  {s_n:<15}: {w_val:.4f}\")\n",
    "#             print(\"-\" * 30)\n",
    "#         except Exception as e_fw: print(f\"ERR display fusion weights: {e_fw}\")\n",
    "#     print(\"\\n--- Script Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65dc7e96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T09:28:27.241356Z",
     "iopub.status.busy": "2025-05-07T09:28:27.241153Z",
     "iopub.status.idle": "2025-05-07T13:42:58.828523Z",
     "shell.execute_reply": "2025-05-07T13:42:58.827641Z"
    },
    "papermill": {
     "duration": 15271.600294,
     "end_time": "2025-05-07T13:42:58.829961",
     "exception": false,
     "start_time": "2025-05-07T09:28:27.229667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|██████████| 333454/333454 [12:16<00:00, 452.69it/s]\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/15] loss 0.0118 | F1 0.738 (P 0.66 R 0.83) thr 0.417 time 961.6s\n",
      "  ↳ saved best_ep01_f10.738.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/15] loss 0.0093 | F1 0.765 (P 0.73 R 0.80) thr 0.429 time 958.5s\n",
      "  ↳ saved best_ep02_f10.765.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/15] loss 0.0078 | F1 0.761 (P 0.70 R 0.83) thr 0.372 time 955.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/15] loss 0.0065 | F1 0.747 (P 0.71 R 0.79) thr 0.384 time 958.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/15] loss 0.0053 | F1 0.728 (P 0.69 R 0.77) thr 0.358 time 958.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/15] loss 0.0044 | F1 0.730 (P 0.68 R 0.79) thr 0.337 time 959.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/15] loss 0.0037 | F1 0.725 (P 0.70 R 0.76) thr 0.309 time 953.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/15] loss 0.0031 | F1 0.732 (P 0.67 R 0.80) thr 0.234 time 958.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/15] loss 0.0027 | F1 0.725 (P 0.67 R 0.79) thr 0.250 time 959.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/15] loss 0.0023 | F1 0.723 (P 0.69 R 0.76) thr 0.214 time 957.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/15] loss 0.0020 | F1 0.703 (P 0.65 R 0.76) thr 0.220 time 961.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/15] loss 0.0019 | F1 0.711 (P 0.65 R 0.78) thr 0.229 time 956.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/15] loss 0.0017 | F1 0.724 (P 0.68 R 0.77) thr 0.191 time 960.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/15] loss 0.0015 | F1 0.720 (P 0.67 R 0.78) thr 0.133 time 959.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/15] loss 0.0014 | F1 0.721 (P 0.66 R 0.80) thr 0.162 time 960.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL VALIDATION =====\n",
      "acc       : 0.9250\n",
      "precision : 0.7341\n",
      "recall    : 0.7981\n",
      "f1        : 0.7648\n",
      "auc       : 0.9678\n",
      "threshold  : 0.429\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# ================================================================\n",
    "#  PIE pedestrian–crossing prediction — precision-oriented version\n",
    "#  • focal-loss     • 40 % positive oversampling + noise\n",
    "#  • automatic threshold search each epoch (best-F1 checkpoint)\n",
    "# ================================================================\n",
    "\n",
    "# ----------------------------- IMPORTS --------------------------\n",
    "import os, sys, time, random, math, gc, pickle, warnings\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.metrics import (accuracy_score, precision_recall_curve,\n",
    "                             roc_auc_score, confusion_matrix,\n",
    "                             ConfusionMatrixDisplay)\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# silence optional libs\n",
    "for _m in (\"pandas\",\"seaborn\"): \n",
    "    try: globals()[_m]=__import__(_m)\n",
    "    except ImportError: warnings.warn(f\"{_m} not installed\")\n",
    "\n",
    "# ------------------------- GLOBAL CONSTANTS ---------------------\n",
    "# --- Paths ------------------------------------------------------\n",
    "PIE_ROOT_PATH            = '/kaggle/working/PIE'\n",
    "VIDEO_INPUT_DIR          = '/kaggle/input'\n",
    "POSE_DATA_DIR            = '/kaggle/input/pose-data/extracted_poses2'\n",
    "PIE_DATABASE_CACHE_PATH  = '/kaggle/input/pie-database/pie_database.pkl'\n",
    "YOLOP_FEATURE_DIR        = '/kaggle/input/yolop-data/yolop features'\n",
    "BALANCED_DATA_PKL_PATH   = '/kaggle/working/aug_balanced_train_data_with_yolop.pkl'\n",
    "SCALERS_PKL_PATH         = '/kaggle/working/scalers.pkl'\n",
    "\n",
    "# --- Streams ----------------------------------------------------\n",
    "ALL_POSSIBLE_STREAMS = [\n",
    "    'bbox','pose','ego_speed','ego_acc','ego_gyro',\n",
    "    'ped_action','ped_look','ped_occlusion',\n",
    "    'traffic_light','static_context',\n",
    "    'yolop'\n",
    "]\n",
    "ACTIVE_STREAMS = [\n",
    "    'bbox',\n",
    "    'ped_action',\n",
    "    'ped_look',\n",
    "    'ego_speed',\n",
    "    'ego_acc',\n",
    "    'yolop'\n",
    "]\n",
    "\n",
    "# --- Sequence & Input sizes ------------------------------------\n",
    "SEQ_LEN = 30; PRED_LEN = 1\n",
    "INPUT_SIZE_BBOX        = 4\n",
    "INPUT_SIZE_POSE        = 34\n",
    "INPUT_SIZE_EGO_SPEED   = 1\n",
    "INPUT_SIZE_EGO_ACC     = 2\n",
    "INPUT_SIZE_EGO_GYRO    = 1\n",
    "INPUT_SIZE_PED_ACTION  = 1\n",
    "INPUT_SIZE_PED_LOOK    = 1\n",
    "INPUT_SIZE_PED_OCC     = 1\n",
    "INPUT_SIZE_TL_STATE    = 4\n",
    "NUM_SIGNALIZED_CATS    = 4\n",
    "NUM_INTERSECTION_CATS  = 5\n",
    "NUM_AGE_CATS           = 4\n",
    "NUM_GENDER_CATS        = 3\n",
    "NUM_TRAFFIC_DIR_CATS   = 2\n",
    "LANE_CATEGORIES        = {1:0,2:1,3:2,4:3,5:4,6:4,7:4,8:4}\n",
    "NUM_LANE_CATS          = len(set(LANE_CATEGORIES.values()))\n",
    "INPUT_SIZE_STATIC      = (NUM_SIGNALIZED_CATS+NUM_INTERSECTION_CATS+\n",
    "                          NUM_AGE_CATS+NUM_GENDER_CATS+\n",
    "                          NUM_TRAFFIC_DIR_CATS+NUM_LANE_CATS)\n",
    "GRID_SIZE = 3\n",
    "INPUT_SIZE_YOLOP = GRID_SIZE**2*2 + 2   # 20\n",
    "\n",
    "# --- Model hyper-parameters ------------------------------------\n",
    "LSTM_HIDDEN_SIZE = 256\n",
    "NUM_LSTM_LAYERS  = 2\n",
    "DROPOUT_RATE     = 0.2     # ←lighter\n",
    "NUM_CLASSES      = 2\n",
    "ATTENTION_DIM    = 128\n",
    "\n",
    "# --- Training ---------------------------------------------------\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE    = 32\n",
    "NUM_EPOCHS    = 15\n",
    "DEVICE        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "TRAIN_SETS_STR = ['set01','set02','set04']\n",
    "VAL_SETS_STR   = ['set05','set06']\n",
    "\n",
    "TL_STATE_MAP   = {'__undefined__':0,'red':1,'yellow':2,'green':3}\n",
    "SIGNALIZED_MAP = {'n/a':0,'C':1,'S':2,'CS':3}\n",
    "INTERSECTION_MAP={'midblock':0,'T':1,'T-left':2,'T-right':3,'four-way':4}\n",
    "AGE_MAP        = {'child':0,'young':1,'adult':2,'senior':3}\n",
    "GENDER_MAP     = {'n/a':0,'female':1,'male':2}\n",
    "TRAFFIC_DIR_MAP={'OW':0,'TW':1}\n",
    "\n",
    "# -------------------- HELPER: one-hot ---------------------------\n",
    "def to_one_hot(idx, n):\n",
    "    v=np.zeros(n,dtype=np.float32)\n",
    "    v[int(np.clip(idx,0,n-1))]=1.0\n",
    "    return v\n",
    "\n",
    "# ========================= DATASETS =============================\n",
    "class PIEDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads sequences from the PIE database and optional pose / YOLOP pkls.\n",
    "    Each item:\n",
    "        dict{stream_name: Tensor(seq_len, feat_dim)}, label Tensor()\n",
    "    \"\"\"\n",
    "    def __init__(self, pie_db, set_names, pose_dir, yolop_dir,\n",
    "                 seq_len, pred_len, scalers=None, streams=None):\n",
    "        super().__init__()\n",
    "        self.pie_db=pie_db; self.set_names=set_names\n",
    "        self.pose_dir=pose_dir; self.yolop_dir=yolop_dir\n",
    "        self.seq_len=seq_len; self.pred_len=pred_len\n",
    "        self.scalers=scalers or {}; self.streams=streams or ALL_POSSIBLE_STREAMS\n",
    "        self._sizes=self._calc_input_sizes()\n",
    "        self.all_pose={}; self.all_yolop={}\n",
    "        if 'pose'  in self.streams:  self._load_pose()\n",
    "        if 'yolop' in self.streams:  self._load_yolop()\n",
    "        self.sequences=[]; self._make_seq_list()\n",
    "        if not self.sequences: raise ValueError(\"No sequences created\")\n",
    "\n",
    "    def _calc_input_sizes(self):\n",
    "        out={}\n",
    "        for s in ALL_POSSIBLE_STREAMS:\n",
    "            if s=='bbox': out[s]=INPUT_SIZE_BBOX\n",
    "            elif s=='pose':out[s]=INPUT_SIZE_POSE\n",
    "            elif s=='ego_speed':out[s]=INPUT_SIZE_EGO_SPEED\n",
    "            elif s=='ego_acc': out[s]=INPUT_SIZE_EGO_ACC\n",
    "            elif s=='ego_gyro':out[s]=INPUT_SIZE_EGO_GYRO\n",
    "            elif s=='ped_action':out[s]=INPUT_SIZE_PED_ACTION\n",
    "            elif s=='ped_look': out[s]=INPUT_SIZE_PED_LOOK\n",
    "            elif s=='ped_occlusion':out[s]=INPUT_SIZE_PED_OCC\n",
    "            elif s=='traffic_light':out[s]=INPUT_SIZE_TL_STATE\n",
    "            elif s=='static_context':out[s]=INPUT_SIZE_STATIC\n",
    "            elif s=='yolop':out[s]=INPUT_SIZE_YOLOP\n",
    "            else: out[s]=1\n",
    "        return out\n",
    "\n",
    "    # ---------- load pose pkls ----------------------------------\n",
    "    def _load_pose(self):\n",
    "        for set_id in self.set_names:\n",
    "            self.all_pose[set_id]={}\n",
    "            pset=os.path.join(self.pose_dir,set_id)\n",
    "            if not os.path.isdir(pset): continue\n",
    "            for f in os.listdir(pset):\n",
    "                if not f.endswith('_poses.pkl'): continue\n",
    "                vid_id=\"_\".join(f.replace('_poses.pkl','').split('_')[1:])\n",
    "                with open(os.path.join(pset,f),'rb') as fh:\n",
    "                    pk=pickle.load(fh)\n",
    "                if len(pk)!=1: continue\n",
    "                key=list(pk.keys())[0]; self.all_pose[set_id][vid_id]=pk[key]\n",
    "\n",
    "    # ---------- load YOLOP pkls ---------------------------------\n",
    "    def _load_yolop(self):\n",
    "        if not os.path.isdir(self.yolop_dir): return\n",
    "        for f in os.listdir(self.yolop_dir):\n",
    "            if not f.endswith('_yolop_features.pkl'): continue\n",
    "            parts=f.replace('_yolop_features.pkl','').split('_')\n",
    "            set_id, vid_id = parts[0], \"_\".join(parts[1:])\n",
    "            if set_id not in self.set_names: continue\n",
    "            self.all_yolop.setdefault(set_id,{})\n",
    "            with open(os.path.join(self.yolop_dir,f),'rb') as fh:\n",
    "                pk=pickle.load(fh)\n",
    "            if len(pk)!=1: continue\n",
    "            key=list(pk.keys())[0]; self.all_yolop[set_id][vid_id]=pk[key]\n",
    "\n",
    "    # ---------- generate (set,video,ped,start_frame) tuples -----\n",
    "    def _make_seq_list(self):\n",
    "        for set_id in self.set_names:\n",
    "            if set_id not in self.pie_db: continue\n",
    "            for vid_id,v in self.pie_db[set_id].items():\n",
    "                for ped_id,p in v.get('ped_annotations',{}).items():\n",
    "                    frames=sorted(p['frames'])\n",
    "                    if len(frames)<self.seq_len+self.pred_len:continue\n",
    "                    for i in range(len(frames)-self.seq_len-self.pred_len+1):\n",
    "                        sfrm=frames[i]\n",
    "                        efrm=frames[i+self.seq_len-1]\n",
    "                        if efrm-sfrm!=self.seq_len-1: continue\n",
    "                        t_idx=i+self.seq_len+self.pred_len-1\n",
    "                        tfrm=frames[t_idx]\n",
    "                        if tfrm-efrm!=self.pred_len: continue\n",
    "                        self.sequences.append((set_id,vid_id,ped_id,sfrm))\n",
    "\n",
    "    def __len__(self): return len(self.sequences)\n",
    "\n",
    "    # ---------- main extraction ---------------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        set_id,vid_id,ped_id,start = self.sequences[idx]\n",
    "        frame_nums=list(range(start,start+self.seq_len))\n",
    "        tgt_frame=start+self.seq_len+self.pred_len-1\n",
    "        vdb=self.pie_db[set_id][vid_id]\n",
    "        pdb=vdb['ped_annotations'][ped_id]\n",
    "        edb=vdb['vehicle_annotations']\n",
    "        tdb=vdb['traffic_annotations']\n",
    "        padd=pdb.get('attributes',{})\n",
    "        feats={s:[] for s in self.streams}\n",
    "        # label\n",
    "        lbl=0\n",
    "        if 'behavior'in pdb and 'cross'in pdb['behavior']:\n",
    "            try:\n",
    "                tidx=pdb['frames'].index(tgt_frame)\n",
    "                lbl=pdb['behavior']['cross'][tidx]\n",
    "                if lbl==-1: lbl=0\n",
    "            except: pass\n",
    "        # static\n",
    "        if 'static_context' in self.streams:\n",
    "            sig= padd.get('signalized',0)\n",
    "            inter=padd.get('intersection',0)\n",
    "            age=padd.get('age',2)\n",
    "            gen=padd.get('gender',0)\n",
    "            td_idx=int(padd.get('traffic_direction',0))\n",
    "            nlcat=LANE_CATEGORIES.get(padd.get('num_lanes',2),\n",
    "                                      LANE_CATEGORIES[max(LANE_CATEGORIES)])\n",
    "            static_vec=np.concatenate([\n",
    "                to_one_hot(sig,NUM_SIGNALIZED_CATS),\n",
    "                to_one_hot(inter,NUM_INTERSECTION_CATS),\n",
    "                to_one_hot(age,NUM_AGE_CATS),\n",
    "                to_one_hot(gen,NUM_GENDER_CATS),\n",
    "                to_one_hot(td_idx,NUM_TRAFFIC_DIR_CATS),\n",
    "                to_one_hot(nlcat,NUM_LANE_CATS)\n",
    "            ]).astype(np.float32)\n",
    "        # per-frame\n",
    "        for f in frame_nums:\n",
    "            try: fidx=pdb['frames'].index(f)\n",
    "            except: fidx=-1\n",
    "            ego=edb.get(f,{})\n",
    "            # bbox --------------------------------------------------\n",
    "            if 'bbox' in self.streams:\n",
    "                b=np.zeros(4,dtype=np.float32)\n",
    "                if fidx!=-1 and len(pdb['bbox'])>fidx:\n",
    "                    x1,y1,x2,y2=pdb['bbox'][fidx]\n",
    "                    w=vdb['width']; h=vdb['height']\n",
    "                    if w>0 and h>0:\n",
    "                        cx=((x1+x2)/2)/w; cy=((y1+y2)/2)/h\n",
    "                        bw=(x2-x1)/w;    bh=(y2-y1)/h\n",
    "                        if bw>0 and bh>0: b=np.array([cx,cy,bw,bh],np.float32)\n",
    "                feats['bbox'].append(b)\n",
    "            # pose ---------------------------------------------------\n",
    "            if 'pose' in self.streams:\n",
    "                pv=np.zeros(INPUT_SIZE_POSE,np.float32)\n",
    "                pose_set=self.all_pose.get(set_id,{}).get(vid_id,{})\n",
    "                if f in pose_set and ped_id in pose_set[f]:\n",
    "                    pv=pose_set[f][ped_id]\n",
    "                feats['pose'].append(pv)\n",
    "            # ego speed ---------------------------------------------\n",
    "            if 'ego_speed' in self.streams:\n",
    "                sp=ego.get('OBD_speed',0.0) or ego.get('GPS_speed',0.0)\n",
    "                sp=(sp-self.scalers.get('ego_speed_mean',0.))/ \\\n",
    "                   self.scalers.get('ego_speed_std',1.)\n",
    "                feats['ego_speed'].append([sp])\n",
    "            # ego acc -----------------------------------------------\n",
    "            if 'ego_acc' in self.streams:\n",
    "                ax=ego.get('accX',0.); ay=ego.get('accY',0.)\n",
    "                ax=(ax-self.scalers.get('accX_mean',0.))/ \\\n",
    "                   self.scalers.get('accX_std',1.)\n",
    "                ay=(ay-self.scalers.get('accY_mean',0.))/ \\\n",
    "                   self.scalers.get('accY_std',1.)\n",
    "                feats['ego_acc'].append([ax,ay])\n",
    "            # ego gyro ----------------------------------------------\n",
    "            if 'ego_gyro' in self.streams:\n",
    "                gz=ego.get('gyroZ',0.)\n",
    "                gz=(gz-self.scalers.get('gyroZ_mean',0.))/ \\\n",
    "                   self.scalers.get('gyroZ_std',1.)\n",
    "                feats['ego_gyro'].append([gz])\n",
    "            # ped action --------------------------------------------\n",
    "            if 'ped_action' in self.streams:\n",
    "                act=0\n",
    "                if fidx!=-1 and len(pdb['behavior']['action'])>fidx:\n",
    "                    act=pdb['behavior']['action'][fidx]\n",
    "                feats['ped_action'].append([float(act)])\n",
    "            # ped look ----------------------------------------------\n",
    "            if 'ped_look' in self.streams:\n",
    "                look=0\n",
    "                if fidx!=-1 and len(pdb['behavior']['look'])>fidx:\n",
    "                    look=pdb['behavior']['look'][fidx]\n",
    "                feats['ped_look'].append([float(look)])\n",
    "            # ped occ ----------------------------------------------\n",
    "            if 'ped_occlusion' in self.streams:\n",
    "                occ=0.\n",
    "                if fidx!=-1 and len(pdb.get('occlusion',[]))>fidx:\n",
    "                    occ=pdb['occlusion'][fidx]/2.0\n",
    "                feats['ped_occlusion'].append([occ])\n",
    "            # traffic light ----------------------------------------\n",
    "            if 'traffic_light' in self.streams:\n",
    "                state=0\n",
    "                for _,td in tdb.items():\n",
    "                    if td.get('obj_class')!='traffic_light': continue\n",
    "                    if f in td.get('frames',[]):\n",
    "                        idx=td['frames'].index(f)\n",
    "                        state=td['state'][idx]; break\n",
    "                feats['traffic_light'].append(to_one_hot(state,len(TL_STATE_MAP)))\n",
    "            # static ------------------------------------------------\n",
    "            if 'static_context' in self.streams:\n",
    "                feats['static_context'].append(static_vec)\n",
    "            # yolop -------------------------------------------------\n",
    "            if 'yolop' in self.streams:\n",
    "                yv=np.zeros(INPUT_SIZE_YOLOP,np.float32)\n",
    "                yset=self.all_yolop.get(set_id,{}).get(vid_id,{})\n",
    "                if f in yset and ped_id in yset[f]:\n",
    "                    yv=yset[f][ped_id]\n",
    "                feats['yolop'].append(yv)\n",
    "\n",
    "        out={s:torch.tensor(np.array(feats[s],np.float32))\n",
    "             for s in self.streams}\n",
    "        return out, torch.tensor(lbl,dtype=torch.long)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "class BalancedDataset(Dataset):\n",
    "    \"\"\"Takes a pre-built dict of numpy sequences + labels.\"\"\"\n",
    "    def __init__(self,data,active,label_key='label'):\n",
    "        self.active=active\n",
    "        self.labels=torch.tensor([l[0] for l in data[label_key]],\n",
    "                                 dtype=torch.long)\n",
    "        self.feats={}\n",
    "        for s in active:\n",
    "            arr=np.array(data[s],dtype=np.float32)\n",
    "            self.feats[s]=torch.tensor(arr)\n",
    "        self.n=len(self.labels)\n",
    "\n",
    "    def __len__(self): return self.n\n",
    "    def __getitem__(self,i):\n",
    "        return {s:self.feats[s][i] for s in self.active}, self.labels[i]\n",
    "\n",
    "# ========================= MODEL =================================\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,hidden,att_dim):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(hidden,att_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(att_dim,1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        a=self.net(x).squeeze(2)\n",
    "        w=torch.softmax(a,dim=1)\n",
    "        ctx=(x*w.unsqueeze(2)).sum(1)\n",
    "        return ctx,w\n",
    "\n",
    "class MultiStreamWeightedAvgLSTM(nn.Module):\n",
    "    def __init__(self,input_sizes,hidden,n_layers,n_classes,\n",
    "                 att_dim,dropout,stream_names):\n",
    "        super().__init__()\n",
    "        self.stream_names=stream_names\n",
    "        self.lstm_out_dim=hidden*2\n",
    "        self.lstms=nn.ModuleDict()\n",
    "        self.atts=nn.ModuleDict()\n",
    "        for s in stream_names:\n",
    "            inp=input_sizes[s]\n",
    "            self.lstms[s]=nn.LSTM(inp,hidden,n_layers,\n",
    "                                  batch_first=True,\n",
    "                                  dropout=dropout if n_layers>1 else 0,\n",
    "                                  bidirectional=True)\n",
    "            self.atts[s]=Attention(self.lstm_out_dim,att_dim)\n",
    "        self.fusion_w=nn.Parameter(torch.ones(len(stream_names)))\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.lstm_out_dim,max(8,self.lstm_out_dim//2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(max(8,self.lstm_out_dim//2),n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        ctx=[]\n",
    "        for s in self.stream_names:\n",
    "            lstm_out,_=self.lstms[s](x[s])\n",
    "            c,_=self.atts[s](lstm_out); ctx.append(c)\n",
    "        ctx=torch.stack(ctx,1)\n",
    "        w=torch.softmax(self.fusion_w,0).view(1,-1,1)\n",
    "        fused=(ctx*w).sum(1)\n",
    "        return self.fc(fused)\n",
    "\n",
    "# ====================== LOSS / METRICS ===========================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self,alpha=.25,gamma=2.):\n",
    "        super().__init__()\n",
    "        self.a=alpha; self.g=gamma\n",
    "        self.ce=nn.CrossEntropyLoss(reduction='none')\n",
    "    def forward(self,logits,targets):\n",
    "        ce=self.ce(logits,targets)\n",
    "        pt=torch.exp(-ce)\n",
    "        return (self.a*(1-pt)**self.g*ce).mean()\n",
    "\n",
    "# ----------------------- TRAIN / EVAL ----------------------------\n",
    "def train_epoch(model,loader,opt,crit,device):\n",
    "    model.train(); tloss=0; yp,yt=[],[]\n",
    "    act=model.stream_names\n",
    "    for X,y in tqdm(loader,desc='train',leave=False):\n",
    "        X={k:v.to(device) for k,v in X.items()}\n",
    "        y=y.to(device)\n",
    "        opt.zero_grad(); out=model(X); loss=crit(out,y)\n",
    "        loss.backward(); opt.step()\n",
    "        tloss+=loss.item()\n",
    "        yp.extend(out.argmax(1).cpu().numpy()); yt.extend(y.cpu().numpy())\n",
    "    return tloss/len(loader), accuracy_score(yt,yp)\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_probs_labels(model,loader,device):\n",
    "    model.eval(); probs,labels=[],[]\n",
    "    for X,y in tqdm(loader,desc='eval',leave=False):\n",
    "        X={k:v.to(device) for k,v in X.items()}\n",
    "        p=torch.softmax(model(X),1)[:,1]\n",
    "        probs.extend(p.cpu().numpy()); labels.extend(y.numpy())\n",
    "    return np.array(probs),np.array(labels)\n",
    "\n",
    "def best_threshold(pr,lbl):\n",
    "    p,r,t=precision_recall_curve(lbl,pr)\n",
    "    f1=2*p*r/(p+r+1e-12)\n",
    "    idx=np.nanargmax(f1); return t[idx],f1[idx],p[idx],r[idx]\n",
    "\n",
    "def metrics(pr,lbl,thr):\n",
    "    pred=(pr>thr).astype(int)\n",
    "    acc=accuracy_score(lbl,pred)\n",
    "    tp=(pred&lbl).sum()\n",
    "    prec=tp/max(pred.sum(),1); rec=tp/max(lbl.sum(),1)\n",
    "    f1=2*prec*rec/(prec+rec+1e-12)\n",
    "    auc=roc_auc_score(lbl,pr) if len(np.unique(lbl))>1 else float('nan')\n",
    "    return dict(acc=acc,precision=prec,recall=rec,f1=f1,auc=auc)\n",
    "\n",
    "# ---------------------- OVERSAMPLING + AUG -----------------------\n",
    "TARGET_POS_RATIO     = 0.40\n",
    "GAUSSIAN_NOISE_STD   = 0.02\n",
    "STREAMS_NOISE_AUG    = ['bbox','pose','ego_speed','ego_acc','ego_gyro','yolop']\n",
    "\n",
    "def make_balanced_dict(full_ds):\n",
    "    d={s:[] for s in ALL_POSSIBLE_STREAMS}; d['label']=[]\n",
    "    pos,neg=[],[]\n",
    "    for i in tqdm(range(len(full_ds)),desc='extract'):\n",
    "        X,y=full_ds[i]; lab=y.item()\n",
    "        d['label'].append([lab])\n",
    "        for s in ALL_POSSIBLE_STREAMS:\n",
    "            d[s].append(X[s].numpy() if s in X\n",
    "                        else np.zeros((SEQ_LEN,full_ds._sizes[s]),np.float32))\n",
    "        (pos if lab else neg).append(i)\n",
    "    need=int(TARGET_POS_RATIO/(1-TARGET_POS_RATIO)*len(neg))-len(pos)\n",
    "    need=max(0,need)\n",
    "    for idx in random.choices(pos,k=need):\n",
    "        d['label'].append([1])\n",
    "        for s in ALL_POSSIBLE_STREAMS:\n",
    "            seq=d[s][idx].copy()\n",
    "            if s in STREAMS_NOISE_AUG:\n",
    "                seq+=np.random.normal(0,GAUSSIAN_NOISE_STD,seq.shape).astype(np.float32)\n",
    "                if s=='bbox':\n",
    "                    seq[:,:2]=np.clip(seq[:,:2],.05,.95)\n",
    "                    seq[:,2:]=np.clip(seq[:,2:],.05,1.)\n",
    "                elif s=='pose':\n",
    "                    seq=np.clip(seq,0.,1.)\n",
    "            d[s].append(seq)\n",
    "    idxs=list(range(len(d['label']))); random.shuffle(idxs)\n",
    "    return {k:[v[i] for i in idxs] for k,v in d.items()}\n",
    "\n",
    "# ============================ MAIN ===============================\n",
    "if __name__ == '__main__':\n",
    "    random.seed(42); np.random.seed(42); torch.manual_seed(42)\n",
    "\n",
    "    # --------- Load PIE database --------------------------------\n",
    "    if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n",
    "        raise FileNotFoundError(\"PIE database cache missing\")\n",
    "    with open(PIE_DATABASE_CACHE_PATH,'rb') as fh:\n",
    "        pie_db=pickle.load(fh)\n",
    "\n",
    "    # --------- Dummy scalers (replace by your saved scalers) ----\n",
    "    if os.path.exists(SCALERS_PKL_PATH):\n",
    "        with open(SCALERS_PKL_PATH,'rb') as fh:\n",
    "            scalers=pickle.load(fh)\n",
    "    else:\n",
    "        scalers={'ego_speed_mean':0,'ego_speed_std':1,\n",
    "                 'accX_mean':0,'accX_std':1,\n",
    "                 'accY_mean':0,'accY_std':1,\n",
    "                 'gyroZ_mean':0,'gyroZ_std':1}\n",
    "\n",
    "    # --------- Build datasets -----------------------------------\n",
    "    full_train=PIEDataset(pie_db,TRAIN_SETS_STR,POSE_DATA_DIR,\n",
    "                          YOLOP_FEATURE_DIR,SEQ_LEN,PRED_LEN,\n",
    "                          scalers,ALL_POSSIBLE_STREAMS)\n",
    "    bal_dict=make_balanced_dict(full_train)\n",
    "    train_ds=BalancedDataset(bal_dict,ACTIVE_STREAMS)\n",
    "\n",
    "    val_ds=PIEDataset(pie_db,VAL_SETS_STR,POSE_DATA_DIR,\n",
    "                      YOLOP_FEATURE_DIR,SEQ_LEN,PRED_LEN,\n",
    "                      scalers,ALL_POSSIBLE_STREAMS)\n",
    "\n",
    "    train_ld=DataLoader(train_ds,BATCH_SIZE,shuffle=True,\n",
    "                        num_workers=2,pin_memory=True)\n",
    "    val_ld  =DataLoader(val_ds,BATCH_SIZE,shuffle=False,\n",
    "                        num_workers=2,pin_memory=True)\n",
    "\n",
    "    # --------- Model --------------------------------------------\n",
    "    inp_sizes={s:(INPUT_SIZE_BBOX if s=='bbox' else\n",
    "                  INPUT_SIZE_POSE if s=='pose' else\n",
    "                  INPUT_SIZE_EGO_SPEED if s=='ego_speed' else\n",
    "                  INPUT_SIZE_EGO_ACC if s=='ego_acc' else\n",
    "                  INPUT_SIZE_EGO_GYRO if s=='ego_gyro' else\n",
    "                  INPUT_SIZE_PED_ACTION if s=='ped_action' else\n",
    "                  INPUT_SIZE_PED_LOOK if s=='ped_look' else\n",
    "                  INPUT_SIZE_PED_OCC if s=='ped_occlusion' else\n",
    "                  INPUT_SIZE_TL_STATE if s=='traffic_light' else\n",
    "                  INPUT_SIZE_STATIC if s=='static_context' else\n",
    "                  INPUT_SIZE_YOLOP if s=='yolop' else 1)\n",
    "                for s in ACTIVE_STREAMS}\n",
    "\n",
    "    model=MultiStreamWeightedAvgLSTM(inp_sizes,LSTM_HIDDEN_SIZE,\n",
    "                                     NUM_LSTM_LAYERS,NUM_CLASSES,\n",
    "                                     ATTENTION_DIM,DROPOUT_RATE,\n",
    "                                     ACTIVE_STREAMS).to(DEVICE)\n",
    "    criterion=FocalLoss(alpha=.25,gamma=2.0)\n",
    "    optimiser=optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "    best_f1,best_thr,best_path=-1.,.5,\"\"\n",
    "    hist={'train_loss':[],'val_f1':[]}\n",
    "\n",
    "    # --------- Training loop ------------------------------------\n",
    "    for ep in range(1,NUM_EPOCHS+1):\n",
    "        t0=time.time()\n",
    "        tr_loss,tr_acc=train_epoch(model,train_ld,optimiser,\n",
    "                                   criterion,DEVICE)\n",
    "        v_probs,v_lbls=collect_probs_labels(model,val_ld,DEVICE)\n",
    "        thr,f1,prec,rec=best_threshold(v_probs,v_lbls)\n",
    "        metr=metrics(v_probs,v_lbls,thr)\n",
    "        hist['train_loss'].append(tr_loss); hist['val_f1'].append(f1)\n",
    "        print(f\"[{ep}/{NUM_EPOCHS}] \"\n",
    "              f\"loss {tr_loss:.4f} | F1 {f1:.3f} \"\n",
    "              f\"(P {prec:.2f} R {rec:.2f}) thr {thr:.3f} \"\n",
    "              f\"time {time.time()-t0:.1f}s\")\n",
    "        if f1>best_f1:\n",
    "            best_f1,best_thr=f1,thr\n",
    "            best_path=f\"best_ep{ep:02d}_f1{best_f1:.3f}.pth\"\n",
    "            torch.save({'state':model.state_dict(),\n",
    "                        'threshold':best_thr},best_path)\n",
    "            print(f\"  ↳ saved {best_path}\")\n",
    "\n",
    "    # --------- Final evaluation ---------------------------------\n",
    "    ckpt=torch.load(best_path,map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt['state']); best_thr=ckpt['threshold']\n",
    "    probs,lbls=collect_probs_labels(model,val_ld,DEVICE)\n",
    "    fin=metrics(probs,lbls,best_thr)\n",
    "    cm=confusion_matrix(lbls,(probs>best_thr).astype(int),labels=[0,1])\n",
    "\n",
    "    print(\"\\n===== FINAL VALIDATION =====\")\n",
    "    for k,v in fin.items(): print(f\"{k:10}: {v:.4f}\")\n",
    "    print(f\"threshold  : {best_thr:.3f}\")\n",
    "\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                           display_labels=['Not-Cross','Cross']).plot(\n",
    "                           cmap=plt.cm.Blues)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ffcb8",
   "metadata": {
    "papermill": {
     "duration": 6.292962,
     "end_time": "2025-05-07T13:43:11.600109",
     "exception": false,
     "start_time": "2025-05-07T13:43:05.307147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6993690,
     "sourceId": 11201333,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6993708,
     "sourceId": 11201362,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6993722,
     "sourceId": 11201388,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6993740,
     "sourceId": 11201422,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6993794,
     "sourceId": 11201506,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6993809,
     "sourceId": 11201543,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7034191,
     "sourceId": 11255589,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7127490,
     "sourceId": 11382982,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7142036,
     "sourceId": 11402679,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7333398,
     "sourceId": 11684148,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 279383,
     "modelInstanceId": 258142,
     "sourceId": 302300,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 283333,
     "modelInstanceId": 262207,
     "sourceId": 307831,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 288527,
     "modelInstanceId": 267476,
     "sourceId": 316944,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 297682,
     "modelInstanceId": 276781,
     "sourceId": 329886,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 297702,
     "modelInstanceId": 276800,
     "sourceId": 329908,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 314775,
     "modelInstanceId": 294156,
     "sourceId": 352620,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15317.627696,
   "end_time": "2025-05-07T13:43:21.495869",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-07T09:28:03.868173",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
