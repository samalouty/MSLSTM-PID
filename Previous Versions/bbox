{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-03-25T21:48:32.806416Z","iopub.execute_input":"2025-03-25T21:48:32.806882Z","iopub.status.idle":"2025-03-25T21:48:33.116651Z","shell.execute_reply.started":"2025-03-25T21:48:32.806858Z","shell.execute_reply":"2025-03-25T21:48:33.115763Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom oauth2client.client import GoogleCredentials\nfrom google.colab import auth\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\nfile_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\nfor file in file_list:\n    print('title: %s, id: %s' % (file['title'], file['id']))\n    folder_id = \"1TmB2W_gw7LfzqIAKNUtzi-rsBXR5Ovb-\"  # Replace this with your folder's ID\nfile_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(folder_id)}).GetList()\n\nfor file in file_list:\n    print(f\"Title: {file['title']}, ID: {file['id']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T01:51:38.692536Z","iopub.execute_input":"2025-03-25T01:51:38.692960Z","iopub.status.idle":"2025-03-25T01:51:39.836833Z","shell.execute_reply.started":"2025-03-25T01:51:38.692929Z","shell.execute_reply":"2025-03-25T01:51:39.835796Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!git clone https://github.com/aras62/PIE.git\n!unzip /content/PIE/annotations/annotations.zip -d /content/PIE\n!unzip /content/PIE/annotations/annotations_vehicle.zip -d /content/PIE\n!git clone https://github.com/hustvl/YOLOP.git\n!mkdir /kaggle/working/PIE/content\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T21:48:41.749908Z","iopub.execute_input":"2025-03-25T21:48:41.750190Z","iopub.status.idle":"2025-03-25T21:48:54.604771Z","shell.execute_reply.started":"2025-03-25T21:48:41.750166Z","shell.execute_reply":"2025-03-25T21:48:54.603593Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'PIE'...\nremote: Enumerating objects: 178, done.\u001b[K\nremote: Counting objects: 100% (93/93), done.\u001b[K\nremote: Compressing objects: 100% (73/73), done.\u001b[K\nremote: Total 178 (delta 33), reused 75 (delta 18), pack-reused 85 (from 1)\u001b[K\nReceiving objects: 100% (178/178), 144.63 MiB | 37.47 MiB/s, done.\nResolving deltas: 100% (74/74), done.\nUpdating files: 100% (41/41), done.\nunzip:  cannot find or open /content/PIE/annotations/annotations.zip, /content/PIE/annotations/annotations.zip.zip or /content/PIE/annotations/annotations.zip.ZIP.\nunzip:  cannot find or open /content/PIE/annotations/annotations_vehicle.zip, /content/PIE/annotations/annotations_vehicle.zip.zip or /content/PIE/annotations/annotations_vehicle.zip.ZIP.\nCloning into 'YOLOP'...\nremote: Enumerating objects: 431, done.\u001b[K\nremote: Counting objects: 100% (426/426), done.\u001b[K\nremote: Compressing objects: 100% (231/231), done.\u001b[K\nremote: Total 431 (delta 214), reused 335 (delta 186), pack-reused 5 (from 1)\u001b[K\nReceiving objects: 100% (431/431), 140.17 MiB | 47.04 MiB/s, done.\nResolving deltas: 100% (214/214), done.\nUpdating files: 100% (95/95), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!mkdir /kaggle/working/PIE/content/set01\n!wget -r --no-parent -P/kaggle/working/PIE/content/set01 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set01/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T21:49:01.141847Z","iopub.execute_input":"2025-03-25T21:49:01.142193Z","iopub.status.idle":"2025-03-25T21:50:20.076873Z","shell.execute_reply.started":"2025-03-25T21:49:01.142162Z","shell.execute_reply":"2025-03-25T21:50:20.076039Z"}},"outputs":[{"name":"stdout","text":"--2025-03-25 21:49:01--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set01/\nResolving data.nvision2.eecs.yorku.ca (data.nvision2.eecs.yorku.ca)... 130.63.94.247\nConnecting to data.nvision2.eecs.yorku.ca (data.nvision2.eecs.yorku.ca)|130.63.94.247|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‘/kaggle/working/PIE/content/set01/index.html’\n\nindex.html              [ <=>                ]     667  --.-KB/s    in 0s      \n\n2025-03-25 21:49:01 (213 MB/s) - ‘/kaggle/working/PIE/content/set01/index.html’ saved [667]\n\nLoading robots.txt; please ignore errors.\n--2025-03-25 21:49:01--  https://data.nvision2.eecs.yorku.ca/robots.txt\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 404 Not Found\n2025-03-25 21:49:01 ERROR 404: Not Found.\n\n--2025-03-25 21:49:01--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set01/video_0001.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1200536469 (1.1G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set01/video_0001.mp4’\n\nvideo_0001.mp4      100%[===================>]   1.12G  53.1MB/s    in 22s     \n\n2025-03-25 21:49:23 (52.0 MB/s) - ‘/kaggle/working/PIE/content/set01/video_0001.mp4’ saved [1200536469/1200536469]\n\n--2025-03-25 21:49:23--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set01/video_0002.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1200434097 (1.1G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set01/video_0002.mp4’\n\nvideo_0002.mp4      100%[===================>]   1.12G  52.6MB/s    in 22s     \n\n2025-03-25 21:49:45 (52.7 MB/s) - ‘/kaggle/working/PIE/content/set01/video_0002.mp4’ saved [1200434097/1200434097]\n\n--2025-03-25 21:49:45--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set01/video_0003.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1199604441 (1.1G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set01/video_0003.mp4’\n\nvideo_0003.mp4      100%[===================>]   1.12G  51.5MB/s    in 22s     \n\n2025-03-25 21:50:07 (52.2 MB/s) - ‘/kaggle/working/PIE/content/set01/video_0003.mp4’ saved [1199604441/1199604441]\n\n--2025-03-25 21:50:07--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set01/video_0004.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 656370477 (626M) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set01/video_0004.mp4’\n\nvideo_0004.mp4      100%[===================>] 625.96M  50.7MB/s    in 12s     \n\n2025-03-25 21:50:19 (50.4 MB/s) - ‘/kaggle/working/PIE/content/set01/video_0004.mp4’ saved [656370477/656370477]\n\nFINISHED --2025-03-25 21:50:19--\nTotal wall clock time: 1m 19s\nDownloaded: 5 files, 4.0G in 1m 18s (52.0 MB/s)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!mkdir /kaggle/working/PIE/content/set02\n!wget -r --no-parent -P/kaggle/working/PIE/content/set02 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set02/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T21:51:42.933663Z","iopub.execute_input":"2025-03-25T21:51:42.933913Z","iopub.status.idle":"2025-03-25T21:52:36.716135Z","shell.execute_reply.started":"2025-03-25T21:51:42.933890Z","shell.execute_reply":"2025-03-25T21:52:36.715320Z"}},"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‘/kaggle/working/PIE/content/set02’: File exists\n--2025-03-25 21:51:43--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set02/\nResolving data.nvision2.eecs.yorku.ca (data.nvision2.eecs.yorku.ca)... 130.63.94.247\nConnecting to data.nvision2.eecs.yorku.ca (data.nvision2.eecs.yorku.ca)|130.63.94.247|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‘/kaggle/working/PIE/content/set02/index.html.1’\n\nindex.html.1            [ <=>                ]     548  --.-KB/s    in 0s      \n\n2025-03-25 21:51:43 (111 MB/s) - ‘/kaggle/working/PIE/content/set02/index.html.1’ saved [548]\n\nLoading robots.txt; please ignore errors.\n--2025-03-25 21:51:43--  https://data.nvision2.eecs.yorku.ca/robots.txt\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 404 Not Found\n2025-03-25 21:51:43 ERROR 404: Not Found.\n\n--2025-03-25 21:51:43--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set02/video_0001.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1200345018 (1.1G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set02/video_0001.mp4.1’\n\nvideo_0001.mp4.1    100%[===================>]   1.12G  53.2MB/s    in 22s     \n\n2025-03-25 21:52:05 (52.0 MB/s) - ‘/kaggle/working/PIE/content/set02/video_0001.mp4.1’ saved [1200345018/1200345018]\n\n--2025-03-25 21:52:05--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set02/video_0002.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1200625008 (1.1G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set02/video_0002.mp4.1’\n\nvideo_0002.mp4.1    100%[===================>]   1.12G  50.6MB/s    in 22s     \n\n2025-03-25 21:52:27 (51.3 MB/s) - ‘/kaggle/working/PIE/content/set02/video_0002.mp4.1’ saved [1200625008/1200625008]\n\n--2025-03-25 21:52:27--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set02/video_0003.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 465517511 (444M) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set02/video_0003.mp4.1’\n\nvideo_0003.mp4.1    100%[===================>] 443.95M  52.1MB/s    in 8.6s    \n\n2025-03-25 21:52:36 (51.6 MB/s) - ‘/kaggle/working/PIE/content/set02/video_0003.mp4.1’ saved [465517511/465517511]\n\nFINISHED --2025-03-25 21:52:36--\nTotal wall clock time: 54s\nDownloaded: 4 files, 2.7G in 53s (51.6 MB/s)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!mkdir /kaggle/working/PIE/content/set03\n!wget -r --no-parent -P/kaggle/working/PIE/content/set03 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set03/\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir /kaggle/working/PIE/content/set04\n!wget -r --no-parent -P/kaggle/working/PIE/content/set04 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set04/\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir /kaggle/working/PIE/content/set05\n!wget -r --no-parent -P/kaggle/working/PIE/content/set05 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set05/\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir /kaggle/working/PIE/content/set06\n!wget -r --no-parent -P/kaggle/working/PIE/content/set06 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T21:52:36.717350Z","iopub.execute_input":"2025-03-25T21:52:36.717707Z","iopub.status.idle":"2025-03-25T21:57:33.234861Z","shell.execute_reply.started":"2025-03-25T21:52:36.717664Z","shell.execute_reply":"2025-03-25T21:57:33.233967Z"}},"outputs":[{"name":"stdout","text":"--2025-03-25 21:52:36--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/\nResolving data.nvision2.eecs.yorku.ca (data.nvision2.eecs.yorku.ca)... 130.63.94.247\nConnecting to data.nvision2.eecs.yorku.ca (data.nvision2.eecs.yorku.ca)|130.63.94.247|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‘/kaggle/working/PIE/content/set06/index.html’\n\nindex.html              [ <=>                ]   1.23K  --.-KB/s    in 0s      \n\n2025-03-25 21:52:37 (311 MB/s) - ‘/kaggle/working/PIE/content/set06/index.html’ saved [1262]\n\nLoading robots.txt; please ignore errors.\n--2025-03-25 21:52:37--  https://data.nvision2.eecs.yorku.ca/robots.txt\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 404 Not Found\n2025-03-25 21:52:37 ERROR 404: Not Found.\n\n--2025-03-25 21:52:37--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0001.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1500150602 (1.4G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0001.mp4’\n\nvideo_0001.mp4      100%[===================>]   1.40G  35.6MB/s    in 40s     \n\n2025-03-25 21:53:17 (35.4 MB/s) - ‘/kaggle/working/PIE/content/set06/video_0001.mp4’ saved [1500150602/1500150602]\n\n--2025-03-25 21:53:17--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0002.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1500301047 (1.4G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0002.mp4’\n\nvideo_0002.mp4      100%[===================>]   1.40G  30.6MB/s    in 42s     \n\n2025-03-25 21:53:59 (34.1 MB/s) - ‘/kaggle/working/PIE/content/set06/video_0002.mp4’ saved [1500301047/1500301047]\n\n--2025-03-25 21:53:59--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0003.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1500582428 (1.4G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0003.mp4’\n\nvideo_0003.mp4      100%[===================>]   1.40G  35.9MB/s    in 40s     \n\n2025-03-25 21:54:40 (35.6 MB/s) - ‘/kaggle/working/PIE/content/set06/video_0003.mp4’ saved [1500582428/1500582428]\n\n--2025-03-25 21:54:40--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0004.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1349801835 (1.3G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0004.mp4’\n\nvideo_0004.mp4      100%[===================>]   1.26G  36.0MB/s    in 36s     \n\n2025-03-25 21:55:15 (35.9 MB/s) - ‘/kaggle/working/PIE/content/set06/video_0004.mp4’ saved [1349801835/1349801835]\n\n--2025-03-25 21:55:15--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0005.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1499881950 (1.4G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0005.mp4’\n\nvideo_0005.mp4      100%[===================>]   1.40G  36.0MB/s    in 42s     \n\n2025-03-25 21:55:58 (33.9 MB/s) - ‘/kaggle/working/PIE/content/set06/video_0005.mp4’ saved [1499881950/1499881950]\n\n--2025-03-25 21:55:58--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0006.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1500032594 (1.4G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0006.mp4’\n\nvideo_0006.mp4      100%[===================>]   1.40G  18.7MB/s    in 53s     \n\n2025-03-25 21:56:51 (26.9 MB/s) - ‘/kaggle/working/PIE/content/set06/video_0006.mp4’ saved [1500032594/1500032594]\n\n--2025-03-25 21:56:51--  https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/video_0007.mp4\nReusing existing connection to data.nvision2.eecs.yorku.ca:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1650227705 (1.5G) [video/mp4]\nSaving to: ‘/kaggle/working/PIE/content/set06/video_0007.mp4’\n\nvideo_0007.mp4       82%[===============>    ]   1.27G  35.1MB/s    in 42s     \n\n\nCannot write to ‘/kaggle/working/PIE/content/set06/video_0007.mp4’ (Success).\nFINISHED --2025-03-25 21:57:33--\nTotal wall clock time: 4m 56s\nDownloaded: 7 files, 8.2G in 4m 14s (33.3 MB/s)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport xml.etree.ElementTree as ET\nimport os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\nfrom tqdm.notebook import tqdm\nimport random\nimport math\nimport zipfile","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:10:27.867851Z","iopub.execute_input":"2025-03-25T22:10:27.868195Z","iopub.status.idle":"2025-03-25T22:10:27.872765Z","shell.execute_reply.started":"2025-03-25T22:10:27.868171Z","shell.execute_reply":"2025-03-25T22:10:27.871967Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extractall(extract_to)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:11:12.423907Z","iopub.execute_input":"2025-03-25T22:11:12.424232Z","iopub.status.idle":"2025-03-25T22:11:13.974618Z","shell.execute_reply.started":"2025-03-25T22:11:12.424203Z","shell.execute_reply":"2025-03-25T22:11:13.973715Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# --- Configuration ---\nBASE_DIR = '/kaggle/working/PIE/content'\nANNOTATION_DIR = '/kaggle/working/PIE/annotations/annotations'\n# CLIP_DIR = os.path.join(BASE_DIR, 'PIE_clips') # Not used directly in this version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:12:02.724483Z","iopub.execute_input":"2025-03-25T22:12:02.724845Z","iopub.status.idle":"2025-03-25T22:12:02.728739Z","shell.execute_reply.started":"2025-03-25T22:12:02.724819Z","shell.execute_reply":"2025-03-25T22:12:02.727748Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Model Hyperparameters\nSEQ_LEN = 15        # Number of past frames to observe\nPRED_LEN = 1        # Predict state at the end of sequence (relative to seq_len)\nINPUT_SIZE_BBOX = 4 # (center_x, center_y, width, height) - normalized\n# INPUT_SIZE_POSE = 34 # Example: 17 keypoints * 2 coords (if using pose)\n# INPUT_SIZE_FLOW = 10 # Example: Aggregated optical flow features (if using flow)\nLSTM_HIDDEN_SIZE = 128\nNUM_LSTM_LAYERS = 2\nDROPOUT_RATE = 0.3\nNUM_CLASSES = 2      # 0: not-crossing, 1: crossing\nATTENTION_DIM = 128 # Dimension for the attention mechanism","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:01:37.482266Z","iopub.execute_input":"2025-03-25T22:01:37.482620Z","iopub.status.idle":"2025-03-25T22:01:37.486781Z","shell.execute_reply.started":"2025-03-25T22:01:37.482587Z","shell.execute_reply":"2025-03-25T22:01:37.485840Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Training Hyperparameters\nLEARNING_RATE = 1e-4\nBATCH_SIZE = 64\nNUM_EPOCHS = 20 # Adjust as needed\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:01:40.365061Z","iopub.execute_input":"2025-03-25T22:01:40.365369Z","iopub.status.idle":"2025-03-25T22:01:40.420119Z","shell.execute_reply.started":"2025-03-25T22:01:40.365340Z","shell.execute_reply":"2025-03-25T22:01:40.419224Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"TRAIN_SETS = ['set06']\nVAL_SETS = ['set01', 'set02'] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:01:43.081114Z","iopub.execute_input":"2025-03-25T22:01:43.081429Z","iopub.status.idle":"2025-03-25T22:01:43.085614Z","shell.execute_reply.started":"2025-03-25T22:01:43.081404Z","shell.execute_reply":"2025-03-25T22:01:43.084618Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# --- Data Preprocessing ---\n\ndef parse_annotations(xml_file):\n    \"\"\"Parses a PIE annotation XML file.\"\"\"\n    try:\n        tree = ET.parse(xml_file)\n        root = tree.getroot()\n    except ET.ParseError:\n        print(f\"Error parsing {xml_file}\")\n        return None, None\n\n    img_width = int(root.find('.//original_size/width').text)\n    img_height = int(root.find('.//original_size/height').text)\n\n    ped_tracks = {} # {ped_id: {frame: {'bbox': [xtl,ytl,xbr,ybr], 'cross': label}}}\n\n    for track in root.findall('.//track[@label=\"pedestrian\"]'):\n        for box in track.findall('.//box'):\n            frame = int(box.get('frame'))\n            ped_id = box.find('.//attribute[@name=\"id\"]').text\n            xtl = float(box.get('xtl'))\n            ytl = float(box.get('ytl'))\n            xbr = float(box.get('xbr'))\n            ybr = float(box.get('ybr'))\n            occluded = int(box.get('occluded', 0)) # Handle missing occluded tag\n\n            # Only process non-occluded boxes for simplicity\n            if occluded > 0:\n                continue\n\n            cross_status = box.find('.//attribute[@name=\"cross\"]').text\n            # Map labels: 1 for crossing, 0 otherwise\n            cross_label = 1 if cross_status == 'crossing' else 0\n\n            if ped_id not in ped_tracks:\n                ped_tracks[ped_id] = {}\n\n            # Normalize bounding box: [center_x, center_y, width, height]\n            center_x = ((xtl + xbr) / 2) / img_width\n            center_y = ((ytl + ybr) / 2) / img_height\n            width = (xbr - xtl) / img_width\n            height = (ybr - ytl) / img_height\n\n            # Basic check for valid bbox dimensions\n            if width <= 0 or height <= 0 or not (0 <= center_x <= 1) or not (0 <= center_y <= 1):\n                continue # Skip invalid boxes\n\n            ped_tracks[ped_id][frame] = {\n                'bbox': [center_x, center_y, width, height],\n                'cross': cross_label\n            }\n\n    return ped_tracks, (img_width, img_height)\n\ndef create_sequences(ped_tracks, seq_len, pred_len):\n    \"\"\"Creates sequences of features and labels from pedestrian tracks.\"\"\"\n    sequences = []\n    for ped_id, frames_data in ped_tracks.items():\n        sorted_frames = sorted(frames_data.keys())\n\n        for i in range(len(sorted_frames) - seq_len - pred_len + 1):\n            seq_frames = sorted_frames[i : i + seq_len]\n            target_frame = sorted_frames[i + seq_len + pred_len - 1]\n\n            # Ensure frames are consecutive (or reasonably close)\n            # Allow for small gaps (e.g., 1-2 frames) if needed, but strict continuity is safer\n            is_continuous = all(seq_frames[j+1] - seq_frames[j] == 1 for j in range(len(seq_frames)-1))\n            is_target_continuous = (target_frame - seq_frames[-1] == pred_len)\n\n            if not (is_continuous and is_target_continuous):\n                continue\n\n            # Extract features (only bbox for now)\n            bbox_seq = [frames_data[f]['bbox'] for f in seq_frames]\n            # pose_seq = ... # Placeholder: Extract pose sequence if available\n            # flow_seq = ... # Placeholder: Extract flow sequence if available\n\n            # Get target label\n            label = frames_data[target_frame]['cross']\n\n            # Append sequence and label\n            # In a multi-stream setup, you'd store dict like {'bbox': bbox_seq, 'pose': pose_seq, ...}\n            sequences.append({'bbox': np.array(bbox_seq, dtype=np.float32), 'label': label})\n\n    return sequences\n\n\nclass PIEDataset(Dataset):\n    def __init__(self, annotation_dir, set_folders, seq_len, pred_len):\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.sequences = []\n\n        print(f\"Loading data from sets: {set_folders}\")\n        for set_folder in tqdm(set_folders):\n            set_path = os.path.join(annotation_dir, set_folder)\n            if not os.path.isdir(set_path):\n                print(f\"Warning: Annotation directory not found for {set_folder}\")\n                continue\n\n            xml_files = [f for f in os.listdir(set_path) if f.endswith('.xml')]\n            for xml_file in tqdm(xml_files, desc=f\"Processing {set_folder}\", leave=False):\n                file_path = os.path.join(set_path, xml_file)\n                ped_tracks, _ = parse_annotations(file_path)\n                if ped_tracks:\n                    video_sequences = create_sequences(ped_tracks, seq_len, pred_len)\n                    self.sequences.extend(video_sequences)\n\n        print(f\"Loaded {len(self.sequences)} sequences.\")\n        # Basic balancing (undersample majority class if highly imbalanced)\n        # More sophisticated balancing might be needed\n        labels = [s['label'] for s in self.sequences]\n        count_0 = labels.count(0)\n        count_1 = labels.count(1)\n        print(f\"Class distribution: 0={count_0}, 1={count_1}\")\n\n        # Example Undersampling (adjust ratio as needed)\n        # if count_0 > count_1 * 2: # If non-crossing is more than double crossing\n        #     print(\"Performing undersampling of class 0...\")\n        #     indices_0 = [i for i, label in enumerate(labels) if label == 0]\n        #     indices_1 = [i for i, label in enumerate(labels) if label == 1]\n        #     indices_0_sampled = random.sample(indices_0, count_1 * 2) # Keep double non-crossing\n        #     sampled_indices = sorted(indices_1 + indices_0_sampled)\n        #     self.sequences = [self.sequences[i] for i in sampled_indices]\n        #     print(f\"Reduced sequences to {len(self.sequences)}\")\n\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        sequence_data = self.sequences[idx]\n\n        # --- Modality Extraction ---\n        # Currently only BBox\n        bbox_features = torch.tensor(sequence_data['bbox'], dtype=torch.float32)\n\n        # Placeholder for other modalities\n        # pose_features = torch.tensor(...)\n        # flow_features = torch.tensor(...)\n\n        label = torch.tensor(sequence_data['label'], dtype=torch.long) # Use long for CrossEntropyLoss\n\n        # Return as a dictionary for clarity, especially with multiple streams\n        features = {\n            'bbox': bbox_features\n            # 'pose': pose_features,\n            # 'flow': flow_features\n        }\n        return features, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:01:45.490840Z","iopub.execute_input":"2025-03-25T22:01:45.491154Z","iopub.status.idle":"2025-03-25T22:01:45.504862Z","shell.execute_reply.started":"2025-03-25T22:01:45.491126Z","shell.execute_reply":"2025-03-25T22:01:45.503989Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# --- Model Architecture ---\n\nclass Attention(nn.Module):\n    \"\"\" Simple Dot-Product Attention or Learned Attention\"\"\"\n    def __init__(self, hidden_dim, attention_dim):\n        super(Attention, self).__init__()\n        # Option 1: Simple Linear Attention\n        self.attention_net = nn.Sequential(\n            nn.Linear(hidden_dim, attention_dim),\n            nn.Tanh(),\n            nn.Linear(attention_dim, 1)\n        )\n        # Option 2: Dot Product (if query is available, e.g., final hidden state)\n        # Or Multi-Head Attention (more complex)\n\n    def forward(self, lstm_output):\n        # lstm_output shape: (batch_size, seq_len, hidden_dim)\n        # Calculate attention scores\n        attention_scores = self.attention_net(lstm_output).squeeze(2) # (batch_size, seq_len)\n        # Normalize scores to get weights\n        attention_weights = torch.softmax(attention_scores, dim=1) # (batch_size, seq_len)\n        # Weighted sum of LSTM outputs\n        # unsqueeze to allow broadcasting: (batch_size, seq_len, 1)\n        context_vector = torch.sum(lstm_output * attention_weights.unsqueeze(2), dim=1) # (batch_size, hidden_dim)\n        return context_vector, attention_weights\n\n\nclass MultiStreamAdaptiveLSTM(nn.Module):\n    def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes,\n                 attention_dim, dropout_rate, stream_names=['bbox']): # Add 'pose', 'flow' etc. later\n        super(MultiStreamAdaptiveLSTM, self).__init__()\n        self.stream_names = stream_names\n        self.lstms = nn.ModuleDict()\n        self.attentions = nn.ModuleDict() # Attention per stream (temporal attention)\n\n        # --- Create LSTM Streams ---\n        for name in self.stream_names:\n            input_size = input_sizes[name]\n            self.lstms[name] = nn.LSTM(input_size, lstm_hidden_size, num_lstm_layers,\n                                       batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0,\n                                       bidirectional=False) # Consider bidirectional=True\n            # Temporal attention for each stream's output sequence\n            self.attentions[name] = Attention(lstm_hidden_size, attention_dim) # Assuming non-bidirectional LSTM\n\n        # --- Fusion Mechanism ---\n        # For now, simple concatenation or weighted sum after temporal attention.\n        # The \"Adaptive Fusion via Attention\" from the prompt would operate *across*\n        # the outputs of the stream-specific attentions (the context_vectors).\n        num_streams = len(self.stream_names)\n        combined_feature_dim = lstm_hidden_size * num_streams # If concatenating context vectors\n\n        # Placeholder for cross-stream attention mechanism\n        # self.fusion_attention = nn.Linear(combined_feature_dim, num_streams) # Example: Learns weights per stream\n\n        # --- Prediction Head ---\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc1 = nn.Linear(combined_feature_dim, combined_feature_dim // 2)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(combined_feature_dim // 2, num_classes)\n\n    def forward(self, x):\n        # x is a dictionary: {'bbox': tensor, 'pose': tensor, ...}\n        stream_outputs = {}\n        stream_context_vectors = []\n        stream_att_weights = {} # Store attention weights for inspection if needed\n\n        # --- Process each stream ---\n        for name in self.stream_names:\n            lstm_input = x[name] # Shape: (batch_size, seq_len, input_size)\n            lstm_out, _ = self.lstms[name](lstm_input) # lstm_out shape: (batch_size, seq_len, hidden_size)\n            # Apply temporal attention to each stream's output sequence\n            context_vector, attention_weights = self.attentions[name](lstm_out) # context_vector shape: (batch_size, hidden_size)\n            stream_context_vectors.append(context_vector)\n            stream_att_weights[name] = attention_weights # Store temporal attention weights\n\n        # --- Fusion ---\n        # Option 1: Simple Concatenation (as implemented here)\n        fused_features = torch.cat(stream_context_vectors, dim=1) # Shape: (batch_size, hidden_size * num_streams)\n\n        # Option 2: Adaptive Attention Fusion (More complex, requires defining attention mechanism over context_vectors)\n        # Example idea:\n        # combined_context = torch.stack(stream_context_vectors, dim=1) # (batch_size, num_streams, hidden_size)\n        # fusion_scores = self.fusion_attention(fused_features) # Or another attention mechanism on combined_context\n        # fusion_weights = torch.softmax(fusion_scores, dim=1).unsqueeze(2) # (batch_size, num_streams, 1)\n        # fused_features = torch.sum(combined_context * fusion_weights, dim=1) # (batch_size, hidden_size)\n\n        # --- Prediction ---\n        x = self.dropout(fused_features)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        logits = self.fc2(x) # Output logits\n\n        return logits # Return logits for CrossEntropyLoss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:01:51.228836Z","iopub.execute_input":"2025-03-25T22:01:51.229116Z","iopub.status.idle":"2025-03-25T22:01:51.238246Z","shell.execute_reply.started":"2025-03-25T22:01:51.229094Z","shell.execute_reply":"2025-03-25T22:01:51.237292Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# --- Training and Evaluation Functions ---\n\ndef train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n\n    for features, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n        # Move data to device\n        input_features = {name: features[name].to(device) for name in model.stream_names}\n        labels = labels.to(device)\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(input_features)\n\n        # Calculate loss\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        # Store predictions and labels for epoch metrics (optional during training)\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    avg_loss = total_loss / len(dataloader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    return avg_loss, accuracy\n\ndef evaluate_epoch(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_labels = []\n    all_preds = []\n    all_probs = [] # For AUC\n\n    with torch.no_grad():\n        for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n            # Move data to device\n            input_features = {name: features[name].to(device) for name in model.stream_names}\n            labels = labels.to(device)\n\n            # Forward pass\n            outputs = model(input_features) # Logits\n\n            # Calculate loss\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n\n            # Calculate probabilities and predictions\n            probs = torch.softmax(outputs, dim=1)\n            preds = torch.argmax(probs, dim=1)\n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy()) # Store probabilities\n\n    avg_loss = total_loss / len(dataloader)\n    all_probs = np.array(all_probs)\n    all_labels = np.array(all_labels)\n    all_preds = np.array(all_preds)\n\n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', pos_label=1, zero_division=0\n    )\n\n    # Calculate AUC using probabilities of the positive class (class 1)\n    if len(np.unique(all_labels)) > 1: # AUC requires both classes present\n       auc = roc_auc_score(all_labels, all_probs[:, 1]) # Probs for class 1\n    else:\n       auc = float('nan') # Not defined if only one class is present\n\n    metrics = {\n        'loss': avg_loss,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'auc': auc\n    }\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:01:55.778831Z","iopub.execute_input":"2025-03-25T22:01:55.779120Z","iopub.status.idle":"2025-03-25T22:01:55.788711Z","shell.execute_reply.started":"2025-03-25T22:01:55.779096Z","shell.execute_reply":"2025-03-25T22:01:55.787853Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# --- Main Execution ---\n\nif __name__ == '__main__':\n    # --- Initialize Datasets and Dataloaders ---\n    train_dataset = PIEDataset(ANNOTATION_DIR, TRAIN_SETS, SEQ_LEN, PRED_LEN)\n    val_dataset = PIEDataset(ANNOTATION_DIR, VAL_SETS, SEQ_LEN, PRED_LEN)\n\n    # Handle potential empty datasets\n    if not train_dataset or not val_dataset or len(train_dataset) == 0 or len(val_dataset) == 0:\n         raise ValueError(\"Dataset loading failed or resulted in empty datasets. Check paths and parsing.\")\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n    # --- Initialize Model, Loss, Optimizer ---\n    input_sizes = {'bbox': INPUT_SIZE_BBOX} # Add other modalities here if implemented\n    model = MultiStreamAdaptiveLSTM(\n        input_sizes=input_sizes,\n        lstm_hidden_size=LSTM_HIDDEN_SIZE,\n        num_lstm_layers=NUM_LSTM_LAYERS,\n        num_classes=NUM_CLASSES,\n        attention_dim=ATTENTION_DIM,\n        dropout_rate=DROPOUT_RATE,\n        stream_names=['bbox'] # Update if using more streams\n    ).to(DEVICE)\n\n    print(model)\n    # Calculate class weights for imbalanced data (optional but recommended)\n    # labels = np.array([s['label'] for s in train_dataset.sequences])\n    # class_counts = np.bincount(labels)\n    # class_weights = torch.tensor([len(labels) / c if c > 0 else 0 for c in class_counts], dtype=torch.float).to(DEVICE)\n    # print(f\"Using class weights: {class_weights}\")\n    # criterion = nn.CrossEntropyLoss(weight=class_weights)\n    criterion = nn.CrossEntropyLoss() # Standard loss\n\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5) # Optional LR scheduler\n\n    # --- Training Loop ---\n    best_val_f1 = -1.0\n    print(\"\\n--- Starting Training ---\")\n    for epoch in range(NUM_EPOCHS):\n        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n        val_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n        # if scheduler: scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}:\")\n        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        print(f\"  Val Loss:   {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n        print(f\"  Val Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n        print(f\"  Val AUC: {val_metrics['auc']:.4f}\")\n\n        # Save best model based on validation F1-score\n        if val_metrics['f1'] > best_val_f1:\n            best_val_f1 = val_metrics['f1']\n            torch.save(model.state_dict(), 'best_model.pth')\n            print(f\"  Saved new best model with F1: {best_val_f1:.4f}\")\n        print(\"-\" * 20)\n\n    print(\"--- Training Finished ---\")\n\n    # --- Final Evaluation on Validation Set (Set 6) ---\n    print(\"\\n--- Final Evaluation on Set 6 (Validation Set) using Best Model ---\")\n    # Load best model\n    model.load_state_dict(torch.load('best_model.pth', map_location=DEVICE))\n    final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n\n    print(\"Final Performance Metrics:\")\n    print(f\"  Accuracy:  {final_metrics['accuracy']:.4f}\")\n    print(f\"  Precision: {final_metrics['precision']:.4f}\")\n    print(f\"  Recall:    {final_metrics['recall']:.4f}\")\n    print(f\"  F1 Score:  {final_metrics['f1']:.4f}\")\n    print(f\"  AUC:       {final_metrics['auc']:.4f}\")\n    print(f\"  Loss:      {final_metrics['loss']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:12:10.041764Z","iopub.execute_input":"2025-03-25T22:12:10.042063Z","iopub.status.idle":"2025-03-25T22:14:36.950001Z","shell.execute_reply.started":"2025-03-25T22:12:10.042040Z","shell.execute_reply":"2025-03-25T22:14:36.948881Z"}},"outputs":[{"name":"stdout","text":"Loading data from sets: ['set06']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e47d74779d048a1abdf49355f0e024f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing set06:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Loaded 46323 sequences.\nClass distribution: 0=37978, 1=8345\nLoading data from sets: ['set01', 'set02']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3b47eb1b2c64dc4bfa217de16c3ce58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing set01:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing set02:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Loaded 64096 sequences.\nClass distribution: 0=49937, 1=14159\nMultiStreamAdaptiveLSTM(\n  (lstms): ModuleDict(\n    (bbox): LSTM(4, 128, num_layers=2, batch_first=True, dropout=0.3)\n  )\n  (attentions): ModuleDict(\n    (bbox): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=128, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc1): Linear(in_features=128, out_features=64, bias=True)\n  (relu): ReLU()\n  (fc2): Linear(in_features=64, out_features=2, bias=True)\n)\n\n--- Starting Training ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1/20:\n  Train Loss: 0.4273, Train Acc: 0.7971\n  Val Loss:   0.4452, Val Acc: 0.8000\n  Val Precision: 0.6000, Recall: 0.2838, F1: 0.3853\n  Val AUC: 0.7796\n  Saved new best model with F1: 0.3853\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2/20:\n  Train Loss: 0.3332, Train Acc: 0.8760\n  Val Loss:   0.4547, Val Acc: 0.7985\n  Val Precision: 0.5543, Recall: 0.4485, F1: 0.4958\n  Val AUC: 0.7796\n  Saved new best model with F1: 0.4958\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3/20:\n  Train Loss: 0.3093, Train Acc: 0.8813\n  Val Loss:   0.4580, Val Acc: 0.8120\n  Val Precision: 0.6110, Recall: 0.4096, F1: 0.4904\n  Val AUC: 0.7864\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4/20:\n  Train Loss: 0.2955, Train Acc: 0.8831\n  Val Loss:   0.4845, Val Acc: 0.8109\n  Val Precision: 0.6028, Recall: 0.4221, F1: 0.4965\n  Val AUC: 0.7899\n  Saved new best model with F1: 0.4965\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5/20:\n  Train Loss: 0.2912, Train Acc: 0.8831\n  Val Loss:   0.4967, Val Acc: 0.7965\n  Val Precision: 0.5448, Recall: 0.4788, F1: 0.5097\n  Val AUC: 0.7907\n  Saved new best model with F1: 0.5097\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6/20:\n  Train Loss: 0.2877, Train Acc: 0.8846\n  Val Loss:   0.5192, Val Acc: 0.7783\n  Val Precision: 0.4984, Recall: 0.5159, F1: 0.5070\n  Val AUC: 0.7919\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7/20:\n  Train Loss: 0.2852, Train Acc: 0.8851\n  Val Loss:   0.5518, Val Acc: 0.7769\n  Val Precision: 0.4955, Recall: 0.5433, F1: 0.5183\n  Val AUC: 0.7916\n  Saved new best model with F1: 0.5183\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8/20:\n  Train Loss: 0.2824, Train Acc: 0.8861\n  Val Loss:   0.5033, Val Acc: 0.7906\n  Val Precision: 0.5270, Recall: 0.5064, F1: 0.5165\n  Val AUC: 0.7980\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9/20:\n  Train Loss: 0.2783, Train Acc: 0.8859\n  Val Loss:   0.5502, Val Acc: 0.7791\n  Val Precision: 0.5000, Recall: 0.5678, F1: 0.5317\n  Val AUC: 0.7956\n  Saved new best model with F1: 0.5317\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10/20:\n  Train Loss: 0.2743, Train Acc: 0.8871\n  Val Loss:   0.5473, Val Acc: 0.7876\n  Val Precision: 0.5176, Recall: 0.5699, F1: 0.5425\n  Val AUC: 0.8023\n  Saved new best model with F1: 0.5425\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11/20:\n  Train Loss: 0.2711, Train Acc: 0.8871\n  Val Loss:   0.5078, Val Acc: 0.7930\n  Val Precision: 0.5312, Recall: 0.5356, F1: 0.5334\n  Val AUC: 0.8059\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12/20:\n  Train Loss: 0.2686, Train Acc: 0.8882\n  Val Loss:   0.6090, Val Acc: 0.7907\n  Val Precision: 0.5224, Recall: 0.6152, F1: 0.5650\n  Val AUC: 0.8078\n  Saved new best model with F1: 0.5650\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13/20:\n  Train Loss: 0.2647, Train Acc: 0.8883\n  Val Loss:   0.5367, Val Acc: 0.7968\n  Val Precision: 0.5385, Recall: 0.5619, F1: 0.5499\n  Val AUC: 0.8019\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 14/20:\n  Train Loss: 0.2629, Train Acc: 0.8886\n  Val Loss:   0.4895, Val Acc: 0.8171\n  Val Precision: 0.6051, Recall: 0.4948, F1: 0.5444\n  Val AUC: 0.8071\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 15/20:\n  Train Loss: 0.2622, Train Acc: 0.8887\n  Val Loss:   0.5955, Val Acc: 0.8040\n  Val Precision: 0.5565, Recall: 0.5551, F1: 0.5558\n  Val AUC: 0.7986\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 16/20:\n  Train Loss: 0.2602, Train Acc: 0.8890\n  Val Loss:   0.6451, Val Acc: 0.8037\n  Val Precision: 0.5552, Recall: 0.5608, F1: 0.5580\n  Val AUC: 0.7988\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 17/20:\n  Train Loss: 0.2583, Train Acc: 0.8886\n  Val Loss:   0.6282, Val Acc: 0.8066\n  Val Precision: 0.5635, Recall: 0.5522, F1: 0.5578\n  Val AUC: 0.7971\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 18/20:\n  Train Loss: 0.2557, Train Acc: 0.8887\n  Val Loss:   0.5783, Val Acc: 0.8156\n  Val Precision: 0.5891, Recall: 0.5462, F1: 0.5668\n  Val AUC: 0.8074\n  Saved new best model with F1: 0.5668\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 19/20:\n  Train Loss: 0.2532, Train Acc: 0.8885\n  Val Loss:   0.6383, Val Acc: 0.8031\n  Val Precision: 0.5516, Recall: 0.5822, F1: 0.5665\n  Val AUC: 0.7956\n--------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 20/20:\n  Train Loss: 0.2495, Train Acc: 0.8878\n  Val Loss:   0.5901, Val Acc: 0.8023\n  Val Precision: 0.5509, Recall: 0.5688, F1: 0.5597\n  Val AUC: 0.8022\n--------------------\n--- Training Finished ---\n\n--- Final Evaluation on Set 6 (Validation Set) using Best Model ---\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-27-56c466e73d67>:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth', map_location=DEVICE))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Final Performance Metrics:\n  Accuracy:  0.8156\n  Precision: 0.5891\n  Recall:    0.5462\n  F1 Score:  0.5668\n  AUC:       0.8074\n  Loss:      0.5783\n","output_type":"stream"}],"execution_count":27}]}