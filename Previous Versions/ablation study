{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11201333,"sourceType":"datasetVersion","datasetId":6993690},{"sourceId":11201362,"sourceType":"datasetVersion","datasetId":6993708},{"sourceId":11201388,"sourceType":"datasetVersion","datasetId":6993722},{"sourceId":11201422,"sourceType":"datasetVersion","datasetId":6993740},{"sourceId":11201506,"sourceType":"datasetVersion","datasetId":6993794},{"sourceId":11201543,"sourceType":"datasetVersion","datasetId":6993809},{"sourceId":11382982,"sourceType":"datasetVersion","datasetId":7127490},{"sourceId":11402679,"sourceType":"datasetVersion","datasetId":7142036},{"sourceId":302300,"sourceType":"modelInstanceVersion","modelInstanceId":258142,"modelId":279383},{"sourceId":307831,"sourceType":"modelInstanceVersion","modelInstanceId":262207,"modelId":283333},{"sourceId":316944,"sourceType":"modelInstanceVersion","modelInstanceId":267476,"modelId":288527},{"sourceId":329886,"sourceType":"modelInstanceVersion","modelInstanceId":276781,"modelId":297682},{"sourceId":329908,"sourceType":"modelInstanceVersion","modelInstanceId":276800,"modelId":297702}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/aras62/PIE.git\n!unzip /content/PIE/annotations/annotations.zip -d /content/PIE\n!unzip /content/PIE/annotations/annotations_vehicle.zip -d /content/PIE\n# !git clone https://github.com/hustvl/YOLOP.git\n!mkdir /kaggle/working/PIE/content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:43:56.024208Z","iopub.execute_input":"2025-04-14T12:43:56.024476Z","iopub.status.idle":"2025-04-14T12:43:56.508330Z","shell.execute_reply.started":"2025-04-14T12:43:56.024445Z","shell.execute_reply":"2025-04-14T12:43:56.507088Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'PIE' already exists and is not an empty directory.\nunzip:  cannot find or open /content/PIE/annotations/annotations.zip, /content/PIE/annotations/annotations.zip.zip or /content/PIE/annotations/annotations.zip.ZIP.\nunzip:  cannot find or open /content/PIE/annotations/annotations_vehicle.zip, /content/PIE/annotations/annotations_vehicle.zip.zip or /content/PIE/annotations/annotations_vehicle.zip.ZIP.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q ultralytics opencv-python-headless # ultralytics includes necessary dependencies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:51:33.496183Z","iopub.execute_input":"2025-04-14T23:51:33.496466Z","iopub.status.idle":"2025-04-14T23:51:39.203272Z","shell.execute_reply.started":"2025-04-14T23:51:33.496445Z","shell.execute_reply":"2025-04-14T23:51:39.202064Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.8/974.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# # Standalone script to generate the PIE database cache (pie_database.pkl)\n# # Run this script ONCE before running the main training script.\n\n# import os\n# import pickle\n# import sys\n# import xml.etree.ElementTree as ET\n# import numpy as np\n# from os.path import join, abspath, isfile, isdir\n# from os import makedirs, listdir\n# from tqdm.notebook import tqdm # Use notebook version for progress bars\n\n# print(\"--- PIE Database Cache Generation Script ---\")\n\n# # --- Configuration ---\n# # <<< --- SET THIS TO THE ROOT OF YOUR PIE DATASET --- >>>\n# PIE_ROOT_PATH = '/kaggle/working/PIE' # Path where PIE repo was cloned/unzipped\n# # <<< --- END CONFIGURATION --- >>>\n\n# # Verify Root Path\n# if not os.path.isdir(PIE_ROOT_PATH):\n#     print(f\"ERROR: PIE_ROOT_PATH '{PIE_ROOT_PATH}' not found or not a directory.\")\n#     print(\"Please ensure the path is correct.\")\n#     exit()\n\n\n# # --- Copy of the PIE Class with necessary methods and the fix ---\n# # (Copied directly from pie_data.py and modified)\n# class PIE_Generator(object): # Renamed slightly to avoid conflicts if imported later\n#     def __init__(self, regen_database=False, data_path=''):\n#         \"\"\"\n#         Class constructor\n#         :param regen_database: Whether generate the database or not\n#         :param data_path: The path to the dataset root folder\n#         \"\"\"\n#         self._name = 'pie_generator'\n#         self._regen_database = regen_database\n\n#         # Paths\n#         self._pie_path = data_path if data_path else self._get_default_path()\n#         if not isdir(self._pie_path):\n#              raise NotADirectoryError(f'PIE path does not exist: {self._pie_path}')\n\n#         # Define paths relative to the root path\n#         self._annotation_path = join(self._pie_path, 'annotations', 'annotations')\n#         self._annotation_attributes_path = join(self._pie_path, 'annotations', 'annotations_attributes')\n#         self._annotation_vehicle_path = join(self._pie_path, 'annotations', 'annotations_vehicle')\n#         self._cache_path_internal = join(self._pie_path, 'data_cache') # Internal variable for cache dir\n\n#         # Ensure necessary annotation directories exist\n#         if not isdir(self._annotation_path):\n#              raise NotADirectoryError(f'Annotation path does not exist: {self._annotation_path}')\n#         # Attributes and Vehicle paths might not exist if only base annotations are used, add checks later if needed\n\n#     # Path generators\n#     @property\n#     def cache_path(self):\n#         \"\"\"\n#         Generates a path to save cache files\n#         :return: Cache file folder path\n#         \"\"\"\n#         if not isdir(self._cache_path_internal):\n#             try:\n#                 makedirs(self._cache_path_internal)\n#                 print(f\"Created cache directory: {self._cache_path_internal}\")\n#             except OSError as e:\n#                 print(f\"Error creating cache directory {self._cache_path_internal}: {e}\")\n#                 raise\n#         return self._cache_path_internal\n\n#     def _get_default_path(self):\n#         \"\"\" Returns the default path where pie is expected to be installed. \"\"\"\n#         # This is unlikely to be used if data_path is provided, but included for completeness\n#         return '.' # Default to current directory if no path given\n\n#     # --- Helper methods needed by generate_database ---\n#     def _map_text_to_scalar(self, label_type, value):\n#         \"\"\" Maps a text label in XML file to scalars \"\"\"\n#         map_dic = {'occlusion': {'none': 0, 'part': 1, 'full': 2},\n#                    'action': {'standing': 0, 'walking': 1},\n#                    'look': {'not-looking': 0, 'looking': 1},\n#                    'gesture': {'__undefined__': 0, 'hand_ack': 1, 'hand_yield': 2,\n#                                'hand_rightofway': 3, 'nod': 4, 'other': 5},\n#                    'cross': {'not-crossing': 0, 'crossing': 1, 'crossing-irrelevant': -1},\n#                    'crossing': {'not-crossing': 0, 'crossing': 1, 'irrelevant': -1},\n#                    'age': {'child': 0, 'young': 1, 'adult': 2, 'senior': 3},\n#                    'designated': {'ND': 0, 'D': 1},\n#                    'gender': {'n/a': 0, 'female': 1, 'male': 2},\n#                    'intersection': {'midblock': 0, 'T': 1, 'T-left': 2, 'T-right': 3, 'four-way': 4},\n#                    'motion_direction': {'n/a': 0, 'LAT': 1, 'LONG': 2},\n#                    'traffic_direction': {'OW': 0, 'TW': 1},\n#                    'signalized': {'n/a': 0, 'C': 1, 'S': 2, 'CS': 3},\n#                    'vehicle': {'car': 0, 'truck': 1, 'bus': 2, 'train': 3, 'bicycle': 4, 'bike': 5},\n#                    'sign': {'ped_blue': 0, 'ped_yellow': 1, 'ped_white': 2, 'ped_text': 3, 'stop_sign': 4,\n#                             'bus_stop': 5, 'train_stop': 6, 'construction': 7, 'other': 8},\n#                    'traffic_light': {'regular': 0, 'transit': 1, 'pedestrian': 2},\n#                    'state': {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}}\n#         try:\n#             return map_dic[label_type][value]\n#         except KeyError:\n#              print(f\"Warning: Unknown value '{value}' for label type '{label_type}'. Returning default/0.\")\n#              # Return a default value, maybe 0 or handle based on context\n#              if label_type == 'cross': return -1 # Specific default for cross?\n#              return 0 # General default\n\n#     def _get_annotations(self, setid, vid):\n#         \"\"\" Generates a dictionary of annotations by parsing the video XML file \"\"\"\n#         path_to_file = join(self._annotation_path, setid, vid + '_annt.xml')\n#         # print(f\"Parsing annotations: {path_to_file}\") # Optional debug\n#         try:\n#             tree = ET.parse(path_to_file)\n#         except (ET.ParseError, FileNotFoundError) as e:\n#             print(f\"Error parsing annotation file {path_to_file}: {e}\")\n#             return None # Return None on error\n\n#         ped_annt = 'ped_annotations'\n#         traffic_annt = 'traffic_annotations'\n\n#         annotations = {}\n#         try:\n#             annotations['num_frames'] = int(tree.find(\"./meta/task/size\").text)\n#             annotations['width'] = int(tree.find(\"./meta/task/original_size/width\").text)\n#             annotations['height'] = int(tree.find(\"./meta/task/original_size/height\").text)\n#         except AttributeError:\n#              print(f\"Warning: Missing meta information (num_frames/size) in {path_to_file}\")\n#              # Decide how critical this is. Maybe return None or set defaults?\n#              annotations['num_frames'] = -1\n#              annotations['width'] = -1\n#              annotations['height'] = -1\n\n#         annotations[ped_annt] = {}\n#         annotations[traffic_annt] = {}\n#         tracks = tree.findall('./track')\n#         for t in tracks:\n#             boxes = t.findall('./box')\n#             if not boxes: continue # Skip tracks with no boxes\n#             obj_label = t.get('label')\n#             obj_id_elem = boxes[0].find('./attribute[@name=\\\"id\\\"]') # Try finding ID in first box\n#             obj_id = obj_id_elem.text if obj_id_elem is not None else None\n#             # Fallback: Check track attributes if not in box (less common in PIE XML structure)\n#             if obj_id is None:\n#                 track_id_elem = t.find('./attribute[@name=\"id\"]')\n#                 if track_id_elem is not None: obj_id = track_id_elem.text\n\n#             if obj_id is None:\n#                  # print(f\"Warning: Cannot find object ID for a track with label '{obj_label}' in {vid}_annt.xml. Skipping track.\")\n#                  continue # Skip if no ID found\n\n#             if obj_label == 'pedestrian':\n#                 annotations[ped_annt][obj_id] = {'frames': [], 'bbox': [], 'occlusion': []}\n#                 annotations[ped_annt][obj_id]['behavior'] = {'gesture': [], 'look': [], 'action': [], 'cross': []}\n#                 for b in boxes:\n#                     if int(b.get('outside', 0)) == 1: continue # Use default 0 if 'outside' missing\n#                     try:\n#                          annotations[ped_annt][obj_id]['bbox'].append([float(b.get('xtl')), float(b.get('ytl')), float(b.get('xbr')), float(b.get('ybr'))])\n#                          occ_elem = b.find('./attribute[@name=\"occlusion\"]')\n#                          occ_text = occ_elem.text if occ_elem is not None else 'none' # Default to none if missing\n#                          occ = self._map_text_to_scalar('occlusion', occ_text)\n#                          annotations[ped_annt][obj_id]['occlusion'].append(occ)\n#                          annotations[ped_annt][obj_id]['frames'].append(int(b.get('frame')))\n#                          for beh in annotations['ped_annotations'][obj_id]['behavior']:\n#                              beh_elem = b.find(f'./attribute[@name=\"{beh}\"]')\n#                              beh_text = beh_elem.text if beh_elem is not None else '__undefined__' # Handle missing behavior tags\n#                              annotations[ped_annt][obj_id]['behavior'][beh].append(self._map_text_to_scalar(beh, beh_text))\n#                     except (TypeError, ValueError, AttributeError) as e:\n#                          print(f\"Warning: Error processing box attributes for ped {obj_id} in frame {b.get('frame')} of {vid}: {e}\")\n#                          continue\n\n#             else: # Traffic objects\n#                 obj_type_elem = boxes[0].find('./attribute[@name=\"type\"]')\n#                 obj_type_text = obj_type_elem.text if obj_type_elem is not None else None\n#                 obj_type = self._map_text_to_scalar(obj_label, obj_type_text) if obj_type_text else None\n\n#                 annotations[traffic_annt][obj_id] = {'frames': [], 'bbox': [], 'occlusion': [],\n#                                                      'obj_class': obj_label,\n#                                                      'obj_type': obj_type,\n#                                                      'state': []}\n#                 for b in boxes:\n#                     if int(b.get('outside', 0)) == 1: continue\n#                     try:\n#                          annotations[traffic_annt][obj_id]['bbox'].append([float(b.get('xtl')), float(b.get('ytl')), float(b.get('xbr')), float(b.get('ybr'))])\n#                          annotations[traffic_annt][obj_id]['occlusion'].append(int(b.get('occluded', 0))) # Default 0 if missing\n#                          annotations[traffic_annt][obj_id]['frames'].append(int(b.get('frame')))\n#                          if obj_label == 'traffic_light':\n#                              state_elem = b.find('./attribute[@name=\"state\"]')\n#                              state_text = state_elem.text if state_elem is not None else '__undefined__'\n#                              annotations[traffic_annt][obj_id]['state'].append(self._map_text_to_scalar('state', state_text))\n#                     except (TypeError, ValueError, AttributeError) as e:\n#                          print(f\"Warning: Error processing box attributes for obj {obj_id} in frame {b.get('frame')} of {vid}: {e}\")\n#                          continue\n#         return annotations\n\n#     def _get_ped_attributes(self, setid, vid):\n#         \"\"\" Generates a dictionary of attributes by parsing the video XML file \"\"\"\n#         path_to_file = join(self._annotation_attributes_path, setid, vid + '_attributes.xml')\n#         attributes = {}\n#         try:\n#             tree = ET.parse(path_to_file)\n#             pedestrians = tree.findall(\"./pedestrian\")\n#             for p in pedestrians:\n#                 ped_id = p.get('id')\n#                 if not ped_id: continue # Skip if pedestrian tag has no id\n#                 attributes[ped_id] = {}\n#                 for k, v in p.items():\n#                     if 'id' in k: continue\n#                     try:\n#                         if k == 'intention_prob': attributes[ped_id][k] = float(v)\n#                         else: attributes[ped_id][k] = int(v)\n#                     except ValueError:\n#                         attributes[ped_id][k] = self._map_text_to_scalar(k, v)\n#         except (ET.ParseError, FileNotFoundError) as e:\n#             print(f\"Warning: Could not parse pedestrian attributes {path_to_file}: {e}. Attributes will be missing.\")\n#         return attributes\n\n#     def _get_vehicle_attributes(self, setid, vid):\n#         \"\"\" Generates a dictionary of vehicle attributes (OBD) by parsing the video XML file \"\"\"\n#         path_to_file = join(self._annotation_vehicle_path, setid, vid + '_obd.xml') # Assumes name is video_id.xml\n#         veh_attributes = {}\n#         try:\n#             tree = ET.parse(path_to_file)\n#             frames = tree.findall(\"./frame\")\n#             for f in frames:\n#                 try:\n#                      frame_id = int(f.get('id'))\n#                      # Attempt to convert all attributes to float, handle errors\n#                      dict_vals = {}\n#                      for k, v in f.attrib.items():\n#                          if k != 'id':\n#                              try: dict_vals[k] = float(v)\n#                              except (ValueError, TypeError): pass # Ignore attributes that cannot be converted\n#                      veh_attributes[frame_id] = dict_vals\n#                 except (TypeError, ValueError, AttributeError):\n#                      # Skip frame if ID is missing or invalid\n#                      pass\n#         except (ET.ParseError, FileNotFoundError) as e:\n#             print(f\"Warning: Could not parse vehicle attributes {path_to_file}: {e}. Ego data will be missing.\")\n#         return veh_attributes\n\n#     def generate_database(self):\n#         \"\"\" Generates and saves a database of the pie dataset by integrating all annotations \"\"\"\n#         print('---------------------------------------------------------')\n#         print(\"Generating database for pie\")\n\n#         cache_file = join(self.cache_path, 'pie_database.pkl')\n#         # Always regenerate if self._regen_database is True\n#         if isfile(cache_file) and not self._regen_database:\n#             print(f\"Database cache found at {cache_file}. Skipping generation.\")\n#             # Optionally load and return existing DB if needed by caller, but for this script we just want to generate.\n#             return None # Indicate cache exists\n\n#         # --- *** Corrected set_ids definition *** ---\n#         try:\n#             # Only list directories within the annotation path\n#             set_ids = [f for f in sorted(listdir(self._annotation_path))\n#                        if isdir(join(self._annotation_path, f))]\n#             if not set_ids:\n#                  print(f\"ERROR: No set directories found in {self._annotation_path}\")\n#                  return None\n#         except Exception as e:\n#             print(f\"ERROR: Could not list set directories in {self._annotation_path}: {e}\")\n#             return None\n#         # --- *** End Correction *** ---\n\n#         database = {}\n#         print(f\"Found sets: {set_ids}\")\n#         for setid in tqdm(set_ids, desc=\"Processing Sets\"):\n#             # Check if necessary sub-annotation dirs exist for this set\n#             set_annot_path = join(self._annotation_path, setid)\n#             set_attr_path = join(self._annotation_attributes_path, setid)\n#             set_vehicle_path = join(self._annotation_vehicle_path, setid)\n#             if not isdir(set_annot_path):\n#                  print(f\"Warning: Annotation dir missing for {setid}, skipping set.\")\n#                  continue\n#             if not isdir(set_attr_path):\n#                  print(f\"Warning: Attribute dir missing for {setid}. Attributes may be incomplete.\")\n#             if not isdir(set_vehicle_path):\n#                  print(f\"Warning: Vehicle dir missing for {setid}. Ego data may be incomplete.\")\n\n#             try:\n#                 video_xmls = [v for v in sorted(listdir(set_annot_path)) if v.endswith(\"_annt.xml\")]\n#                 if not video_xmls:\n#                     print(f\"Warning: No '*_annt.xml' files found in {set_annot_path}\")\n#                     continue\n#                 video_ids = [v.replace('_annt.xml', '') for v in video_xmls]\n#             except Exception as e:\n#                 print(f\"Error listing video XMLs for {setid}: {e}\")\n#                 continue\n\n#             database[setid] = {}\n#             for vid in tqdm(video_ids, desc=f\"Videos in {setid}\", leave=False):\n#                 # print(f'Getting annotations for {setid}, {vid}') # Can be too verbose\n#                 vid_annotations = self._get_annotations(setid, vid)\n#                 if vid_annotations is None: # Skip video if base annotation parsing failed\n#                      print(f\"Skipping video {vid} due to annotation parsing error.\")\n#                      continue\n\n#                 vid_ped_attributes = self._get_ped_attributes(setid, vid)\n#                 vid_vehicle_attributes = self._get_vehicle_attributes(setid, vid)\n\n#                 database[setid][vid] = vid_annotations\n#                 # Add vehicle data if parsed\n#                 database[setid][vid]['vehicle_annotations'] = vid_vehicle_attributes\n#                 # Add pedestrian attributes if parsed\n#                 if 'ped_annotations' in database[setid][vid]:\n#                      for ped_id in list(database[setid][vid]['ped_annotations'].keys()): # Iterate over copy of keys\n#                           if ped_id in vid_ped_attributes:\n#                                 database[setid][vid]['ped_annotations'][ped_id]['attributes'] = vid_ped_attributes[ped_id]\n#                           else:\n#                                # print(f\"Warning: No attributes found for ped {ped_id} in video {vid}, set {setid}. Assigning empty dict.\")\n#                                database[setid][vid]['ped_annotations'][ped_id]['attributes'] = {} # Ensure key exists\n\n\n#         # --- Save the generated database ---\n#         try:\n#             print(f\"\\nAttempting to save database to {cache_file}...\")\n#             with open(cache_file, 'wb') as fid:\n#                 pickle.dump(database, fid, pickle.HIGHEST_PROTOCOL)\n#             print(f'Database successfully written to {cache_file}')\n#         except Exception as e:\n#             print(f\"\\nERROR: Failed to write database cache file {cache_file}: {e}\")\n#             print(\"Database generation completed but saving failed.\")\n#             # Optionally: return database anyway, or re-raise error\n#             # return database\n#             raise e # Re-raise error if saving is critical\n\n#         return database # Return the generated database\n\n# # --- Main Execution Block ---\n# if __name__ == '__main__':\n#     print(f\"Using PIE Root Path: {PIE_ROOT_PATH}\")\n\n#     # Instantiate the generator class - force regeneration if cache exists\n#     # Set regen_database=False if you only want it to generate IF the cache is missing\n#     force_regeneration = True\n#     print(f\"Force Regeneration set to: {force_regeneration}\")\n\n#     pie_generator = PIE_Generator(data_path=PIE_ROOT_PATH, regen_database=force_regeneration)\n\n#     try:\n#         generated_db = pie_generator.generate_database()\n#         if generated_db is not None:\n#              print(\"\\nDatabase generation process completed successfully.\")\n#              # You could add basic checks here like number of sets/videos processed if needed\n#         elif os.path.exists(os.path.join(pie_generator.cache_path, 'pie_database.pkl')):\n#              print(\"\\nDatabase generation skipped as cache file already exists (and regen_database=False).\")\n#         else:\n#              print(\"\\nDatabase generation may have failed (returned None and cache doesn't exist). Check warnings/errors.\")\n#     except Exception as e:\n#         print(f\"\\nAn unexpected error occurred during database generation: {e}\")\n\n#     print(\"\\n--- Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:44:02.054822Z","iopub.execute_input":"2025-04-14T12:44:02.055076Z","iopub.status.idle":"2025-04-14T12:44:02.062421Z","shell.execute_reply.started":"2025-04-14T12:44:02.055055Z","shell.execute_reply":"2025-04-14T12:44:02.061477Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# import os\n# import pickle\n# import sys\n# import numpy as np\n# from tqdm.notebook import tqdm\n# import random\n\n# print(\"--- PIE Database Cache Verification Script ---\")\n\n# # --- Configuration ---\n# # <<< --- SET THIS TO THE LOCATION OF YOUR GENERATED CACHE --- >>>\n# PKL_FILE_PATH = \"/kaggle/working/PIE/data_cache/pie_database.pkl\"\n# # <<< --- END CONFIGURATION --- >>>\n\n# # Define expected top-level keys (these are the set IDs)\n# EXPECTED_SETS = {'set01', 'set02', 'set03', 'set04', 'set05', 'set06'}\n\n# # Define expected keys within each video dictionary\n# EXPECTED_VIDEO_KEYS = {'num_frames', 'width', 'height',\n#                        'ped_annotations', 'traffic_annotations', 'vehicle_annotations'}\n\n# # Define expected keys within each pedestrian annotation dictionary\n# EXPECTED_PED_KEYS = {'frames', 'bbox', 'occlusion', 'behavior', 'attributes'}\n# EXPECTED_PED_BEHAVIOR_KEYS = {'gesture', 'look', 'action', 'cross'}\n# # Attributes can vary, so we won't check all exhaustively here, but check presence\n\n# # Define expected keys within vehicle (ego) frame dictionary (sample)\n# EXPECTED_EGO_FRAME_KEYS = {'OBD_speed', 'GPS_speed', 'accX', 'accY', 'accZ',\n#                            'gyroX', 'gyroY', 'gyroZ', 'heading_angle', 'latitude',\n#                            'longitude', 'pitch', 'roll', 'yaw'}\n\n\n# # --- Verification Parameters ---\n# MAX_FRAMES_TO_CHECK_PER_VIDEO = 50 # Limit checks per video for speed\n# MAX_PEDS_TO_CHECK_PER_VIDEO = 20  # Limit checks per video for speed\n# MAX_TRAFFIC_OBJS_TO_CHECK_PER_VIDEO = 20 # Limit checks\n# PRINT_SAMPLE_COUNT = 3 # How many sample data lines to print\n\n# # --- Counters and Flags ---\n# errors_found = 0\n# warnings_found = 0\n# checked_sets = 0\n# checked_videos = 0\n# checked_peds = 0\n# checked_ped_frames = 0\n# checked_ego_frames = 0\n\n# # --- Helper Function for Reporting ---\n# def report_error(message):\n#     global errors_found\n#     print(f\"  ERROR: {message}\")\n#     errors_found += 1\n\n# def report_warning(message):\n#     global warnings_found\n#     print(f\"  Warning: {message}\")\n#     warnings_found += 1\n\n# # --- 1. Load the PKL File ---\n# print(f\"\\n[1] Loading PKL file: {PKL_FILE_PATH}\")\n# if not os.path.exists(PKL_FILE_PATH):\n#     print(f\"  ERROR: PKL file not found at the specified path.\")\n#     exit()\n\n# try:\n#     with open(PKL_FILE_PATH, 'rb') as f:\n#         pie_database = pickle.load(f)\n#     print(\"  -> PKL file loaded successfully.\")\n# except Exception as e:\n#     print(f\"  ERROR: Failed to load PKL file: {e}\")\n#     exit()\n\n# # --- 2. Basic Structure Checks ---\n# print(f\"\\n[2] Checking Top-Level Structure...\")\n# if not isinstance(pie_database, dict):\n#     report_error(f\"Loaded data is not a dictionary (Type: {type(pie_database)}).\")\n#     exit()\n# print(f\"  -> Top level is a dictionary: OK\")\n\n# found_sets = set(pie_database.keys())\n# if found_sets != EXPECTED_SETS:\n#     report_warning(f\"Set keys mismatch. Found: {found_sets}, Expected: {EXPECTED_SETS}\")\n# else:\n#     print(f\"  -> Found expected set keys: OK\")\n# checked_sets = len(found_sets)\n\n# # --- 3. Detailed Content Checks ---\n# print(f\"\\n[3] Checking Set/Video/Annotation Structures...\")\n\n# sample_data_to_print = []\n\n# for set_id in tqdm(found_sets, desc=\"Checking Sets\"):\n#     if not isinstance(pie_database[set_id], dict):\n#         report_error(f\"Data for set '{set_id}' is not a dictionary.\")\n#         continue\n\n#     video_ids = list(pie_database[set_id].keys())\n#     checked_videos += len(video_ids)\n\n#     for video_id in tqdm(video_ids, desc=f\"Videos in {set_id}\", leave=False):\n#         video_data = pie_database[set_id][video_id]\n#         if not isinstance(video_data, dict):\n#             report_error(f\"Data for video '{set_id}/{video_id}' is not a dictionary.\")\n#             continue\n\n#         # Check video-level keys\n#         missing_vid_keys = EXPECTED_VIDEO_KEYS - set(video_data.keys())\n#         if missing_vid_keys:\n#             report_warning(f\"Video '{set_id}/{video_id}' missing keys: {missing_vid_keys}\")\n\n#         # Basic type checks for video keys\n#         img_width = video_data.get('width', -1)\n#         img_height = video_data.get('height', -1)\n#         if not isinstance(video_data.get('num_frames'), int): report_warning(f\"num_frames type mismatch in {set_id}/{video_id}\")\n#         if not isinstance(img_width, int): report_warning(f\"width type mismatch in {set_id}/{video_id}\")\n#         if not isinstance(img_height, int): report_warning(f\"height type mismatch in {set_id}/{video_id}\")\n#         if not isinstance(video_data.get('ped_annotations'), dict): report_error(f\"ped_annotations not a dict in {set_id}/{video_id}\"); continue # Stop checking peds for this video if structure wrong\n#         if not isinstance(video_data.get('traffic_annotations'), dict): report_warning(f\"traffic_annotations not a dict in {set_id}/{video_id}\")\n#         if not isinstance(video_data.get('vehicle_annotations'), dict): report_warning(f\"vehicle_annotations not a dict in {set_id}/{video_id}\")\n\n\n#         # --- Check Pedestrian Annotations ---\n#         ped_annotations = video_data.get('ped_annotations', {})\n#         ped_ids_to_check = list(ped_annotations.keys())\n#         random.shuffle(ped_ids_to_check) # Check a random subset\n\n#         for i, ped_id in enumerate(ped_ids_to_check):\n#             if i >= MAX_PEDS_TO_CHECK_PER_VIDEO: break # Limit checks\n#             checked_peds += 1\n#             ped_data = ped_annotations[ped_id]\n#             if not isinstance(ped_data, dict): report_error(f\"Data for ped '{ped_id}' in {set_id}/{video_id} is not a dict.\"); continue\n\n#             missing_ped_keys = EXPECTED_PED_KEYS - set(ped_data.keys())\n#             if missing_ped_keys: report_warning(f\"Ped '{ped_id}' in {set_id}/{video_id} missing keys: {missing_ped_keys}\")\n\n#             # Check structure of essential lists/dicts\n#             frames = ped_data.get('frames', [])\n#             bboxes = ped_data.get('bbox', [])\n#             occlusions = ped_data.get('occlusion', [])\n#             behavior = ped_data.get('behavior', {})\n#             attributes = ped_data.get('attributes', {})\n\n#             if not isinstance(frames, list): report_error(f\"Ped '{ped_id}' frames not a list.\"); continue\n#             if not isinstance(bboxes, list): report_error(f\"Ped '{ped_id}' bbox not a list.\"); continue\n#             if not isinstance(occlusions, list): report_error(f\"Ped '{ped_id}' occlusion not a list.\"); continue\n#             if not isinstance(behavior, dict): report_error(f\"Ped '{ped_id}' behavior not a dict.\"); continue\n#             if not isinstance(attributes, dict): report_warning(f\"Ped '{ped_id}' attributes not a dict.\"); continue # Attributes might be empty\n\n#             # Check list lengths consistency\n#             n_frames = len(frames)\n#             if n_frames == 0 and (len(bboxes) > 0 or len(occlusions) > 0): report_warning(f\"Ped '{ped_id}' has bboxes/occlusions but 0 frames listed.\")\n#             if len(bboxes) != n_frames: report_error(f\"Ped '{ped_id}' bbox length ({len(bboxes)}) != frames length ({n_frames}).\")\n#             if len(occlusions) != n_frames: report_error(f\"Ped '{ped_id}' occlusion length ({len(occlusions)}) != frames length ({n_frames}).\")\n\n#             missing_beh_keys = EXPECTED_PED_BEHAVIOR_KEYS - set(behavior.keys())\n#             if missing_beh_keys: report_warning(f\"Ped '{ped_id}' behavior missing keys: {missing_beh_keys}\")\n\n#             for beh_key, beh_list in behavior.items():\n#                 if not isinstance(beh_list, list): report_error(f\"Ped '{ped_id}' behavior '{beh_key}' not a list.\"); continue\n#                 if len(beh_list) != n_frames: report_error(f\"Ped '{ped_id}' behavior '{beh_key}' length ({len(beh_list)}) != frames length ({n_frames}).\")\n\n#             # Check sample frame content\n#             frames_to_check_in_ped = list(range(n_frames))\n#             random.shuffle(frames_to_check_in_ped)\n#             for k, frame_idx in enumerate(frames_to_check_in_ped):\n#                  if k >= MAX_FRAMES_TO_CHECK_PER_VIDEO: break\n#                  checked_ped_frames +=1\n#                  # Check frame number type\n#                  if not isinstance(frames[frame_idx], int): report_warning(f\"Ped '{ped_id}' frame value at index {frame_idx} not int.\")\n#                  # Check bbox format and range\n#                  if len(bboxes) > frame_idx:\n#                       bbox = bboxes[frame_idx]\n#                       if not isinstance(bbox, list) or len(bbox) != 4: report_error(f\"Ped '{ped_id}' frame {frames[frame_idx]} bbox invalid format: {bbox}\"); continue\n#                       try:\n#                            x1,y1,x2,y2 = map(float, bbox)\n#                            if img_width>0 and img_height>0 and not (0 <= x1 < x2 <= img_width and 0 <= y1 < y2 <= img_height): report_warning(f\"Ped '{ped_id}' frame {frames[frame_idx]} bbox out of bounds: {[int(x) for x in bbox]} vs {img_width}x{img_height}\")\n#                       except (ValueError, TypeError): report_error(f\"Ped '{ped_id}' frame {frames[frame_idx]} bbox contains non-numeric values: {bbox}\")\n#                  # Check occlusion value\n#                  if len(occlusions) > frame_idx:\n#                       occ = occlusions[frame_idx]\n#                       if not isinstance(occ, int) or occ not in [0, 1, 2]: report_warning(f\"Ped '{ped_id}' frame {frames[frame_idx]} invalid occlusion value: {occ}\")\n\n#                  # Add sample for printing later\n#                  if len(sample_data_to_print) < PRINT_SAMPLE_COUNT:\n#                       sample_data_to_print.append(f\" Sample Ped Data: Set={set_id}, Vid={video_id}, Ped={ped_id}, Frame={frames[frame_idx]}, BBox={bboxes[frame_idx] if len(bboxes) > frame_idx else 'N/A'}, Occ={occlusions[frame_idx] if len(occlusions) > frame_idx else 'N/A'}\")\n\n#         # --- Check Vehicle Annotations (Ego Data) ---\n#         vehicle_annotations = video_data.get('vehicle_annotations', {})\n#         ego_frames_to_check = list(vehicle_annotations.keys())\n#         random.shuffle(ego_frames_to_check)\n\n#         for k, frame_num in enumerate(ego_frames_to_check):\n#             if k >= MAX_FRAMES_TO_CHECK_PER_VIDEO: break # Limit checks\n#             checked_ego_frames += 1\n#             if not isinstance(frame_num, int): report_warning(f\"Ego data frame key '{frame_num}' in {set_id}/{video_id} is not int.\") ; continue\n#             ego_frame_data = vehicle_annotations[frame_num]\n#             if not isinstance(ego_frame_data, dict): report_error(f\"Ego data for frame {frame_num} in {set_id}/{video_id} is not dict.\"); continue\n\n#             missing_ego_keys = EXPECTED_EGO_FRAME_KEYS - set(ego_frame_data.keys())\n#             # Don't warn about every missing key, just check a few critical ones\n#             if 'OBD_speed' not in ego_frame_data and 'GPS_speed' not in ego_frame_data: report_warning(f\"Ego frame {frame_num} in {set_id}/{video_id} missing speed data.\")\n#             for key in EXPECTED_EGO_FRAME_KEYS:\n#                 if key in ego_frame_data and not isinstance(ego_frame_data[key], (float, int)): report_warning(f\"Ego frame {frame_num} key '{key}' value is not float/int (type: {type(ego_frame_data[key])})\")\n\n#             # Add sample for printing later\n#             if len(sample_data_to_print) < PRINT_SAMPLE_COUNT * 2 and k < 5: # Print a few ego samples too\n#                   sample_data_to_print.append(f\" Sample Ego Data: Set={set_id}, Vid={video_id}, Frame={frame_num}, Speed={ego_frame_data.get('OBD_speed', 'N/A'):.2f}, AccX={ego_frame_data.get('accX', 'N/A'):.2f}\")\n\n# # --- 4. Print Summary ---\n# print(f\"\\n[4] Verification Summary ---\")\n# print(f\"  - Checked {checked_sets} sets.\")\n# print(f\"  - Checked {checked_videos} videos.\")\n# print(f\"  - Checked {checked_peds} pedestrian tracks (sampled max {MAX_PEDS_TO_CHECK_PER_VIDEO} per video).\")\n# print(f\"  - Checked {checked_ped_frames} pedestrian frame entries (sampled max {MAX_FRAMES_TO_CHECK_PER_VIDEO} per ped).\")\n# print(f\"  - Checked {checked_ego_frames} ego data frame entries (sampled max {MAX_FRAMES_TO_CHECK_PER_VIDEO} per video).\")\n# print(f\"  - Total Errors Found: {errors_found}\")\n# print(f\"  - Total Warnings Found: {warnings_found}\")\n\n# if errors_found == 0:\n#     print(\"\\n  >>> Structure and basic content checks PASSED (with potential warnings). <<<\")\n# else:\n#     print(\"\\n  >>> ERRORS FOUND during structural/content checks. Review messages above. <<<\")\n\n# # --- 5. Print Sample Data ---\n# if sample_data_to_print:\n#     print(\"\\n[5] Sample Data Points ---\")\n#     for line in sample_data_to_print:\n#         print(line)\n\n# print(\"\\n--- Verification Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:44:02.063760Z","iopub.execute_input":"2025-04-14T12:44:02.064049Z","iopub.status.idle":"2025-04-14T12:44:02.081954Z","shell.execute_reply.started":"2025-04-14T12:44:02.064030Z","shell.execute_reply":"2025-04-14T12:44:02.081278Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport xml.etree.ElementTree as ET\nimport os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\nfrom tqdm.notebook import tqdm\nimport random\nimport math\nimport zipfile\nimport cv2\nfrom ultralytics import YOLO\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:44:02.082789Z","iopub.execute_input":"2025-04-14T12:44:02.083081Z","iopub.status.idle":"2025-04-14T12:44:07.585406Z","shell.execute_reply.started":"2025-04-14T12:44:02.083054Z","shell.execute_reply":"2025-04-14T12:44:07.584740Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + 'annotations'):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:44:07.586132Z","iopub.execute_input":"2025-04-14T12:44:07.586439Z","iopub.status.idle":"2025-04-14T12:44:07.591611Z","shell.execute_reply.started":"2025-04-14T12:44:07.586419Z","shell.execute_reply":"2025-04-14T12:44:07.590679Z"}},"outputs":[{"name":"stdout","text":"Exists already. Not unzipping.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations_vehicle.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + 'annotations_vehicle'):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:44:07.592471Z","iopub.execute_input":"2025-04-14T12:44:07.592667Z","iopub.status.idle":"2025-04-14T12:44:07.608939Z","shell.execute_reply.started":"2025-04-14T12:44:07.592649Z","shell.execute_reply":"2025-04-14T12:44:07.608251Z"}},"outputs":[{"name":"stdout","text":"Exists already. Not unzipping.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations_attributes.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + \"annotations_attributes\"):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:44:07.610564Z","iopub.execute_input":"2025-04-14T12:44:07.610792Z","iopub.status.idle":"2025-04-14T12:44:07.623671Z","shell.execute_reply.started":"2025-04-14T12:44:07.610767Z","shell.execute_reply":"2025-04-14T12:44:07.623005Z"}},"outputs":[{"name":"stdout","text":"Exists already. Not unzipping.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# BASE_DIR = '/kaggle/input'\n# ANNOTATION_DIR = '/kaggle/working/PIE/annotations/annotations'\n# # CLIP_DIR = os.path.join(BASE_DIR, 'PIE_clips') # Not used directly in this version\n\n# # Model Hyperparameters\n# SEQ_LEN = 30        # Number of past frames to observe\n# PRED_LEN = 1        # Predict state at the end of sequence (relative to seq_len)\n# INPUT_SIZE_BBOX = 4 # (center_x, center_y, width, height) - normalized\n# INPUT_SIZE_POSE = 34  \n# LSTM_HIDDEN_SIZE = 256\n# NUM_LSTM_LAYERS = 2\n# DROPOUT_RATE = 0.3\n# NUM_CLASSES = 2      # 0: not-crossing, 1: crossing\n# ATTENTION_DIM = 128  # Dimension for the attention mechanism\n\n# # Training Hyperparameters\n# LEARNING_RATE = 1e-3\n# BATCH_SIZE = 64\n# NUM_EPOCHS = 2 # Adjust as needed\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {DEVICE}\")\n\n# TRAIN_SETS = ['pie-set01', 'pie-set02', 'pie-set03', 'pie-set04', 'pie-set05']\n# VAL_SETS = ['pie-set06']\n\n\n# # --- Helper Function: Compute IOU ---\n# def compute_iou(boxA, boxB):\n#     # boxA and boxB are in [x1, y1, x2, y2] format\n#     xA = max(boxA[0], boxB[0])\n#     yA = max(boxA[1], boxB[1])\n#     xB = min(boxA[2], boxB[2])\n#     yB = min(boxA[3], boxB[3])\n#     interW = max(0, xB - xA)\n#     interH = max(0, yB - yA)\n#     interArea = interW * interH\n#     boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n#     boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n#     iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n#     return iou\n\n# # --- Data Preprocessing Functions ---\n\n# def parse_annotations(xml_file):\n#     \"\"\"Parses a PIE annotation XML file.\"\"\"\n#     try:\n#         tree = ET.parse(xml_file)\n#         root = tree.getroot()\n#     except ET.ParseError:\n#         print(f\"Error parsing {xml_file}\")\n#         return None, None\n\n#     img_width = int(root.find('.//original_size/width').text)\n#     img_height = int(root.find('.//original_size/height').text)\n\n#     ped_tracks = {} # {ped_id: {frame: {'bbox': [xtl,ytl,xbr,ybr], 'cross': label}}}\n\n#     for track in root.findall('.//track[@label=\"pedestrian\"]'):\n#         for box in track.findall('.//box'):\n#             frame = int(box.get('frame'))\n#             ped_id = box.find('.//attribute[@name=\"id\"]').text\n#             xtl = float(box.get('xtl'))\n#             ytl = float(box.get('ytl'))\n#             xbr = float(box.get('xbr'))\n#             ybr = float(box.get('ybr'))\n#             occluded = int(box.get('occluded', 0)) # Handle missing occluded tag\n\n#             # Only process non-occluded boxes for simplicity\n#             if occluded > 0:\n#                 continue\n\n#             cross_status = box.find('.//attribute[@name=\"cross\"]').text\n#             # Map labels: 1 for crossing, 0 otherwise\n#             cross_label = 1 if cross_status == 'crossing' else 0\n\n#             if ped_id not in ped_tracks:\n#                 ped_tracks[ped_id] = {}\n\n#             # Normalize bounding box: [center_x, center_y, width, height]\n#             center_x = ((xtl + xbr) / 2) / img_width\n#             center_y = ((ytl + ybr) / 2) / img_height\n#             width = (xbr - xtl) / img_width\n#             height = (ybr - ytl) / img_height\n#             # Basic check for valid bbox dimensions\n#             if width <= 0 or height <= 0 or not (0 <= center_x <= 1) or not (0 <= center_y <= 1):\n#                 continue # Skip invalid boxes\n\n#             ped_tracks[ped_id][frame] = {\n#                 'bbox': [center_x, center_y, width, height],\n#                 'cross': cross_label\n#             }\n\n#     return ped_tracks, (img_width, img_height)\n\n# def create_sequences(ped_tracks, seq_len, pred_len, set_folder):\n#     \"\"\"Creates sequences of features and labels from pedestrian tracks.\n#        Now also stores frame numbers and image dimensions for pose extraction.\"\"\"\n#     sequences = []\n#     for ped_id, frames_data in ped_tracks.items():\n#         sorted_frames = sorted(frames_data.keys())\n\n#         for i in range(len(sorted_frames) - seq_len - pred_len + 1):\n#             seq_frames = sorted_frames[i : i + seq_len]\n#             target_frame = sorted_frames[i + seq_len + pred_len - 1]\n\n#             # Ensure frames are consecutive\n#             is_continuous = all(seq_frames[j+1] - seq_frames[j] == 1 for j in range(len(seq_frames)-1))\n#             is_target_continuous = (target_frame - seq_frames[-1] == pred_len)\n\n#             if not (is_continuous and is_target_continuous):\n#                 continue\n\n#             # Extract bbox features\n#             bbox_seq = [frames_data[f]['bbox'] for f in seq_frames]\n#             label = frames_data[target_frame]['cross']\n\n#             # For pose extraction, store the frame numbers and image dimensions.\n#             # (Assume all frames in the XML share the same image dimensions)\n#             # Note: set_folder helps locate images in BASE_DIR.\n#             sequence = {\n#                 'bbox': np.array(bbox_seq, dtype=np.float32),\n#                 'label': label,\n#                 'frames': seq_frames,\n#                 'set_folder': set_folder\n#             }\n#             # Also store image dimensions from the first frame\n#             # (Assuming they are constant within a video)\n#             # Here, we pick the first frame to get dims from the parsed annotation.\n#             # You may adjust if different frames have different dims.\n#             sequence['img_dims'] = None  # will be set outside if needed\n#             sequences.append(sequence)\n#     return sequences\n\n# # --- Dataset Class ---\n\n# class PIEDataset(Dataset):\n#     def __init__(self, annotation_dir, set_folders, seq_len, pred_len):\n#         self.seq_len = seq_len\n#         self.pred_len = pred_len\n#         self.sequences = []\n#         self.BASE_DIR = BASE_DIR  # Use global BASE_DIR for images\n\n#         self.pose_model = YOLO('/kaggle/input/yolov11n/pytorch/default/1/yolo11n-pose.pt') \n#         self.pose_model.to(DEVICE)\n\n\n#         print(f\"Loading data from sets: {set_folders}\")\n#         for set_folder in tqdm(set_folders):\n#             annotation_set_folder = set_folder.replace('pie-', '')              \n#             set_path = os.path.join(annotation_dir, annotation_set_folder)\n#             if not os.path.isdir(set_path):\n#                 print(f\"Warning: Annotation directory not found for {annotation_set_folder}\")\n#                 continue\n\n#             xml_files = [f for f in os.listdir(set_path) if f.endswith('.xml')]\n#             for xml_file in tqdm(xml_files, desc=f\"Processing {annotation_set_folder}\", leave=False):\n#                 file_path = os.path.join(set_path, xml_file)\n#                 ped_tracks, dims = parse_annotations(file_path)\n#                 if dims is None:\n#                     continue\n#                 img_width, img_height = dims\n#                 if ped_tracks:\n#                     video_sequences = create_sequences(ped_tracks, seq_len, pred_len, annotation_set_folder)\n#                     # Set image dimensions for each sequence\n#                     for seq in video_sequences:\n#                         seq['img_dims'] = (img_width, img_height)\n#                     self.sequences.extend(video_sequences)\n\n#         print(f\"Loaded {len(self.sequences)} sequences.\")\n#         # Basic balancing (undersampling majority class if highly imbalanced)\n#         labels = [s['label'] for s in self.sequences]\n#         count_0 = labels.count(0)\n#         count_1 = labels.count(1)\n#         print(f\"Class distribution: 0={count_0}, 1={count_1}\")\n\n#     def __len__(self):\n#         return len(self.sequences)\n\n#     def __getitem__(self, idx):\n#         sequence_data = self.sequences[idx]\n\n#         # --- Modality Extraction for BBox ---\n#         bbox_features = torch.tensor(sequence_data['bbox'], dtype=torch.float32)\n\n#         # For each frame in the sequence, load the image and extract keypoints\n#         set_folder = sequence_data['set_folder']\n#         img_dims = sequence_data['img_dims']  # (img_width, img_height)\n#         img_width, img_height = img_dims\n#         pose_seq = []\n\n#         for frame in sequence_data['frames']:\n#             # Assumption: images are stored in BASE_DIR/<set_folder> and named as \"<frame>.jpg\"\n#             image_path = os.path.join(self.BASE_DIR, set_folder, f\"{frame}.jpg\")\n#             # Load image using OpenCV\n#             image = cv2.imread(image_path)\n#             if image is None:\n#                 # If image is not found, use a zero vector\n#                 pose_seq.append(np.zeros(INPUT_SIZE_POSE, dtype=np.float32))\n#                 continue\n\n#             # Convert BGR to RGB\n#             image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n#             results = self.pose_model(image_rgb, verbose=False)  # results is a list\n#             keypoints_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)  # default if no match is found\n\n#             # Convert annotation bbox from normalized (center_x, center_y, w, h) to pixel coordinates\n#             ann_bbox = sequence_data['bbox'][sequence_data['frames'].index(frame)]\n#             ann_cx, ann_cy, ann_w, ann_h = ann_bbox\n#             ann_x1 = (ann_cx - ann_w/2) * img_width\n#             ann_y1 = (ann_cy - ann_h/2) * img_height\n#             ann_x2 = (ann_cx + ann_w/2) * img_width\n#             ann_y2 = (ann_cy + ann_h/2) * img_height\n#             ann_box_pixels = [ann_x1, ann_y1, ann_x2, ann_y2]\n\n#             # Search for the detection with best IOU with the annotation bbox\n#             best_iou = 0.0\n#             best_keypoints = None\n#             if results and len(results) > 0:\n#                 # Each result may contain multiple detections.\n#                 res = results[0]\n#                 # res.boxes.xyxy gives detections; res.keypoints.data gives keypoints if available.\n#                 if res.boxes is not None and res.keypoints is not None:\n#                     boxes = res.boxes.xyxy.cpu().numpy()  # shape: (n, 4)\n#                     kps = res.keypoints.cpu().numpy()       # shape: (n, 17, 3)\n#                     for i in range(len(boxes)):\n#                         det_box = boxes[i]\n#                         iou = compute_iou(ann_box_pixels, det_box)\n#                         if iou > best_iou:\n#                             best_iou = iou\n#                             best_keypoints = kps[i]\n\n#             # If a detection with sufficient overlap is found, process its keypoints\n#             if best_keypoints is not None and best_iou > 0.3:\n#                 # Extract x,y coordinates (ignoring confidence) and normalize them\n#                 norm_keypoints = []\n#                 for (x, y, conf) in best_keypoints:\n#                     norm_keypoints.extend([x / img_width, y / img_height])\n#                 keypoints_vector = np.array(norm_keypoints, dtype=np.float32)\n#                 # Ensure it is of length 34\n#                 if keypoints_vector.shape[0] != INPUT_SIZE_POSE:\n#                     keypoints_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n#             # Else, keypoints_vector remains zeros\n\n#             pose_seq.append(keypoints_vector)\n#         pose_features = torch.tensor(np.array(pose_seq), dtype=torch.float32)\n\n#         # --- Return combined modalities ---\n#         features = {\n#             'bbox': bbox_features,\n#             'pose': pose_features\n#         }\n#         label = torch.tensor(sequence_data['label'], dtype=torch.long) # Use long for CrossEntropyLoss\n\n#         return features, label\n    \n\n# # --- Model Architecture ---\n\n# class Attention(nn.Module):\n#     \"\"\" Simple Dot-Product Attention or Learned Attention\"\"\"\n#     def __init__(self, hidden_dim, attention_dim):\n#         super(Attention, self).__init__()\n#         self.attention_net = nn.Sequential(\n#             nn.Linear(hidden_dim, attention_dim),\n#             nn.Tanh(),\n#             nn.Linear(attention_dim, 1)\n#         )\n\n#     def forward(self, lstm_output):\n#         attention_scores = self.attention_net(lstm_output).squeeze(2)\n#         attention_weights = torch.softmax(attention_scores, dim=1)\n#         context_vector = torch.sum(lstm_output * attention_weights.unsqueeze(2), dim=1)\n#         return context_vector, attention_weights\n\n# class MultiStreamAdaptiveLSTM(nn.Module):\n#     def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes,\n#                  attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n#         super(MultiStreamAdaptiveLSTM, self).__init__()\n#         self.stream_names = stream_names\n#         self.lstms = nn.ModuleDict()\n#         self.attentions = nn.ModuleDict()\n\n#         for name in self.stream_names:\n#             input_size = input_sizes[name]\n#             self.lstms[name] = nn.LSTM(input_size, lstm_hidden_size, num_lstm_layers,\n#                                        batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0,\n#                                        bidirectional=True)\n#             self.attentions[name] = Attention(lstm_hidden_size * 2 , attention_dim)\n\n#         num_streams = len(self.stream_names)\n#         combined_feature_dim = lstm_hidden_size * 2 * len(self.stream_names)\n\n#         self.dropout = nn.Dropout(dropout_rate)\n#         self.fc1 = nn.Linear(combined_feature_dim, combined_feature_dim // 2)\n#         self.relu = nn.ReLU()\n#         self.fc2 = nn.Linear(combined_feature_dim // 2, num_classes)\n\n#     def forward(self, x):\n#         stream_context_vectors = []\n#         stream_att_weights = {}\n\n#         for name in self.stream_names:\n#             lstm_input = x[name]\n#             lstm_out, _ = self.lstms[name](lstm_input)\n#             context_vector, attention_weights = self.attentions[name](lstm_out)\n#             stream_context_vectors.append(context_vector)\n#             stream_att_weights[name] = attention_weights\n\n#         fused_features = torch.cat(stream_context_vectors, dim=1)\n#         x = self.dropout(fused_features)\n#         x = self.fc1(x)\n#         x = self.relu(x)\n#         x = self.dropout(x)\n#         logits = self.fc2(x)\n#         return logits\n\n# # --- Training and Evaluation Functions ---\n\n# def train_epoch(model, dataloader, optimizer, criterion, device):\n#     model.train()\n#     total_loss = 0.0\n#     all_preds = []\n#     all_labels = []\n\n#     for features, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n#         input_features = {name: features[name].to(device) for name in model.stream_names}\n#         labels = labels.to(device)\n#         optimizer.zero_grad()\n#         outputs = model(input_features)\n#         loss = criterion(outputs, labels)\n#         loss.backward()\n#         optimizer.step()\n#         total_loss += loss.item()\n#         preds = torch.argmax(outputs, dim=1)\n#         all_preds.extend(preds.cpu().numpy())\n#         all_labels.extend(labels.cpu().numpy())\n\n#     avg_loss = total_loss / len(dataloader)\n#     accuracy = accuracy_score(all_labels, all_preds)\n#     return avg_loss, accuracy\n\n# def evaluate_epoch(model, dataloader, criterion, device):\n#     model.eval()\n#     total_loss = 0.0\n#     all_labels = []\n#     all_preds = []\n#     all_probs = []\n\n#     with torch.no_grad():\n#         for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n#             input_features = {name: features[name].to(device) for name in model.stream_names}\n#             labels = labels.to(device)\n#             outputs = model(input_features)\n#             loss = criterion(outputs, labels)\n#             total_loss += loss.item()\n#             probs = torch.softmax(outputs, dim=1)\n#             preds = torch.argmax(probs, dim=1)\n#             all_labels.extend(labels.cpu().numpy())\n#             all_preds.extend(preds.cpu().numpy())\n#             all_probs.extend(probs.cpu().numpy())\n\n#     avg_loss = total_loss / len(dataloader)\n#     all_probs = np.array(all_probs)\n#     all_labels = np.array(all_labels)\n#     all_preds = np.array(all_preds)\n\n#     accuracy = accuracy_score(all_labels, all_preds)\n#     precision, recall, f1, _ = precision_recall_fscore_support(\n#         all_labels, all_preds, average='binary', pos_label=1, zero_division=0\n#     )\n#     if len(np.unique(all_labels)) > 1:\n#        auc = roc_auc_score(all_labels, all_probs[:, 1])\n#     else:\n#        auc = float('nan')\n#     metrics = {\n#         'loss': avg_loss,\n#         'accuracy': accuracy,\n#         'precision': precision,\n#         'recall': recall,\n#         'f1': f1,\n#         'auc': auc\n#     }\n#     return metrics\n\n# def get_predictions_and_labels(model, dataloader, device):\n#     model.eval()\n#     all_labels = []\n#     all_preds = []\n\n#     with torch.no_grad():\n#         for features, labels in tqdm(dataloader, desc=\"Generating Confusion Matrix\", leave=False):\n#             input_features = {name: features[name].to(device) for name in model.stream_names}\n#             labels = labels.to(device)\n#             outputs = model(input_features)\n#             preds = torch.argmax(outputs, dim=1)\n#             all_labels.extend(labels.cpu().numpy())\n#             all_preds.extend(preds.cpu().numpy())\n\n#     return np.array(all_labels), np.array(all_preds)\n\n# # --- Main Execution ---\n\n# if __name__ == '__main__':\n#     # Initialize Datasets and Dataloaders\n#     train_dataset = PIEDataset(ANNOTATION_DIR, TRAIN_SETS, SEQ_LEN, PRED_LEN)\n#     val_dataset = PIEDataset(ANNOTATION_DIR, VAL_SETS, SEQ_LEN, PRED_LEN)\n\n#     if len(train_dataset) == 0 or len(val_dataset) == 0:\n#          raise ValueError(\"Dataset loading failed or resulted in empty datasets. Check paths and parsing.\")\n\n#     train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n#     val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n\n#     # Initialize Model, Loss, and Optimizer\n#     input_sizes = {'bbox': INPUT_SIZE_BBOX, 'pose': INPUT_SIZE_POSE}\n#     model = MultiStreamAdaptiveLSTM(\n#         input_sizes=input_sizes,\n#         lstm_hidden_size=LSTM_HIDDEN_SIZE,\n#         num_lstm_layers=NUM_LSTM_LAYERS,\n#         num_classes=NUM_CLASSES,\n#         attention_dim=ATTENTION_DIM,\n#         dropout_rate=DROPOUT_RATE,\n#         stream_names=['bbox', 'pose']\n#     ).to(DEVICE)\n\n#     print(model)\n#     criterion = nn.CrossEntropyLoss()\n#     optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n#     # Training Loop\n#     best_val_f1 = -1.0\n#     print(\"\\n--- Starting Training ---\")\n#     for epoch in range(NUM_EPOCHS):\n#         train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n#         val_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n#         print(f\"Epoch {epoch+1}/{NUM_EPOCHS}:\")\n#         print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n#         print(f\"  Val Loss:   {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n#         print(f\"  Val Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n#         print(f\"  Val AUC: {val_metrics['auc']:.4f}\")\n\n#         if val_metrics['f1'] > best_val_f1:\n#             best_val_f1 = val_metrics['f1']\n#             torch.save(model.state_dict(), 'best_model.pth')\n#             print(f\"  Saved new best model with F1: {best_val_f1:.4f}\")\n#         print(\"-\" * 20)\n\n#     print(\"--- Training Finished ---\")\n\n#     print(\"\\n--- Final Evaluation on Validation Set using Best Model ---\")\n#     model.load_state_dict(torch.load('best_model.pth', map_location=DEVICE))\n#     final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n#     true_labels, pred_labels = get_predictions_and_labels(model, val_loader, DEVICE)\n#     cm = confusion_matrix(true_labels, pred_labels)\n\n#     labels = ['Not Crossing', 'Crossing']\n#     print(\"Final Performance Metrics:\")\n#     print(f\"  Accuracy:  {final_metrics['accuracy']:.4f}\")\n#     print(f\"  Precision: {final_metrics['precision']:.4f}\")\n#     print(f\"  Recall:    {final_metrics['recall']:.4f}\")\n#     print(f\"  F1 Score:  {final_metrics['f1']:.4f}\")\n#     print(f\"  AUC:       {final_metrics['auc']:.4f}\")\n#     print(f\"  Loss:      {final_metrics['loss']:.4f}\")\n\n#     # Plot confusion matrix\n#     plt.figure(figsize=(6, 5))\n#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n#     plt.xlabel('Predicted')\n#     plt.ylabel('Actual')\n#     plt.title('Confusion Matrix')\n#     plt.tight_layout()\n#     plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:44:07.624790Z","iopub.execute_input":"2025-04-14T12:44:07.625100Z","iopub.status.idle":"2025-04-14T12:44:07.637757Z","shell.execute_reply.started":"2025-04-14T12:44:07.625073Z","shell.execute_reply":"2025-04-14T12:44:07.637124Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# import cv2\n# import os\n# import xml.etree.ElementTree as ET\n# import numpy as np\n# import matplotlib.pyplot as plt\n# from ultralytics import YOLO\n# import torch\n# import csv        # <<< Use csv module\n# import random     # <<< Use random module\n\n# # --- Configuration ---\n# # <<< MAKE SURE THESE PATHS ARE CORRECT FOR YOUR KAGGLE SETUP >>>\n# VIDEO_PATH = '/kaggle/input/pie-set01/video_0001.mp4' # Video file location\n# ANNOTATION_DIR = '/kaggle/working/PIE/annotations/annotations/set01' # Directory containing the XML files for set01\n# YOLO_MODEL_PATH = '/kaggle/input/yolo11x-pose/pytorch/default/1/yolo11x-pose.pt' # Using standard name - downloads if not present\n# CSV_ANNOTATED_FRAMES = '/kaggle/working/PIE/annotations/annotations/set01/set01_annotated_frames.csv' # Path to your CSV\n# # ---\n\n# VIDEO_ID = 'video_0001' # The video we are inspecting\n\n# # --- Select Target Frame from CSV (using csv module) ---\n# TARGET_FRAME_XML = None # Initialize\n# print(f\"Reading annotated frames CSV line by line: {CSV_ANNOTATED_FRAMES}\")\n# annotated_frames = []\n# try:\n#     with open(CSV_ANNOTATED_FRAMES, 'r', newline='') as f:\n#         reader = csv.reader(f)\n#         for row in reader:\n#             # Check if the first element matches the VIDEO_ID and row is not empty\n#             if row and row[0] == VIDEO_ID:\n#                 # Extract frames from the second element onwards\n#                 # Clean the frame numbers: convert to int, remove empty/non-digit strings\n#                 annotated_frames = [int(f) for f in row[1:] if f.strip().isdigit()]\n#                 break # Stop reading once the target row is found\n\n#     if not annotated_frames:\n#          # If loop finished without finding frames for VIDEO_ID\n#          raise ValueError(f\"VIDEO_ID '{VIDEO_ID}' not found or no valid frame numbers found (after column 1) in {CSV_ANNOTATED_FRAMES}\")\n\n#     # Randomly select one frame from the list\n#     TARGET_FRAME_XML = random.choice(annotated_frames)\n#     print(f\"Randomly selected annotated frame for {VIDEO_ID}: {TARGET_FRAME_XML}\")\n\n# except FileNotFoundError:\n#     raise FileNotFoundError(f\"Annotated frames CSV not found at: {CSV_ANNOTATED_FRAMES}\")\n# except ValueError as e: # Catch the specific error we might raise\n#     print(f\"Error processing CSV: {e}\")\n#     raise\n# except Exception as e:\n#     print(f\"An unexpected error occurred processing CSV: {e}\")\n#     raise\n\n# # Ensure a frame was selected before proceeding\n# if TARGET_FRAME_XML is None:\n#      raise ValueError(\"TARGET_FRAME_XML was not set. CSV processing failed.\")\n\n# # --- Device Setup ---\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {DEVICE}\")\n\n# # --- Helper Functions ---\n# def compute_iou(boxA, boxB):\n#     \"\"\"Computes the Intersection over Union (IoU) between two boxes.\"\"\"\n#     # boxA and boxB are in [x1, y1, x2, y2] format\n#     xA = max(boxA[0], boxB[0])\n#     yA = max(boxA[1], boxB[1])\n#     xB = min(boxA[2], boxB[2])\n#     yB = min(boxA[3], boxB[3])\n#     interW = max(0, xB - xA)\n#     interH = max(0, yB - yA)\n#     interArea = interW * interH\n#     boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n#     boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n#     iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6) # Add epsilon for stability\n#     return iou\n\n# def parse_specific_frame_annotation(xml_file, target_frame):\n#     \"\"\"Parses a PIE annotation XML file for pedestrian annotations in a specific frame.\"\"\"\n#     try:\n#         tree = ET.parse(xml_file)\n#         root = tree.getroot()\n#     except ET.ParseError:\n#         print(f\"Error parsing {xml_file}\")\n#         return None, None\n#     except FileNotFoundError:\n#         print(f\"Error: XML file not found at {xml_file}\")\n#         return None, None\n\n#     img_width = None\n#     img_height = None\n#     try:\n#         img_width = int(root.find('.//original_size/width').text)\n#         img_height = int(root.find('.//original_size/height').text)\n#     except AttributeError:\n#          print(f\"Warning: Could not find original_size in {xml_file}. Dimensions needed.\")\n\n#     frame_annotations = {} # {ped_id: {'bbox_pixels': [xtl,ytl,xbr,ybr], 'cross': label}}\n#     pedestrian_tracks = root.findall('.//track[@label=\"pedestrian\"]')\n#     if not pedestrian_tracks:\n#         print(f\"Warning: No tracks with label='pedestrian' found in {xml_file}\")\n#         return {}, (img_width, img_height) if img_width else None\n\n#     found_frame_annotation = False\n#     for track in pedestrian_tracks:\n#         # Determine ped_id for the track (robustly checking different places)\n#         ped_id = None\n#         track_id_elem = track.find('.//attribute[@name=\"id\"]')\n#         if track_id_elem is not None:\n#             ped_id = track_id_elem.text\n#         if ped_id is None: # Fallback to first box if not on track element\n#              first_box = track.find('.//box')\n#              if first_box is not None:\n#                  id_elem = first_box.find('.//attribute[@name=\"id\"]')\n#                  if id_elem is not None:\n#                      ped_id = id_elem.text\n\n#         # Iterate boxes within this track\n#         for box in track.findall('.//box'):\n#             # Verify ID consistency if possible (optional, can be complex)\n#             current_box_id_elem = box.find('.//attribute[@name=\"id\"]')\n#             current_box_id = current_box_id_elem.text if current_box_id_elem is not None else ped_id\n#             if current_box_id is None: continue # Cannot process box without ID\n#             if ped_id is None: ped_id = current_box_id # Assign ID if track didn't have one\n\n#             frame = int(box.get('frame'))\n#             if frame == target_frame:\n#                 found_frame_annotation = True\n#                 xtl = float(box.get('xtl'))\n#                 ytl = float(box.get('ytl'))\n#                 xbr = float(box.get('xbr'))\n#                 ybr = float(box.get('ybr'))\n#                 occluded = int(box.get('occluded', 0))\n#                 if occluded > 0: print(f\"Note: Pedestrian {ped_id} is occluded in frame {target_frame}\")\n\n#                 # Find 'cross' attribute\n#                 cross_status = \"unknown\"\n#                 cross_elem = box.find('.//attribute[@name=\"cross\"]')\n#                 if cross_elem is not None: cross_status = cross_elem.text\n#                 cross_label = 1 if cross_status == 'crossing' else 0\n\n#                 if ped_id is not None:\n#                     frame_annotations[ped_id] = {\n#                         'bbox_pixels': [xtl, ytl, xbr, ybr],\n#                         'cross': cross_label}\n#                 else: print(f\"Warning: Found annotation box for frame {target_frame} but couldn't determine ped_id.\")\n#                 break # Found target frame in this track, move to next track\n\n#     # Check results after iterating all tracks\n#     if not found_frame_annotation:\n#          print(f\"Warning: Frame number {target_frame} was not found within any pedestrian track in {xml_file}.\")\n#          return {}, (img_width, img_height) if img_width else None # Return empty dict\n#     elif not frame_annotations:\n#          print(f\"Warning: Frame {target_frame} found, but no valid pedestrian annotations could be extracted (e.g., missing IDs).\")\n#          return {}, (img_width, img_height) if img_width else None # Return empty dict\n\n#     return frame_annotations, (img_width, img_height)\n\n# # --- Main Verification Logic ---\n\n# # 1. Load YOLO Pose Model\n# print(f\"Loading YOLO model: {YOLO_MODEL_PATH}\")\n# # Allow ultralytics to download if path is just the name and exists in their repo\n# # Or load from specific path if it exists\n# if not os.path.exists(YOLO_MODEL_PATH) and YOLO_MODEL_PATH not in ['yolov8s-pose.pt', 'yolov8n-pose.pt', 'yolov8m-pose.pt', 'yolov8l-pose.pt', 'yolov8x-pose.pt']:\n#     raise FileNotFoundError(f\"YOLO model not found at specified path: {YOLO_MODEL_PATH} and is not a standard downloadable name.\")\n# pose_model = YOLO(YOLO_MODEL_PATH)\n# pose_model.to(DEVICE)\n# print(\"YOLO model loaded.\")\n\n# # 2. Construct the expected XML annotation filename\n# xml_filename = f\"{VIDEO_ID}_annt.xml\" # Assuming _annt suffix based on previous errors\n# xml_filepath = os.path.join(ANNOTATION_DIR, xml_filename)\n# print(f\"Looking for annotation file: {xml_filepath}\")\n\n# # Check video file existence\n# if not os.path.exists(VIDEO_PATH):\n#     raise FileNotFoundError(f\"Video file not found: {VIDEO_PATH}\")\n# print(f\"Found video file: {VIDEO_PATH}\")\n\n# # 3. Parse Annotations for the RANDOMLY SELECTED target frame\n# print(f\"Parsing annotations for frame {TARGET_FRAME_XML} from {xml_filename}...\")\n# annotations, img_dims_ann = parse_specific_frame_annotation(xml_filepath, TARGET_FRAME_XML)\n\n# # Handle parsing results\n# if annotations is None:\n#      # Error message already printed inside function if file not found/parsed\n#      raise ValueError(f\"Failed to parse or access annotation file {xml_filepath}\")\n# elif not annotations: # Check if dictionary is empty\n#      print(f\"Warning: No pedestrian annotations were successfully extracted for frame {TARGET_FRAME_XML} in {xml_filepath}.\")\n#      # Proceed to show frame anyway for visual inspection\n#      pass\n# else:\n#     print(f\"Found {len(annotations)} pedestrian annotation(s) in frame {TARGET_FRAME_XML}.\")\n\n\n# # 4. Open Video and Read the Target Frame\n# print(f\"Opening video file: {VIDEO_PATH}\")\n# cap = cv2.VideoCapture(VIDEO_PATH)\n# if not cap.isOpened(): raise IOError(f\"Error: Could not open video: {VIDEO_PATH}\")\n\n# # Get video properties\n# vid_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n# vid_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n# total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n# fps = cap.get(cv2.CAP_PROP_FPS)\n# print(f\"Video properties: {vid_width}x{vid_height}, {total_frames} frames, {fps:.2f} FPS\")\n\n# # Compare dimensions if annotation dimensions were found\n# if img_dims_ann:\n#     ann_width, ann_height = img_dims_ann\n#     if ann_width != vid_width or ann_height != vid_height:\n#         print(f\"Warning: Annotation dimensions ({ann_width}x{ann_height}) do not match video dimensions ({vid_width}x{vid_height})!\")\n\n# # Frame numbers in OpenCV are 0-based\n# target_frame_cv = TARGET_FRAME_XML - 1\n\n# # Check if target frame index is valid for the video\n# if target_frame_cv < 0 or target_frame_cv >= total_frames:\n#     cap.release()\n#     raise IndexError(f\"Error: Target frame {TARGET_FRAME_XML} (index {target_frame_cv}) is out of video bounds (0-{total_frames-1}).\")\n\n# # Seek to the frame and read it\n# print(f\"Seeking to frame index {target_frame_cv} (Annotation frame {TARGET_FRAME_XML})...\")\n# cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame_cv)\n# ret, frame = cap.read()\n# if not ret:\n#     cap.release()\n#     raise IOError(f\"Error: Failed to read frame at index {target_frame_cv} from video {VIDEO_PATH}.\")\n\n# print(\"Frame read successfully.\")\n# cap.release() # Release video file\n\n# # 5. Run YOLO Pose Detection on the Frame\n# print(\"Running YOLO pose detection...\")\n# frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Convert for YOLO\n# results = pose_model(frame_rgb, device=DEVICE, verbose=False) # Run inference\n\n# # Determine number of detected persons\n# detected_persons = 0\n# if results and hasattr(results[0], 'boxes') and results[0].boxes is not None:\n#      # Use .shape[0] to count detected boxes\n#      if hasattr(results[0].boxes, 'shape'):\n#           detected_persons = results[0].boxes.shape[0]\n#      elif hasattr(results[0].boxes, 'data'): # Fallback if shape isn't direct attribute\n#           detected_persons = results[0].boxes.data.shape[0]\n\n# print(f\"YOLO detected {detected_persons} potential persons.\")\n\n\n# # 6. Draw Annotations and Detections\n# vis_frame = frame.copy() # Create a copy to draw on\n\n# # Draw Ground Truth Annotations (Green)\n# if annotations: # Only draw if annotations were found for this frame\n#     for ped_id, data in annotations.items():\n#         xtl, ytl, xbr, ybr = [int(c) for c in data['bbox_pixels']]\n#         cv2.rectangle(vis_frame, (xtl, ytl), (xbr, ybr), (0, 255, 0), 2) # Green rectangle\n#         cv2.putText(vis_frame, f\"Ann ID: {ped_id}\", (xtl, ytl - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1) # Green text\n\n# # Draw YOLO Detections (Blue Boxes, Red Keypoints)\n# # Check necessary attributes exist before proceeding\n# if detected_persons > 0 \\\n#    and hasattr(results[0], 'keypoints') and results[0].keypoints is not None \\\n#    and hasattr(results[0].boxes, 'xyxy') \\\n#    and hasattr(results[0].keypoints, 'data'):\n\n#     boxes = results[0].boxes.xyxy.cpu().numpy()\n#     keypoints_data = results[0].keypoints.data.cpu().numpy()\n\n#     # Verify consistency between boxes and keypoints array shapes\n#     if len(boxes) > 0 and keypoints_data.ndim >= 2 and len(boxes) == keypoints_data.shape[0]:\n#         # Loop through each detected person 'i'\n#         for i in range(len(boxes)):\n#             # Draw detection bounding box\n#             det_xtl, det_ytl, det_xbr, det_ybr = [int(c) for c in boxes[i]]\n#             cv2.rectangle(vis_frame, (det_xtl, det_ytl), (det_xbr, det_ybr), (255, 0, 0), 1) # Blue rectangle\n\n#             # Get keypoints for this person 'i'\n#             kps = keypoints_data[i] # Expected shape: (num_keypoints, 3)\n\n#             # Check if keypoints have the expected structure (17 keypoints, 3 values: x,y,conf)\n#             if kps.ndim == 2 and kps.shape[1] == 3:\n#                  num_keypoints = kps.shape[0] # Should typically be 17\n#                  # Loop through each keypoint 'kp_idx' for this person\n#                  for kp_idx in range(num_keypoints):\n#                      x    = kps[kp_idx, 0]\n#                      y    = kps[kp_idx, 1]\n#                      conf = kps[kp_idx, 2]\n#                      # Only draw keypoints with confidence above threshold and valid coordinates\n#                      if conf > 0.3 and x > 0 and y > 0: # Add check for non-zero coords if needed\n#                          cv2.circle(vis_frame, (int(x), int(y)), 3, (0, 0, 255), -1) # Red dot\n#             else:\n#                  # Print warning if shape is unexpected for this specific person's keypoints\n#                  print(f\"Warning: Unexpected keypoints shape for detection {i}: {kps.shape}\")\n#     elif detected_persons > 0: # Print warning only if detections were expected but shapes mismatch\n#         print(f\"Warning: Mismatch or issue between boxes ({len(boxes)}) and keypoints ({keypoints_data.shape}). Skipping keypoint drawing.\")\n\n\n# # 7. Display the Result\n# print(\"Displaying the frame. Annotations=Green boxes. YOLO=Blue boxes, Red keypoints.\")\n# plt.figure(figsize=(16, 10)) # Use a larger figure size\n# plt.imshow(cv2.cvtColor(vis_frame, cv2.COLOR_BGR2RGB)) # Convert BGR (cv2) to RGB (matplotlib)\n# plt.title(f\"Video: {VIDEO_ID} - Frame: {TARGET_FRAME_XML} (Index: {target_frame_cv})\")\n# plt.axis('off') # Hide axes\n# plt.tight_layout() # Adjust layout\n# plt.show()\n\n# print(\"Verification script finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:44:07.638501Z","iopub.execute_input":"2025-04-14T12:44:07.638720Z","iopub.status.idle":"2025-04-14T12:44:07.651702Z","shell.execute_reply.started":"2025-04-14T12:44:07.638693Z","shell.execute_reply":"2025-04-14T12:44:07.651044Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# !wget -P /kaggle/working https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-pose.pt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:44:07.652500Z","iopub.execute_input":"2025-04-14T12:44:07.652771Z","iopub.status.idle":"2025-04-14T12:44:07.667334Z","shell.execute_reply.started":"2025-04-14T12:44:07.652744Z","shell.execute_reply":"2025-04-14T12:44:07.666537Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# # !git clone https://github.com/aras62/PIE.git # Assuming already cloned or data exists\n# # !unzip /content/PIE/annotations/annotations.zip -d /content/PIE # Assuming already unzipped\n# # !unzip /content/PIE/annotations/annotations_vehicle.zip -d /content/PIE # Assuming already unzipped\n\n# # !pip install -q ultralytics opencv-python-headless # Assuming already installed\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import Dataset, DataLoader\n# import xml.etree.ElementTree as ET\n# import os\n# import numpy as np\n# from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n# from tqdm.notebook import tqdm\n# import random\n# import math\n# import zipfile\n# import cv2 # Still needed for helper functions if used\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# import pickle # Needed to load PKL files\n# import time # Added for training time measurement\n\n# # --- Configuration ---\n# BASE_VIDEO_INPUT_DIR = '/kaggle/input' # Where pie-setXX video dataset folders are\n# ANNOTATION_DIR = '/kaggle/working/PIE/annotations/annotations' # Where setXX annotation XML folders are\n\n# # <<< --- Corrected Path to Pre-extracted Pose Data --- >>>\n# POSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2' # Base dir containing setXX subdirs with PKLs\n# # <<< --- End Pose Data Path --- >>>\n\n# # Model Hyperparameters\n# SEQ_LEN = 30\n# PRED_LEN = 1\n# INPUT_SIZE_BBOX = 4\n# INPUT_SIZE_POSE = 34\n# LSTM_HIDDEN_SIZE = 256\n# NUM_LSTM_LAYERS = 2\n# DROPOUT_RATE = 0.3\n# NUM_CLASSES = 2\n# ATTENTION_DIM = 128\n\n# # Training Hyperparameters\n# LEARNING_RATE = 1e-3\n# BATCH_SIZE = 64\n# NUM_EPOCHS = 30\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {DEVICE}\")\n\n# TRAIN_SETS = ['set01', 'set02', 'set03', 'set04', 'set05']\n# VAL_SETS = ['set06']\n\n\n# # --- Helper Function: Compute IOU (Unchanged) ---\n# def compute_iou(boxA, boxB):\n#     xA = max(boxA[0], boxB[0]); yA = max(boxA[1], boxB[1])\n#     xB = min(boxA[2], boxB[2]); yB = min(boxA[3], boxB[3])\n#     interW = max(0, xB - xA); interH = max(0, yB - yA)\n#     interArea = interW * interH\n#     boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n#     boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n#     iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n#     return iou\n\n# # --- Data Preprocessing Functions (Unchanged from previous version) ---\n# def parse_annotations(xml_file):\n#     video_id = os.path.basename(xml_file).replace('_annt.xml', '').replace('.xml', '')\n#     try: tree = ET.parse(xml_file); root = tree.getroot()\n#     except (ET.ParseError, FileNotFoundError) as e: print(f\"Error parsing {xml_file}: {e}\"); return None, None, None\n#     try: img_width = int(root.find('.//original_size/width').text); img_height = int(root.find('.//original_size/height').text)\n#     except AttributeError: print(f\"Warning: Cannot find original_size in {xml_file}\"); img_width, img_height = -1, -1\n#     ped_tracks = {}\n#     for track in root.findall('.//track[@label=\"pedestrian\"]'):\n#         ped_id = None; track_id_elem = track.find('.//attribute[@name=\"id\"]');\n#         if track_id_elem is not None: ped_id = track_id_elem.text\n#         if ped_id is None:\n#              first_box = track.find('.//box');\n#              if first_box is not None: id_elem = first_box.find('.//attribute[@name=\"id\"]');\n#              if id_elem is not None: ped_id = id_elem.text\n#         if ped_id is None: continue\n#         for box in track.findall('.//box'):\n#             current_box_id_elem = box.find('.//attribute[@name=\"id\"]'); current_box_id = current_box_id_elem.text if current_box_id_elem is not None else ped_id\n#             if current_box_id != ped_id: pass\n#             frame = int(box.get('frame')); cross_status = \"unknown\"; cross_elem = box.find('.//attribute[@name=\"cross\"]')\n#             if cross_elem is not None: cross_status = cross_elem.text\n#             cross_label = 1 if cross_status == 'crossing' else 0\n#             try: xtl, ytl, xbr, ybr = float(box.get('xtl')), float(box.get('ytl')), float(box.get('xbr')), float(box.get('ybr'))\n#             except (TypeError, ValueError): print(f\"Warning: Invalid bbox coords F:{frame} P:{ped_id} in {xml_file}. Skip.\"); continue\n#             if img_width > 0 and img_height > 0:\n#                 center_x = ((xtl + xbr) / 2) / img_width; center_y = ((ytl + ybr) / 2) / img_height\n#                 width = (xbr - xtl) / img_width; height = (ybr - ytl) / img_height\n#                 if width <= 0 or height <= 0 or not (0 <= center_x <= 1) or not (0 <= center_y <= 1): continue\n#                 bbox_norm = [center_x, center_y, width, height]\n#             else: continue\n#             if ped_id not in ped_tracks: ped_tracks[ped_id] = {}\n#             ped_tracks[ped_id][frame] = {'bbox': bbox_norm, 'cross': cross_label}\n#     if not ped_tracks: return None, None, None\n#     return ped_tracks, (img_width, img_height), video_id\n\n# def create_sequences(ped_tracks, video_id, set_folder, seq_len, pred_len):\n#     sequences = [];\n#     if not ped_tracks: return sequences\n#     for ped_id, frames_data in ped_tracks.items():\n#         sorted_frames = sorted(frames_data.keys())\n#         for i in range(len(sorted_frames) - seq_len - pred_len + 1):\n#             seq_frames = sorted_frames[i : i + seq_len]; target_frame = sorted_frames[i + seq_len + pred_len - 1]\n#             is_continuous = all(seq_frames[j+1] - seq_frames[j] == 1 for j in range(len(seq_frames)-1))\n#             is_target_continuous = (target_frame - seq_frames[-1] == pred_len)\n#             if not (is_continuous and is_target_continuous): continue\n#             bbox_seq = [frames_data[f]['bbox'] for f in seq_frames]; label = frames_data[target_frame]['cross']\n#             sequence = {'bbox': np.array(bbox_seq, dtype=np.float32), 'label': label, 'frames': seq_frames,\n#                         'set_folder': set_folder, 'video_id': video_id, 'ped_id': ped_id}\n#             sequences.append(sequence)\n#     return sequences\n\n# # --- Dataset Class (Corrected PKL Loading Path) ---\n# class PIEDataset(Dataset):\n#     def __init__(self, annotation_dir, set_folders, pose_data_dir, seq_len, pred_len):\n#         self.seq_len = seq_len\n#         self.pred_len = pred_len\n#         self.sequences = []\n#         self.pose_data_dir = pose_data_dir\n#         self.all_pose_data = {} # Store loaded pose data here: {set_id: {video_id: {frame: {ped_id: pose_vec}}}}\n\n#         # --- Load All Relevant Pose PKL Data ---\n#         print(f\"Loading pose data for sets: {set_folders} from {self.pose_data_dir}\")\n#         sets_loaded_count = 0\n#         for set_id in tqdm(set_folders, desc=\"Loading Pose Sets\"):\n#             self.all_pose_data[set_id] = {}\n#             expected_pkl_files = []\n#             annotation_set_folder_path = os.path.join(annotation_dir, set_id)\n#             if not os.path.isdir(annotation_set_folder_path): print(f\"Warning: Annotation dir not found for {set_id}, cannot determine PKLs.\"); continue\n#             try:\n#                 xml_filenames = [f for f in os.listdir(annotation_set_folder_path) if f.endswith('_annt.xml')]\n#                 expected_pkl_files = [f\"{set_id}_{f.replace('_annt.xml', '')}_poses.pkl\" for f in xml_filenames]\n#             except Exception as e: print(f\"Warning: Error listing XMLs for {set_id}: {e}\")\n#             if not expected_pkl_files: print(f\"Warning: No expected PKL files for {set_id}.\"); continue\n\n#             loaded_video_count = 0\n#             for pkl_filename in tqdm(expected_pkl_files, desc=f\"Loading PKLs for {set_id}\", leave=False):\n#                 # --- *** CORRECTED PATH CONSTRUCTION *** ---\n#                 # Look inside the setXX subdirectory within POSE_DATA_DIR\n#                 pkl_file_path = os.path.join(self.pose_data_dir, set_id, pkl_filename)\n#                 # --- *** END CORRECTION *** ---\n#                 try:\n#                     with open(pkl_file_path, 'rb') as f:\n#                         loaded_pkl_content = pickle.load(f)\n#                         if len(loaded_pkl_content) != 1: print(f\"Warning: PKL {pkl_filename} format issue. Skip.\"); continue\n#                         unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n#                         video_id = \"_\".join(unique_video_key.split('_')[1:])\n#                         self.all_pose_data[set_id][video_id] = video_data\n#                         loaded_video_count += 1\n#                 except FileNotFoundError: pass # Silently ignore missing PKLs\n#                 except Exception as e: print(f\"Error loading pose PKL {pkl_file_path}: {e}\")\n\n#             print(f\" -> Loaded pose data for {loaded_video_count}/{len(expected_pkl_files)} videos in {set_id}\")\n#             if loaded_video_count > 0: sets_loaded_count+=1\n#         print(f\"Finished loading pose data for {sets_loaded_count} sets.\")\n#         # --- End Pose Data Loading ---\n\n#         # --- Load Sequence Information from Annotations ---\n#         print(f\"Loading sequence info from annotation sets: {set_folders}\")\n#         for set_folder_name in tqdm(set_folders, desc=\"Processing Annotation Sets\"):\n#             annotation_set_path = os.path.join(annotation_dir, set_folder_name)\n#             if not os.path.isdir(annotation_set_path): print(f\"Warning: Annotation dir not found for {set_folder_name}\"); continue\n#             xml_files = [f for f in os.listdir(annotation_set_path) if f.endswith('_annt.xml')]\n#             for xml_file in tqdm(xml_files, desc=f\"Processing {set_folder_name}\", leave=False):\n#                 file_path = os.path.join(annotation_set_path, xml_file)\n#                 ped_tracks, _, video_id = parse_annotations(file_path)\n#                 if ped_tracks and video_id:\n#                     video_sequences = create_sequences(ped_tracks, video_id, set_folder_name, seq_len, pred_len)\n#                     self.sequences.extend(video_sequences)\n#         # --- End Sequence Info Loading ---\n\n#         print(f\"Loaded {len(self.sequences)} sequences in total.\")\n#         if not self.sequences: raise ValueError(\"Dataset initialization failed: No sequences loaded.\")\n#         labels = [s['label'] for s in self.sequences]; count_0 = labels.count(0); count_1 = labels.count(1); print(f\"Class distribution: 0={count_0}, 1={count_1}\")\n\n#     def __len__(self): return len(self.sequences)\n\n#     def __getitem__(self, idx):\n#         # --- (This function remains unchanged from previous version) ---\n#         sequence_data = self.sequences[idx]\n#         bbox_features = torch.tensor(sequence_data['bbox'], dtype=torch.float32)\n#         set_folder = sequence_data['set_folder']; video_id = sequence_data['video_id']; ped_id = sequence_data['ped_id']\n#         pose_seq = []\n#         set_pose_data = self.all_pose_data.get(set_folder, {}); video_pose_data = set_pose_data.get(video_id, {})\n#         for frame_num in sequence_data['frames']:\n#             frame_data = video_pose_data.get(frame_num, {}); pose_vector = frame_data.get(ped_id)\n#             if pose_vector is not None:\n#                 if isinstance(pose_vector, np.ndarray): pose_seq.append(pose_vector)\n#                 else: print(f\"Warn: Type {type(pose_vector)} F:{frame_num} P:{ped_id} V:{video_id}. Use zeros.\"); pose_seq.append(np.zeros(INPUT_SIZE_POSE, dtype=np.float32))\n#             else: pose_seq.append(np.zeros(INPUT_SIZE_POSE, dtype=np.float32))\n#         try:\n#              pose_features = torch.tensor(np.array(pose_seq, dtype=np.float32), dtype=torch.float32)\n#              if pose_features.shape != (self.seq_len, INPUT_SIZE_POSE):\n#                   print(f\"Warn: Shape {pose_features.shape} idx {idx}. Exp ({self.seq_len}, {INPUT_SIZE_POSE}). Pad/Trunc or Use zeros.\")\n#                   pose_features = torch.zeros((self.seq_len, INPUT_SIZE_POSE), dtype=torch.float32)\n#         except Exception as e: print(f\"Error converting pose_seq idx {idx}: {e}. Use zeros.\"); pose_features = torch.zeros((self.seq_len, INPUT_SIZE_POSE), dtype=torch.float32)\n#         features = {'bbox': bbox_features, 'pose': pose_features}\n#         label = torch.tensor(sequence_data['label'], dtype=torch.long)\n#         return features, label\n\n# # --- Model Architecture (Unchanged) ---\n# class Attention(nn.Module):\n#     def __init__(self, hidden_dim, attention_dim): super(Attention, self).__init__(); self.attention_net = nn.Sequential(nn.Linear(hidden_dim, attention_dim), nn.Tanh(), nn.Linear(attention_dim, 1) )\n#     def forward(self, lstm_output): att_scores = self.attention_net(lstm_output).squeeze(2); att_weights = torch.softmax(att_scores, dim=1); context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1); return context_vector, att_weights\n# class MultiStreamAdaptiveLSTM(nn.Module):\n#     def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes, attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n#         super(MultiStreamAdaptiveLSTM, self).__init__(); self.stream_names = stream_names; self.lstms = nn.ModuleDict(); self.attentions = nn.ModuleDict()\n#         for name in self.stream_names: self.lstms[name] = nn.LSTM(input_sizes[name], lstm_hidden_size, num_lstm_layers, batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0, bidirectional=True); self.attentions[name] = Attention(lstm_hidden_size * 2 , attention_dim)\n#         combined_feature_dim = lstm_hidden_size * 2 * len(self.stream_names); self.dropout = nn.Dropout(dropout_rate); self.fc1 = nn.Linear(combined_feature_dim, combined_feature_dim // 2); self.relu = nn.ReLU(); self.fc2 = nn.Linear(combined_feature_dim // 2, num_classes)\n#     def forward(self, x):\n#         stream_context_vectors = []; stream_att_weights = {}\n#         for name in self.stream_names: lstm_out, _ = self.lstms[name](x[name]); context_vector, att_weights = self.attentions[name](lstm_out); stream_context_vectors.append(context_vector); stream_att_weights[name] = att_weights\n#         fused_features = torch.cat(stream_context_vectors, dim=1); out = self.dropout(fused_features); out = self.relu(self.fc1(out)); out = self.dropout(out); logits = self.fc2(out); return logits\n\n# # --- Training and Evaluation Functions (Unchanged) ---\n# def train_epoch(model, dataloader, optimizer, criterion, device):\n#     model.train(); total_loss = 0.0; all_preds = []; all_labels = []\n#     for features, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n#         input_features = {name: features[name].to(device) for name in model.stream_names}; labels = labels.to(device)\n#         optimizer.zero_grad(); outputs = model(input_features); loss = criterion(outputs, labels); loss.backward(); optimizer.step()\n#         total_loss += loss.item(); preds = torch.argmax(outputs, dim=1); all_preds.extend(preds.cpu().numpy()); all_labels.extend(labels.cpu().numpy())\n#     avg_loss = total_loss / len(dataloader); accuracy = accuracy_score(all_labels, all_preds); return avg_loss, accuracy\n# def evaluate_epoch(model, dataloader, criterion, device):\n#     model.eval(); total_loss = 0.0; all_labels = []; all_preds = []; all_probs = []\n#     with torch.no_grad():\n#         for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n#             input_features = {name: features[name].to(device) for name in model.stream_names}; labels = labels.to(device)\n#             outputs = model(input_features); loss = criterion(outputs, labels); total_loss += loss.item(); probs = torch.softmax(outputs, dim=1); preds = torch.argmax(probs, dim=1)\n#             all_labels.extend(labels.cpu().numpy()); all_preds.extend(preds.cpu().numpy()); all_probs.extend(probs.cpu().numpy())\n#     avg_loss = total_loss / len(dataloader); all_probs = np.array(all_probs); all_labels = np.array(all_labels); all_preds = np.array(all_preds)\n#     accuracy = accuracy_score(all_labels, all_preds); precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', pos_label=1, zero_division=0)\n#     auc = roc_auc_score(all_labels, all_probs[:, 1]) if len(np.unique(all_labels)) > 1 else float('nan'); return {'loss': avg_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}\n# def get_predictions_and_labels(model, dataloader, device):\n#     model.eval(); all_labels = []; all_preds = []\n#     with torch.no_grad():\n#         for features, labels in tqdm(dataloader, desc=\"Generating CM Data\", leave=False):\n#              input_features = {name: features[name].to(device) for name in model.stream_names}; labels = labels.to(device); outputs = model(input_features); preds = torch.argmax(outputs, dim=1)\n#              all_labels.extend(labels.cpu().numpy()); all_preds.extend(preds.cpu().numpy())\n#     return np.array(all_labels), np.array(all_preds)\n\n# # --- Main Execution (Unchanged) ---\n# if __name__ == '__main__':\n#     print(f\"Using Pose Data Directory: {POSE_DATA_DIR}\")\n#     if not os.path.isdir(POSE_DATA_DIR): raise FileNotFoundError(f\"Pose data directory not found: {POSE_DATA_DIR}\")\n#     train_dataset = PIEDataset(ANNOTATION_DIR, TRAIN_SETS, POSE_DATA_DIR, SEQ_LEN, PRED_LEN)\n#     val_dataset = PIEDataset(ANNOTATION_DIR, VAL_SETS, POSE_DATA_DIR, SEQ_LEN, PRED_LEN)\n#     if len(train_dataset) == 0 or len(val_dataset) == 0: raise ValueError(\"Dataset loading failed.\")\n#     train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n#     val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n#     input_sizes = {'bbox': INPUT_SIZE_BBOX, 'pose': INPUT_SIZE_POSE}\n#     model = MultiStreamAdaptiveLSTM(input_sizes=input_sizes, lstm_hidden_size=LSTM_HIDDEN_SIZE, num_lstm_layers=NUM_LSTM_LAYERS, num_classes=NUM_CLASSES, attention_dim=ATTENTION_DIM, dropout_rate=DROPOUT_RATE).to(DEVICE)\n#     print(\"\\n--- Model Architecture ---\"); print(model); print(\"-\" * 30)\n#     criterion = nn.CrossEntropyLoss(); optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n#     best_val_f1 = -1.0; train_losses, val_losses = [], []; train_accs, val_accs = [], []; val_f1s = []\n#     print(\"\\n--- Starting Training ---\")\n#     for epoch in range(NUM_EPOCHS):\n#         epoch_start_time = time.time(); train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE); val_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE); epoch_duration = time.time() - epoch_start_time\n#         train_losses.append(train_loss); val_losses.append(val_metrics['loss']); train_accs.append(train_acc); val_accs.append(val_metrics['accuracy']); val_f1s.append(val_metrics['f1'])\n#         print(f\"--- Epoch {epoch+1}/{NUM_EPOCHS} ({epoch_duration:.2f} sec) ---\")\n#         print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\"); print(f\"  Val Loss:   {val_metrics['loss']:.4f}, Val Acc:  {val_metrics['accuracy']:.4f}\"); print(f\"  Val Prec:   {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\"); print(f\"  Val AUC:    {val_metrics['auc']:.4f}\")\n#         if val_metrics['f1'] > best_val_f1: best_val_f1 = val_metrics['f1']; torch.save(model.state_dict(), 'best_model_with_pose.pth'); print(f\"  >> Saved new best model with F1: {best_val_f1:.4f}\")\n#         print(\"-\" * 30)\n#     print(\"--- Training Finished ---\")\n#     print(\"\\n--- Plotting Training History ---\")\n#     fig, axes = plt.subplots(1, 2, figsize=(12, 5)); axes[0].plot(range(1, NUM_EPOCHS + 1), train_losses, label='Train Loss'); axes[0].plot(range(1, NUM_EPOCHS + 1), val_losses, label='Val Loss'); axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].set_title('Loss Curve'); axes[0].legend(); axes[0].grid(True)\n#     axes[1].plot(range(1, NUM_EPOCHS + 1), train_accs, label='Train Accuracy'); axes[1].plot(range(1, NUM_EPOCHS + 1), val_accs, label='Val Accuracy'); axes[1].plot(range(1, NUM_EPOCHS + 1), val_f1s, label='Val F1-Score', linestyle='--'); axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Metric'); axes[1].set_title('Accuracy & F1-Score Curve'); axes[1].legend(); axes[1].grid(True)\n#     plt.tight_layout(); plt.show()\n#     print(\"\\n--- Final Evaluation on Validation Set using Best Model ---\")\n#     if os.path.exists('best_model_with_pose.pth'): print(\"Loading best saved model 'best_model_with_pose.pth'\"); model.load_state_dict(torch.load('best_model_with_pose.pth', map_location=DEVICE))\n#     else: print(\"Warning: No saved best model found. Evaluating final model.\")\n#     final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE); true_labels, pred_labels = get_predictions_and_labels(model, val_loader, DEVICE); cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1])\n#     labels_display = ['Not Crossing', 'Crossing']; print(\"\\n--- Final Performance Metrics ---\"); print(f\"  Accuracy:  {final_metrics['accuracy']:.4f}\"); print(f\"  Precision: {final_metrics['precision']:.4f}\"); print(f\"  Recall:    {final_metrics['recall']:.4f}\"); print(f\"  F1 Score:  {final_metrics['f1']:.4f}\"); print(f\"  AUC:       {final_metrics['auc']:.4f}\"); print(f\"  Loss:      {final_metrics['loss']:.4f}\")\n#     print(\"\\n--- Confusion Matrix ---\"); disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_display); disp.plot(cmap=plt.cm.Blues); plt.title('Confusion Matrix (Validation Set)'); plt.show()\n#     print(\"\\n--- Script Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:44:07.668234Z","iopub.execute_input":"2025-04-14T12:44:07.668514Z","iopub.status.idle":"2025-04-14T12:44:07.682132Z","shell.execute_reply.started":"2025-04-14T12:44:07.668486Z","shell.execute_reply":"2025-04-14T12:44:07.681289Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# # Uncomment the following lines if you need to clone or unzip resources\n# # !git clone https://github.com/aras62/PIE.git  # Assuming already cloned or data exists\n# # !unzip /content/PIE/annotations/annotations.zip -d /content/PIE  # Assuming already unzipped\n# # !unzip /content/PIE/annotations/annotations_vehicle.zip -d /content/PIE  # Assuming already unzipped\n\n# # !pip install -q ultralytics opencv-python-headless  # Assuming already installed\n\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import Dataset, DataLoader\n# import xml.etree.ElementTree as ET\n# import os\n# import numpy as np\n# from sklearn.metrics import (accuracy_score, precision_recall_fscore_support, \n#                              roc_auc_score, confusion_matrix, ConfusionMatrixDisplay)\n# from tqdm.notebook import tqdm\n# import random\n# import math\n# import zipfile\n# import cv2  # Keep for potential future use or helper functions\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# import pickle  # Needed to load PKL files\n# import time\n\n# # --- Configuration ---\n# BASE_VIDEO_INPUT_DIR = '/kaggle/input'  # Where pie-setXX video dataset folders are\n# ANNOTATION_DIR = '/kaggle/working/PIE/annotations/annotations'  # Where setXX annotation XML folders are (*_annt.xml)\n# EGO_DATA_DIR = '/kaggle/working/PIE/annotations/annotations_vehicle'  # Where setXX vehicle XML folders are (*.xml)\n# POSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2'  # Where setXX subdirs with PKLs are\n\n# # --- Stream Control ---\n# # <<< --- Comment out streams here to disable them --- >>>\n# ACTIVE_STREAMS = [\n#     'bbox',\n#     # 'pose',\n#     'ego_speed',\n#     # 'scene',\n# ]\n# # <<< --- End Stream Control --- >>>\n# print(f\"Active Streams: {ACTIVE_STREAMS}\")\n\n# # Model Hyperparameters\n# SEQ_LEN = 30\n# PRED_LEN = 1\n# INPUT_SIZE_BBOX = 4\n# INPUT_SIZE_POSE = 34\n# INPUT_SIZE_EGO = 1  # Speed (normalized)\n# INPUT_SIZE_SCENE = 4  # Traffic Light State (one-hot: None/Unknown, Red, Yellow, Green)\n# LSTM_HIDDEN_SIZE = 256\n# NUM_LSTM_LAYERS = 2\n# DROPOUT_RATE = 0.3\n# NUM_CLASSES = 2\n# ATTENTION_DIM = 128\n\n# # Training Hyperparameters\n# LEARNING_RATE = 1e-4\n# BATCH_SIZE = 64\n# NUM_EPOCHS = 20\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {DEVICE}\")\n\n# TRAIN_SETS = ['set01', 'set02', 'set03', 'set04', 'set05']\n# VAL_SETS = ['set06']\n\n# # Normalization Constants (Adjust if necessary)\n# MAX_EGO_SPEED = 80.0  # km/h, used for simple normalization\n\n# # Traffic Light State Mapping\n# TL_STATE_MAP = {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}\n# NUM_TL_STATES = len(TL_STATE_MAP)  # Should match INPUT_SIZE_SCENE if using one-hot\n\n# # --- Helper Functions ---\n# def compute_iou(boxA, boxB):\n#     xA = max(boxA[0], boxB[0])\n#     yA = max(boxA[1], boxB[1])\n#     xB = min(boxA[2], boxB[2])\n#     yB = min(boxA[3], boxB[3])\n#     interW = max(0, xB - xA)\n#     interH = max(0, yB - yA)\n#     interArea = interW * interH\n#     boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n#     boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n#     iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n#     return iou\n\n# def parse_ego_vehicle_data(ego_xml_file):\n#     \"\"\"\n#     Parse ego vehicle speed data from an XML file.\n#     \"\"\"\n#     ego_data = {}  # {frame_num: speed}\n#     try:\n#         tree = ET.parse(ego_xml_file)\n#         root = tree.getroot()\n#         for frame_elem in root.findall('.//frame'):\n#             try:\n#                 frame_num = int(frame_elem.get('id'))\n#                 speed = float(frame_elem.get('OBD_speed', 0.0))\n#                 if speed == 0.0:\n#                     speed = float(frame_elem.get('GPS_speed', 0.0))\n#                 ego_data[frame_num] = speed\n#             except (TypeError, ValueError, AttributeError):\n#                 pass\n#     except (ET.ParseError, FileNotFoundError) as e:\n#         print(f\"Warning: Could not parse ego {ego_xml_file}: {e}\")\n#     return ego_data\n\n# def parse_annotations(xml_file):\n#     \"\"\"\n#     Parse pedestrian and traffic light annotations from an XML file.\n#     \"\"\"\n#     video_id = os.path.basename(xml_file).replace('_annt.xml', '').replace('.xml', '')\n#     ped_tracks = {}\n#     traffic_light_states = {}\n#     try:\n#         tree = ET.parse(xml_file)\n#         root = tree.getroot()\n#     except (ET.ParseError, FileNotFoundError) as e:\n#         print(f\"Error parsing {xml_file}: {e}\")\n#         return None, None, None, None\n\n#     try:\n#         img_width = int(root.find('.//original_size/width').text)\n#         img_height = int(root.find('.//original_size/height').text)\n#     except AttributeError:\n#         print(f\"Warning: Cannot find original_size in {xml_file}\")\n#         img_width, img_height = -1, -1\n\n#     # Process pedestrian tracks\n#     for track in root.findall('.//track[@label=\"pedestrian\"]'):\n#         ped_id = None\n#         track_id_elem = track.find('.//attribute[@name=\"id\"]')\n#         if track_id_elem is not None:\n#             ped_id = track_id_elem.text\n#         if ped_id is None:\n#             first_box = track.find('.//box')\n#             if first_box is not None:\n#                 id_elem = first_box.find('.//attribute[@name=\"id\"]')\n#                 if id_elem is not None:\n#                     ped_id = id_elem.text\n#         if ped_id is None:\n#             continue\n#         for box in track.findall('.//box'):\n#             current_box_id_elem = box.find('.//attribute[@name=\"id\"]')\n#             current_box_id = current_box_id_elem.text if current_box_id_elem is not None else ped_id\n#             # This check is redundant in current code but preserved in case of logic changes\n#             if current_box_id != ped_id:\n#                 pass\n#             frame = int(box.get('frame'))\n#             cross_status = \"unknown\"\n#             cross_elem = box.find('.//attribute[@name=\"cross\"]')\n#             if cross_elem is not None:\n#                 cross_status = cross_elem.text\n#             cross_label = 1 if cross_status == 'crossing' else 0\n#             try:\n#                 xtl = float(box.get('xtl'))\n#                 ytl = float(box.get('ytl'))\n#                 xbr = float(box.get('xbr'))\n#                 ybr = float(box.get('ybr'))\n#             except (TypeError, ValueError):\n#                 continue\n#             if img_width > 0 and img_height > 0:\n#                 center_x = ((xtl + xbr) / 2) / img_width\n#                 center_y = ((ytl + ybr) / 2) / img_height\n#                 width = (xbr - xtl) / img_width\n#                 height = (ybr - ytl) / img_height\n#                 if width <= 0 or height <= 0 or not (0 <= center_x <= 1) or not (0 <= center_y <= 1):\n#                     continue\n#                 bbox_norm = [center_x, center_y, width, height]\n#             else:\n#                 continue\n#             if ped_id not in ped_tracks:\n#                 ped_tracks[ped_id] = {}\n#             ped_tracks[ped_id][frame] = {'bbox': bbox_norm, 'cross': cross_label}\n\n#     # Process traffic light annotations\n#     processed_frames_tl = set()\n#     for track in root.findall('.//track[@label=\"traffic_light\"]'):\n#         for box in track.findall('.//box'):\n#             try:\n#                 frame = int(box.get('frame'))\n#                 if frame in processed_frames_tl:\n#                     continue\n#                 state_elem = box.find('.//attribute[@name=\"state\"]')\n#                 if state_elem is not None:\n#                     state_str = state_elem.text\n#                     if state_str != '__undefined__':\n#                         traffic_light_states[frame] = state_str\n#                         processed_frames_tl.add(frame)\n#             except (TypeError, ValueError, AttributeError):\n#                 continue\n\n#     if not ped_tracks:\n#         return None, None, None, None\n\n#     return ped_tracks, traffic_light_states, (img_width, img_height), video_id\n\n# def create_sequences(ped_tracks, video_id, set_folder, seq_len, pred_len):\n#     \"\"\"\n#     Create a list of sequence dictionaries from the pedestrian tracks.\n#     \"\"\"\n#     sequences = []\n#     if not ped_tracks:\n#         return sequences\n#     for ped_id, frames_data in ped_tracks.items():\n#         sorted_frames = sorted(frames_data.keys())\n#         for i in range(len(sorted_frames) - seq_len - pred_len + 1):\n#             seq_frames = sorted_frames[i : i + seq_len]\n#             target_frame = sorted_frames[i + seq_len + pred_len - 1]\n#             is_continuous = all(seq_frames[j+1] - seq_frames[j] == 1 for j in range(len(seq_frames) - 1))\n#             is_target_continuous = (target_frame - seq_frames[-1] == pred_len)\n#             if not (is_continuous and is_target_continuous):\n#                 continue\n#             bbox_seq = [frames_data[f]['bbox'] for f in seq_frames]\n#             label = frames_data[target_frame]['cross']\n#             sequence = {\n#                 'bbox': np.array(bbox_seq, dtype=np.float32),\n#                 'label': label,\n#                 'frames': seq_frames,\n#                 'set_folder': set_folder,\n#                 'video_id': video_id,\n#                 'ped_id': ped_id,\n#             }\n#             sequences.append(sequence)\n#     return sequences\n\n# # --- Dataset Class ---\n# class PIEDataset(Dataset):\n#     def __init__(self, annotation_dir, ego_data_dir, set_folders, pose_data_dir, seq_len, pred_len):\n#         self.seq_len = seq_len\n#         self.pred_len = pred_len\n#         self.sequences = []\n#         self.pose_data_dir = pose_data_dir\n#         self.all_pose_data = {}\n#         self.all_ego_data = {}\n#         self.all_traffic_light_data = {}\n\n#         print(f\"\\nLoading pose data for sets: {set_folders} from {self.pose_data_dir}\")\n#         sets_loaded_count_pose = 0\n#         for set_id in tqdm(set_folders, desc=\"Loading Pose Sets\"):\n#             self.all_pose_data[set_id] = {}\n#             expected_pkl_files = []\n#             annotation_set_folder_path = os.path.join(annotation_dir, set_id)\n#             if not os.path.isdir(annotation_set_folder_path):\n#                 continue\n#             try:\n#                 xml_filenames = [f for f in os.listdir(annotation_set_folder_path)\n#                                  if f.endswith('_annt.xml')]\n#                 expected_pkl_files = [f\"{set_id}_{f.replace('_annt.xml', '')}_poses.pkl\" for f in xml_filenames]\n#             except Exception as e:\n#                 print(f\"Warning: Error listing XMLs for {set_id}: {e}\")\n#             if not expected_pkl_files:\n#                 continue\n#             loaded_video_count = 0\n#             for pkl_filename in tqdm(expected_pkl_files, desc=f\"Loading PKLs for {set_id}\", leave=False):\n#                 pkl_file_path = os.path.join(self.pose_data_dir, set_id, pkl_filename)\n#                 try:\n#                     with open(pkl_file_path, 'rb') as f:\n#                         loaded_pkl_content = pickle.load(f)\n#                     if len(loaded_pkl_content) != 1:\n#                         continue\n#                     unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n#                     video_id = \"_\".join(unique_video_key.split('_')[1:])\n#                     self.all_pose_data[set_id][video_id] = video_data\n#                     loaded_video_count += 1\n#                 except FileNotFoundError:\n#                     pass\n#                 except Exception as e:\n#                     print(f\"Error loading pose PKL {pkl_file_path}: {e}\")\n#             if loaded_video_count > 0:\n#                 sets_loaded_count_pose += 1\n#         print(f\"Finished loading pose data for {sets_loaded_count_pose} sets.\")\n\n#         print(f\"\\nLoading ego vehicle data for sets: {set_folders} from {ego_data_dir}\")\n#         sets_loaded_count_ego = 0\n#         for set_id in tqdm(set_folders, desc=\"Loading Ego Data Sets\"):\n#             self.all_ego_data[set_id] = {}\n#             ego_set_path = os.path.join(ego_data_dir, set_id)\n#             if not os.path.isdir(ego_set_path):\n#                 print(f\"Warning: Ego data dir not found for {set_id}.\")\n#                 continue\n#             try:\n#                 ego_xml_files = [f for f in os.listdir(ego_set_path)\n#                                  if f.endswith('.xml') and not f.endswith('_annt.xml')]\n#             except Exception as e:\n#                 print(f\"Warning: Error listing ego XMLs for {set_id}: {e}\")\n#                 continue\n#             loaded_ego_count = 0\n#             for ego_xml_file in tqdm(ego_xml_files, desc=f\"Loading Ego XMLs for {set_id}\", leave=False):\n#                 video_id = ego_xml_file.replace('.xml', '')\n#                 full_ego_xml_path = os.path.join(ego_set_path, ego_xml_file)\n#                 self.all_ego_data[set_id][video_id] = parse_ego_vehicle_data(full_ego_xml_path)\n#                 loaded_ego_count += 1\n#             if loaded_ego_count > 0:\n#                 sets_loaded_count_ego += 1\n#         print(f\"Finished loading ego data for {sets_loaded_count_ego} sets.\")\n\n#         print(f\"\\nLoading sequence info & scene context from annotation sets: {set_folders}\")\n#         sets_loaded_count_ann = 0\n#         for set_folder_name in tqdm(set_folders, desc=\"Processing Annotation Sets\"):\n#             self.all_traffic_light_data[set_folder_name] = {}\n#             annotation_set_path = os.path.join(annotation_dir, set_folder_name)\n#             if not os.path.isdir(annotation_set_path):\n#                 print(f\"Warning: Annotation dir not found for {set_folder_name}\")\n#                 continue\n#             xml_files = [f for f in os.listdir(annotation_set_path) if f.endswith('_annt.xml')]\n#             loaded_ann_count = 0\n#             for xml_file in tqdm(xml_files, desc=f\"Processing {set_folder_name}\", leave=False):\n#                 file_path = os.path.join(annotation_set_path, xml_file)\n#                 ped_tracks, traffic_lights, _, video_id = parse_annotations(file_path)\n#                 if ped_tracks and video_id:\n#                     loaded_ann_count += 1\n#                 if traffic_lights:\n#                     self.all_traffic_light_data[set_folder_name][video_id] = traffic_lights\n#                 video_sequences = create_sequences(ped_tracks, video_id, set_folder_name, seq_len, pred_len)\n#                 self.sequences.extend(video_sequences)\n#             if loaded_ann_count > 0:\n#                 sets_loaded_count_ann += 1\n#         print(f\"Finished loading annotations for {sets_loaded_count_ann} sets.\")\n\n#         print(f\"\\nLoaded {len(self.sequences)} sequences in total.\")\n#         if not self.sequences:\n#             raise ValueError(\"Dataset init failed: No sequences loaded.\")\n#         labels = [s['label'] for s in self.sequences]\n#         count_0 = labels.count(0)\n#         count_1 = labels.count(1)\n#         print(f\"Class distribution: 0={count_0}, 1={count_1}\")\n\n#     def __len__(self):\n#         return len(self.sequences)\n\n#     def __getitem__(self, idx):\n#         sequence_data = self.sequences[idx]\n#         set_folder = sequence_data['set_folder']\n#         video_id = sequence_data['video_id']\n#         ped_id = sequence_data['ped_id']\n#         frames = sequence_data['frames']\n#         bbox_seq = []\n#         pose_seq = []\n#         ego_speed_seq = []\n#         scene_seq = []\n\n#         set_pose_data = self.all_pose_data.get(set_folder, {})\n#         video_pose_data = set_pose_data.get(video_id, {})\n#         set_ego_data = self.all_ego_data.get(set_folder, {})\n#         video_ego_data = set_ego_data.get(video_id, {})\n#         set_tl_data = self.all_traffic_light_data.get(set_folder, {})\n#         video_tl_data = set_tl_data.get(video_id, {})\n\n#         for frame_num in frames:\n#             frame_idx_in_seq = frames.index(frame_num)\n#             bbox_seq.append(sequence_data['bbox'][frame_idx_in_seq])\n#             frame_pose_ped_data = video_pose_data.get(frame_num, {})\n#             pose_vector = frame_pose_ped_data.get(ped_id)\n#             if pose_vector is not None and isinstance(pose_vector, np.ndarray):\n#                 pose_seq.append(pose_vector)\n#             else:\n#                 pose_seq.append(np.zeros(INPUT_SIZE_POSE, dtype=np.float32))\n#             speed = video_ego_data.get(frame_num, 0.0)\n#             normalized_speed = np.clip(speed / MAX_EGO_SPEED, 0.0, 1.0)\n#             ego_speed_seq.append([normalized_speed])\n#             state_str = video_tl_data.get(frame_num, '__undefined__')\n#             state_int = TL_STATE_MAP.get(state_str, 0)\n#             state_one_hot = np.zeros(NUM_TL_STATES, dtype=np.float32)\n#             state_one_hot[state_int] = 1.0\n#             scene_seq.append(state_one_hot)\n\n#         try:\n#             bbox_features = torch.tensor(np.array(bbox_seq, dtype=np.float32), dtype=torch.float32)\n#             pose_features = torch.tensor(np.array(pose_seq, dtype=np.float32), dtype=torch.float32)\n#             ego_speed_features = torch.tensor(np.array(ego_speed_seq, dtype=np.float32), dtype=torch.float32)\n#             scene_features = torch.tensor(np.array(scene_seq, dtype=np.float32), dtype=torch.float32)\n#             if bbox_features.shape != (self.seq_len, INPUT_SIZE_BBOX):\n#                 raise ValueError(f\"Bbox shape mismatch {bbox_features.shape}\")\n#             if pose_features.shape != (self.seq_len, INPUT_SIZE_POSE):\n#                 raise ValueError(f\"Pose shape mismatch {pose_features.shape}\")\n#             if ego_speed_features.shape != (self.seq_len, INPUT_SIZE_EGO):\n#                 raise ValueError(f\"Ego shape mismatch {ego_speed_features.shape}\")\n#             if scene_features.shape != (self.seq_len, INPUT_SIZE_SCENE):\n#                 raise ValueError(f\"Scene shape mismatch {scene_features.shape}\")\n#         except Exception as e:\n#             print(f\"Error converting features idx {idx}: {e}. Return dummy.\")\n#             bbox_features = torch.zeros((self.seq_len, INPUT_SIZE_BBOX), dtype=torch.float32)\n#             pose_features = torch.zeros((self.seq_len, INPUT_SIZE_POSE), dtype=torch.float32)\n#             ego_speed_features = torch.zeros((self.seq_len, INPUT_SIZE_EGO), dtype=torch.float32)\n#             scene_features = torch.zeros((self.seq_len, INPUT_SIZE_SCENE), dtype=torch.float32)\n\n#         features = {\n#             'bbox': bbox_features,\n#             'pose': pose_features,\n#             'ego_speed': ego_speed_features,\n#             'scene': scene_features\n#         }\n#         label = torch.tensor(sequence_data['label'], dtype=torch.long)\n#         return features, label\n\n# # --- Model Architecture ---\n# class Attention(nn.Module):\n#     def __init__(self, hidden_dim, attention_dim):\n#         super(Attention, self).__init__()\n#         self.attention_net = nn.Sequential(\n#             nn.Linear(hidden_dim, attention_dim),\n#             nn.Tanh(),\n#             nn.Linear(attention_dim, 1)\n#         )\n\n#     def forward(self, lstm_output):\n#         att_scores = self.attention_net(lstm_output).squeeze(2)\n#         att_weights = torch.softmax(att_scores, dim=1)\n#         context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1)\n#         return context_vector, att_weights\n\n# class MultiStreamAdaptiveLSTM(nn.Module):\n#     def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes,\n#                  attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n#         super(MultiStreamAdaptiveLSTM, self).__init__()\n#         if not stream_names:\n#             raise ValueError(\"stream_names cannot be empty.\")\n#         self.stream_names = stream_names\n#         self.lstms = nn.ModuleDict()\n#         self.attentions = nn.ModuleDict()\n#         print(f\"Initializing model with streams: {self.stream_names}\")\n#         for name in self.stream_names:\n#             if name not in input_sizes:\n#                 raise KeyError(f\"Input size for stream '{name}' not provided.\")\n#             current_input_size = input_sizes[name]\n#             print(f\"  - Adding stream '{name}' with input size {current_input_size}\")\n#             self.lstms[name] = nn.LSTM(\n#                 current_input_size,\n#                 lstm_hidden_size,\n#                 num_lstm_layers,\n#                 batch_first=True,\n#                 dropout=dropout_rate if num_lstm_layers > 1 else 0,\n#                 bidirectional=True\n#             )\n#             self.attentions[name] = Attention(lstm_hidden_size * 2, attention_dim)\n#         num_active_streams = len(self.stream_names)\n#         combined_feature_dim = lstm_hidden_size * 2 * num_active_streams\n#         print(f\"  Combined feature dimension: {combined_feature_dim}\")\n#         self.dropout = nn.Dropout(dropout_rate)\n#         intermediate_dim = max(num_classes * 4, combined_feature_dim // 2)\n#         self.fc1 = nn.Linear(combined_feature_dim, intermediate_dim)\n#         self.relu = nn.ReLU()\n#         self.fc2 = nn.Linear(intermediate_dim, num_classes)\n\n#     def forward(self, x):\n#         stream_context_vectors = []\n#         stream_att_weights = {}\n#         for name in self.stream_names:\n#             if name not in x:\n#                 print(f\"Warning: Stream '{name}' expected but not in input data.\")\n#                 continue\n#             lstm_out, _ = self.lstms[name](x[name])\n#             context_vector, attention_weights = self.attentions[name](lstm_out)\n#             stream_context_vectors.append(context_vector)\n#             stream_att_weights[name] = attention_weights\n#         if not stream_context_vectors:\n#             raise RuntimeError(\"No stream outputs generated.\")\n#         fused_features = torch.cat(stream_context_vectors, dim=1)\n#         out = self.dropout(fused_features)\n#         out = self.relu(self.fc1(out))\n#         out = self.dropout(out)\n#         logits = self.fc2(out)\n#         return logits\n\n# # --- Training and Evaluation Functions ---\n# def train_epoch(model, dataloader, optimizer, criterion, device):\n#     model.train()\n#     total_loss = 0.0\n#     all_preds = []\n#     all_labels = []\n#     active_streams = model.stream_names  # Get active streams from model\n\n#     for features, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n#         # Select only the features for active streams\n#         input_features = {name: features[name].to(device) for name in active_streams if name in features}\n#         labels = labels.to(device)\n\n#         optimizer.zero_grad()\n#         outputs = model(input_features)  # Pass only active stream data\n#         loss = criterion(outputs, labels)\n#         loss.backward()\n#         optimizer.step()\n\n#         total_loss += loss.item()\n#         preds = torch.argmax(outputs, dim=1)\n#         all_preds.extend(preds.cpu().numpy())\n#         all_labels.extend(labels.cpu().numpy())\n\n#     avg_loss = total_loss / len(dataloader)\n#     accuracy = accuracy_score(all_labels, all_preds)\n#     return avg_loss, accuracy\n\n# def evaluate_epoch(model, dataloader, criterion, device):\n#     model.eval()\n#     total_loss = 0.0\n#     all_labels = []\n#     all_preds = []\n#     all_probs = []\n#     active_streams = model.stream_names  # Get active streams from model\n\n#     with torch.no_grad():\n#         for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n#             input_features = {name: features[name].to(device) for name in active_streams if name in features}\n#             labels = labels.to(device)\n\n#             outputs = model(input_features)\n#             loss = criterion(outputs, labels)\n#             total_loss += loss.item()\n\n#             probs = torch.softmax(outputs, dim=1)\n#             preds = torch.argmax(probs, dim=1)\n#             all_labels.extend(labels.cpu().numpy())\n#             all_preds.extend(preds.cpu().numpy())\n#             all_probs.extend(probs.cpu().numpy())\n\n#     avg_loss = total_loss / len(dataloader)\n#     all_probs = np.array(all_probs)\n#     all_labels = np.array(all_labels)\n#     all_preds = np.array(all_preds)\n#     accuracy = accuracy_score(all_labels, all_preds)\n#     precision, recall, f1, _ = precision_recall_fscore_support(\n#         all_labels, all_preds, average='binary', pos_label=1, zero_division=0\n#     )\n#     auc = roc_auc_score(all_labels, all_probs[:, 1]) if len(np.unique(all_labels)) > 1 else float('nan')\n#     return {\n#         'loss': avg_loss,\n#         'accuracy': accuracy,\n#         'precision': precision,\n#         'recall': recall,\n#         'f1': f1,\n#         'auc': auc\n#     }\n\n# def get_predictions_and_labels(model, dataloader, device):\n#     model.eval()\n#     all_labels = []\n#     all_preds = []\n#     active_streams = model.stream_names  # Get active streams from model\n\n#     with torch.no_grad():\n#         for features, labels in tqdm(dataloader, desc=\"Generating CM Data\", leave=False):\n#             input_features = {name: features[name].to(device) for name in active_streams if name in features}\n#             labels = labels.to(device)\n#             outputs = model(input_features)\n#             preds = torch.argmax(outputs, dim=1)\n#             all_labels.extend(labels.cpu().numpy())\n#             all_preds.extend(preds.cpu().numpy())\n#     return np.array(all_labels), np.array(all_preds)\n\n# # --- Main Execution ---\n# if __name__ == '__main__':\n#     print(\"Verifying required directories...\")\n#     if not os.path.isdir(POSE_DATA_DIR):\n#         raise FileNotFoundError(f\"Pose data dir not found: {POSE_DATA_DIR}\")\n#     if not os.path.isdir(ANNOTATION_DIR):\n#         raise FileNotFoundError(f\"Annotation dir not found: {ANNOTATION_DIR}\")\n#     if not os.path.isdir(EGO_DATA_DIR):\n#         raise FileNotFoundError(f\"Ego vehicle data dir not found: {EGO_DATA_DIR}\")\n#     print(\"Input directories verified.\")\n\n#     train_dataset = PIEDataset(ANNOTATION_DIR, EGO_DATA_DIR, TRAIN_SETS, POSE_DATA_DIR, SEQ_LEN, PRED_LEN)\n#     val_dataset = PIEDataset(ANNOTATION_DIR, EGO_DATA_DIR, VAL_SETS, POSE_DATA_DIR, SEQ_LEN, PRED_LEN)\n#     if len(train_dataset) == 0 or len(val_dataset) == 0:\n#         raise ValueError(\"Dataset loading failed.\")\n#     train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n#     val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n#     input_sizes = {\n#         'bbox': INPUT_SIZE_BBOX,\n#         'pose': INPUT_SIZE_POSE,\n#         'ego_speed': INPUT_SIZE_EGO,\n#         'scene': INPUT_SIZE_SCENE\n#     }\n#     model = MultiStreamAdaptiveLSTM(\n#         input_sizes=input_sizes,\n#         lstm_hidden_size=LSTM_HIDDEN_SIZE,\n#         num_lstm_layers=NUM_LSTM_LAYERS,\n#         num_classes=NUM_CLASSES,\n#         attention_dim=ATTENTION_DIM,\n#         dropout_rate=DROPOUT_RATE,\n#         stream_names=ACTIVE_STREAMS\n#     ).to(DEVICE)\n\n#     print(\"\\n--- Model Architecture ---\")\n#     print(model)\n#     num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n#     print(f\"Total Trainable Parameters: {num_params:,}\")\n#     print(\"-\" * 30)\n\n#     criterion = nn.CrossEntropyLoss()\n#     optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n#     best_val_f1 = -1.0\n#     train_losses, val_losses = [], []\n#     train_accs, val_accs = [], []\n#     val_f1s = []\n\n#     print(\"\\n--- Starting Training ---\")\n#     for epoch in range(NUM_EPOCHS):\n#         epoch_start_time = time.time()\n#         train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n#         val_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n#         epoch_duration = time.time() - epoch_start_time\n\n#         train_losses.append(train_loss)\n#         val_losses.append(val_metrics['loss'])\n#         train_accs.append(train_acc)\n#         val_accs.append(val_metrics['accuracy'])\n#         val_f1s.append(val_metrics['f1'])\n\n#         print(f\"--- Epoch {epoch+1}/{NUM_EPOCHS} ({epoch_duration:.2f} sec) ---\")\n#         print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n#         print(f\"  Val Loss:   {val_metrics['loss']:.4f}, Val Acc:  {val_metrics['accuracy']:.4f}\")\n#         print(f\"  Val Prec:   {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n#         print(f\"  Val AUC:    {val_metrics['auc']:.4f}\")\n#         if val_metrics['f1'] > best_val_f1:\n#             best_val_f1 = val_metrics['f1']\n#             torch.save(model.state_dict(), 'best_model_multistream.pth')\n#             print(f\"  >> Saved new best model with F1: {best_val_f1:.4f}\")\n#         print(\"-\" * 30)\n\n#     print(\"--- Training Finished ---\")\n#     print(\"\\n--- Plotting Training History ---\")\n#     fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n#     axes[0].plot(range(1, NUM_EPOCHS + 1), train_losses, label='Train Loss')\n#     axes[0].plot(range(1, NUM_EPOCHS + 1), val_losses, label='Val Loss')\n#     axes[0].set_xlabel('Epoch')\n#     axes[0].set_ylabel('Loss')\n#     axes[0].set_title('Loss Curve')\n#     axes[0].legend()\n#     axes[0].grid(True)\n\n#     axes[1].plot(range(1, NUM_EPOCHS + 1), train_accs, label='Train Accuracy')\n#     axes[1].plot(range(1, NUM_EPOCHS + 1), val_accs, label='Val Accuracy')\n#     axes[1].plot(range(1, NUM_EPOCHS + 1), val_f1s, label='Val F1-Score', linestyle='--')\n#     axes[1].set_xlabel('Epoch')\n#     axes[1].set_ylabel('Metric')\n#     axes[1].set_title('Accuracy & F1-Score Curve')\n#     axes[1].legend()\n#     axes[1].grid(True)\n\n#     plt.tight_layout()\n#     plt.show()\n\n#     print(\"\\n--- Final Evaluation on Validation Set using Best Model ---\")\n#     best_model_path = 'best_model_multistream.pth'\n#     if os.path.exists(best_model_path):\n#         print(f\"Loading best saved model '{best_model_path}'\")\n#         model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n#     else:\n#         print(\"Warning: No saved best model found. Evaluating final model.\")\n\n#     final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n#     true_labels, pred_labels = get_predictions_and_labels(model, val_loader, DEVICE)\n#     cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1])\n#     labels_display = ['Not Crossing', 'Crossing']\n\n#     print(\"\\n--- Final Performance Metrics ---\")\n#     print(f\"  Accuracy:  {final_metrics['accuracy']:.4f}\")\n#     print(f\"  Precision: {final_metrics['precision']:.4f}\")\n#     print(f\"  Recall:    {final_metrics['recall']:.4f}\")\n#     print(f\"  F1 Score:  {final_metrics['f1']:.4f}\")\n#     print(f\"  AUC:       {final_metrics['auc']:.4f}\")\n#     print(f\"  Loss:      {final_metrics['loss']:.4f}\")\n\n#     print(\"\\n--- Confusion Matrix ---\")\n#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_display)\n#     disp.plot(cmap=plt.cm.Blues)\n#     plt.title('Confusion Matrix (Validation Set)')\n#     plt.show()\n\n#     print(\"\\n--- Script Complete ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:44:07.683050Z","iopub.execute_input":"2025-04-14T12:44:07.683250Z","iopub.status.idle":"2025-04-14T12:44:07.699790Z","shell.execute_reply.started":"2025-04-14T12:44:07.683233Z","shell.execute_reply":"2025-04-14T12:44:07.699153Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# run this sam search\n\nimport sys\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom sklearn.metrics import (\n    accuracy_score, precision_recall_fscore_support, \n    roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n)\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm.notebook import tqdm\nimport random\nimport math\nimport zipfile\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport time\n\n# --- Add PIE utilities to Python path ---\n# Adjust this path if your PIE repo clone is elsewhere\npie_utilities_path = '/kaggle/working/PIE/utilities'  \nif pie_utilities_path not in sys.path:\n    sys.path.insert(0, pie_utilities_path)\n# ---\n\n# --- Import from PIE utilities ---\ntry:\n    from pie_data import PIE\n    from utils import print_msg, exception  # And potentially others if needed later\nexcept ImportError as e:\n    print(\"ERROR: Could not import from PIE utilities.\")\n    print(f\"Ensure '{pie_utilities_path}' is correct and contains pie_data.py and utils.py\")\n    print(f\"ImportError: {e}\")\n    # Stop execution if utilities not found\n    raise\n\n# --- Configuration ---\nPIE_ROOT_PATH = '/kaggle/working/PIE'  # Path where PIE repo was cloned/unzipped\nVIDEO_INPUT_DIR = '/kaggle/input'        # Where pie-setXX video dataset folders are\nPOSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2'  # Where setXX subdirs with PKLs are\n\n# --- Stream Control ---\n# <<< --- Comment out streams here to disable them --- >>>\nACTIVE_STREAMS = [\n    'bbox',           # Normalized BBox [cx, cy, w, h]\n    # 'pose',           # Pre-extracted normalized pose keypoints\n    'ego_speed',      # Standardized Ego Vehicle OBD Speed\n    'ego_acc',        # Standardized Ego Accel X, Y\n    # 'ego_gyro',       # Standardized Ego Gyro Z (Yaw rate)\n    'ped_action',     # Walking/Standing (0/1)\n    'ped_look',       # Looking/Not-Looking (0/1)\n    # 'ped_occlusion',  # Occlusion state (0/1/2 - normalized)\n    'traffic_light',  # One-hot encoded state\n    'static_context'  # Repeated static features (signalized, intersection, age, gender)\n]\n# <<< --- End Stream Control --- >>>\nprint(f\"Active Streams: {ACTIVE_STREAMS}\")\n\n# Model Hyperparameters\nSEQ_LEN = 30\nPRED_LEN = 1  # Predict state at frame (SEQ_LEN + PRED_LEN - 1) relative to start\n\n# Input Sizes (Define for ALL potential streams)\nINPUT_SIZE_BBOX = 4\nINPUT_SIZE_POSE = 34\nINPUT_SIZE_EGO_SPEED = 1\nINPUT_SIZE_EGO_ACC = 2   # X, Y components\nINPUT_SIZE_EGO_GYRO = 1  # Z component (Yaw rate)\nINPUT_SIZE_PED_ACTION = 1  # 0:standing, 1:walking\nINPUT_SIZE_PED_LOOK = 1   # 0:not-looking, 1:looking\nINPUT_SIZE_PED_OCC = 1    # 0:none, 0.5:part, 1:full (normalized)\nINPUT_SIZE_TL_STATE = 4   # 0:Undef, 1:Red, 2:Yellow, 3:Green (One-Hot)\nINPUT_SIZE_STATIC = 4 + 5 + 4 + 3  # One-hot: Signalized(4) + Intersection(5) + Age(4) + Gender(3) = 16\n\nLSTM_HIDDEN_SIZE = 256\nNUM_LSTM_LAYERS = 2\nDROPOUT_RATE = 0.3\nNUM_CLASSES = 2\nATTENTION_DIM = 128\n\n# Training Hyperparameters\nLEARNING_RATE = 1e-3\nBATCH_SIZE = 32  # Reduced batch size due to potentially larger feature vectors\nNUM_EPOCHS = 30   \nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\n# Dataset Splits (Using PIE default)\nTRAIN_SETS_STR = ['set01', 'set02', 'set04']  # PIE default train\nVAL_SETS_STR = ['set05', 'set06']  # PIE default val\nTEST_SETS_STR = ['set03']  # PIE default test\n\n# Traffic Light State Mapping (Matches pie_data.py)\nTL_STATE_MAP = {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}\nNUM_TL_STATES = len(TL_STATE_MAP)\n\n# Static Feature Mappings (Matches pie_data.py)\nSIGNALIZED_MAP = {'n/a': 0, 'C': 1, 'S': 2, 'CS': 3}\nINTERSECTION_MAP = {'midblock': 0, 'T': 1, 'T-left': 2, 'T-right': 3, 'four-way': 4}\nAGE_MAP = {'child': 0, 'young': 1, 'adult': 2, 'senior': 3}  # Note: PIE map has young, adjust if needed\nGENDER_MAP = {'n/a': 0, 'female': 1, 'male': 2}\nNUM_SIGNALIZED_CATS = len(SIGNALIZED_MAP)\nNUM_INTERSECTION_CATS = len(INTERSECTION_MAP)\nNUM_AGE_CATS = len(AGE_MAP)\nNUM_GENDER_CATS = len(GENDER_MAP)\n# Verify total static size\nassert INPUT_SIZE_STATIC == (\n    NUM_SIGNALIZED_CATS + NUM_INTERSECTION_CATS + NUM_AGE_CATS + NUM_GENDER_CATS\n), \"Static input size mismatch!\"\n\n# --- Helper: One-Hot Encoding ---\ndef to_one_hot(index, num_classes):\n    \"\"\"Converts an integer index to a one-hot vector.\"\"\"\n    vec = np.zeros(num_classes, dtype=np.float32)\n    if 0 <= index < num_classes:\n        vec[index] = 1.0\n    else:\n        # Handle unexpected index (e.g., map to 'unknown' if first class is reserved)\n        vec[0] = 1.0  # Default to first class if index is invalid\n    return vec\n\n# --- Dataset Class (Leveraging pie_data.py database) ---\nclass PIEDataset(Dataset):\n    def __init__(self, pie_database, set_names, pose_data_dir, seq_len, pred_len, scalers=None, active_streams=None):\n        self.pie_db = pie_database\n        self.set_names = set_names  # e.g., ['set01', 'set02']\n        self.pose_data_dir = pose_data_dir\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.scalers = scalers or {}  # Dictionary for scalers (mean/std)\n        self.active_streams = active_streams or []  # List of streams to actually generate\n\n        self.sequences = []  # List of tuples: (set_id, video_id, ped_id, start_frame)\n        self.all_pose_data = {}  # Store loaded pose data here: {set_id: {video_id: {frame: {ped_id: pose_vec}}}}\n\n        self._load_pose_data()\n        self._generate_sequence_list()\n\n        if not self.sequences:\n            raise ValueError(f\"Dataset initialization failed: No sequences found for sets {self.set_names}\")\n\n    def _load_pose_data(self):\n        # --- Load Pre-extracted Pose Data ---\n        print(f\"\\nLoading pose data for sets: {self.set_names} from {self.pose_data_dir}\")\n        sets_loaded_count = 0\n        for set_id in tqdm(self.set_names, desc=\"Loading Pose Sets\"):\n            self.all_pose_data[set_id] = {}\n            pose_set_path = os.path.join(self.pose_data_dir, set_id)\n            if not os.path.isdir(pose_set_path):\n                print(f\"Warning: Pose directory not found for {set_id} at {pose_set_path}, skipping.\")\n                continue\n\n            pkl_files_in_set = [\n                f for f in os.listdir(pose_set_path)\n                if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")\n            ]\n            if not pkl_files_in_set:\n                continue\n\n            loaded_video_count = 0\n            for pkl_filename in tqdm(pkl_files_in_set, desc=f\"Loading PKLs for {set_id}\", leave=False):\n                pkl_file_path = os.path.join(pose_set_path, pkl_filename)\n                try:\n                    with open(pkl_file_path, 'rb') as f:\n                        loaded_pkl_content = pickle.load(f)\n                    if len(loaded_pkl_content) != 1:\n                        print(f\"Warn: PKL {pkl_filename} format issue.\")\n                        continue\n                    unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n                    video_id = \"_\".join(unique_video_key.split('_')[1:])  # Extract video_id\n                    self.all_pose_data[set_id][video_id] = video_data\n                    loaded_video_count += 1\n                except FileNotFoundError:\n                    pass\n                except Exception as e:\n                    print(f\"Error loading pose PKL {pkl_file_path}: {e}\")\n\n            if loaded_video_count > 0:\n                sets_loaded_count += 1\n        print(f\"Finished loading pose data for {sets_loaded_count} sets.\")\n\n    def _generate_sequence_list(self):\n        \"\"\"Iterates through the loaded PIE database to find valid sequences.\"\"\"\n        print(f\"Generating sequence list from PIE database for sets: {self.set_names}\")\n        sequence_count = 0\n        ped_count = 0\n        for set_id in tqdm(self.set_names, desc=\"Generating Sequences\"):\n            if set_id not in self.pie_db:\n                continue\n            for video_id, video_data in self.pie_db[set_id].items():\n                if 'ped_annotations' not in video_data:\n                    continue\n                for ped_id, ped_data in video_data['ped_annotations'].items():\n                    ped_count += 1\n                    if 'frames' not in ped_data or len(ped_data['frames']) < self.seq_len + self.pred_len:\n                        continue  # Skip tracks that are too short\n\n                    sorted_frames = sorted(ped_data['frames'])\n\n                    for i in range(len(sorted_frames) - self.seq_len - self.pred_len + 1):\n                        start_frame = sorted_frames[i]\n                        end_frame_observe = sorted_frames[i + self.seq_len - 1]\n                        # Check for frame continuity within observation window\n                        if end_frame_observe - start_frame != self.seq_len - 1:\n                            continue\n                        # Check continuity to prediction frame\n                        target_frame_actual_idx = i + self.seq_len + self.pred_len - 1\n                        target_frame = sorted_frames[target_frame_actual_idx]\n                        if target_frame - end_frame_observe != self.pred_len:\n                            continue\n\n                        # If checks pass, add sequence info\n                        self.sequences.append((set_id, video_id, ped_id, start_frame))\n                        sequence_count += 1\n\n        print(f\"Found {sequence_count} valid sequences from {ped_count} pedestrian tracks.\")\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        set_id, video_id, ped_id, start_frame = self.sequences[idx]\n\n        # Determine frame numbers for the sequence\n        frame_nums = list(range(start_frame, start_frame + self.seq_len))\n        target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n\n        # --- Get data from the loaded PIE database ---\n        video_db = self.pie_db.get(set_id, {}).get(video_id, {})\n        ped_db = video_db.get('ped_annotations', {}).get(ped_id, {})\n        ego_db = video_db.get('vehicle_annotations', {})  # Frame -> {sensor: value}\n        traffic_db = video_db.get('traffic_annotations', {})  # ObjID -> {frames:[], state:[], ...}\n        ped_attributes = ped_db.get('attributes', {})\n\n        # --- Initialize feature sequences ---\n        feature_sequences = {stream: [] for stream in self.active_streams}\n\n        # --- Static Features (calculated once) ---\n        if 'static_context' in self.active_streams:\n            sig_idx = ped_attributes.get('signalized', 0)  # Default to 'n/a' index\n            int_idx = ped_attributes.get('intersection', 0)  # Default to 'midblock' index\n            age_idx = ped_attributes.get('age', 2)  # Default to 'adult' index\n            gen_idx = ped_attributes.get('gender', 0)  # Default to 'n/a' index\n\n            static_vec = np.concatenate([\n                to_one_hot(sig_idx, NUM_SIGNALIZED_CATS),\n                to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n                to_one_hot(age_idx, NUM_AGE_CATS),\n                to_one_hot(gen_idx, NUM_GENDER_CATS)\n            ])\n            if static_vec.shape[0] != INPUT_SIZE_STATIC:  # Sanity check\n                print(f\"Error: Static vector size mismatch! Expected {INPUT_SIZE_STATIC}, got {static_vec.shape[0]}. Using zeros.\")\n                static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n\n        # --- Get Target Label ---\n        label = 0  # Default to not-crossing\n        if 'frames' in ped_db and 'behavior' in ped_db and 'cross' in ped_db['behavior']:\n            try:\n                target_frame_db_idx = ped_db['frames'].index(target_frame_num)\n                label = ped_db['behavior']['cross'][target_frame_db_idx]  # Get 0/1/-1 label\n                if label == -1:\n                    label = 0  # Map irrelevant to not-crossing\n            except (ValueError, IndexError):\n                pass  # Keep default label if target frame not found for this ped\n\n        # --- Iterate through sequence frames ---\n        for frame_num in frame_nums:\n            # --- Get Data for Current Frame ---\n            frame_db_idx = -1\n            if 'frames' in ped_db:\n                try:\n                    frame_db_idx = ped_db['frames'].index(frame_num)\n                except ValueError:\n                    pass  # Frame not found for this ped\n\n            ego_frame_data = ego_db.get(frame_num, {})  # Get ego data dict for frame\n\n            # --- Extract for ACTIVE streams ---\n            if 'bbox' in self.active_streams:\n                bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32)\n                if frame_db_idx != -1 and 'bbox' in ped_db and len(ped_db['bbox']) > frame_db_idx:\n                    # Convert [x1,y1,x2,y2] from DB to normalized [cx,cy,w,h]\n                    try:\n                        x1, y1, x2, y2 = ped_db['bbox'][frame_db_idx]\n                        img_w = video_db.get('width', 1920)\n                        img_h = video_db.get('height', 1080)\n                        if img_w > 0 and img_h > 0:\n                            cx = ((x1 + x2) / 2) / img_w\n                            cy = ((y1 + y2) / 2) / img_h\n                            w = (x2 - x1) / img_w\n                            h = (y2 - y1) / img_h\n                            if w > 0 and h > 0 and 0 <= cx <= 1 and 0 <= cy <= 1:\n                                bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n                    except Exception:\n                        pass  # Keep zeros on error\n                feature_sequences['bbox'].append(bbox_norm)\n\n            if 'pose' in self.active_streams:\n                pose_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n                # Look up in pre-loaded pose data\n                vid_pose_data = self.all_pose_data.get(set_id, {}).get(video_id, {})\n                frame_pose_data = vid_pose_data.get(frame_num, {})\n                loaded_pose = frame_pose_data.get(ped_id)\n                if loaded_pose is not None and isinstance(loaded_pose, np.ndarray):\n                    if loaded_pose.shape == (INPUT_SIZE_POSE,):\n                        pose_vector = loaded_pose\n                feature_sequences['pose'].append(pose_vector)\n\n            if 'ego_speed' in self.active_streams:\n                speed = ego_frame_data.get('OBD_speed', 0.0)\n                if speed == 0.0:\n                    speed = ego_frame_data.get('GPS_speed', 0.0)\n                # Standardize\n                speed_scaled = (speed - self.scalers.get('ego_speed_mean', 0.0)) / self.scalers.get('ego_speed_std', 1.0)\n                feature_sequences['ego_speed'].append([speed_scaled])\n\n            if 'ego_acc' in self.active_streams:\n                accX = ego_frame_data.get('accX', 0.0)\n                accY = ego_frame_data.get('accY', 0.0)\n                # Standardize\n                accX_scaled = (accX - self.scalers.get('accX_mean', 0.0)) / self.scalers.get('accX_std', 1.0)\n                accY_scaled = (accY - self.scalers.get('accY_mean', 0.0)) / self.scalers.get('accY_std', 1.0)\n                feature_sequences['ego_acc'].append([accX_scaled, accY_scaled])\n\n            if 'ego_gyro' in self.active_streams:\n                gyroZ = ego_frame_data.get('gyroZ', 0.0)  # Yaw rate\n                # Standardize\n                gyroZ_scaled = (gyroZ - self.scalers.get('gyroZ_mean', 0.0)) / self.scalers.get('gyroZ_std', 1.0)\n                feature_sequences['ego_gyro'].append([gyroZ_scaled])\n\n            if 'ped_action' in self.active_streams:\n                action = 0  # Default standing\n                if frame_db_idx != -1 and 'behavior' in ped_db and 'action' in ped_db['behavior'] and len(ped_db['behavior']['action']) > frame_db_idx:\n                    action = ped_db['behavior']['action'][frame_db_idx]  # 0 or 1\n                feature_sequences['ped_action'].append([float(action)])\n\n            if 'ped_look' in self.active_streams:\n                look = 0  # Default not-looking\n                if frame_db_idx != -1 and 'behavior' in ped_db and 'look' in ped_db['behavior'] and len(ped_db['behavior']['look']) > frame_db_idx:\n                    look = ped_db['behavior']['look'][frame_db_idx]  # 0 or 1\n                feature_sequences['ped_look'].append([float(look)])\n\n            if 'ped_occlusion' in self.active_streams:\n                occ = 0.0  # Default none\n                if frame_db_idx != -1 and 'occlusion' in ped_db and len(ped_db['occlusion']) > frame_db_idx:\n                    occ_val = ped_db['occlusion'][frame_db_idx]  # 0, 1, 2\n                    occ = float(occ_val) / 2.0  # Normalize to [0, 0.5, 1.0]\n                feature_sequences['ped_occlusion'].append([occ])\n\n            if 'traffic_light' in self.active_streams:\n                # Simple approach: Find *any* relevant TL state for this frame\n                state_int = 0  # Default undefined\n                # Iterate through traffic objects in DB to find TL state for this frame\n                for obj_id, obj_data in traffic_db.items():\n                    if obj_data.get('obj_class') == 'traffic_light' and 'frames' in obj_data and 'state' in obj_data:\n                        try:\n                            tl_frame_idx = obj_data['frames'].index(frame_num)\n                            state_val = obj_data['state'][tl_frame_idx]\n                            if state_val != 0:  # If not undefined\n                                state_int = state_val\n                                break  # Found first non-undefined state\n                        except (ValueError, IndexError):\n                            continue\n                feature_sequences['traffic_light'].append(to_one_hot(state_int, NUM_TL_STATES))\n\n            if 'static_context' in self.active_streams:\n                feature_sequences['static_context'].append(static_vec)  # Append the same vector each time\n\n        # --- Convert lists to Tensors ---\n        features = {}\n        try:\n            for stream_name in self.active_streams:\n                stream_array = np.array(feature_sequences[stream_name], dtype=np.float32)\n                features[stream_name] = torch.tensor(stream_array, dtype=torch.float32)\n        except Exception as e:\n            print(f\"Error converting features to tensor (after loop) for idx {idx}: {e}. Returning dummy data.\")\n            # Return dummy tensors for all expected streams on error\n            features = {\n                name: torch.zeros((self.seq_len, INPUT_SIZE_BBOX), dtype=torch.float32)\n                for name in self.active_streams\n            }\n        return features, torch.tensor(label, dtype=torch.long)\n\n# --- Model Architecture (Dynamic Multi-Stream Adaptive LSTM) ---\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.attention_net = nn.Sequential(\n            nn.Linear(hidden_dim, attention_dim),\n            nn.Tanh(),\n            nn.Linear(attention_dim, 1)\n        )\n\n    def forward(self, lstm_output):\n        att_scores = self.attention_net(lstm_output).squeeze(2)\n        att_weights = torch.softmax(att_scores, dim=1)\n        context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1)\n        return context_vector, att_weights\n\nclass MultiStreamAdaptiveLSTM(nn.Module):\n    def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes,\n                 attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n        super(MultiStreamAdaptiveLSTM, self).__init__()\n        if not stream_names:\n            raise ValueError(\"stream_names cannot be empty.\")\n        self.stream_names = stream_names\n        self.lstms = nn.ModuleDict()\n        self.attentions = nn.ModuleDict()\n        print(f\"Initializing model with streams: {self.stream_names}\")\n        for name in self.stream_names:\n            if name not in input_sizes:\n                raise KeyError(f\"Input size for stream '{name}' not provided.\")\n            current_input_size = input_sizes[name]\n            print(f\"  - Adding stream '{name}' with input size {current_input_size}\")\n            self.lstms[name] = nn.LSTM(\n                current_input_size, lstm_hidden_size, num_lstm_layers,\n                batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0,\n                bidirectional=True\n            )\n            self.attentions[name] = Attention(lstm_hidden_size * 2, attention_dim)\n        num_active_streams = len(self.stream_names)\n        combined_feature_dim = lstm_hidden_size * 2 * num_active_streams\n        print(f\"  Combined feature dimension: {combined_feature_dim}\")\n        self.dropout = nn.Dropout(dropout_rate)\n        intermediate_dim = max(num_classes * 4, combined_feature_dim // 2)\n        self.fc1 = nn.Linear(combined_feature_dim, intermediate_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(intermediate_dim, num_classes)\n\n    def forward(self, x):\n        stream_context_vectors = []\n        stream_att_weights = {}\n        for name in self.stream_names:\n            if name not in x:\n                print(f\"Warning: Stream '{name}' expected but not in input data.\")\n                continue\n            lstm_out, _ = self.lstms[name](x[name])\n            context_vector, attention_weights = self.attentions[name](lstm_out)\n            stream_context_vectors.append(context_vector)\n            stream_att_weights[name] = attention_weights\n        if not stream_context_vectors:\n            raise RuntimeError(\"No stream outputs generated.\")\n        fused_features = torch.cat(stream_context_vectors, dim=1)\n        out = self.dropout(fused_features)\n        out = self.relu(self.fc1(out))\n        out = self.dropout(out)\n        logits = self.fc2(out)\n        return logits\n\n# --- Training and Evaluation Functions ---\ndef train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n    active_streams = model.stream_names\n\n    for features, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n        input_features = {name: features[name].to(device) for name in active_streams if name in features}\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(input_features)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    avg_loss = total_loss / len(dataloader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    return avg_loss, accuracy\n\ndef evaluate_epoch(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_labels = []\n    all_preds = []\n    all_probs = []\n    active_streams = model.stream_names\n\n    with torch.no_grad():\n        for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n            input_features = {name: features[name].to(device) for name in active_streams if name in features}\n            labels = labels.to(device)\n            outputs = model(input_features)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            probs = torch.softmax(outputs, dim=1)\n            preds = torch.argmax(probs, dim=1)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n\n    avg_loss = total_loss / len(dataloader)\n    all_probs = np.array(all_probs)\n    all_labels = np.array(all_labels)\n    all_preds = np.array(all_preds)\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', pos_label=1, zero_division=0\n    )\n    auc = roc_auc_score(all_labels, all_probs[:, 1]) if len(np.unique(all_labels)) > 1 else float('nan')\n    return {'loss': avg_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}\n\ndef get_predictions_and_labels(model, dataloader, device):\n    model.eval()\n    all_labels = []\n    all_preds = []\n    active_streams = model.stream_names\n\n    with torch.no_grad():\n        for features, labels in tqdm(dataloader, desc=\"Generating CM Data\", leave=False):\n            input_features = {name: features[name].to(device) for name in active_streams if name in features}\n            labels = labels.to(device)\n            outputs = model(input_features)\n            preds = torch.argmax(outputs, dim=1)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n    return np.array(all_labels), np.array(all_preds)\n\n# --- Main Execution ---\nif __name__ == '__main__':\n    # --- Preliminary Step: Generate or Load PIE Database ---\n    pie_db_path = '/kaggle/input/pie-database/pie_database.pkl'\n    print(f\"Checking for PIE database cache at: {pie_db_path}\")\n    if not os.path.exists(pie_db_path):\n        print(\"PIE database cache not found. Generating... (This may take a while)\")\n        # Ensure the PIE class can find the annotations and vehicle data\n        pie_dataset_interface = PIE(data_path=PIE_ROOT_PATH, regen_database=True)\n        pie_database = pie_dataset_interface.generate_database()\n        if not pie_database:\n            raise RuntimeError(\"Failed to generate PIE database.\")\n        print(\"PIE database generated successfully.\")\n    else:\n        print(\"Loading PIE database from cache...\")\n        try:\n            with open(pie_db_path, 'rb') as f:\n                pie_database = pickle.load(f)\n            print(\"PIE database loaded successfully.\")\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load PIE database from {pie_db_path}: {e}\")\n    # --- End Database Loading ---\n\n    # --- Calculate Mean/Std for Standardization (on Training Set) ---\n    print(\"\\nCalculating standardization parameters from training set...\")\n    all_train_ego_speeds = []\n    all_train_accX = []\n    all_train_accY = []\n    all_train_gyroZ = []\n    for set_id in TRAIN_SETS_STR:\n        if set_id in pie_database:\n            for video_id, video_data in pie_database[set_id].items():\n                if 'vehicle_annotations' in video_data:\n                    for frame_num, ego_frame_data in video_data['vehicle_annotations'].items():\n                        speed = ego_frame_data.get('OBD_speed', 0.0)\n                        if speed == 0.0:\n                            speed = ego_frame_data.get('GPS_speed', 0.0)\n                        all_train_ego_speeds.append(speed)\n                        all_train_accX.append(ego_frame_data.get('accX', 0.0))\n                        all_train_accY.append(ego_frame_data.get('accY', 0.0))\n                        all_train_gyroZ.append(ego_frame_data.get('gyroZ', 0.0))\n\n    scalers = {}\n    if all_train_ego_speeds:\n        scalers['ego_speed_mean'] = np.mean(all_train_ego_speeds)\n        scalers['ego_speed_std'] = np.std(all_train_ego_speeds) if np.std(all_train_ego_speeds) > 1e-6 else 1.0\n        print(f\"  Ego Speed: Mean={scalers['ego_speed_mean']:.2f}, Std={scalers['ego_speed_std']:.2f}\")\n    if all_train_accX:\n        scalers['accX_mean'] = np.mean(all_train_accX)\n        scalers['accX_std'] = np.std(all_train_accX) if np.std(all_train_accX) > 1e-6 else 1.0\n        scalers['accY_mean'] = np.mean(all_train_accY)\n        scalers['accY_std'] = np.std(all_train_accY) if np.std(all_train_accY) > 1e-6 else 1.0\n        print(f\"  Ego AccX: Mean={scalers['accX_mean']:.2f}, Std={scalers['accX_std']:.2f}\")\n        print(f\"  Ego AccY: Mean={scalers['accY_mean']:.2f}, Std={scalers['accY_std']:.2f}\")\n    if all_train_gyroZ:\n        scalers['gyroZ_mean'] = np.mean(all_train_gyroZ)\n        scalers['gyroZ_std'] = np.std(all_train_gyroZ) if np.std(all_train_gyroZ) > 1e-6 else 1.0\n        print(f\"  Ego GyroZ: Mean={scalers['gyroZ_mean']:.2f}, Std={scalers['gyroZ_std']:.2f}\")\n    print(\"Standardization parameters calculated.\")\n    # --- End Scaler Calculation ---\n\n    print(f\"\\nVerifying required directories...\")\n    if not os.path.isdir(POSE_DATA_DIR):\n        raise FileNotFoundError(f\"Pose data dir not found: {POSE_DATA_DIR}\")\n    print(\"Input directories verified.\")\n\n    # Initialize Datasets and Dataloaders\n    train_dataset = PIEDataset(pie_database, TRAIN_SETS_STR, POSE_DATA_DIR, SEQ_LEN, PRED_LEN, scalers, ACTIVE_STREAMS)\n    val_dataset = PIEDataset(pie_database, VAL_SETS_STR, POSE_DATA_DIR, SEQ_LEN, PRED_LEN, scalers, ACTIVE_STREAMS)\n\n    if len(train_dataset) == 0 or len(val_dataset) == 0:\n        raise ValueError(\"Dataset loading failed.\")\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n    # Define Input Sizes for ALL potential streams\n    input_sizes = {\n        'bbox': INPUT_SIZE_BBOX,\n        'pose': INPUT_SIZE_POSE,\n        'ego_speed': INPUT_SIZE_EGO_SPEED,\n        'ego_acc': INPUT_SIZE_EGO_ACC,\n        'ego_gyro': INPUT_SIZE_EGO_GYRO,\n        'ped_action': INPUT_SIZE_PED_ACTION,\n        'ped_look': INPUT_SIZE_PED_LOOK,\n        'ped_occlusion': INPUT_SIZE_PED_OCC,\n        'traffic_light': INPUT_SIZE_TL_STATE,\n        'static_context': INPUT_SIZE_STATIC\n    }\n\n    # Initialize Model, Loss, and Optimizer\n    model = MultiStreamAdaptiveLSTM(\n        input_sizes=input_sizes,\n        lstm_hidden_size=LSTM_HIDDEN_SIZE,\n        num_lstm_layers=NUM_LSTM_LAYERS,\n        num_classes=NUM_CLASSES,\n        attention_dim=ATTENTION_DIM,\n        dropout_rate=DROPOUT_RATE,\n        stream_names=ACTIVE_STREAMS\n    ).to(DEVICE)\n\n    print(\"\\n--- Model Architecture ---\")\n    print(model)\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total Trainable Parameters: {num_params:,}\")\n    print(\"-\" * 30)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    best_val_f1 = -1.0\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n    val_f1s = []\n\n    print(\"\\n--- Starting Training ---\")\n    # --- Training Loop ---\n    for epoch in range(NUM_EPOCHS):\n        epoch_start_time = time.time()\n        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n        val_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n        epoch_duration = time.time() - epoch_start_time\n\n        train_losses.append(train_loss)\n        val_losses.append(val_metrics['loss'])\n        train_accs.append(train_acc)\n        val_accs.append(val_metrics['accuracy'])\n        val_f1s.append(val_metrics['f1'])\n\n        print(f\"--- Epoch {epoch+1}/{NUM_EPOCHS} ({epoch_duration:.2f} sec) ---\")\n        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        print(f\"  Val Loss:   {val_metrics['loss']:.4f}, Val Acc:  {val_metrics['accuracy']:.4f}\")\n        print(f\"  Val Prec:   {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n        print(f\"  Val AUC:    {val_metrics['auc']:.4f}\")\n        if val_metrics['f1'] > best_val_f1:\n            best_val_f1 = val_metrics['f1']\n            torch.save(model.state_dict(), 'best_model_multistream.pth')\n            print(f\"  >> Saved new best model with F1: {best_val_f1:.4f}\")\n        print(\"-\" * 30)\n    print(\"--- Training Finished ---\")\n\n    print(\"\\n--- Plotting Training History ---\")\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    axes[0].plot(range(1, NUM_EPOCHS + 1), train_losses, label='Train Loss')\n    axes[0].plot(range(1, NUM_EPOCHS + 1), val_losses, label='Val Loss')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Loss Curve')\n    axes[0].legend()\n    axes[0].grid(True)\n\n    axes[1].plot(range(1, NUM_EPOCHS + 1), train_accs, label='Train Accuracy')\n    axes[1].plot(range(1, NUM_EPOCHS + 1), val_accs, label='Val Accuracy')\n    axes[1].plot(range(1, NUM_EPOCHS + 1), val_f1s, label='Val F1-Score', linestyle='--')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Metric')\n    axes[1].set_title('Accuracy & F1-Score Curve')\n    axes[1].legend()\n    axes[1].grid(True)\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n--- Final Evaluation on Validation Set using Best Model ---\")\n    best_model_path = 'best_model_multistream.pth'\n    if os.path.exists(best_model_path):\n        print(f\"Loading best saved model '{best_model_path}'\")\n        model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n    else:\n        print(\"Warning: No saved best model found. Evaluating final model.\")\n\n    final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n    true_labels, pred_labels = get_predictions_and_labels(model, val_loader, DEVICE)\n    cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1])\n    labels_display = ['Not Crossing', 'Crossing']\n    print(\"\\n--- Final Performance Metrics ---\")\n    print(f\"  Accuracy:  {final_metrics['accuracy']:.4f}\")\n    print(f\"  Precision: {final_metrics['precision']:.4f}\")\n    print(f\"  Recall:    {final_metrics['recall']:.4f}\")\n    print(f\"  F1 Score:  {final_metrics['f1']:.4f}\")\n    print(f\"  AUC:       {final_metrics['auc']:.4f}\")\n    print(f\"  Loss:      {final_metrics['loss']:.4f}\")\n\n    print(\"\\n--- Confusion Matrix ---\")\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_display)\n    disp.plot(cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix (Validation Set)')\n    plt.show()\n\n    print(\"\\n--- Script Complete ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:44:07.700588Z","iopub.execute_input":"2025-04-14T12:44:07.700861Z","execution_failed":"2025-04-14T12:57:35.244Z"}},"outputs":[{"name":"stdout","text":"Active Streams: ['bbox', 'ego_speed', 'ego_acc', 'ped_action', 'ped_look', 'traffic_light', 'static_context']\nUsing device: cuda\nChecking for PIE database cache at: /kaggle/working/PIE/data_cache/pie_database.pkl\nLoading PIE database from cache...\nPIE database loaded successfully.\n\nCalculating standardization parameters from training set...\n  Ego Speed: Mean=13.43, Std=13.31\n  Ego AccX: Mean=-0.03, Std=0.08\n  Ego AccY: Mean=-0.52, Std=0.85\n  Ego GyroZ: Mean=-0.04, Std=4.48\nStandardization parameters calculated.\n\nVerifying required directories...\nInput directories verified.\n\nLoading pose data for sets: ['set01', 'set02', 'set04'] from /kaggle/input/pose-data/extracted_poses2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading Pose Sets:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55e76d16cc7449a5a5da8c110d489182"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set01:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set02:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set04:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Finished loading pose data for 3 sets.\nGenerating sequence list from PIE database for sets: ['set01', 'set02', 'set04']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating Sequences:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db6b8fbb41964231a31cd2ec287ea538"}},"metadata":{}},{"name":"stdout","text":"Found 333454 valid sequences from 880 pedestrian tracks.\n\nLoading pose data for sets: ['set05', 'set06'] from /kaggle/input/pose-data/extracted_poses2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading Pose Sets:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9de572b156142bab8e6924a971c88d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set05:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set06:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Finished loading pose data for 2 sets.\nGenerating sequence list from PIE database for sets: ['set05', 'set06']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating Sequences:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"194cff48b1b343fab43d70eeb0bd29cc"}},"metadata":{}},{"name":"stdout","text":"Found 77288 valid sequences from 243 pedestrian tracks.\nInitializing model with streams: ['bbox', 'ego_speed', 'ego_acc', 'ped_action', 'ped_look', 'traffic_light', 'static_context']\n  - Adding stream 'bbox' with input size 4\n  - Adding stream 'ego_speed' with input size 1\n  - Adding stream 'ego_acc' with input size 2\n  - Adding stream 'ped_action' with input size 1\n  - Adding stream 'ped_look' with input size 1\n  - Adding stream 'traffic_light' with input size 4\n  - Adding stream 'static_context' with input size 16\n  Combined feature dimension: 3584\n\n--- Model Architecture ---\nMultiStreamAdaptiveLSTM(\n  (lstms): ModuleDict(\n    (bbox): LSTM(4, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ego_speed): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ego_acc): LSTM(2, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ped_action): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ped_look): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (traffic_light): LSTM(4, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (static_context): LSTM(16, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n  )\n  (attentions): ModuleDict(\n    (bbox): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ego_speed): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ego_acc): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ped_action): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ped_look): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (traffic_light): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (static_context): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc1): Linear(in_features=3584, out_features=1792, bias=True)\n  (relu): ReLU()\n  (fc2): Linear(in_features=1792, out_features=2, bias=True)\n)\nTotal Trainable Parameters: 21,685,257\n------------------------------\n\n--- Starting Training ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/10421 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 1/30 (770.61 sec) ---\n  Train Loss: 0.1466, Train Acc: 0.9393\n  Val Loss:   0.2672, Val Acc:  0.9185\n  Val Prec:   0.8400, Recall: 0.5769, F1: 0.6840\n  Val AUC:    0.9489\n  >> Saved new best model with F1: 0.6840\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/10421 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b1d3861214842e7b56aea7661df4111"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# --- IMPORTS ---\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport xml.etree.ElementTree as ET\nimport os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.preprocessing import StandardScaler # For standardizing ego features\nfrom tqdm.notebook import tqdm\nimport random\nimport math\nimport zipfile\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport time\nimport sys # For path manipulation if needed\n\n# --- Add PIE utilities path if necessary (adjust path) ---\npie_utilities_path = '/kaggle/working/PIE/utilities'\nif pie_utilities_path not in sys.path:\n    sys.path.insert(0, pie_utilities_path)\ntry:\n    # We only need PIE class to generate the database if needed\n    from pie_data import PIE\nexcept ImportError as e:\n    print(f\"Warning: Could not import PIE class from {pie_utilities_path}. Database must already exist. Error: {e}\")\n    PIE = None # Define PIE as None if import fails\n\n# --- Configuration ---\nPIE_ROOT_PATH = '/kaggle/working/PIE' # Path where PIE repo was cloned/unzipped\nVIDEO_INPUT_DIR = '/kaggle/input' # Where pie-setXX video dataset folders are\nPOSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2' # Where setXX subdirs with PKLs are\nPIE_DATABASE_CACHE_PATH = os.path.join(PIE_ROOT_PATH, 'data_cache', 'pie_database.pkl')\n\n# --- Stream Control ---\nACTIVE_STREAMS = [\n    'bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro',\n    'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context'\n]\nprint(f\"Active Streams: {ACTIVE_STREAMS}\")\n\n# Model Hyperparameters\nSEQ_LEN = 30\nPRED_LEN = 1\n# Input Sizes (Define for ALL potential streams)\nINPUT_SIZE_BBOX = 4\nINPUT_SIZE_POSE = 34\nINPUT_SIZE_EGO_SPEED = 1\nINPUT_SIZE_EGO_ACC = 2 # X, Y components\nINPUT_SIZE_EGO_GYRO = 1 # Z component (Yaw rate)\nINPUT_SIZE_PED_ACTION = 1 # 0:standing, 1:walking\nINPUT_SIZE_PED_LOOK = 1 # 0:not-looking, 1:looking\nINPUT_SIZE_PED_OCC = 1 # 0:none, 0.5:part, 1:full (normalized)\nINPUT_SIZE_TL_STATE = 4 # 0:Undef, 1:Red, 2:Yellow, 3:Green (One-Hot)\n# Static Feature Sizes (Matches pie_data.py mappings)\nNUM_SIGNALIZED_CATS = 4\nNUM_INTERSECTION_CATS = 5\nNUM_AGE_CATS = 4\nNUM_GENDER_CATS = 3\nINPUT_SIZE_STATIC = NUM_SIGNALIZED_CATS + NUM_INTERSECTION_CATS + NUM_AGE_CATS + NUM_GENDER_CATS # = 16\n\nLSTM_HIDDEN_SIZE = 256\nNUM_LSTM_LAYERS = 2\nDROPOUT_RATE = 0.3\nNUM_CLASSES = 2\nATTENTION_DIM = 128\n\n# Training Hyperparameters\nLEARNING_RATE = 1e-4 # Possibly lower LR needed after balancing/more features\nBATCH_SIZE = 32\nNUM_EPOCHS = 10 # Increase epochs slightly for balanced data\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\n# Dataset Splits (Using PIE default)\nTRAIN_SETS_STR = ['set01', 'set02', 'set04']\nVAL_SETS_STR = ['set05', 'set06']\nTEST_SETS_STR = ['set03'] # Although not used in training loop\n\n# Mappings (Matches pie_data.py)\nTL_STATE_MAP = {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}\nNUM_TL_STATES = len(TL_STATE_MAP)\nSIGNALIZED_MAP = {'n/a': 0, 'C': 1, 'S': 2, 'CS': 3}\nINTERSECTION_MAP = {'midblock': 0, 'T': 1, 'T-left': 2, 'T-right': 3, 'four-way': 4}\nAGE_MAP = {'child': 0, 'young': 1, 'adult': 2, 'senior': 3}\nGENDER_MAP = {'n/a': 0, 'female': 1, 'male': 2}\n\n# --- Helper: One-Hot Encoding ---\ndef to_one_hot(index, num_classes):\n    vec = np.zeros(num_classes, dtype=np.float32)\n    if 0 <= index < num_classes:\n        vec[index] = 1.0\n    else: # Handle unexpected index\n        vec[0] = 1.0 # Default to first class\n    return vec\n\n# --- Balancing Function ---\ndef balance_samples_count(seq_data, label_type, random_seed=42):\n    print('---------------------------------------------------------')\n    print(f\"Balancing samples based on '{label_type}' key\")\n    if label_type not in seq_data:\n        raise KeyError(f\"Label type '{label_type}' not found.\")\n    try:\n        gt_labels = [lbl[0] for lbl in seq_data[label_type]]\n    except (IndexError, TypeError):\n        raise ValueError(f\"Labels under '{label_type}' not in expected format [[label_val]].\")\n\n    if not all(l in [0, 1] for l in gt_labels):\n        print(f\"Warning: Labels for balancing contain values other than 0 or 1.\")\n\n    num_pos_samples = np.count_nonzero(np.array(gt_labels))\n    num_neg_samples = len(gt_labels) - num_pos_samples\n    new_seq_data = {}\n\n    if num_neg_samples == num_pos_samples:\n        print('Samples already balanced.')\n        return seq_data.copy()\n    else:\n        print(f'Unbalanced: Positive (1): {num_pos_samples} | Negative (0): {num_neg_samples}')\n        majority_label = 0 if num_neg_samples > num_pos_samples else 1\n        minority_count = min(num_neg_samples, num_pos_samples)\n        print(f\"Undersampling majority class ({majority_label}) to match count ({minority_count}).\")\n\n        majority_indices = np.where(np.array(gt_labels) == majority_label)[0]\n        minority_indices = np.where(np.array(gt_labels) != majority_label)[0]\n        np.random.seed(random_seed)\n        keep_majority_indices = np.random.choice(majority_indices, size=minority_count, replace=False)\n        final_indices = np.concatenate((minority_indices, keep_majority_indices))\n        np.random.shuffle(final_indices)\n\n        for k, v_list in seq_data.items():\n            if isinstance(v_list, list) and len(v_list) == len(gt_labels):\n                 try:\n                     # Check if the list contains numpy arrays before converting the whole list\n                     if v_list and isinstance(v_list[0], np.ndarray):\n                         v_array = np.array(v_list)\n                         new_seq_data[k] = list(v_array[final_indices])\n                     else: # Assume list of lists or list of scalars\n                          new_seq_data[k] = [v_list[i] for i in final_indices]\n                 except Exception as e:\n                      print(f\"Error processing key '{k}' during balancing: {e}. Skip.\")\n                      new_seq_data[k] = []\n            else:\n                 print(f\"Warn: Skipping key '{k}' in balancing (not list or len mismatch).\")\n                 new_seq_data[k] = v_list\n\n        # Check if label key still exists after potential errors\n        if label_type in new_seq_data:\n             new_gt_labels = [lbl[0] for lbl in new_seq_data[label_type]]\n             final_pos = np.count_nonzero(np.array(new_gt_labels))\n             final_neg = len(new_gt_labels) - final_pos\n             print(f'Balanced:   Positive (1): {final_pos} | Negative (0): {final_neg}')\n        else:\n             print(\"Error: Label key was lost during balancing process.\")\n\n        print('---------------------------------------------------------')\n        return new_seq_data\n\n\n# --- Dataset Class ---\nclass PIEDataset(Dataset):\n    def __init__(self, pie_database, set_names, pose_data_dir, seq_len, pred_len, scalers=None, active_streams=None):\n        self.pie_db = pie_database\n        self.set_names = set_names\n        self.pose_data_dir = pose_data_dir\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.scalers = scalers or {}\n        self.active_streams = active_streams or []\n        self.sequences = []\n        self.all_pose_data = {}\n        # Store input sizes needed for error handling in __getitem__\n        self._input_sizes_for_error = self._get_input_sizes_dict()\n        self._load_pose_data()\n        self._generate_sequence_list()\n        if not self.sequences:\n            raise ValueError(f\"Dataset init failed: No sequences for {self.set_names}\")\n\n    def _get_input_sizes_dict(self):\n        # Helper to create input sizes dict, needed for error fallback in __getitem__\n        input_sizes = {}\n        for stream in self.active_streams:\n            size_constant_name = f'INPUT_SIZE_{stream.upper()}'\n            special_cases = {'TRAFFIC_LIGHT': 'TL_STATE', 'STATIC_CONTEXT': 'STATIC', 'EGO_SPEED': 'EGO_SPEED', 'EGO_ACC': 'EGO_ACC', 'EGO_GYRO': 'EGO_GYRO', 'PED_ACTION': 'PED_ACTION', 'PED_LOOK': 'PED_LOOK', 'PED_OCCLUSION': 'PED_OCC'}\n            stream_upper_key = stream.upper()\n            suffix = special_cases.get(stream_upper_key)\n            if suffix:\n                 size_constant_name = f'INPUT_SIZE_{suffix}'\n            elif stream == 'bbox': size_constant_name = 'INPUT_SIZE_BBOX'\n            elif stream == 'pose': size_constant_name = 'INPUT_SIZE_POSE'\n            if size_constant_name in globals(): input_sizes[stream] = globals()[size_constant_name]\n            else: input_sizes[stream] = 1 # Default size 1 if not found (should not happen ideally)\n        return input_sizes\n\n    def _load_pose_data(self):\n        print(f\"\\nLoading pose data for sets: {self.set_names} from {self.pose_data_dir}\")\n        sets_loaded_count = 0\n        for set_id in tqdm(self.set_names, desc=\"Loading Pose Sets\"):\n            self.all_pose_data[set_id] = {}\n            pose_set_path = os.path.join(self.pose_data_dir, set_id)\n            if not os.path.isdir(pose_set_path):\n                print(f\"Warn: Pose dir missing for {set_id} at {pose_set_path}\")\n                continue\n            pkl_files_in_set = [f for f in os.listdir(pose_set_path) if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")]\n            if not pkl_files_in_set:\n                continue\n            loaded_video_count = 0\n            for pkl_filename in tqdm(pkl_files_in_set, desc=f\"Loading PKLs for {set_id}\", leave=False):\n                pkl_file_path = os.path.join(pose_set_path, pkl_filename)\n                try:\n                    with open(pkl_file_path, 'rb') as f:\n                        loaded_pkl_content = pickle.load(f)\n                    if len(loaded_pkl_content) != 1:\n                        continue\n                    unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n                    video_id = \"_\".join(unique_video_key.split('_')[1:])\n                    self.all_pose_data[set_id][video_id] = video_data\n                    loaded_video_count += 1\n                except FileNotFoundError:\n                    pass # Expected if some videos failed extraction\n                except Exception as e:\n                    print(f\"Error loading pose PKL {pkl_file_path}: {e}\")\n            if loaded_video_count > 0:\n                sets_loaded_count += 1\n        print(f\"Finished loading pose data for {sets_loaded_count} sets.\")\n\n    def _generate_sequence_list(self):\n        print(f\"Generating sequence list from PIE database for sets: {self.set_names}\")\n        sequence_count = 0\n        ped_count = 0\n        for set_id in tqdm(self.set_names, desc=\"Generating Sequences\"):\n            if set_id not in self.pie_db:\n                continue\n            for video_id, video_data in self.pie_db[set_id].items():\n                if 'ped_annotations' not in video_data:\n                    continue\n                for ped_id, ped_data in video_data['ped_annotations'].items():\n                    ped_count += 1\n                    if 'frames' not in ped_data or len(ped_data['frames']) < self.seq_len + self.pred_len:\n                        continue\n                    sorted_frames = sorted(ped_data['frames'])\n                    for i in range(len(sorted_frames) - self.seq_len - self.pred_len + 1):\n                        start_frame = sorted_frames[i]\n                        end_frame_observe = sorted_frames[i + self.seq_len - 1]\n                        if end_frame_observe - start_frame != self.seq_len - 1:\n                            continue\n                        target_frame_actual_idx = i + self.seq_len + self.pred_len - 1\n                        if target_frame_actual_idx >= len(sorted_frames):\n                             continue # Avoid index out of bounds\n                        target_frame = sorted_frames[target_frame_actual_idx]\n                        if target_frame - end_frame_observe != self.pred_len:\n                             continue\n                        self.sequences.append((set_id, video_id, ped_id, start_frame))\n                        sequence_count += 1\n        print(f\"Found {sequence_count} valid sequences from {ped_count} pedestrian tracks.\")\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        set_id, video_id, ped_id, start_frame = self.sequences[idx]\n        frame_nums = list(range(start_frame, start_frame + self.seq_len))\n        target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n\n        # Get data from the loaded PIE database\n        video_db = self.pie_db.get(set_id, {}).get(video_id, {})\n        ped_db = video_db.get('ped_annotations', {}).get(ped_id, {})\n        ego_db = video_db.get('vehicle_annotations', {}) # Frame -> {sensor: value}\n        traffic_db = video_db.get('traffic_annotations', {}) # ObjID -> {frames:[], state:[], ...}\n        ped_attributes = ped_db.get('attributes', {})\n\n        # Initialize feature sequences\n        feature_sequences = {stream: [] for stream in self.active_streams}\n\n        # Static Features (calculated once)\n        static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32) # Default static\n        if 'static_context' in self.active_streams:\n            sig_idx = ped_attributes.get('signalized', 0)\n            int_idx = ped_attributes.get('intersection', 0)\n            age_idx = ped_attributes.get('age', 2) # Default to 'adult'\n            gen_idx = ped_attributes.get('gender', 0)\n            static_vec = np.concatenate([\n                to_one_hot(sig_idx, NUM_SIGNALIZED_CATS),\n                to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n                to_one_hot(age_idx, NUM_AGE_CATS),\n                to_one_hot(gen_idx, NUM_GENDER_CATS)\n            ])\n            if static_vec.shape[0] != INPUT_SIZE_STATIC: # Sanity check\n                 static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n\n        # Get Target Label\n        label = 0 # Default to not-crossing\n        if 'frames' in ped_db and 'behavior' in ped_db and 'cross' in ped_db['behavior']:\n             try:\n                 target_frame_db_idx = ped_db['frames'].index(target_frame_num)\n                 label = ped_db['behavior']['cross'][target_frame_db_idx]\n                 if label == -1: label = 0 # Map irrelevant to not-crossing\n             except (ValueError, IndexError):\n                 pass # Keep default label\n\n        # Iterate through sequence frames\n        for frame_num in frame_nums:\n            frame_db_idx = -1\n            if 'frames' in ped_db:\n                 try:\n                     frame_db_idx = ped_db['frames'].index(frame_num)\n                 except ValueError:\n                     pass # Frame not found for this pedestrian in this sequence part\n            ego_frame_data = ego_db.get(frame_num, {})\n\n            # --- Extract for ACTIVE streams ---\n            if 'bbox' in self.active_streams:\n                bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32) # Default\n                if frame_db_idx != -1 and 'bbox' in ped_db and len(ped_db['bbox']) > frame_db_idx:\n                    # --- Start Corrected Try/Except ---\n                     try:\n                          x1, y1, x2, y2 = ped_db['bbox'][frame_db_idx]\n                          img_w = video_db.get('width', 1920) # Use default if missing\n                          img_h = video_db.get('height', 1080)\n                          if img_w > 0 and img_h > 0: # Check for valid image dimensions\n                               cx = ((x1 + x2) / 2) / img_w\n                               cy = ((y1 + y2) / 2) / img_h\n                               w = (x2 - x1) / img_w\n                               h = (y2 - y1) / img_h\n                               # Check for valid normalized bbox dimensions\n                               if w > 0 and h > 0 and 0 <= cx <= 1 and 0 <= cy <= 1:\n                                    bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n                     except Exception as e:\n                          # Keep default zero vector if any error occurs during processing\n                          # print(f\"Warning: Error processing bbox F:{frame_num} P:{ped_id} V:{video_id} - {e}\") # Optional warning\n                          pass\n                    # --- End Corrected Try/Except ---\n                feature_sequences['bbox'].append(bbox_norm)\n\n            # --- (Rest of stream extractions - unchanged logic but ensure proper indentation) ---\n            if 'pose' in self.active_streams:\n                pose_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n                vid_pose_data = self.all_pose_data.get(set_id, {}).get(video_id, {})\n                frame_pose_data = vid_pose_data.get(frame_num, {})\n                loaded_pose = frame_pose_data.get(ped_id)\n                if loaded_pose is not None and isinstance(loaded_pose, np.ndarray) and loaded_pose.shape == (INPUT_SIZE_POSE,):\n                    pose_vector = loaded_pose\n                feature_sequences['pose'].append(pose_vector)\n\n            if 'ego_speed' in self.active_streams:\n                speed = ego_frame_data.get('OBD_speed', 0.0)\n                if speed == 0.0: speed = ego_frame_data.get('GPS_speed', 0.0)\n                speed_scaled = (speed - self.scalers.get('ego_speed_mean', 0.0)) / self.scalers.get('ego_speed_std', 1.0)\n                feature_sequences['ego_speed'].append([speed_scaled])\n\n            if 'ego_acc' in self.active_streams:\n                accX = ego_frame_data.get('accX', 0.0)\n                accY = ego_frame_data.get('accY', 0.0)\n                accX_scaled = (accX - self.scalers.get('accX_mean', 0.0)) / self.scalers.get('accX_std', 1.0)\n                accY_scaled = (accY - self.scalers.get('accY_mean', 0.0)) / self.scalers.get('accY_std', 1.0)\n                feature_sequences['ego_acc'].append([accX_scaled, accY_scaled])\n\n            if 'ego_gyro' in self.active_streams:\n                gyroZ = ego_frame_data.get('gyroZ', 0.0)\n                gyroZ_scaled = (gyroZ - self.scalers.get('gyroZ_mean', 0.0)) / self.scalers.get('gyroZ_std', 1.0)\n                feature_sequences['ego_gyro'].append([gyroZ_scaled])\n\n            if 'ped_action' in self.active_streams:\n                action = 0 # Default standing\n                if frame_db_idx != -1 and 'behavior' in ped_db and 'action' in ped_db['behavior'] and len(ped_db['behavior']['action']) > frame_db_idx:\n                     action = ped_db['behavior']['action'][frame_db_idx]\n                feature_sequences['ped_action'].append([float(action)])\n\n            if 'ped_look' in self.active_streams:\n                look = 0 # Default not-looking\n                if frame_db_idx != -1 and 'behavior' in ped_db and 'look' in ped_db['behavior'] and len(ped_db['behavior']['look']) > frame_db_idx:\n                     look = ped_db['behavior']['look'][frame_db_idx]\n                feature_sequences['ped_look'].append([float(look)])\n\n            if 'ped_occlusion' in self.active_streams:\n                occ = 0.0 # Default none\n                if frame_db_idx != -1 and 'occlusion' in ped_db and len(ped_db['occlusion']) > frame_db_idx:\n                     occ_val = ped_db['occlusion'][frame_db_idx]\n                     occ = float(occ_val) / 2.0 # Normalize\n                feature_sequences['ped_occlusion'].append([occ])\n\n            if 'traffic_light' in self.active_streams:\n                state_int = 0\n                for obj_id, obj_data in traffic_db.items():\n                     if obj_data.get('obj_class') == 'traffic_light' and 'frames' in obj_data and 'state' in obj_data:\n                          try:\n                              tl_frame_idx = obj_data['frames'].index(frame_num)\n                              state_val = obj_data['state'][tl_frame_idx]\n                              if state_val != 0:\n                                  state_int = state_val\n                                  break # Found first non-undefined state\n                          except (ValueError, IndexError):\n                              continue\n                feature_sequences['traffic_light'].append(to_one_hot(state_int, NUM_TL_STATES))\n\n            if 'static_context' in self.active_streams:\n                feature_sequences['static_context'].append(static_vec)\n\n        # --- Convert lists to Tensors ---\n        features = {}\n        try:\n            for stream_name in self.active_streams:\n                 features[stream_name] = torch.tensor(np.array(feature_sequences[stream_name], dtype=np.float32), dtype=torch.float32)\n\n        except Exception as e:\n             print(f\"Error converting features idx {idx}: {e}. Return dummy.\")\n             # Use the pre-calculated sizes dictionary for fallback\n             features = {\n                 name: torch.zeros((self.seq_len, self._input_sizes_for_error.get(name, 1)), dtype=torch.float32)\n                 for name in self.active_streams\n             }\n\n        return features, torch.tensor(label, dtype=torch.long)\n\n# --- Wrapper Dataset for Balanced Data ---\nclass BalancedDataset(Dataset):\n    def __init__(self, data_dict, active_streams, label_key='label'):\n        self.active_streams = active_streams\n        self.label_key = label_key\n        if self.label_key not in data_dict or not data_dict[self.label_key]:\n             raise ValueError(f\"Label key '{self.label_key}' missing/empty.\")\n        self.num_samples = len(data_dict[self.label_key])\n        if self.num_samples == 0:\n            print(\"Warning: BalancedDataset initialized with zero samples.\")\n\n        self.features = {}\n        for stream in self.active_streams:\n             if stream in data_dict and data_dict[stream]:\n                 try:\n                     self.features[stream] = torch.tensor(np.array(data_dict[stream]), dtype=torch.float32)\n                 except ValueError as e:\n                      raise ValueError(f\"Error converting stream '{stream}' data: {e}\")\n             else:\n                  raise KeyError(f\"Stream '{stream}' missing/empty in balanced data.\")\n        try:\n            self.labels = torch.tensor([lbl[0] for lbl in data_dict[self.label_key]], dtype=torch.long)\n        except (IndexError, TypeError) as e:\n             raise ValueError(f\"Error converting labels: {e}\")\n\n        for stream in self.active_streams:\n             if len(self.features[stream]) != self.num_samples:\n                 raise ValueError(f\"Len mismatch: Stream '{stream}' ({len(self.features[stream])}) vs Labels ({self.num_samples})\")\n    def __len__(self):\n        return self.num_samples\n    def __getitem__(self, idx):\n        feature_dict = {stream: self.features[stream][idx] for stream in self.active_streams}\n        label = self.labels[idx]\n        return feature_dict, label\n\n# --- Model Architecture ---\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.attention_net = nn.Sequential(\n            nn.Linear(hidden_dim, attention_dim),\n            nn.Tanh(),\n            nn.Linear(attention_dim, 1)\n        )\n    def forward(self, lstm_output):\n        att_scores = self.attention_net(lstm_output).squeeze(2)\n        att_weights = torch.softmax(att_scores, dim=1)\n        context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1)\n        return context_vector, att_weights\nclass MultiStreamAdaptiveLSTM(nn.Module):\n    def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes, attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n        super(MultiStreamAdaptiveLSTM, self).__init__()\n        if not stream_names: raise ValueError(\"stream_names cannot be empty.\")\n        self.stream_names = stream_names\n        self.lstms = nn.ModuleDict()\n        self.attentions = nn.ModuleDict()\n        print(f\"Initializing model with streams: {self.stream_names}\")\n        for name in self.stream_names:\n            if name not in input_sizes: raise KeyError(f\"Input size for stream '{name}' not provided.\")\n            current_input_size = input_sizes[name]\n            print(f\"  - Adding stream '{name}' with input size {current_input_size}\")\n            self.lstms[name] = nn.LSTM(current_input_size, lstm_hidden_size, num_lstm_layers,\n                                       batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0,\n                                       bidirectional=True)\n            self.attentions[name] = Attention(lstm_hidden_size * 2 , attention_dim)\n        num_active_streams = len(self.stream_names)\n        combined_feature_dim = lstm_hidden_size * 2 * num_active_streams\n        print(f\"  Combined feature dimension: {combined_feature_dim}\")\n        self.dropout = nn.Dropout(dropout_rate)\n        intermediate_dim = max(num_classes * 4, combined_feature_dim // 2)\n        self.fc1 = nn.Linear(combined_feature_dim, intermediate_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(intermediate_dim, num_classes)\n    def forward(self, x):\n        stream_context_vectors = []\n        stream_att_weights = {}\n        for name in self.stream_names:\n            if name not in x: print(f\"Warning: Stream '{name}' expected but not in input data.\"); continue\n            lstm_out, _ = self.lstms[name](x[name])\n            context_vector, attention_weights = self.attentions[name](lstm_out)\n            stream_context_vectors.append(context_vector)\n            stream_att_weights[name] = attention_weights\n        if not stream_context_vectors: raise RuntimeError(\"No stream outputs generated.\")\n        fused_features = torch.cat(stream_context_vectors, dim=1)\n        out = self.dropout(fused_features)\n        out = self.relu(self.fc1(out))\n        out = self.dropout(out)\n        logits = self.fc2(out)\n        return logits\n\n# --- Training and Evaluation Functions ---\ndef train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n    active_streams = model.stream_names\n    for features, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n        input_features = {name: features[name].to(device) for name in active_streams if name in features}\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(input_features)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n    avg_loss = total_loss / len(dataloader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    return avg_loss, accuracy\n\ndef evaluate_epoch(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_labels = []\n    all_preds = []\n    all_probs = []\n    active_streams = model.stream_names\n    with torch.no_grad():\n        for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n            input_features = {name: features[name].to(device) for name in active_streams if name in features}\n            labels = labels.to(device)\n            outputs = model(input_features)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            probs = torch.softmax(outputs, dim=1)\n            preds = torch.argmax(probs, dim=1)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n    avg_loss = total_loss / len(dataloader)\n    all_probs = np.array(all_probs); all_labels = np.array(all_labels); all_preds = np.array(all_preds)\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', pos_label=1, zero_division=0)\n    auc = roc_auc_score(all_labels, all_probs[:, 1]) if len(np.unique(all_labels)) > 1 else float('nan')\n    return {'loss': avg_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}\n\ndef get_predictions_and_labels(model, dataloader, device):\n    model.eval(); all_labels = []; all_preds = []\n    active_streams = model.stream_names\n    with torch.no_grad():\n        for features, labels in tqdm(dataloader, desc=\"Generating CM Data\", leave=False):\n             input_features = {name: features[name].to(device) for name in active_streams if name in features}\n             labels = labels.to(device); outputs = model(input_features); preds = torch.argmax(outputs, dim=1)\n             all_labels.extend(labels.cpu().numpy()); all_preds.extend(preds.cpu().numpy())\n    return np.array(all_labels), np.array(all_preds)\n\n# --- Main Execution ---\nif __name__ == '__main__':\n\n    # --- Generate/Load PIE Database ---\n    print(f\"Checking for PIE database cache at: {PIE_DATABASE_CACHE_PATH}\")\n    if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n        if PIE is None: raise ImportError(\"PIE class not imported, cannot generate database.\")\n        print(\"PIE database cache not found. Generating...\");\n        pie_dataset_interface = PIE(data_path=PIE_ROOT_PATH, regen_database=True)\n        pie_database = pie_dataset_interface.generate_database()\n        if not pie_database: raise RuntimeError(\"Failed to generate PIE database.\")\n        print(\"PIE database generated successfully.\")\n    else:\n        print(\"Loading PIE database from cache...\")\n        try:\n            with open(PIE_DATABASE_CACHE_PATH, 'rb') as f: pie_database = pickle.load(f)\n            print(\"PIE database loaded successfully.\")\n        except Exception as e: raise RuntimeError(f\"Failed to load PIE database: {e}\")\n\n    # --- Calculate Standardization Parameters ---\n    print(\"\\nCalculating standardization parameters from training set...\")\n    all_train_ego_speeds = []; all_train_accX = []; all_train_accY = []; all_train_gyroZ = []\n    for set_id in TRAIN_SETS_STR:\n         if set_id in pie_database:\n             for video_id, video_data in pie_database[set_id].items():\n                  if 'vehicle_annotations' in video_data:\n                       for frame_num, ego_frame_data in video_data['vehicle_annotations'].items():\n                           speed = ego_frame_data.get('OBD_speed', 0.0);\n                           if speed == 0.0: speed = ego_frame_data.get('GPS_speed', 0.0)\n                           all_train_ego_speeds.append(speed); all_train_accX.append(ego_frame_data.get('accX', 0.0));\n                           all_train_accY.append(ego_frame_data.get('accY', 0.0)); all_train_gyroZ.append(ego_frame_data.get('gyroZ', 0.0))\n    scalers = {}\n    if all_train_ego_speeds: scalers['ego_speed_mean'] = np.mean(all_train_ego_speeds); scalers['ego_speed_std'] = np.std(all_train_ego_speeds) if np.std(all_train_ego_speeds) > 1e-6 else 1.0; print(f\"  Ego Speed: Mean={scalers['ego_speed_mean']:.2f}, Std={scalers['ego_speed_std']:.2f}\")\n    if all_train_accX: scalers['accX_mean'] = np.mean(all_train_accX); scalers['accX_std'] = np.std(all_train_accX) if np.std(all_train_accX) > 1e-6 else 1.0; scalers['accY_mean'] = np.mean(all_train_accY); scalers['accY_std'] = np.std(all_train_accY) if np.std(all_train_accY) > 1e-6 else 1.0; print(f\"  Ego AccX: Mean={scalers['accX_mean']:.2f}, Std={scalers['accX_std']:.2f}\"); print(f\"  Ego AccY: Mean={scalers['accY_mean']:.2f}, Std={scalers['accY_std']:.2f}\")\n    if all_train_gyroZ: scalers['gyroZ_mean'] = np.mean(all_train_gyroZ); scalers['gyroZ_std'] = np.std(all_train_gyroZ) if np.std(all_train_gyroZ) > 1e-6 else 1.0; print(f\"  Ego GyroZ: Mean={scalers['gyroZ_mean']:.2f}, Std={scalers['gyroZ_std']:.2f}\")\n    print(\"Standardization parameters calculated.\")\n\n    # --- Initialize FULL Datasets ---\n    print(\"\\nInitializing full datasets...\")\n    full_train_dataset = PIEDataset(pie_database, TRAIN_SETS_STR, POSE_DATA_DIR, SEQ_LEN, PRED_LEN, scalers, ACTIVE_STREAMS)\n    val_dataset = PIEDataset(pie_database, VAL_SETS_STR, POSE_DATA_DIR, SEQ_LEN, PRED_LEN, scalers, ACTIVE_STREAMS)\n    if len(full_train_dataset) == 0 or len(val_dataset) == 0: raise ValueError(\"Dataset loading failed.\")\n\n    # --- Prepare and Balance Training Data ---\n    print(\"\\nExtracting training data for balancing...\")\n    training_data_dict = {stream: [] for stream in ACTIVE_STREAMS}; training_data_dict['label'] = []\n    for i in tqdm(range(len(full_train_dataset)), desc=\"Extracting data\"):\n         features, label = full_train_dataset[i]\n         for stream_name in ACTIVE_STREAMS: training_data_dict[stream_name].append(features[stream_name].numpy())\n         training_data_dict['label'].append([label.item()]) # Store label as list containing the item\n    print(f\"Original training samples: {len(training_data_dict['label'])}\")\n    del full_train_dataset # Free memory\n\n    label_key_for_balancing = 'label' # Key used in training_data_dict\n    balanced_train_data_dict = balance_samples_count(training_data_dict, label_type=label_key_for_balancing)\n    del training_data_dict # Free up memory\n\n    # --- Create Balanced Training Dataset and DataLoaders ---\n    print(\"\\nCreating DataLoaders...\")\n    try: balanced_train_dataset = BalancedDataset(balanced_train_data_dict, ACTIVE_STREAMS, label_key=label_key_for_balancing); del balanced_train_data_dict\n    except Exception as e: print(f\"Error creating BalancedDataset: {e}\"); raise\n    if len(balanced_train_dataset) == 0: raise ValueError(\"Balanced training dataset is empty!\")\n\n    train_loader = DataLoader(balanced_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n    print(\"DataLoaders created.\")\n\n    # --- Initialize Model ---\n    input_sizes = {}\n    for stream in ACTIVE_STREAMS:\n        size_constant_name = f'INPUT_SIZE_{stream.upper()}'\n        special_cases = {'TRAFFIC_LIGHT': 'TL_STATE', 'STATIC_CONTEXT': 'STATIC', 'EGO_SPEED': 'EGO_SPEED', 'EGO_ACC': 'EGO_ACC', 'EGO_GYRO': 'EGO_GYRO', 'PED_ACTION': 'PED_ACTION', 'PED_LOOK': 'PED_LOOK', 'PED_OCCLUSION': 'PED_OCC'}\n        stream_upper_key = stream.upper()\n        suffix = special_cases.get(stream_upper_key)\n        if suffix: size_constant_name = f'INPUT_SIZE_{suffix}'\n        elif stream == 'bbox': size_constant_name = 'INPUT_SIZE_BBOX'\n        elif stream == 'pose': size_constant_name = 'INPUT_SIZE_POSE'\n        if size_constant_name in globals(): input_sizes[stream] = globals()[size_constant_name]\n        else: raise ValueError(f\"Input size constant {size_constant_name} not found for stream {stream}\")\n\n    model = MultiStreamAdaptiveLSTM(\n        input_sizes=input_sizes, lstm_hidden_size=LSTM_HIDDEN_SIZE,\n        num_lstm_layers=NUM_LSTM_LAYERS, num_classes=NUM_CLASSES, attention_dim=ATTENTION_DIM,\n        dropout_rate=DROPOUT_RATE, stream_names=ACTIVE_STREAMS ).to(DEVICE)\n\n    print(\"\\n--- Model Architecture ---\"); print(model); num_params = sum(p.numel() for p in model.parameters() if p.requires_grad); print(f\"Total Trainable Parameters: {num_params:,}\"); print(\"-\" * 30)\n\n    # --- Class Weighting & Optimizer ---\n    print(\"\\nCalculating Class Weights for Loss Function...\")\n    balanced_train_labels_list = balanced_train_dataset.labels.tolist() # Use balanced list\n    count_0 = balanced_train_labels_list.count(0); count_1 = balanced_train_labels_list.count(1)\n    total = len(balanced_train_labels_list)\n    if total == 0: print(\"Warning: Balanced dataset empty. Use equal weights.\"); weight_0 = 1.0; weight_1 = 1.0\n    elif count_0 == 0: print(\"Warning: Class 0 missing. Adjust weights.\"); weight_0 = 0.0; weight_1 = 1.0\n    elif count_1 == 0: print(\"Warning: Class 1 missing. Adjust weights.\"); weight_0 = 1.0; weight_1 = 0.0\n    else: weight_0 = total / (2.0 * count_0); weight_1 = total / (2.0 * count_1) # Inverse frequency\n    class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float32).to(DEVICE)\n    print(f\"Using Class Weights for Loss: 0={weight_0:.2f}, 1={weight_1:.2f}\")\n    criterion = nn.CrossEntropyLoss(weight=class_weights)\n    # criterion = nn.CrossEntropyLoss() # Uncomment to disable class weighting\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    best_val_f1 = -1.0; train_losses, val_losses = [], []; train_accs, val_accs = [], []; val_f1s = []\n\n    # --- Training Loop ---\n    print(\"\\n--- Starting Training on Balanced Data---\")\n    for epoch in range(NUM_EPOCHS):\n        epoch_start_time = time.time()\n        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n        val_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n        epoch_duration = time.time() - epoch_start_time\n        train_losses.append(train_loss); val_losses.append(val_metrics['loss'])\n        train_accs.append(train_acc); val_accs.append(val_metrics['accuracy'])\n        val_f1s.append(val_metrics['f1'])\n        print(f\"--- Epoch {epoch+1}/{NUM_EPOCHS} ({epoch_duration:.2f} sec) ---\")\n        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        print(f\"  Val Loss:   {val_metrics['loss']:.4f}, Val Acc:  {val_metrics['accuracy']:.4f}\")\n        print(f\"  Val Prec:   {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n        print(f\"  Val AUC:    {val_metrics['auc']:.4f}\")\n        if val_metrics['f1'] > best_val_f1:\n            best_val_f1 = val_metrics['f1']\n            torch.save(model.state_dict(), 'best_model_balanced.pth')\n            print(f\"  >> Saved new best model with F1: {best_val_f1:.4f}\")\n        print(\"-\" * 30)\n    print(\"--- Training Finished ---\")\n\n    # --- Plotting ---\n    print(\"\\n--- Plotting Training History ---\")\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    axes[0].plot(range(1, NUM_EPOCHS + 1), train_losses, label='Train Loss')\n    axes[0].plot(range(1, NUM_EPOCHS + 1), val_losses, label='Val Loss')\n    axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].set_title('Loss Curve'); axes[0].legend(); axes[0].grid(True)\n    axes[1].plot(range(1, NUM_EPOCHS + 1), train_accs, label='Train Accuracy')\n    axes[1].plot(range(1, NUM_EPOCHS + 1), val_accs, label='Val Accuracy')\n    axes[1].plot(range(1, NUM_EPOCHS + 1), val_f1s, label='Val F1-Score', linestyle='--')\n    axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Metric'); axes[1].set_title('Accuracy & F1-Score Curve'); axes[1].legend(); axes[1].grid(True)\n    plt.tight_layout(); plt.show()\n\n    # --- Final Evaluation ---\n    print(\"\\n--- Final Evaluation on Validation Set using Best Model ---\")\n    best_model_path = 'best_model_balanced.pth'\n    if os.path.exists(best_model_path):\n        print(f\"Loading best saved model '{best_model_path}'\")\n        model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n    else: print(\"Warning: No saved best model found. Evaluating final model.\")\n    final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n    true_labels, pred_labels = get_predictions_and_labels(model, val_loader, DEVICE)\n    cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1])\n    labels_display = ['Not Crossing', 'Crossing']\n    print(\"\\n--- Final Performance Metrics ---\")\n    print(f\"  Accuracy:  {final_metrics['accuracy']:.4f}\"); print(f\"  Precision: {final_metrics['precision']:.4f}\"); print(f\"  Recall:    {final_metrics['recall']:.4f}\"); print(f\"  F1 Score:  {final_metrics['f1']:.4f}\"); print(f\"  AUC:       {final_metrics['auc']:.4f}\"); print(f\"  Loss:      {final_metrics['loss']:.4f}\")\n    print(\"\\n--- Confusion Matrix ---\")\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_display); disp.plot(cmap=plt.cm.Blues); plt.title('Confusion Matrix (Validation Set)'); plt.show()\n\n    print(\"\\n--- Script Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:09:41.051348Z","iopub.execute_input":"2025-04-15T00:09:41.051773Z"}},"outputs":[{"name":"stdout","text":"Active Streams: ['bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro', 'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context']\nUsing device: cuda\nChecking for PIE database cache at: /kaggle/working/PIE/data_cache/pie_database.pkl\nLoading PIE database from cache...\nPIE database loaded successfully.\n\nCalculating standardization parameters from training set...\n  Ego Speed: Mean=13.43, Std=13.31\n  Ego AccX: Mean=-0.03, Std=0.08\n  Ego AccY: Mean=-0.52, Std=0.85\n  Ego GyroZ: Mean=-0.04, Std=4.48\nStandardization parameters calculated.\n\nInitializing full datasets...\n\nLoading pose data for sets: ['set01', 'set02', 'set04'] from /kaggle/input/pose-data/extracted_poses2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading Pose Sets:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9523563ec56426fb5d788494a14335c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set01:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set02:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set04:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Finished loading pose data for 3 sets.\nGenerating sequence list from PIE database for sets: ['set01', 'set02', 'set04']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating Sequences:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"620ae1e8e8fa45bc8414e62e3d8183f7"}},"metadata":{}},{"name":"stdout","text":"Found 333454 valid sequences from 880 pedestrian tracks.\n\nLoading pose data for sets: ['set05', 'set06'] from /kaggle/input/pose-data/extracted_poses2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading Pose Sets:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e019687e68f41c5a644638af6b2fde1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set05:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set06:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Finished loading pose data for 2 sets.\nGenerating sequence list from PIE database for sets: ['set05', 'set06']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating Sequences:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c7ed6ea443949c99e636b98a1a09252"}},"metadata":{}},{"name":"stdout","text":"Found 77288 valid sequences from 243 pedestrian tracks.\n\nExtracting training data for balancing...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting data:   0%|          | 0/333454 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b1656d157f840e499f8a061eaad1479"}},"metadata":{}},{"name":"stdout","text":"Original training samples: 333454\n---------------------------------------------------------\nBalancing samples based on 'label' key\nUnbalanced: Positive (1): 54967 | Negative (0): 278487\nUndersampling majority class (0) to match count (54967).\nBalanced:   Positive (1): 54967 | Negative (0): 54967\n---------------------------------------------------------\n\nCreating DataLoaders...\nDataLoaders created.\nInitializing model with streams: ['bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro', 'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context']\n  - Adding stream 'bbox' with input size 4\n  - Adding stream 'pose' with input size 34\n  - Adding stream 'ego_speed' with input size 1\n  - Adding stream 'ego_acc' with input size 2\n  - Adding stream 'ego_gyro' with input size 1\n  - Adding stream 'ped_action' with input size 1\n  - Adding stream 'ped_look' with input size 1\n  - Adding stream 'ped_occlusion' with input size 1\n  - Adding stream 'traffic_light' with input size 4\n  - Adding stream 'static_context' with input size 16\n  Combined feature dimension: 5120\n\n--- Model Architecture ---\nMultiStreamAdaptiveLSTM(\n  (lstms): ModuleDict(\n    (bbox): LSTM(4, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (pose): LSTM(34, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ego_speed): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ego_acc): LSTM(2, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ego_gyro): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ped_action): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ped_look): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ped_occlusion): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (traffic_light): LSTM(4, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (static_context): LSTM(16, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n  )\n  (attentions): ModuleDict(\n    (bbox): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (pose): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ego_speed): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ego_acc): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ego_gyro): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ped_action): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ped_look): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ped_occlusion): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (traffic_light): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (static_context): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc1): Linear(in_features=5120, out_features=2560, bias=True)\n  (relu): ReLU()\n  (fc2): Linear(in_features=2560, out_features=2, bias=True)\n)\nTotal Trainable Parameters: 34,959,372\n------------------------------\n\nCalculating Class Weights for Loss Function...\nUsing Class Weights for Loss: 0=1.00, 1=1.00\n\n--- Starting Training on Balanced Data---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 1/10 (464.35 sec) ---\n  Train Loss: 0.2201, Train Acc: 0.9059\n  Val Loss:   0.1794, Val Acc:  0.9323\n  Val Prec:   0.7811, Recall: 0.7743, F1: 0.7777\n  Val AUC:    0.9589\n  >> Saved new best model with F1: 0.7777\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 2/10 (462.73 sec) ---\n  Train Loss: 0.1269, Train Acc: 0.9500\n  Val Loss:   0.2194, Val Acc:  0.9281\n  Val Prec:   0.7812, Recall: 0.7358, F1: 0.7578\n  Val AUC:    0.9543\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b74b24b0efea45fcb7da1182c3531cbc"}},"metadata":{}}],"execution_count":null}]}