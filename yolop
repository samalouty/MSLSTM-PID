{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11201333,"sourceType":"datasetVersion","datasetId":6993690},{"sourceId":11201362,"sourceType":"datasetVersion","datasetId":6993708},{"sourceId":11201388,"sourceType":"datasetVersion","datasetId":6993722},{"sourceId":11201422,"sourceType":"datasetVersion","datasetId":6993740},{"sourceId":11201506,"sourceType":"datasetVersion","datasetId":6993794},{"sourceId":11201543,"sourceType":"datasetVersion","datasetId":6993809},{"sourceId":11255589,"sourceType":"datasetVersion","datasetId":7034191},{"sourceId":11382982,"sourceType":"datasetVersion","datasetId":7127490},{"sourceId":11402679,"sourceType":"datasetVersion","datasetId":7142036},{"sourceId":302300,"sourceType":"modelInstanceVersion","modelInstanceId":258142,"modelId":279383},{"sourceId":307831,"sourceType":"modelInstanceVersion","modelInstanceId":262207,"modelId":283333},{"sourceId":316944,"sourceType":"modelInstanceVersion","modelInstanceId":267476,"modelId":288527},{"sourceId":329886,"sourceType":"modelInstanceVersion","modelInstanceId":276781,"modelId":297682},{"sourceId":329908,"sourceType":"modelInstanceVersion","modelInstanceId":276800,"modelId":297702},{"sourceId":352620,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":294156,"modelId":314775}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/aras62/PIE.git\n!unzip /content/PIE/annotations/annotations.zip -d /content/PIE\n!unzip /content/PIE/annotations/annotations_vehicle.zip -d /content/PIE\n# !git clone https://github.com/hustvl/YOLOP.git\n!mkdir /kaggle/working/PIE/content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:27:07.771729Z","iopub.execute_input":"2025-04-19T13:27:07.772029Z","iopub.status.idle":"2025-04-19T13:27:08.263278Z","shell.execute_reply.started":"2025-04-19T13:27:07.771999Z","shell.execute_reply":"2025-04-19T13:27:08.262097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q ultralytics opencv-python-headless ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:05:30.756957Z","iopub.execute_input":"2025-05-01T07:05:30.757137Z","iopub.status.idle":"2025-05-01T07:05:36.274800Z","shell.execute_reply.started":"2025-05-01T07:05:30.757119Z","shell.execute_reply":"2025-05-01T07:05:36.273824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport xml.etree.ElementTree as ET\nimport os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\nfrom tqdm.notebook import tqdm\nimport random\nimport math\nimport zipfile\nimport cv2\nfrom ultralytics import YOLO\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:10.154360Z","iopub.execute_input":"2025-04-15T19:55:10.154719Z","iopub.status.idle":"2025-04-15T19:55:16.590012Z","shell.execute_reply.started":"2025-04-15T19:55:10.154687Z","shell.execute_reply":"2025-04-15T19:55:16.589278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + 'annotations'):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.590843Z","iopub.execute_input":"2025-04-15T19:55:16.591210Z","iopub.status.idle":"2025-04-15T19:55:16.596284Z","shell.execute_reply.started":"2025-04-15T19:55:16.591187Z","shell.execute_reply":"2025-04-15T19:55:16.595261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations_vehicle.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + 'annotations_vehicle'):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.597390Z","iopub.execute_input":"2025-04-15T19:55:16.597839Z","iopub.status.idle":"2025-04-15T19:55:16.618074Z","shell.execute_reply.started":"2025-04-15T19:55:16.597803Z","shell.execute_reply":"2025-04-15T19:55:16.617129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations_attributes.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + \"annotations_attributes\"):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.619029Z","iopub.execute_input":"2025-04-15T19:55:16.619360Z","iopub.status.idle":"2025-04-15T19:55:16.635884Z","shell.execute_reply.started":"2025-04-15T19:55:16.619328Z","shell.execute_reply":"2025-04-15T19:55:16.634975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # -----------------------------------------------------------------------------\n# # CELL 1: DATA PREPARATION & BALANCING  (run once before training)\n# # -----------------------------------------------------------------------------\n# #  This cell:\n# #    1. Loads (or regenerates) the PIE database\n# #    2. Computes per-signal standardisation scalers\n# #    3. Extracts ALL training sequences for every stream\n# #    4. Balances the dataset 50 / 50 on the crossing label\n# #    5. Writes two pickles:\n# #         - /kaggle/working/balanced_train_data.pkl\n# #         - /kaggle/working/scalers.pkl\n# # -----------------------------------------------------------------------------\n\n# import os\n# import sys\n# import time\n# import pickle\n# import gc\n# from pathlib import Path\n\n# import cv2                               # used internally by PIE utilities\n# import numpy as np\n# import torch\n# from torch.utils.data import Dataset\n# from tqdm.notebook import tqdm\n\n# # -----------------------------------------------------------------------------#\n# #                                PIE utilities                                 #\n# # -----------------------------------------------------------------------------#\n# pie_utilities_path = \"/kaggle/working/PIE/utilities\"\n# if pie_utilities_path not in sys.path:\n#     sys.path.insert(0, pie_utilities_path)\n\n# try:\n#     from pie_data import PIE\n# except ImportError as e:\n#     print(\n#         f\"[WARN] Could not import PIE from {pie_utilities_path}. \"\n#         f\"If the DB cache already exists this is fine.\\n→ {e}\"\n#     )\n#     PIE = None\n\n# # -----------------------------------------------------------------------------#\n# #                              configuration                                   #\n# # -----------------------------------------------------------------------------#\n# PIE_ROOT_PATH           = \"/kaggle/working/PIE\"\n# POSE_DATA_DIR           = \"/kaggle/input/pose-data/extracted_poses2\"\n# PIE_DATABASE_CACHE_PATH = \"/kaggle/input/pie-database/pie_database.pkl\"\n\n# TRAIN_SETS_STR = [\"set01\", \"set02\", \"set04\"]\n\n# BALANCED_DATA_PKL_PATH  = \"/kaggle/working/balanced_train_data.pkl\"\n# SCALERS_PKL_PATH        = \"/kaggle/working/scalers.pkl\"\n\n# # Streams used throughout the project ----------------------------------------\n# ALL_POSSIBLE_STREAMS = [\n#     \"bbox\",\n#     \"pose\",\n#     \"ego_speed\",\n#     \"ego_acc\",\n#     \"ego_gyro\",\n#     \"ped_action\",\n#     \"ped_look\",\n#     \"ped_occlusion\",\n#     \"traffic_light\",\n#     \"static_context\",\n# ]\n\n# # Feature sizes & categorical constants --------------------------------------\n# SEQ_LEN, PRED_LEN = 30, 1\n\n# INPUT_SIZE_BBOX       = 4\n# INPUT_SIZE_POSE       = 34\n# INPUT_SIZE_EGO_SPEED  = 1\n# INPUT_SIZE_EGO_ACC    = 2\n# INPUT_SIZE_EGO_GYRO   = 1\n# INPUT_SIZE_PED_ACTION = 1\n# INPUT_SIZE_PED_LOOK   = 1\n# INPUT_SIZE_PED_OCC    = 1\n# INPUT_SIZE_TL_STATE   = 4\n\n# NUM_SIGNALIZED_CATS   = 4\n# NUM_INTERSECTION_CATS = 5\n# NUM_AGE_CATS          = 4\n# NUM_GENDER_CATS       = 3\n# NUM_TRAFFIC_DIR_CATS  = 2\n\n# LANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7: 4, 8: 4}\n# NUM_LANE_CATS   = len(set(LANE_CATEGORIES.values()))\n\n# INPUT_SIZE_STATIC = (\n#     NUM_SIGNALIZED_CATS\n#     + NUM_INTERSECTION_CATS\n#     + NUM_AGE_CATS\n#     + NUM_GENDER_CATS\n#     + NUM_TRAFFIC_DIR_CATS\n#     + NUM_LANE_CATS\n# )  # → 23\n\n# TL_STATE_MAP = {\"__undefined__\": 0, \"red\": 1, \"yellow\": 2, \"green\": 3}\n# NUM_TL_STATES = len(TL_STATE_MAP)\n\n# # -----------------------------------------------------------------------------#\n# #                               helper utils                                   #\n# # -----------------------------------------------------------------------------#\n\n\n# def to_one_hot(index: int, num_classes: int) -> np.ndarray:\n#     vec = np.zeros(num_classes, dtype=np.float32)\n#     vec[int(np.clip(index, 0, num_classes - 1))] = 1.0\n#     return vec\n\n\n# def balance_samples_count(seq_data: dict, label_key: str, seed: int = 42) -> dict:\n#     \"\"\"Undersample majority class so positive and negative labels are equal.\"\"\"\n#     labels = [lbl[0] for lbl in seq_data[label_key]]\n#     n_pos  = int(np.sum(labels))\n#     n_neg  = len(labels) - n_pos\n\n#     if n_pos == n_neg:\n#         print(\"Dataset already balanced.\")\n#         return seq_data.copy()\n\n#     majority_label    = 0 if n_neg > n_pos else 1\n#     minority_count    = min(n_pos, n_neg)\n#     majority_indices  = np.where(np.array(labels) == majority_label)[0]\n#     minority_indices  = np.where(np.array(labels) != majority_label)[0]\n\n#     rng = np.random.default_rng(seed)\n#     keep_majority = rng.choice(majority_indices, size=minority_count, replace=False)\n#     final_indices = np.concatenate([minority_indices, keep_majority])\n#     rng.shuffle(final_indices)\n\n#     balanced = {}\n#     for k, v in seq_data.items():\n#         balanced[k] = [v[i] for i in final_indices]\n\n#     print(f\"Balanced: 1s={minority_count} | 0s={minority_count}\")\n#     return balanced\n\n\n# # -----------------------------------------------------------------------------#\n# #                                PIEDataset                                    #\n# # -----------------------------------------------------------------------------#\n# class PIEDataset(Dataset):\n#     \"\"\"\n#     Lightweight dataset that can generate any subset of the PIE feature streams.\n#     \"\"\"\n\n#     def __init__(\n#         self,\n#         pie_db: dict,\n#         set_names: list[str],\n#         pose_dir: str,\n#         seq_len: int,\n#         pred_len: int,\n#         scalers: dict,\n#         streams_to_generate: list[str],\n#     ):\n#         self.pie_db            = pie_db\n#         self.set_names         = set_names\n#         self.pose_dir          = pose_dir\n#         self.seq_len           = seq_len\n#         self.pred_len          = pred_len\n#         self.scalers           = scalers\n#         self.streams           = streams_to_generate\n#         self._input_sizes      = self._build_input_size_map()\n#         self.all_pose_data     = {}\n#         self.sequences         = []\n\n#         if \"pose\" in self.streams:\n#             self._load_pose_pkls()\n#         self._enumerate_sequences()\n\n#     # ------------------------ internal helpers -------------------------------\n#     def _build_input_size_map(self) -> dict:\n#         special = {\n#             \"TRAFFIC_LIGHT\": \"TL_STATE\",\n#             \"STATIC_CONTEXT\": \"STATIC\",\n#             \"EGO_SPEED\": \"EGO_SPEED\",\n#             \"EGO_ACC\": \"EGO_ACC\",\n#             \"EGO_GYRO\": \"EGO_GYRO\",\n#             \"PED_ACTION\": \"PED_ACTION\",\n#             \"PED_LOOK\": \"PED_LOOK\",\n#             \"PED_OCCLUSION\": \"PED_OCC\",\n#         }\n#         sizes = {}\n#         for s in ALL_POSSIBLE_STREAMS:\n#             const = f\"INPUT_SIZE_{special.get(s.upper(), s.upper())}\"\n#             if s == \"bbox\":\n#                 const = \"INPUT_SIZE_BBOX\"\n#             elif s == \"pose\":\n#                 const = \"INPUT_SIZE_POSE\"\n#             sizes[s] = globals().get(const, 1)\n#         return sizes\n\n#     def _load_pose_pkls(self):\n#         print(\"Loading pose PKLs …\")\n#         for set_id in self.set_names:\n#             set_dir = Path(self.pose_dir) / set_id\n#             if not set_dir.is_dir():\n#                 continue\n#             self.all_pose_data[set_id] = {}\n#             for pkl_path in tqdm(set_dir.glob(f\"{set_id}_*_poses.pkl\"), leave=False):\n#                 try:\n#                     with open(pkl_path, \"rb\") as fp:\n#                         loaded = pickle.load(fp)\n#                 except Exception as e:\n#                     print(f\"[pose load] {pkl_path}: {e}\")\n#                     continue\n\n#                 if len(loaded) != 1:\n#                     continue\n#                 (key, data), *_ = loaded.items()\n#                 vid = \"_\".join(key.split(\"_\")[1:])\n#                 if vid in self.pie_db.get(set_id, {}):\n#                     self.all_pose_data[set_id][vid] = data\n\n#     def _enumerate_sequences(self):\n#         print(\"Enumerating sequences …\")\n#         for set_id in self.set_names:\n#             for vid, vdb in self.pie_db.get(set_id, {}).items():\n#                 for pid, pdb in vdb.get(\"ped_annotations\", {}).items():\n#                     frames = pdb.get(\"frames\", [])\n#                     if len(frames) < self.seq_len + self.pred_len:\n#                         continue\n#                     frames = sorted(frames)\n#                     for i in range(len(frames) - self.seq_len - self.pred_len + 1):\n#                         start = frames[i]\n#                         obs_end = frames[i + self.seq_len - 1]\n#                         if obs_end - start != self.seq_len - 1:\n#                             continue\n#                         target = frames[i + self.seq_len + self.pred_len - 1]\n#                         if target - obs_end != self.pred_len:\n#                             continue\n#                         self.sequences.append((set_id, vid, pid, start))\n#         print(f\"Total sequences: {len(self.sequences)}\")\n\n#     # ------------------ Dataset API ------------------------------------------\n#     def __len__(self):\n#         return len(self.sequences)\n\n#     def __getitem__(self, idx: int):\n#         set_id, vid, pid, start = self.sequences[idx]\n#         vdb  = self.pie_db[set_id][vid]\n#         pdb  = vdb[\"ped_annotations\"][pid]\n#         ego  = vdb.get(\"vehicle_annotations\", {})\n#         tldb = vdb.get(\"traffic_annotations\", {})\n\n#         frame_nums = list(range(start, start + self.seq_len))\n#         target_f   = start + self.seq_len + self.pred_len - 1\n\n#         # label ---------------------------------------------------------------\n#         label = 0\n#         if (\n#             \"frames\" in pdb\n#             and \"behavior\" in pdb\n#             and \"cross\" in pdb[\"behavior\"]\n#             and target_f in pdb[\"frames\"]\n#         ):\n#             try:\n#                 j = pdb[\"frames\"].index(target_f)\n#                 label = pdb[\"behavior\"][\"cross\"][j]\n#                 if label == -1:\n#                     label = 0\n#             except (ValueError, IndexError):\n#                 pass\n\n#         # static context ------------------------------------------------------\n#         static_vec = np.zeros(INPUT_SIZE_STATIC, np.float32)\n#         if \"static_context\" in self.streams:\n#             attr  = pdb.get(\"attributes\", {})\n#             sig   = attr.get(\"signalized\", 0)\n#             intr  = attr.get(\"intersection\", 0)\n#             age   = attr.get(\"age\", 2)\n#             gen   = attr.get(\"gender\", 0)\n#             tdir  = int(attr.get(\"traffic_direction\", 0))\n#             ln    = attr.get(\"num_lanes\", 2)\n#             lncat = LANE_CATEGORIES.get(ln, LANE_CATEGORIES[max(LANE_CATEGORIES)])\n#             static_vec = np.concatenate(\n#                 [\n#                     to_one_hot(sig,  NUM_SIGNALIZED_CATS),\n#                     to_one_hot(intr, NUM_INTERSECTION_CATS),\n#                     to_one_hot(age,  NUM_AGE_CATS),\n#                     to_one_hot(gen,  NUM_GENDER_CATS),\n#                     to_one_hot(tdir, NUM_TRAFFIC_DIR_CATS),\n#                     to_one_hot(lncat, NUM_LANE_CATS),\n#                 ]\n#             ).astype(np.float32)\n\n#         # per-frame feature assembly -----------------------------------------\n#         feats = {s: [] for s in self.streams}\n\n#         for fn in frame_nums:\n#             fidx = -1\n#             if \"frames\" in pdb:\n#                 try:\n#                     fidx = pdb[\"frames\"].index(fn)\n#                 except ValueError:\n#                     pass\n\n#             ego_f = ego.get(fn, {})\n\n#             # bbox ----------------------------------------------------------\n#             if \"bbox\" in self.streams:\n#                 bb = np.zeros(INPUT_SIZE_BBOX, np.float32)\n#                 if (\n#                     fidx != -1\n#                     and \"bbox\" in pdb\n#                     and len(pdb[\"bbox\"]) > fidx\n#                 ):\n#                     try:\n#                         x1, y1, x2, y2 = pdb[\"bbox\"][fidx]\n#                         w_img = vdb.get(\"width\", 1920)\n#                         h_img = vdb.get(\"height\", 1080)\n#                         if w_img > 0 and h_img > 0:\n#                             cx = ((x1 + x2) / 2) / w_img\n#                             cy = ((y1 + y2) / 2) / h_img\n#                             w  = (x2 - x1) / w_img\n#                             h  = (y2 - y1) / h_img\n#                             if 0 < w and 0 < h and 0 <= cx <= 1 and 0 <= cy <= 1:\n#                                 bb = np.array([cx, cy, w, h], np.float32)\n#                     except Exception:\n#                         pass\n#                 feats[\"bbox\"].append(bb)\n\n#             # pose ----------------------------------------------------------\n#             if \"pose\" in self.streams:\n#                 pvec = np.zeros(INPUT_SIZE_POSE, np.float32)\n#                 pose_set = self.all_pose_data.get(set_id, {}).get(vid, {})\n#                 p_loaded = pose_set.get(fn, {}).get(pid)\n#                 if (\n#                     isinstance(p_loaded, np.ndarray)\n#                     and p_loaded.shape == (INPUT_SIZE_POSE,)\n#                 ):\n#                     pvec = p_loaded\n#                 feats[\"pose\"].append(pvec)\n\n#             # ego signals ---------------------------------------------------\n#             if \"ego_speed\" in self.streams:\n#                 s = ego_f.get(\"OBD_speed\", 0.0) or ego_f.get(\"GPS_speed\", 0.0)\n#                 s = (s - self.scalers.get(\"ego_speed_mean\", 0.0)) / self.scalers.get(\n#                     \"ego_speed_std\", 1.0\n#                 )\n#                 feats[\"ego_speed\"].append([s])\n\n#             if \"ego_acc\" in self.streams:\n#                 ax = ego_f.get(\"accX\", 0.0)\n#                 ay = ego_f.get(\"accY\", 0.0)\n#                 ax = (ax - self.scalers.get(\"accX_mean\", 0.0)) / self.scalers.get(\n#                     \"accX_std\", 1.0\n#                 )\n#                 ay = (ay - self.scalers.get(\"accY_mean\", 0.0)) / self.scalers.get(\n#                     \"accY_std\", 1.0\n#                 )\n#                 feats[\"ego_acc\"].append([ax, ay])\n\n#             if \"ego_gyro\" in self.streams:\n#                 gz = ego_f.get(\"gyroZ\", 0.0)\n#                 gz = (gz - self.scalers.get(\"gyroZ_mean\", 0.0)) / self.scalers.get(\n#                     \"gyroZ_std\", 1.0\n#                 )\n#                 feats[\"ego_gyro\"].append([gz])\n\n#             # pedestrian behaviour -----------------------------------------\n#             if \"ped_action\" in self.streams:\n#                 action = (\n#                     pdb[\"behavior\"][\"action\"][fidx]\n#                     if fidx != -1\n#                     and \"behavior\" in pdb\n#                     and \"action\" in pdb[\"behavior\"]\n#                     and len(pdb[\"behavior\"][\"action\"]) > fidx\n#                     else 0\n#                 )\n#                 feats[\"ped_action\"].append([float(action)])\n\n#             if \"ped_look\" in self.streams:\n#                 look = (\n#                     pdb[\"behavior\"][\"look\"][fidx]\n#                     if fidx != -1\n#                     and \"behavior\" in pdb\n#                     and \"look\" in pdb[\"behavior\"]\n#                     and len(pdb[\"behavior\"][\"look\"]) > fidx\n#                     else 0\n#                 )\n#                 feats[\"ped_look\"].append([float(look)])\n\n#             if \"ped_occlusion\" in self.streams:\n#                 occ = (\n#                     float(pdb[\"occlusion\"][fidx]) / 2.0\n#                     if fidx != -1\n#                     and \"occlusion\" in pdb\n#                     and len(pdb[\"occlusion\"]) > fidx\n#                     else 0.0\n#                 )\n#                 feats[\"ped_occlusion\"].append([occ])\n\n#             # traffic light -------------------------------------------------\n#             if \"traffic_light\" in self.streams:\n#                 tl_state = 0\n#                 for obj in tldb.values():\n#                     if obj.get(\"obj_class\") != \"traffic_light\":\n#                         continue\n#                     if \"frames\" not in obj or \"state\" not in obj:\n#                         continue\n#                     try:\n#                         j = obj[\"frames\"].index(fn)\n#                         if obj[\"state\"][j] != 0:\n#                             tl_state = obj[\"state\"][j]\n#                             break\n#                     except (ValueError, IndexError):\n#                         continue\n#                 feats[\"traffic_light\"].append(to_one_hot(tl_state, NUM_TL_STATES))\n\n#             # static context -----------------------------------------------\n#             if \"static_context\" in self.streams:\n#                 feats[\"static_context\"].append(static_vec)\n\n#         # numpy → torch ------------------------------------------------------\n#         out = {\n#             s: torch.tensor(np.asarray(feats[s], np.float32), dtype=torch.float32)\n#             for s in self.streams\n#         }\n#         return out, torch.tensor(label, dtype=torch.long)\n\n\n# # =============================================================================\n# #                       MAIN: build balanced training set\n# # =============================================================================\n# if __name__ == \"__main__\":\n#     print(\"\\n--- DATA PREPARATION ---\")\n\n#     # 1) load / regenerate PIE DB -------------------------------------------\n#     cache = Path(PIE_DATABASE_CACHE_PATH)\n#     if cache.is_file():\n#         print(\"Loading PIE database cache …\")\n#         with cache.open(\"rb\") as fp:\n#             pie_db = pickle.load(fp)\n#         print(\"✓ PIE DB loaded.\")\n#     else:\n#         if PIE is None:\n#             raise RuntimeError(\"PIE class unavailable: cannot rebuild database.\")\n#         print(\"Cache not found – regenerating PIE DB …\")\n#         pie_db = PIE(data_path=PIE_ROOT_PATH, regen_database=True).generate_database()\n#         if not pie_db:\n#             raise RuntimeError(\"PIE DB generation failed.\")\n#         print(\"✓ PIE DB generated.\")\n\n#     # 2) compute scalers -----------------------------------------------------\n#     print(\"\\nComputing scalers …\")\n#     spd, accx, accy, gyz = [], [], [], []\n#     for sid in TRAIN_SETS_STR:\n#         for vid, vdb in pie_db.get(sid, {}).items():\n#             for frame, e in vdb.get(\"vehicle_annotations\", {}).items():\n#                 s  = e.get(\"OBD_speed\", 0.0) or e.get(\"GPS_speed\", 0.0)\n#                 spd.append(s)\n#                 accx.append(e.get(\"accX\", 0.0))\n#                 accy.append(e.get(\"accY\", 0.0))\n#                 gyz.append(e.get(\"gyroZ\", 0.0))\n\n#     scalers = {}\n#     if spd:\n#         scalers[\"ego_speed_mean\"] = float(np.mean(spd))\n#         scalers[\"ego_speed_std\"]  = float(max(np.std(spd), 1e-6))\n#     if accx:\n#         scalers[\"accX_mean\"] = float(np.mean(accx))\n#         scalers[\"accX_std\"]  = float(max(np.std(accx), 1e-6))\n#         scalers[\"accY_mean\"] = float(np.mean(accy))\n#         scalers[\"accY_std\"]  = float(max(np.std(accy), 1e-6))\n#     if gyz:\n#         scalers[\"gyroZ_mean\"] = float(np.mean(gyz))\n#         scalers[\"gyroZ_std\"]  = float(max(np.std(gyz), 1e-6))\n\n#     print(\"Scalers:\", scalers)\n\n#     # 3) extract full training dataset --------------------------------------\n#     print(\"\\nExtracting training sequences (all streams) …\")\n#     full_ds = PIEDataset(\n#         pie_db,\n#         TRAIN_SETS_STR,\n#         POSE_DATA_DIR,\n#         SEQ_LEN,\n#         PRED_LEN,\n#         scalers,\n#         ALL_POSSIBLE_STREAMS,\n#     )\n\n#     train_dict = {s: [] for s in ALL_POSSIBLE_STREAMS}\n#     train_dict[\"label\"] = []\n\n#     for i in tqdm(range(len(full_ds)), desc=\"seq\"):\n#         feat, lbl = full_ds[i]\n#         for s in ALL_POSSIBLE_STREAMS:\n#             train_dict[s].append(feat[s].numpy())\n#         train_dict[\"label\"].append([lbl.item()])\n\n#     print(f\"Raw training samples: {len(train_dict['label'])}\")\n\n#     # 4) balance -------------------------------------------------------------\n#     balanced = balance_samples_count(train_dict, \"label\")\n#     del train_dict, full_ds\n#     gc.collect()\n\n#     # 5) write pickles -------------------------------------------------------\n#     print(\"\\nSaving balanced data …\")\n#     with open(BALANCED_DATA_PKL_PATH, \"wb\") as fp:\n#         pickle.dump(balanced, fp, pickle.HIGHEST_PROTOCOL)\n#     print(f\"✓ {BALANCED_DATA_PKL_PATH}\")\n\n#     print(\"Saving scalers …\")\n#     with open(SCALERS_PKL_PATH, \"wb\") as fp:\n#         pickle.dump(scalers, fp, pickle.HIGHEST_PROTOCOL)\n#     print(f\"✓ {SCALERS_PKL_PATH}\")\n\n#     del pie_db\n#     gc.collect()\n\n#     print(\"\\n--- DATA PREPARATION COMPLETE ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- CELL 2: ABLATION STUDY – MODEL TRAINING AND EVALUATION (with Weighted Average Fusion) ---\n\n# import os\n# import sys\n# import gc\n# import time\n# import math\n# import random\n# import pickle\n# import torch\n# import numpy as np\n# import pandas as pd                      # results-summary table\n# import torch.nn as nn\n# import torch.optim as optim\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# from tqdm.notebook import tqdm\n# from torch.utils.data import Dataset, DataLoader\n# from sklearn.metrics import (\n#     accuracy_score,\n#     precision_recall_fscore_support,\n#     roc_auc_score,\n#     confusion_matrix,\n#     ConfusionMatrixDisplay,\n# )\n\n# # --- Add PIE utilities path if necessary (adjust path) ------------------------\n# pie_utilities_path = \"/kaggle/working/PIE/utilities\"\n# if pie_utilities_path not in sys.path:\n#     sys.path.insert(0, pie_utilities_path)\n\n# try:\n#     from pie_data import PIE\n# except ImportError as e:\n#     print(f\"Warn: Could not import PIE class: {e}\")\n#     PIE = None\n\n# # --- Configuration ------------------------------------------------------------\n# PIE_ROOT_PATH = \"/kaggle/working/PIE\"\n# POSE_DATA_DIR = \"/kaggle/input/pose-data/extracted_poses2\"\n# PIE_DATABASE_CACHE_PATH = \"/kaggle/input/pie-database/pie_database.pkl\"\n\n# # --- Define ALL possible streams (used by Dataset class) ----------------------\n# ALL_POSSIBLE_STREAMS = [\n#     \"bbox\",\n#     \"pose\",\n#     \"ego_speed\",\n#     \"ego_acc\",\n#     \"ego_gyro\",\n#     \"ped_action\",\n#     \"ped_look\",\n#     \"ped_occlusion\",\n#     \"traffic_light\",\n#     \"static_context\",\n# ]\n\n# # --- *** CHOOSE ACTIVE STREAMS FOR THIS EXPERIMENT *** ------------------------\n# ACTIVE_STREAMS = [\n#     \"bbox\",\n#     \"pose\",\n#     \"ego_speed\",\n#     \"ego_acc\",\n#     \"ego_gyro\",\n#     \"ped_action\",\n#     \"ped_look\",\n#     \"ped_occlusion\",\n#     \"traffic_light\",\n#     \"static_context\",\n# ]\n# # ------------------------------------------------------------------------------\n\n# print(f\"--- Running Weighted Average Fusion With Active Streams: {ACTIVE_STREAMS} ---\")\n\n# # --- Model Hyper-parameters ---------------------------------------------------\n# SEQ_LEN, PRED_LEN = 30, 1\n# INPUT_SIZE_BBOX = 4\n# INPUT_SIZE_POSE = 34\n# INPUT_SIZE_EGO_SPEED = 1\n# INPUT_SIZE_EGO_ACC = 2\n# INPUT_SIZE_EGO_GYRO = 1\n# INPUT_SIZE_PED_ACTION = 1\n# INPUT_SIZE_PED_LOOK = 1\n# INPUT_SIZE_PED_OCC = 1\n# INPUT_SIZE_TL_STATE = 4\n# NUM_SIGNALIZED_CATS = 4\n# NUM_INTERSECTION_CATS = 5\n# NUM_AGE_CATS = 4\n# NUM_GENDER_CATS = 3\n# NUM_TRAFFIC_DIR_CATS = 2\n# LANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7: 4, 8: 4}\n# NUM_LANE_CATS = len(set(LANE_CATEGORIES.values()))\n# INPUT_SIZE_STATIC = (\n#     NUM_SIGNALIZED_CATS\n#     + NUM_INTERSECTION_CATS\n#     + NUM_AGE_CATS\n#     + NUM_GENDER_CATS\n#     + NUM_TRAFFIC_DIR_CATS\n#     + NUM_LANE_CATS\n# )\n\n# LSTM_HIDDEN_SIZE = 256\n# NUM_LSTM_LAYERS = 2\n# DROPOUT_RATE = 0.3\n# NUM_CLASSES = 2\n# ATTENTION_DIM = 128\n\n# # --- Training Hyper-parameters ------------------------------------------------\n# LEARNING_RATE = 1e-4\n# BATCH_SIZE = 32\n# NUM_EPOCHS = 30  \n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {DEVICE}\")\n\n# # --- Dataset splits -----------------------------------------------------------\n# VAL_SETS_STR = [\"set05\", \"set06\"]\n\n# # --- Paths for pre-processed data --------------------------------------------\n# BALANCED_DATA_PKL_PATH = \"/kaggle/working/balanced_train_data.pkl\"\n# SCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\"\n\n# # -----------------------------------------------------------------------------#\n# #                               Helper classes                                #\n# # -----------------------------------------------------------------------------\n\n\n# def to_one_hot(index, num_classes):\n#     vec = np.zeros(num_classes, dtype=np.float32)\n#     safe_index = int(np.clip(index, 0, num_classes - 1))\n#     vec[safe_index] = 1.0\n#     return vec\n\n\n# class PIEDataset(Dataset):\n#     \"\"\"\n#     Dataset that can dynamically enable/disable streams.\n#     \"\"\"\n\n#     def __init__(\n#         self,\n#         pie_database,\n#         set_names,\n#         pose_data_dir,\n#         seq_len,\n#         pred_len,\n#         scalers=None,\n#         active_streams=None,\n#     ):\n#         self.pie_db = pie_database\n#         self.set_names = set_names\n#         self.pose_data_dir = pose_data_dir\n#         self.seq_len = seq_len\n#         self.pred_len = pred_len\n#         self.scalers = scalers or {}\n#         self.active_streams = active_streams or ALL_POSSIBLE_STREAMS\n#         self.sequences = []\n#         self.all_pose_data = {}\n\n#         self._input_sizes_for_error = self._get_input_sizes_dict()\n\n#         if \"pose\" in self.active_streams:\n#             self._load_pose_data()\n\n#         self._generate_sequence_list()\n#         if not self.sequences:\n#             raise ValueError(f\"Dataset init failed: No sequences for {self.set_names}\")\n\n#     # --------------------------------------------------------------------- #\n#     #                        internal helper methods                        #\n#     # --------------------------------------------------------------------- #\n\n#     def _get_input_sizes_dict(self):\n#         \"\"\"\n#         Build a dict {stream_name: feature_size}.\n#         \"\"\"\n#         input_sizes = {}\n#         special_cases = {\n#             \"TRAFFIC_LIGHT\": \"TL_STATE\",\n#             \"STATIC_CONTEXT\": \"STATIC\",\n#             \"EGO_SPEED\": \"EGO_SPEED\",\n#             \"EGO_ACC\": \"EGO_ACC\",\n#             \"EGO_GYRO\": \"EGO_GYRO\",\n#             \"PED_ACTION\": \"PED_ACTION\",\n#             \"PED_LOOK\": \"PED_LOOK\",\n#             \"PED_OCCLUSION\": \"PED_OCC\",\n#         }\n\n#         for stream in ALL_POSSIBLE_STREAMS:\n#             size_constant_name = f\"INPUT_SIZE_{stream.upper()}\"\n#             stream_upper_key = stream.upper()\n#             suffix = special_cases.get(stream_upper_key)\n\n#             if suffix:\n#                 size_constant_name = f\"INPUT_SIZE_{suffix}\"\n#             elif stream == \"bbox\":\n#                 size_constant_name = \"INPUT_SIZE_BBOX\"\n#             elif stream == \"pose\":\n#                 size_constant_name = \"INPUT_SIZE_POSE\"\n\n#             input_sizes[stream] = globals().get(size_constant_name, 1)\n\n#         return input_sizes\n\n#     def _load_pose_data(self):\n#         \"\"\"\n#         Load pose dictionaries once per dataset instance.\n#         \"\"\"\n#         sets_loaded_count = 0\n#         for set_id in self.set_names:\n#             self.all_pose_data[set_id] = {}\n#             pose_set_path = os.path.join(self.pose_data_dir, set_id)\n#             if not os.path.isdir(pose_set_path):\n#                 continue\n\n#             pkl_files_in_set = [\n#                 f\n#                 for f in os.listdir(pose_set_path)\n#                 if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")\n#             ]\n#             if not pkl_files_in_set:\n#                 continue\n\n#             loaded_video_count = 0\n#             for pkl_filename in pkl_files_in_set:\n#                 pkl_file_path = os.path.join(pose_set_path, pkl_filename)\n#                 try:\n#                     with open(pkl_file_path, \"rb\") as f:\n#                         loaded_pkl_content = pickle.load(f)\n#                 except FileNotFoundError:\n#                     continue\n#                 except Exception as e:\n#                     print(f\"Error loading pose PKL {pkl_file_path}: {e}\")\n#                     continue\n\n#                 # Every pose-PKL contains a single key (video), by convention\n#                 if len(loaded_pkl_content) != 1:\n#                     continue\n\n#                 unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n#                 video_id = \"_\".join(unique_video_key.split(\"_\")[1:])\n#                 if video_id in self.pie_db.get(set_id, {}):\n#                     self.all_pose_data[set_id][video_id] = video_data\n#                     loaded_video_count += 1\n\n#             if loaded_video_count > 0:\n#                 sets_loaded_count += 1\n\n#     def _generate_sequence_list(self):\n#         \"\"\"\n#         Enumerate every sliding window that satisfies length + prediction horizon.\n#         \"\"\"\n#         sequence_count = 0\n#         for set_id in self.set_names:\n#             if set_id not in self.pie_db:\n#                 continue\n#             for video_id, video_data in self.pie_db[set_id].items():\n#                 if \"ped_annotations\" not in video_data:\n#                     continue\n#                 for ped_id, ped_data in video_data[\"ped_annotations\"].items():\n#                     frames = ped_data.get(\"frames\", [])\n#                     if len(frames) < self.seq_len + self.pred_len:\n#                         continue\n\n#                     frames_sorted = sorted(frames)\n#                     for i in range(len(frames_sorted) - self.seq_len - self.pred_len + 1):\n#                         start_f = frames_sorted[i]\n#                         obs_end_f = frames_sorted[i + self.seq_len - 1]\n\n#                         # consecutiveness check\n#                         if obs_end_f - start_f != self.seq_len - 1:\n#                             continue\n\n#                         target_idx = i + self.seq_len + self.pred_len - 1\n#                         if target_idx >= len(frames_sorted):\n#                             continue\n\n#                         target_f = frames_sorted[target_idx]\n#                         if target_f - obs_end_f != self.pred_len:\n#                             continue\n\n#                         self.sequences.append((set_id, video_id, ped_id, start_f))\n#                         sequence_count += 1\n\n#         print(f\"Dataset initialized with {sequence_count} sequences for sets {self.set_names}.\")\n\n#     # --------------------------------------------------------------------- #\n#     #                              overrides                                #\n#     # --------------------------------------------------------------------- #\n\n#     def __len__(self):\n#         return len(self.sequences)\n\n#     def __getitem__(self, idx):\n#         \"\"\"\n#         Returns:\n#             features_dict   {stream_name: Tensor(seq_len, feat_dim)}\n#             label_tensor    Tensor([])\n#         \"\"\"\n#         set_id, video_id, ped_id, start_frame = self.sequences[idx]\n#         frame_nums = list(range(start_frame, start_frame + self.seq_len))\n#         target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n\n#         # convenient aliases\n#         video_db = self.pie_db.get(set_id, {}).get(video_id, {})\n#         ped_db = video_db.get(\"ped_annotations\", {}).get(ped_id, {})\n#         ego_db = video_db.get(\"vehicle_annotations\", {})\n#         traffic_db = video_db.get(\"traffic_annotations\", {})\n#         ped_attributes = ped_db.get(\"attributes\", {})\n\n#         feature_sequences = {s: [] for s in self.active_streams}\n#         label = 0\n\n#         if (\n#             \"frames\" in ped_db\n#             and \"behavior\" in ped_db\n#             and \"cross\" in ped_db[\"behavior\"]\n#         ):\n#             try:\n#                 target_idx = ped_db[\"frames\"].index(target_frame_num)\n#                 label = ped_db[\"behavior\"][\"cross\"][target_idx]\n#                 if label == -1:\n#                     label = 0\n#             except (ValueError, IndexError):\n#                 pass\n\n#         # --- static context ------------------------------------------------\n#         static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n#         if \"static_context\" in self.active_streams:\n#             sig_idx = ped_attributes.get(\"signalized\", 0)\n#             int_idx = ped_attributes.get(\"intersection\", 0)\n#             age_idx = ped_attributes.get(\"age\", 2)\n#             gen_idx = ped_attributes.get(\"gender\", 0)\n#             td_idx = int(ped_attributes.get(\"traffic_direction\", 0))\n#             nl_val = ped_attributes.get(\"num_lanes\", 2)\n#             nl_cat_idx = LANE_CATEGORIES.get(\n#                 nl_val, LANE_CATEGORIES[max(LANE_CATEGORIES.keys())]\n#             )\n\n#             static_vec = np.concatenate(\n#                 [\n#                     to_one_hot(sig_idx, NUM_SIGNALIZED_CATS),\n#                     to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n#                     to_one_hot(age_idx, NUM_AGE_CATS),\n#                     to_one_hot(gen_idx, NUM_GENDER_CATS),\n#                     to_one_hot(td_idx, NUM_TRAFFIC_DIR_CATS),\n#                     to_one_hot(nl_cat_idx, NUM_LANE_CATS),\n#                 ]\n#             )\n\n#             if static_vec.shape[0] != INPUT_SIZE_STATIC:\n#                 static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n\n#         # -------------------------------------------------------------------\n#         #                    per-frame feature extraction                    #\n#         # -------------------------------------------------------------------\n#         for frame_num in frame_nums:\n#             frame_db_idx = -1\n#             if \"frames\" in ped_db:\n#                 try:\n#                     frame_db_idx = ped_db[\"frames\"].index(frame_num)\n#                 except ValueError:\n#                     pass\n\n#             ego_frame_data = ego_db.get(frame_num, {})\n\n#             # ---------- bbox ------------------------------------------------\n#             if \"bbox\" in self.active_streams:\n#                 bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32)\n#                 if (\n#                     frame_db_idx != -1\n#                     and \"bbox\" in ped_db\n#                     and len(ped_db[\"bbox\"]) > frame_db_idx\n#                 ):\n#                     try:\n#                         x1, y1, x2, y2 = ped_db[\"bbox\"][frame_db_idx]\n#                         img_w = video_db.get(\"width\", 1920)\n#                         img_h = video_db.get(\"height\", 1080)\n#                         if img_w > 0 and img_h > 0:\n#                             cx = ((x1 + x2) / 2) / img_w\n#                             cy = ((y1 + y2) / 2) / img_h\n#                             w = (x2 - x1) / img_w\n#                             h = (y2 - y1) / img_h\n#                             if 0 < w and 0 < h and 0 <= cx <= 1 and 0 <= cy <= 1:\n#                                 bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n#                     except Exception:\n#                         pass\n\n#                 feature_sequences[\"bbox\"].append(bbox_norm)\n\n#             # ---------- pose -----------------------------------------------\n#             if \"pose\" in self.active_streams:\n#                 pose_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n#                 vid_pose = self.all_pose_data.get(set_id, {}).get(video_id, {})\n#                 frame_pose = vid_pose.get(frame_num, {})\n#                 loaded_pose = frame_pose.get(ped_id)\n\n#                 if (\n#                     loaded_pose is not None\n#                     and isinstance(loaded_pose, np.ndarray)\n#                     and loaded_pose.shape == (INPUT_SIZE_POSE,)\n#                 ):\n#                     pose_vector = loaded_pose\n\n#                 feature_sequences[\"pose\"].append(pose_vector)\n\n#             # ---------- ego-speed ------------------------------------------\n#             if \"ego_speed\" in self.active_streams:\n#                 speed = ego_frame_data.get(\"OBD_speed\", 0.0)\n#                 if speed == 0.0:\n#                     speed = ego_frame_data.get(\"GPS_speed\", 0.0)\n\n#                 speed_scaled = (\n#                     speed - self.scalers.get(\"ego_speed_mean\", 0.0)\n#                 ) / self.scalers.get(\"ego_speed_std\", 1.0)\n#                 feature_sequences[\"ego_speed\"].append([speed_scaled])\n\n#             # ---------- ego-acc --------------------------------------------\n#             if \"ego_acc\" in self.active_streams:\n#                 acc_x = ego_frame_data.get(\"accX\", 0.0)\n#                 acc_y = ego_frame_data.get(\"accY\", 0.0)\n#                 acc_x_scaled = (\n#                     acc_x - self.scalers.get(\"accX_mean\", 0.0)\n#                 ) / self.scalers.get(\"accX_std\", 1.0)\n#                 acc_y_scaled = (\n#                     acc_y - self.scalers.get(\"accY_mean\", 0.0)\n#                 ) / self.scalers.get(\"accY_std\", 1.0)\n#                 feature_sequences[\"ego_acc\"].append([acc_x_scaled, acc_y_scaled])\n\n#             # ---------- ego-gyro -------------------------------------------\n#             if \"ego_gyro\" in self.active_streams:\n#                 gyro_z = ego_frame_data.get(\"gyroZ\", 0.0)\n#                 gyro_z_scaled = (\n#                     gyro_z - self.scalers.get(\"gyroZ_mean\", 0.0)\n#                 ) / self.scalers.get(\"gyroZ_std\", 1.0)\n#                 feature_sequences[\"ego_gyro\"].append([gyro_z_scaled])\n\n#             # ---------- ped_action -----------------------------------------\n#             if \"ped_action\" in self.active_streams:\n#                 action = 0\n#                 if (\n#                     frame_db_idx != -1\n#                     and \"behavior\" in ped_db\n#                     and \"action\" in ped_db[\"behavior\"]\n#                     and len(ped_db[\"behavior\"][\"action\"]) > frame_db_idx\n#                 ):\n#                     action = ped_db[\"behavior\"][\"action\"][frame_db_idx]\n#                 feature_sequences[\"ped_action\"].append([float(action)])\n\n#             # ---------- ped_look -------------------------------------------\n#             if \"ped_look\" in self.active_streams:\n#                 look = 0\n#                 if (\n#                     frame_db_idx != -1\n#                     and \"behavior\" in ped_db\n#                     and \"look\" in ped_db[\"behavior\"]\n#                     and len(ped_db[\"behavior\"][\"look\"]) > frame_db_idx\n#                 ):\n#                     look = ped_db[\"behavior\"][\"look\"][frame_db_idx]\n#                 feature_sequences[\"ped_look\"].append([float(look)])\n\n#             # ---------- ped_occlusion --------------------------------------\n#             if \"ped_occlusion\" in self.active_streams:\n#                 occ = 0.0\n#                 if (\n#                     frame_db_idx != -1\n#                     and \"occlusion\" in ped_db\n#                     and len(ped_db[\"occlusion\"]) > frame_db_idx\n#                 ):\n#                     occ_val = ped_db[\"occlusion\"][frame_db_idx]\n#                     occ = float(occ_val) / 2.0\n#                 feature_sequences[\"ped_occlusion\"].append([occ])\n\n#             # ---------- traffic_light --------------------------------------\n#             if \"traffic_light\" in self.active_streams:\n#                 state_int = 0\n#                 for obj_id, obj_data in traffic_db.items():\n#                     if (\n#                         obj_data.get(\"obj_class\") == \"traffic_light\"\n#                         and \"frames\" in obj_data\n#                         and \"state\" in obj_data\n#                     ):\n#                         try:\n#                             tl_idx = obj_data[\"frames\"].index(frame_num)\n#                             state_val = obj_data[\"state\"][tl_idx]\n#                             if state_val != 0:\n#                                 state_int = state_val\n#                                 break\n#                         except (ValueError, IndexError):\n#                             continue\n#                 feature_sequences[\"traffic_light\"].append(\n#                     to_one_hot(state_int, INPUT_SIZE_TL_STATE)\n#                 )\n\n#             # ---------- static_context (per-frame replicate) ---------------\n#             if \"static_context\" in self.active_streams:\n#                 feature_sequences[\"static_context\"].append(static_vec)\n\n#         # -------------------------------------------------------------------\n#         # convert to tensors / safe fallback\n#         # -------------------------------------------------------------------\n#         features = {}\n#         try:\n#             for name in self.active_streams:\n#                 features[name] = torch.tensor(\n#                     np.asarray(feature_sequences[name], dtype=np.float32),\n#                     dtype=torch.float32,\n#                 )\n#         except Exception as e:\n#             print(f\"Error converting features idx {idx}: {e}. Returning zeros.\")\n#             features = {\n#                 name: torch.zeros(\n#                     (self.seq_len, self._input_sizes_for_error.get(name, 1)),\n#                     dtype=torch.float32,\n#                 )\n#                 for name in self.active_streams\n#             }\n\n#         return features, torch.tensor(label, dtype=torch.long)\n\n\n# class BalancedDataset(Dataset):\n#     \"\"\"\n#     Memory-based balanced dataset generated by the prep notebook cell.\n#     \"\"\"\n\n#     def __init__(self, data_dict, active_streams, label_key=\"label\"):\n#         self.active_streams = active_streams\n#         self.label_key = label_key\n\n#         if self.label_key not in data_dict or not data_dict[self.label_key]:\n#             raise ValueError(f\"Label key '{self.label_key}' missing/empty.\")\n\n#         self.num_samples = len(data_dict[self.label_key])\n#         if self.num_samples == 0:\n#             print(\"Warning: BalancedDataset initialized with zero samples.\")\n\n#         # convert every requested stream to tensor\n#         self.features = {}\n#         for stream in self.active_streams:\n#             if stream in data_dict and data_dict[stream]:\n#                 try:\n#                     self.features[stream] = torch.tensor(\n#                         np.asarray(data_dict[stream]), dtype=torch.float32\n#                     )\n#                 except ValueError as e:\n#                     raise ValueError(f\"Error converting stream '{stream}': {e}\")\n#             else:\n#                 raise KeyError(f\"Stream '{stream}' missing or empty in data.\")\n\n#         try:\n#             self.labels = torch.tensor(\n#                 [lbl[0] for lbl in data_dict[self.label_key]], dtype=torch.long\n#             )\n#         except (IndexError, TypeError) as e:\n#             raise ValueError(f\"Error converting labels: {e}\")\n\n#         for stream in self.active_streams:\n#             if len(self.features[stream]) != self.num_samples:\n#                 raise ValueError(\n#                     f\"Len mismatch: '{stream}' ({len(self.features[stream])}) vs labels ({self.num_samples})\"\n#                 )\n\n#     def __len__(self):\n#         return self.num_samples\n\n#     def __getitem__(self, idx):\n#         feature_dict = {s: self.features[s][idx] for s in self.active_streams}\n#         label = self.labels[idx]\n#         return feature_dict, label\n\n\n# class Attention(nn.Module):\n#     def __init__(self, hidden_dim, attention_dim):\n#         super().__init__()\n#         self.attention_net = nn.Sequential(\n#             nn.Linear(hidden_dim, attention_dim),\n#             nn.Tanh(),\n#             nn.Linear(attention_dim, 1),\n#         )\n\n#     def forward(self, lstm_output):\n#         att_scores = self.attention_net(lstm_output).squeeze(2)\n#         att_weights = torch.softmax(att_scores, dim=1)\n#         context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1)\n#         return context_vector, att_weights\n\n\n# # -----------------------------------------------------------------------------#\n# #                  ***  MODEL WITH WEIGHTED AVERAGE FUSION  ***                #\n# # -----------------------------------------------------------------------------\n\n\n# class MultiStreamWeightedAvgLSTM(nn.Module):\n#     def __init__(\n#         self,\n#         input_sizes,\n#         lstm_hidden_size,\n#         num_lstm_layers,\n#         num_classes,\n#         attention_dim,\n#         dropout_rate,\n#         stream_names=None,\n#     ):\n#         super().__init__()\n\n#         if not stream_names:\n#             raise ValueError(\"stream_names cannot be empty.\")\n#         self.stream_names = stream_names\n#         self.num_active_streams = len(stream_names)\n#         self.lstm_output_dim = lstm_hidden_size * 2  # Bi-LSTM doubles hidden\n\n#         self.lstms = nn.ModuleDict()\n#         self.attentions = nn.ModuleDict()\n\n#         print(f\"Initializing Weighted-Avg model with streams: {self.stream_names}\")\n\n#         for name in self.stream_names:\n#             if name not in input_sizes:\n#                 raise KeyError(f\"Input size for stream '{name}' not provided.\")\n\n#             in_size = input_sizes[name]\n#             print(f\"  – Adding stream '{name}' (input {in_size})\")\n\n#             self.lstms[name] = nn.LSTM(\n#                 in_size,\n#                 lstm_hidden_size,\n#                 num_lstm_layers,\n#                 batch_first=True,\n#                 dropout=dropout_rate if num_lstm_layers > 1 else 0,\n#                 bidirectional=True,\n#             )\n#             self.attentions[name] = Attention(self.lstm_output_dim, attention_dim)\n\n#         # learnable fusion weights (one per stream)\n#         self.fusion_weights = nn.Parameter(torch.ones(self.num_active_streams))\n\n#         # classification head\n#         fused_dim = self.lstm_output_dim\n#         self.dropout = nn.Dropout(dropout_rate)\n#         inter_dim = max(num_classes * 4, fused_dim // 2)\n#         self.fc1 = nn.Linear(fused_dim, inter_dim)\n#         self.relu = nn.ReLU()\n#         self.fc2 = nn.Linear(inter_dim, num_classes)\n\n#     # --------------------------------------------------------------------- #\n\n#     def forward(self, x):\n#         # gather context vectors ------------------------------------------------\n#         ctx_vecs = []\n#         for name in self.stream_names:\n#             if name not in x:\n#                 # gracefully handle a missing stream during inference\n#                 zero_ctx = torch.zeros(\n#                     x[next(iter(x))].shape[0],\n#                     self.lstm_output_dim,\n#                     device=x[next(iter(x))].device,\n#                 )\n#                 ctx_vecs.append(zero_ctx)\n#                 continue\n\n#             lstm_out, _ = self.lstms[name](x[name])\n#             context_vector, _ = self.attentions[name](lstm_out)\n#             ctx_vecs.append(context_vector)\n\n#         if len(ctx_vecs) != self.num_active_streams:\n#             raise RuntimeError(\n#                 f\"context_vectors({len(ctx_vecs)}) != num_streams({self.num_active_streams})\"\n#             )\n\n#         # weighted average fusion ---------------------------------------------\n#         stacked = torch.stack(ctx_vecs, dim=1)  # (B, N, D)\n#         weights = torch.softmax(self.fusion_weights, dim=0).view(1, -1, 1)\n#         fused = torch.sum(stacked * weights, dim=1)\n\n#         # classification head --------------------------------------------------\n#         out = self.dropout(fused)\n#         out = self.relu(self.fc1(out))\n#         out = self.dropout(out)\n#         logits = self.fc2(out)\n#         return logits\n\n\n# # -----------------------------------------------------------------------------#\n# #                       Training / evaluation helpers                          #\n# # -----------------------------------------------------------------------------\n\n\n# def train_epoch(model, dataloader, optimizer, criterion, device):\n#     model.train()\n#     total_loss = 0.0\n#     all_preds, all_labels = [], []\n#     active = model.stream_names\n\n#     for feats, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n#         inputs = {n: feats[n].to(device) for n in active if n in feats}\n#         labels = labels.to(device)\n\n#         optimizer.zero_grad()\n#         outputs = model(inputs)\n#         loss = criterion(outputs, labels)\n#         loss.backward()\n#         optimizer.step()\n\n#         total_loss += loss.item()\n#         all_preds.extend(torch.argmax(outputs, 1).cpu().numpy())\n#         all_labels.extend(labels.cpu().numpy())\n\n#     return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n\n\n# def evaluate_epoch(model, dataloader, criterion, device):\n#     model.eval()\n#     total_loss = 0.0\n#     all_labels, all_preds, all_probs = [], [], []\n#     active = model.stream_names\n\n#     with torch.no_grad():\n#         for feats, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n#             inputs = {n: feats[n].to(device) for n in active if n in feats}\n#             labels = labels.to(device)\n\n#             outputs = model(inputs)\n#             loss = criterion(outputs, labels)\n#             total_loss += loss.item()\n\n#             probs = torch.softmax(outputs, 1)\n#             preds = torch.argmax(probs, 1)\n\n#             all_labels.extend(labels.cpu().numpy())\n#             all_preds.extend(preds.cpu().numpy())\n#             all_probs.extend(probs.cpu().numpy())\n\n#     avg_loss = total_loss / max(1, len(dataloader))\n#     all_probs = np.asarray(all_probs)\n#     all_labels = np.asarray(all_labels)\n#     all_preds = np.asarray(all_preds)\n\n#     acc = accuracy_score(all_labels, all_preds)\n#     prec, rec, f1, _ = precision_recall_fscore_support(\n#         all_labels, all_preds, average=\"binary\", pos_label=1, zero_division=0\n#     )\n#     auc = (\n#         roc_auc_score(all_labels, all_probs[:, 1])\n#         if len(np.unique(all_labels)) > 1\n#         else float(\"nan\")\n#     )\n\n#     return {\n#         \"loss\": avg_loss,\n#         \"accuracy\": acc,\n#         \"precision\": prec,\n#         \"recall\": rec,\n#         \"f1\": f1,\n#         \"auc\": auc,\n#     }\n\n\n# def get_predictions_and_labels(model, dataloader, device):\n#     model.eval()\n#     labels_all, preds_all = [], []\n#     active = model.stream_names\n\n#     with torch.no_grad():\n#         for feats, labels in tqdm(dataloader, desc=\"Generating CM Data\", leave=False):\n#             inputs = {n: feats[n].to(device) for n in active if n in feats}\n#             outputs = model(inputs)\n#             preds = torch.argmax(outputs, 1)\n#             labels_all.extend(labels.cpu().numpy())\n#             preds_all.extend(preds.cpu().numpy())\n\n#     return np.asarray(labels_all), np.asarray(preds_all)\n\n\n# # -----------------------------------------------------------------------------#\n# #                            Main execution block                              #\n# # -----------------------------------------------------------------------------\n\n\n# if __name__ == \"__main__\":\n#     print(\"--- Running Model Training/Evaluation with Weighted Fusion ---\")\n#     print(f\"Active Streams: {ACTIVE_STREAMS}\")\n\n#     # ------------------ load balanced data & scalers -------------------------\n#     print(\"\\nLoading balanced training data …\")\n#     try:\n#         with open(BALANCED_DATA_PKL_PATH, \"rb\") as f:\n#             balanced_train_data_dict = pickle.load(f)\n#         with open(SCALERS_PKL_PATH, \"rb\") as f:\n#             scalers = pickle.load(f)\n#         print(\"   ✓ pre-processed data loaded.\")\n#     except FileNotFoundError as e:\n#         print(f\"ERROR: {e}.  Run the preprocessing cell first.\")\n#         sys.exit(1)\n#     except Exception as e:\n#         print(f\"Error loading pre-processed data: {e}\")\n#         sys.exit(1)\n\n#     # -------------------------- load PIE database ----------------------------\n#     print(\"\\nLoading PIE database cache for validation …\")\n#     if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n#         raise FileNotFoundError(\"PIE database cache not found.\")\n\n#     try:\n#         with open(PIE_DATABASE_CACHE_PATH, \"rb\") as f:\n#             pie_database = pickle.load(f)\n#     except Exception as e:\n#         raise RuntimeError(f\"Failed to load PIE database: {e}\")\n#     print(\"   ✓ PIE database loaded.\")\n\n#     # ------------------- create datasets / dataloaders -----------------------\n#     print(\"\\nCreating Datasets and DataLoaders …\")\n#     try:\n#         train_dataset = BalancedDataset(\n#             balanced_train_data_dict, ACTIVE_STREAMS, label_key=\"label\"\n#         )\n#         del balanced_train_data_dict\n\n#         val_dataset = PIEDataset(\n#             pie_database,\n#             VAL_SETS_STR,\n#             POSE_DATA_DIR,\n#             SEQ_LEN,\n#             PRED_LEN,\n#             scalers,\n#             ALL_POSSIBLE_STREAMS,  # provide all streams for val\n#         )\n#     except Exception as e:\n#         print(f\"Error creating datasets: {e}\")\n#         raise\n\n#     if len(train_dataset) == 0 or len(val_dataset) == 0:\n#         raise ValueError(\"One of the datasets is empty!\")\n\n#     train_loader = DataLoader(\n#         train_dataset,\n#         batch_size=BATCH_SIZE,\n#         shuffle=True,\n#         num_workers=2,\n#         pin_memory=True,\n#     )\n#     val_loader = DataLoader(\n#         val_dataset,\n#         batch_size=BATCH_SIZE,\n#         shuffle=False,\n#         num_workers=2,\n#         pin_memory=True,\n#     )\n#     print(\"   ✓ DataLoaders ready.\")\n#     del pie_database\n#     gc.collect()\n\n#     # ------------------------- initialise the model --------------------------\n#     print(\"\\nInitialising model …\")\n#     current_input_sizes = {}\n#     SPECIAL = {\n#         \"TRAFFIC_LIGHT\": \"TL_STATE\",\n#         \"STATIC_CONTEXT\": \"STATIC\",\n#         \"EGO_SPEED\": \"EGO_SPEED\",\n#         \"EGO_ACC\": \"EGO_ACC\",\n#         \"EGO_GYRO\": \"EGO_GYRO\",\n#         \"PED_ACTION\": \"PED_ACTION\",\n#         \"PED_LOOK\": \"PED_LOOK\",\n#         \"PED_OCCLUSION\": \"PED_OCC\",\n#     }\n\n#     for s in ACTIVE_STREAMS:\n#         name = f\"INPUT_SIZE_{SPECIAL.get(s.upper(), s.upper())}\"\n#         if s == \"bbox\":\n#             name = \"INPUT_SIZE_BBOX\"\n#         elif s == \"pose\":\n#             name = \"INPUT_SIZE_POSE\"\n\n#         if name not in globals():\n#             raise ValueError(f\"Input-size constant {name} not found.\")\n\n#         current_input_sizes[s] = globals()[name]\n\n#     model = MultiStreamWeightedAvgLSTM(\n#         current_input_sizes,\n#         LSTM_HIDDEN_SIZE,\n#         NUM_LSTM_LAYERS,\n#         NUM_CLASSES,\n#         ATTENTION_DIM,\n#         DROPOUT_RATE,\n#         stream_names=ACTIVE_STREAMS,\n#     ).to(DEVICE)\n\n#     print(\"\\n--- Model architecture ---\")\n#     print(model)\n#     total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n#     print(f\"Trainable parameters: {total_params:,}\")\n#     print(\"-\" * 30)\n\n#     # ---------------- loss / optimiser --------------------------------------\n#     print(\"\\nCalculating class weights …\")\n#     train_labels = train_dataset.labels.tolist()\n#     n0, n1 = train_labels.count(0), train_labels.count(1)\n#     total = len(train_labels)\n#     if total == 0:\n#         w0, w1 = 1.0, 1.0\n#     elif n0 == 0:\n#         w0, w1 = 0.0, 1.0\n#     elif n1 == 0:\n#         w0, w1 = 1.0, 0.0\n#     else:\n#         w0, w1 = total / (2.0 * n0), total / (2.0 * n1)\n\n#     class_weights = torch.tensor([w0, w1], dtype=torch.float32).to(DEVICE)\n#     print(f\"Loss weights → 0: {w0:.2f}, 1: {w1:.2f}\")\n\n#     criterion = nn.CrossEntropyLoss(weight=class_weights)\n#     optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n#     best_val_f1 = -1.0\n#     history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": [], \"val_f1\": []}\n#     best_model_path = \"\"\n\n#     # --------------------------- training loop -------------------------------\n#     print(\"\\n--- Starting training ---\")\n#     for epoch in range(NUM_EPOCHS):\n#         t0 = time.time()\n\n#         train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n#         metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n\n#         history[\"train_loss\"].append(train_loss)\n#         history[\"val_loss\"].append(metrics[\"loss\"])\n#         history[\"train_acc\"].append(train_acc)\n#         history[\"val_acc\"].append(metrics[\"accuracy\"])\n#         history[\"val_f1\"].append(metrics[\"f1\"])\n\n#         print(f\"\\nEpoch {epoch + 1:02d}/{NUM_EPOCHS} – {time.time() - t0:.1f}s\")\n#         print(f\"  train loss {train_loss:.4f} | acc {train_acc:.4f}\")\n#         print(f\"  val   loss {metrics['loss']:.4f} | acc {metrics['accuracy']:.4f}\")\n#         print(\n#             f\"           prec {metrics['precision']:.4f} | rec {metrics['recall']:.4f} | f1 {metrics['f1']:.4f} | auc {metrics['auc']:.4f}\"\n#         )\n\n#         if metrics[\"f1\"] > best_val_f1:\n#             best_val_f1 = metrics[\"f1\"]\n#             best_model_path = f\"best_model_weighted_{'_'.join(sorted(ACTIVE_STREAMS))}_ep{epoch + 1}.pth\"\n#             torch.save(model.state_dict(), best_model_path)\n#             print(f\"  ✓ new best model saved → {best_model_path} (F1 {best_val_f1:.4f})\")\n\n#     print(\"\\n--- Training finished ---\")\n\n#     # --------------------------- plots ---------------------------------------\n#     print(\"\\nPlotting training curves …\")\n#     fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\n#     ax[0].plot(range(1, NUM_EPOCHS + 1), history[\"train_loss\"], label=\"Train\")\n#     ax[0].plot(range(1, NUM_EPOCHS + 1), history[\"val_loss\"], label=\"Val\")\n#     ax[0].set_xlabel(\"Epoch\")\n#     ax[0].set_ylabel(\"Loss\")\n#     ax[0].set_title(\"Loss curve\")\n#     ax[0].legend()\n#     ax[0].grid(True)\n\n#     ax[1].plot(range(1, NUM_EPOCHS + 1), history[\"train_acc\"], label=\"Train Acc\")\n#     ax[1].plot(range(1, NUM_EPOCHS + 1), history[\"val_acc\"], label=\"Val Acc\")\n#     ax[1].plot(range(1, NUM_EPOCHS + 1), history[\"val_f1\"], \"--\", label=\"Val F1\")\n#     ax[1].set_xlabel(\"Epoch\")\n#     ax[1].set_ylabel(\"Metric\")\n#     ax[1].set_title(\"Accuracy & F1\")\n#     ax[1].legend()\n#     ax[1].grid(True)\n\n#     plt.tight_layout()\n#     plt.show()\n\n#     # ------------------- final evaluation (best model) -----------------------\n#     print(\"\\n--- Final Evaluation on Validation set ---\")\n#     if best_model_path and os.path.exists(best_model_path):\n#         try:\n#             model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n#             print(f\"Loaded best model: {best_model_path}\")\n#         except Exception as e:\n#             print(f\"Warning: could not load best model ({e}).  Using last epoch params.\")\n#     else:\n#         print(\"Warning: best model not found, using last epoch parameters.\")\n\n#     final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n#     y_true, y_pred = get_predictions_and_labels(model, val_loader, DEVICE)\n#     cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n\n#     print(\"\\n--- Final metrics (Weighted Avg Fusion) ---\")\n#     for k, v in final_metrics.items():\n#         print(f\"{k:<10}: {v:.4f}\")\n\n#     print(f\"(Best validation F1 during training: {best_val_f1:.4f})\")\n\n#     ConfusionMatrixDisplay(cm, display_labels=[\"Not Crossing\", \"Crossing\"]).plot(\n#         cmap=plt.cm.Blues\n#     )\n#     plt.title(\"Confusion Matrix\")\n#     plt.show()\n\n#     # ------------------- inspect learned fusion weights ----------------------\n#     if hasattr(model, \"fusion_weights\"):\n#         w = torch.softmax(model.fusion_weights, 0).detach().cpu().numpy()\n#         print(\"\\n--- Learned fusion weights ---\")\n#         for stream, weight in zip(model.stream_names, w):\n#             print(f\"{stream:<15}: {weight:.4f}\")\n#         print(\"-\" * 30)\n\n#     print(\"\\n--- Script complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:09:55.674418Z","iopub.execute_input":"2025-05-01T07:09:55.674755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CELL 1: DATA PREPARATION & BALANCING (Including YOLOP – Run Once) ---\nimport torch\nfrom torch.utils.data import Dataset\nimport os\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport random\nimport pickle\nimport time\nimport sys\nimport gc\nimport cv2  # required for PIE class\n\n# --- Add PIE utilities path ---\npie_utilities_path = '/kaggle/working/PIE/utilities'\nif pie_utilities_path not in sys.path:\n    sys.path.insert(0, pie_utilities_path)\ntry:\n    from pie_data import PIE\nexcept ImportError as e:\n    print(f\"Warn: Could not import PIE class: {e}.  Database must exist.\")\n    PIE = None\n\n# --- Configuration ---\nPIE_ROOT_PATH = '/kaggle/working/PIE'\nPOSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2'\nPIE_DATABASE_CACHE_PATH = '/kaggle/input/pie-database/pie_database.pkl'\nYOLOP_FEATURE_DIR = '/kaggle/input/yolop-data/yolop features'  # setXX/setYY_vidZZ_yolop_features.pkl\n\n# --- Streams ---\nALL_POSSIBLE_STREAMS = [\n    'bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro',\n    'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light',\n    'static_context', 'yolop'\n]\nprint(f\"Data will be prepared for streams: {ALL_POSSIBLE_STREAMS}\")\n\n# --- Sizes & constants ---\nSEQ_LEN = 30\nPRED_LEN = 1\nINPUT_SIZE_BBOX = 4\nINPUT_SIZE_POSE = 34\nINPUT_SIZE_EGO_SPEED = 1\nINPUT_SIZE_EGO_ACC = 2\nINPUT_SIZE_EGO_GYRO = 1\nINPUT_SIZE_PED_ACTION = 1\nINPUT_SIZE_PED_LOOK = 1\nINPUT_SIZE_PED_OCC = 1\nINPUT_SIZE_TL_STATE = 4\n\nNUM_SIGNALIZED_CATS = 4\nNUM_INTERSECTION_CATS = 5\nNUM_AGE_CATS = 4\nNUM_GENDER_CATS = 3\nNUM_TRAFFIC_DIR_CATS = 2\nLANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7: 4, 8: 4}\nNUM_LANE_CATS = len(set(LANE_CATEGORIES.values()))\nINPUT_SIZE_STATIC = (\n    NUM_SIGNALIZED_CATS + NUM_INTERSECTION_CATS + NUM_AGE_CATS +\n    NUM_GENDER_CATS + NUM_TRAFFIC_DIR_CATS + NUM_LANE_CATS\n)\n\n# YOLOP feature size\nGRID_SIZE = 3\nYOLOP_DRIVABLE_FEATURES_DIM = GRID_SIZE * GRID_SIZE\nYOLOP_LANE_FEATURES_DIM = GRID_SIZE * GRID_SIZE\nYOLOP_OBJECT_FEATURES_DIM = 2\nINPUT_SIZE_YOLOP = (\n    YOLOP_DRIVABLE_FEATURES_DIM + YOLOP_LANE_FEATURES_DIM + YOLOP_OBJECT_FEATURES_DIM\n)\n\n# --- Dataset splits ---\nTRAIN_SETS_STR = ['set01', 'set02', 'set04']\n\n# --- Label/state maps ---\nTL_STATE_MAP = {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}\nNUM_TL_STATES = len(TL_STATE_MAP)\n\n# --- Output paths ---\nBALANCED_DATA_PKL_PATH = \"/kaggle/working/balanced_train_data_with_yolop.pkl\"\nSCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\"\n\n# --- Helpers ---------------------------------------------------------------\ndef to_one_hot(index, num_classes):\n    vec = np.zeros(num_classes, dtype=np.float32)\n    safe_index = int(np.clip(index, 0, num_classes - 1))\n    vec[safe_index] = 1.0\n    return vec\n\n\ndef balance_samples_count(seq_data, label_type, random_seed=42):\n    print('---------------------------------------------------------')\n    print(f\"Balancing samples based on '{label_type}' key\")\n\n    if label_type not in seq_data:\n        raise KeyError(f\"Label type '{label_type}' not found.\")\n\n    try:\n        gt_labels = [lbl[0] for lbl in seq_data[label_type]]\n    except (IndexError, TypeError):\n        raise ValueError(\n            f\"Labels under '{label_type}' not in expected format [[label_val]].\"\n        )\n\n    if not all(l in [0, 1] for l in gt_labels):\n        print(\"Warning: labels contain values other than 0 or 1.\")\n\n    num_pos = np.count_nonzero(gt_labels)\n    num_neg = len(gt_labels) - num_pos\n    new_seq_data = {}\n\n    if num_neg == num_pos:\n        print(\"Samples already balanced.\")\n        return seq_data.copy()\n\n    majority_label = 0 if num_neg > num_pos else 1\n    minority_count = min(num_neg, num_pos)\n    print(\n        f\"Unbalanced: Positive (1): {num_pos} | Negative (0): {num_neg}\\n\"\n        f\"Undersampling majority class ({majority_label}) to {minority_count}\"\n    )\n\n    majority_idx = np.where(np.array(gt_labels) == majority_label)[0]\n    minority_idx = np.where(np.array(gt_labels) != majority_label)[0]\n    np.random.seed(random_seed)\n    keep_majority = np.random.choice(majority_idx, minority_count, replace=False)\n    final_idx = np.concatenate((minority_idx, keep_majority))\n    np.random.shuffle(final_idx)\n\n    for k, v_list in seq_data.items():\n        if isinstance(v_list, list) and len(v_list) == len(gt_labels):\n            try:\n                if v_list and isinstance(v_list[0], np.ndarray):\n                    v_array = np.array(v_list)\n                    new_seq_data[k] = list(v_array[final_idx])\n                else:\n                    new_seq_data[k] = [v_list[i] for i in final_idx]\n            except Exception as e:\n                print(f\"Error processing key '{k}': {e}.  Skipped.\")\n                new_seq_data[k] = []\n        else:\n            new_seq_data[k] = v_list\n\n    new_gt = [lbl[0] for lbl in new_seq_data[label_type]]\n    final_pos = np.count_nonzero(new_gt)\n    final_neg = len(new_gt) - final_pos\n    print(f\"Balanced:   Positive (1): {final_pos} | Negative (0): {final_neg}\")\n    print('---------------------------------------------------------')\n    return new_seq_data\n\n\n# --- Dataset class ---------------------------------------------------------\nclass PIEDataset(Dataset):\n    def __init__(\n        self,\n        pie_database,\n        set_names,\n        pose_data_dir,\n        yolop_data_dir,\n        seq_len,\n        pred_len,\n        scalers=None,\n        streams_to_generate=None,\n    ):\n        self.pie_db = pie_database\n        self.set_names = set_names\n        self.pose_data_dir = pose_data_dir\n        self.yolop_data_dir = yolop_data_dir\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.scalers = scalers or {}\n        self.streams_to_generate = streams_to_generate or ALL_POSSIBLE_STREAMS\n        self.sequences = []\n        self.all_pose_data = {}\n        self.all_yolop_data = {}\n        self._input_sizes_for_error = self._get_input_sizes_dict()\n\n        if 'pose' in self.streams_to_generate:\n            self._load_pose_data()\n        if 'yolop' in self.streams_to_generate:\n            self._load_yolop_data()\n\n        self._generate_sequence_list()\n\n        if not self.sequences:\n            raise ValueError(f\"No sequences found for sets {self.set_names}\")\n\n    # ------------------------------------------------------------------\n    def _get_input_sizes_dict(self):\n        input_sizes = {}\n        special = {\n            'TRAFFIC_LIGHT': 'TL_STATE',\n            'STATIC_CONTEXT': 'STATIC',\n            'EGO_SPEED': 'EGO_SPEED',\n            'EGO_ACC': 'EGO_ACC',\n            'EGO_GYRO': 'EGO_GYRO',\n            'PED_ACTION': 'PED_ACTION',\n            'PED_LOOK': 'PED_LOOK',\n            'PED_OCCLUSION': 'PED_OCC',\n            'YOLOP': 'YOLOP',\n        }\n        for stream in ALL_POSSIBLE_STREAMS:\n            size_name = f'INPUT_SIZE_{stream.upper()}'\n            key = stream.upper()\n            suffix = special.get(key)\n            if suffix:\n                size_name = f'INPUT_SIZE_{suffix}'\n            elif stream == 'bbox':\n                size_name = 'INPUT_SIZE_BBOX'\n            elif stream == 'pose':\n                size_name = 'INPUT_SIZE_POSE'\n\n            input_sizes[stream] = globals().get(size_name, 1)\n\n        return input_sizes\n\n    # ------------------------------------------------------------------\n    def _load_pose_data(self):\n        for set_id in self.set_names:\n            self.all_pose_data[set_id] = {}\n            set_path = os.path.join(self.pose_data_dir, set_id)\n            if not os.path.isdir(set_path):\n                continue\n            pkl_files = [\n                f for f in os.listdir(set_path)\n                if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")\n            ]\n            for pkl_name in pkl_files:\n                pkl_path = os.path.join(set_path, pkl_name)\n                try:\n                    with open(pkl_path, 'rb') as f:\n                        content = pickle.load(f)\n                    if len(content) != 1:\n                        continue\n                    uniq_key, video_data = list(content.items())[0]\n                    vid = \"_\".join(uniq_key.split('_')[1:])\n                    if vid in self.pie_db.get(set_id, {}):\n                        self.all_pose_data[set_id][vid] = video_data\n                except Exception as e:\n                    print(f\"Pose load error {pkl_path}: {e}\")\n\n    # ------------------------------------------------------------------\n    \n\n    def _load_yolop_data(self): # New method to load YOLOP features\n        print(f\"\\nLoading YOLOP data for sets: {self.set_names} from {self.yolop_data_dir}\")\n        sets_loaded_count = 0\n        # Initialize dictionaries for all sets requested by this dataset instance\n        for set_id in self.set_names:\n            self.all_yolop_data[set_id] = {}\n\n        if not os.path.isdir(self.yolop_data_dir):\n            print(f\"Error: YOLOP feature directory not found: {self.yolop_data_dir}\")\n            return\n\n        try:\n            all_pkl_files = [f for f in os.listdir(self.yolop_data_dir)\n                             if f.endswith(\"_yolop_features.pkl\")]\n        except Exception as e:\n            print(f\"Error listing files in YOLOP directory {self.yolop_data_dir}: {e}\")\n            return\n\n        if not all_pkl_files:\n            print(f\"Warning: No '*_yolop_features.pkl' files found directly in {self.yolop_data_dir}\")\n\n        # --- Initialize count BEFORE the loop ---\n        loaded_file_count = 0\n        files_for_needed_sets = 0\n        # ---\n\n        for pkl_filename in tqdm(all_pkl_files, desc=\"Loading YOLOP PKLs\"):\n            try:\n                parts = pkl_filename.replace(\"_yolop_features.pkl\", \"\").split('_')\n                set_id_from_file = parts[0]\n                video_id = \"_\".join(parts[1:])\n            except IndexError:\n                print(f\"Warning: Could not parse set/video ID from filename: {pkl_filename}\")\n                continue\n\n            if set_id_from_file in self.set_names:\n                files_for_needed_sets += 1\n                pkl_file_path = os.path.join(self.yolop_data_dir, pkl_filename)\n                try:\n                    with open(pkl_file_path, 'rb') as f:\n                        loaded_pkl_content = pickle.load(f)\n\n                    if len(loaded_pkl_content) != 1:\n                        print(f\"Warn: PKL {pkl_filename} format issue (expected 1 key). Skip.\")\n                        continue\n\n                    unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n                    expected_key = f\"{set_id_from_file}_{video_id}\"\n                    if unique_video_key != expected_key:\n                         print(f\"Warn: Key mismatch in {pkl_filename}. Expected '{expected_key}', found '{unique_video_key}'. Trying to use found key.\")\n\n                    self.all_yolop_data[set_id_from_file][video_id] = video_data\n                    loaded_file_count += 1 # Increment only on success\n\n                except FileNotFoundError:\n                     print(f\"Warn: YOLOP feature file not found during loading: {pkl_file_path}\")\n                except Exception as e:\n                    print(f\"Error loading YOLOP PKL {pkl_file_path}: {e}\")\n\n        print(f\"Finished loading YOLOP data. Found {loaded_file_count} relevant files out of {files_for_needed_sets} expected for sets {self.set_names}.\")\n        if loaded_file_count > 0:\n             actual_sets_loaded = sum(1 for s in self.set_names if s in self.all_yolop_data and self.all_yolop_data[s])\n             print(f\"Data loaded for {actual_sets_loaded} sets.\")\n    # ------------------------------------------------------------------\n    def _generate_sequence_list(self):\n        for set_id in tqdm(self.set_names, desc=\"Generating sequences\"):\n            if set_id not in self.pie_db:\n                continue\n            for video_id, video_data in self.pie_db[set_id].items():\n                ped_ann = video_data.get('ped_annotations', {})\n                for ped_id, ped_data in ped_ann.items():\n                    if 'frames' not in ped_data or len(ped_data['frames']) < self.seq_len + self.pred_len:\n                        continue\n                    frames_sorted = sorted(ped_data['frames'])\n                    for i in range(len(frames_sorted) - self.seq_len - self.pred_len + 1):\n                        start_f = frames_sorted[i]\n                        end_obs = frames_sorted[i + self.seq_len - 1]\n                        if end_obs - start_f != self.seq_len - 1:\n                            continue\n                        target_idx = i + self.seq_len + self.pred_len - 1\n                        target_f = frames_sorted[target_idx]\n                        if target_f - end_obs != self.pred_len:\n                            continue\n                        self.sequences.append((set_id, video_id, ped_id, start_f))\n\n    # ------------------------------------------------------------------\n    def __len__(self):\n        return len(self.sequences)\n\n    # ------------------------------------------------------------------\n    def __getitem__(self, idx):\n        set_id, video_id, ped_id, start_frame = self.sequences[idx]\n        frame_nums = list(range(start_frame, start_frame + self.seq_len))\n        target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n\n        video_db = self.pie_db.get(set_id, {}).get(video_id, {})\n        ped_db = video_db.get('ped_annotations', {}).get(ped_id, {})\n        ego_db = video_db.get('vehicle_annotations', {})\n        traffic_db = video_db.get('traffic_annotations', {})\n        ped_attr = ped_db.get('attributes', {})\n\n        feat_seq = {s: [] for s in self.streams_to_generate}\n        label = 0\n\n        if 'frames' in ped_db and 'behavior' in ped_db and 'cross' in ped_db['behavior']:\n            try:\n                idx_target = ped_db['frames'].index(target_frame_num)\n                label = ped_db['behavior']['cross'][idx_target]\n                if label == -1:\n                    label = 0\n            except (ValueError, IndexError):\n                pass\n\n        static_vec = None\n        if 'static_context' in self.streams_to_generate:\n            sig_idx = ped_attr.get('signalized', 0)\n            int_idx = ped_attr.get('intersection', 0)\n            age_idx = ped_attr.get('age', 2)\n            gen_idx = ped_attr.get('gender', 0)\n            td_idx = int(ped_attr.get('traffic_direction', 0))\n            nl_val = ped_attr.get('num_lanes', 2)\n            nl_idx = LANE_CATEGORIES.get(nl_val, LANE_CATEGORIES[max(LANE_CATEGORIES)])\n            static_vec = np.concatenate([\n                to_one_hot(sig_idx, NUM_SIGNALIZED_CATS),\n                to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n                to_one_hot(age_idx, NUM_AGE_CATS),\n                to_one_hot(gen_idx, NUM_GENDER_CATS),\n                to_one_hot(td_idx, NUM_TRAFFIC_DIR_CATS),\n                to_one_hot(nl_idx, NUM_LANE_CATS),\n            ]).astype(np.float32)\n            if static_vec.shape[0] != INPUT_SIZE_STATIC:\n                static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n\n        for frame in frame_nums:\n            frame_idx = -1\n            if 'frames' in ped_db:\n                try:\n                    frame_idx = ped_db['frames'].index(frame)\n                except ValueError:\n                    pass\n\n            ego = ego_db.get(frame, {})\n\n            # ------------------------------ bbox\n            if 'bbox' in self.streams_to_generate:\n                bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32)\n                if frame_idx != -1 and 'bbox' in ped_db and len(ped_db['bbox']) > frame_idx:\n                    try:\n                        x1, y1, x2, y2 = ped_db['bbox'][frame_idx]\n                        iw = video_db.get('width', 1920)\n                        ih = video_db.get('height', 1080)\n                        if iw > 0 and ih > 0:\n                            cx = ((x1 + x2) / 2) / iw\n                            cy = ((y1 + y2) / 2) / ih\n                            w = (x2 - x1) / iw\n                            h = (y2 - y1) / ih\n                            if 0 <= cx <= 1 and 0 <= cy <= 1 and w > 0 and h > 0:\n                                bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n                    except Exception:\n                        pass\n                feat_seq['bbox'].append(bbox_norm)\n\n            # ------------------------------ pose\n            if 'pose' in self.streams_to_generate:\n                pose_vec = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n                pose_vid = self.all_pose_data.get(set_id, {}).get(video_id, {})\n                pose_frame = pose_vid.get(frame, {})\n                loaded_pose = pose_frame.get(ped_id)\n                if (\n                    loaded_pose is not None\n                    and isinstance(loaded_pose, np.ndarray)\n                    and loaded_pose.shape == (INPUT_SIZE_POSE,)\n                ):\n                    pose_vec = loaded_pose\n                feat_seq['pose'].append(pose_vec)\n\n            # ------------------------------ ego_speed\n            if 'ego_speed' in self.streams_to_generate:\n                speed = ego.get('OBD_speed', 0.0) or ego.get('GPS_speed', 0.0)\n                sp_scaled = (\n                    speed - self.scalers.get('ego_speed_mean', 0.0)\n                ) / self.scalers.get('ego_speed_std', 1.0)\n                feat_seq['ego_speed'].append([sp_scaled])\n\n            # ------------------------------ ego_acc\n            if 'ego_acc' in self.streams_to_generate:\n                accX = ego.get('accX', 0.0)\n                accY = ego.get('accY', 0.0)\n                accX_s = (accX - self.scalers.get('accX_mean', 0.0)) / self.scalers.get('accX_std', 1.0)\n                accY_s = (accY - self.scalers.get('accY_mean', 0.0)) / self.scalers.get('accY_std', 1.0)\n                feat_seq['ego_acc'].append([accX_s, accY_s])\n\n            # ------------------------------ ego_gyro\n            if 'ego_gyro' in self.streams_to_generate:\n                gyroZ = ego.get('gyroZ', 0.0)\n                gyroZ_s = (gyroZ - self.scalers.get('gyroZ_mean', 0.0)) / self.scalers.get('gyroZ_std', 1.0)\n                feat_seq['ego_gyro'].append([gyroZ_s])\n\n            # ------------------------------ ped_action\n            if 'ped_action' in self.streams_to_generate:\n                action = 0\n                if (\n                    frame_idx != -1\n                    and 'behavior' in ped_db\n                    and 'action' in ped_db['behavior']\n                    and len(ped_db['behavior']['action']) > frame_idx\n                ):\n                    action = ped_db['behavior']['action'][frame_idx]\n                feat_seq['ped_action'].append([float(action)])\n\n            # ------------------------------ ped_look\n            if 'ped_look' in self.streams_to_generate:\n                look = 0\n                if (\n                    frame_idx != -1\n                    and 'behavior' in ped_db\n                    and 'look' in ped_db['behavior']\n                    and len(ped_db['behavior']['look']) > frame_idx\n                ):\n                    look = ped_db['behavior']['look'][frame_idx]\n                feat_seq['ped_look'].append([float(look)])\n\n            # ------------------------------ ped_occlusion\n            if 'ped_occlusion' in self.streams_to_generate:\n                occ = 0.0\n                if (\n                    frame_idx != -1\n                    and 'occlusion' in ped_db\n                    and len(ped_db['occlusion']) > frame_idx\n                ):\n                    occ_val = ped_db['occlusion'][frame_idx]\n                    occ = float(occ_val) / 2.0\n                feat_seq['ped_occlusion'].append([occ])\n\n            # ------------------------------ traffic_light\n            if 'traffic_light' in self.streams_to_generate:\n                state_int = 0\n                for obj_id, obj in traffic_db.items():\n                    if (\n                        obj.get('obj_class') == 'traffic_light'\n                        and 'frames' in obj and 'state' in obj\n                    ):\n                        try:\n                            tl_idx = obj['frames'].index(frame)\n                            state_val = obj['state'][tl_idx]\n                            if state_val != 0:\n                                state_int = state_val\n                                break\n                        except (ValueError, IndexError):\n                            continue\n                feat_seq['traffic_light'].append(to_one_hot(state_int, NUM_TL_STATES))\n\n            # ------------------------------ static_context\n            if 'static_context' in self.streams_to_generate:\n                feat_seq['static_context'].append(\n                    static_vec if static_vec is not None else np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n                )\n\n            # ------------------------------ YOLOP\n            if 'yolop' in self.streams_to_generate:\n                yolop_vec = np.zeros(INPUT_SIZE_YOLOP, dtype=np.float32)\n                if set_id in self.all_yolop_data and video_id in self.all_yolop_data[set_id]:\n                    frame_yolo = self.all_yolop_data[set_id][video_id].get(frame, {})\n                    loaded_yolo = frame_yolo.get(ped_id)\n                    if (\n                        loaded_yolo is not None\n                        and isinstance(loaded_yolo, np.ndarray)\n                        and loaded_yolo.shape == (INPUT_SIZE_YOLOP,)\n                    ):\n                        yolop_vec = loaded_yolo\n                feat_seq['yolop'].append(yolop_vec)\n\n        features = {}\n        try:\n            for s in self.streams_to_generate:\n                features[s] = torch.tensor(np.array(feat_seq[s], dtype=np.float32))\n        except Exception as e:\n            print(f\"Tensor conversion error idx {idx}: {e}\")\n            features = {\n                name: torch.zeros(\n                    (self.seq_len, self._input_sizes_for_error.get(name, 1)),\n                    dtype=torch.float32,\n                )\n                for name in self.streams_to_generate\n            }\n\n        return features, torch.tensor(label, dtype=torch.long)\n\n\n# --- Main data-preparation execution ---------------------------------------\nif __name__ == '__main__':\n\n    print(\"--- Running Data Preparation ---\")\n\n    # Load or create PIE database\n    if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n        if PIE is None:\n            raise ImportError(\"PIE class not imported.\")\n        print(\"Generating PIE database cache …\")\n        pie_intf = PIE(data_path=PIE_ROOT_PATH, regen_database=True)\n        pie_database = pie_intf.generate_database()\n        if not pie_database:\n            raise RuntimeError(\"Failed to generate PIE database.\")\n    else:\n        print(\"Loading PIE database cache …\")\n        with open(PIE_DATABASE_CACHE_PATH, 'rb') as f:\n            pie_database = pickle.load(f)\n\n    # Standardization scalers\n    print(\"Calculating standardization parameters …\")\n    all_sp, all_ax, all_ay, all_gz = [], [], [], []\n    for set_id in TRAIN_SETS_STR:\n        for vid, vdata in pie_database.get(set_id, {}).items():\n            for _, ego in vdata.get('vehicle_annotations', {}).items():\n                speed = ego.get('OBD_speed', 0.0) or ego.get('GPS_speed', 0.0)\n                all_sp.append(speed)\n                all_ax.append(ego.get('accX', 0.0))\n                all_ay.append(ego.get('accY', 0.0))\n                all_gz.append(ego.get('gyroZ', 0.0))\n\n    scalers = {}\n    if all_sp:\n        scalers['ego_speed_mean'] = np.mean(all_sp)\n        scalers['ego_speed_std'] = max(np.std(all_sp), 1e-6)\n    if all_ax:\n        scalers['accX_mean'] = np.mean(all_ax)\n        scalers['accX_std'] = max(np.std(all_ax), 1e-6)\n        scalers['accY_mean'] = np.mean(all_ay)\n        scalers['accY_std'] = max(np.std(all_ay), 1e-6)\n    if all_gz:\n        scalers['gyroZ_mean'] = np.mean(all_gz)\n        scalers['gyroZ_std'] = max(np.std(all_gz), 1e-6)\n\n    print(\"Initializing full training dataset …\")\n    full_train_ds = PIEDataset(\n        pie_database,\n        TRAIN_SETS_STR,\n        POSE_DATA_DIR,\n        YOLOP_FEATURE_DIR,\n        SEQ_LEN,\n        PRED_LEN,\n        scalers,\n        ALL_POSSIBLE_STREAMS,\n    )\n\n    # Extract all features for balancing\n    print(\"Extracting data for balancing …\")\n    data_dict = {s: [] for s in ALL_POSSIBLE_STREAMS}\n    data_dict['label'] = []\n    for i in tqdm(range(len(full_train_ds)), desc=\"Extracting\"):\n        feats, lbl = full_train_ds[i]\n        for s in ALL_POSSIBLE_STREAMS:\n            data_dict[s].append(feats[s].numpy())\n        data_dict['label'].append([lbl.item()])\n\n    balanced_dict = balance_samples_count(data_dict, 'label')\n\n    # Save outputs\n    print(\"Saving balanced data …\")\n    with open(BALANCED_DATA_PKL_PATH, 'wb') as f:\n        pickle.dump(balanced_dict, f, pickle.HIGHEST_PROTOCOL)\n    with open(SCALERS_PKL_PATH, 'wb') as f:\n        pickle.dump(scalers, f, pickle.HIGHEST_PROTOCOL)\n\n    del pie_database, full_train_ds, balanced_dict\n    gc.collect()\n    print(\"--- Data preparation complete ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T12:39:52.814621Z","iopub.execute_input":"2025-05-05T12:39:52.814935Z","iopub.status.idle":"2025-05-05T12:54:05.839264Z","shell.execute_reply.started":"2025-05-05T12:39:52.814914Z","shell.execute_reply":"2025-05-05T12:54:05.838522Z"}},"outputs":[{"name":"stdout","text":"Data will be prepared for streams: ['bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro', 'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context', 'yolop']\n--- Running Data Preparation ---\nLoading PIE database cache …\nCalculating standardization parameters …\nInitializing full training dataset …\n\nLoading YOLOP data for sets: ['set01', 'set02', 'set04'] from /kaggle/input/yolop-data/yolop features\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading YOLOP PKLs:   0%|          | 0/53 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b56047bee5254d15b3460d01198c76a2"}},"metadata":{}},{"name":"stdout","text":"Finished loading YOLOP data. Found 23 relevant files out of 23 expected for sets ['set01', 'set02', 'set04'].\nData loaded for 3 sets.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating sequences:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"961e501ebbbd483eb612198c4f928f19"}},"metadata":{}},{"name":"stdout","text":"Extracting data for balancing …\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting:   0%|          | 0/333454 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5357eadfa494475ab1f886e2785e3d67"}},"metadata":{}},{"name":"stdout","text":"---------------------------------------------------------\nBalancing samples based on 'label' key\nUnbalanced: Positive (1): 54967 | Negative (0): 278487\nUndersampling majority class (0) to 54967\nBalanced:   Positive (1): 54967 | Negative (0): 54967\n---------------------------------------------------------\nSaving balanced data …\n--- Data preparation complete ---\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# --- IMPORTS ---\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset # Import Subset\nimport xml.etree.ElementTree as ET\nimport os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm.notebook import tqdm\nimport random\nimport math\nimport zipfile\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport time\nimport sys\nimport gc\n\n# --- Add PIE utilities path if necessary (adjust path) ---\npie_utilities_path = '/kaggle/working/PIE/utilities'\nif pie_utilities_path not in sys.path:\n    sys.path.insert(0, pie_utilities_path)\nPIE = None # Declare PIE before try block\ntry:\n    from pie_data import PIE\nexcept ImportError as e:\n    print(f\"Warning: Could not import PIE class from {pie_utilities_path}. Database must already exist. Error: {e}\")\n    # PIE remains None, handled later if needed\n\n# --- Configuration ---\nPIE_ROOT_PATH = '/kaggle/working/PIE'\nVIDEO_INPUT_DIR = '/kaggle/input'\nPOSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2'\nPIE_DATABASE_CACHE_PATH = '/kaggle/input/pie-database/pie_database.pkl'\nYOLOP_FEATURE_DIR = '/kaggle/input/yolop-data/yolop features'\n\n# --- Define ALL possible streams (used by Dataset class) ---\nALL_POSSIBLE_STREAMS = [\n    'bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro',\n    'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context',\n    'yolop'\n]\nprint(f\"All possible streams: {ALL_POSSIBLE_STREAMS}\")\n\n# --- *** CHOOSE ACTIVE STREAMS FOR THIS EXPERIMENT RUN *** ---\nACTIVE_STREAMS = [\n    'bbox',\n    'ped_action',\n    'ped_look',\n    'ego_speed',\n    'ego_acc',\n    'yolop'\n]\nprint(f\"--- Running Experiment With Active Streams: {ACTIVE_STREAMS} ---\")\n# --- *** END ACTIVE STREAM SELECTION *** ---\n\n# --- Model Hyperparameters ---\nSEQ_LEN = 30\nPRED_LEN = 1\n# --- Input Sizes ---\nINPUT_SIZE_BBOX = 4\nINPUT_SIZE_POSE = 34\nINPUT_SIZE_EGO_SPEED = 1\nINPUT_SIZE_EGO_ACC = 2\nINPUT_SIZE_EGO_GYRO = 1\nINPUT_SIZE_PED_ACTION = 1\nINPUT_SIZE_PED_LOOK = 1\nINPUT_SIZE_PED_OCC = 1\nINPUT_SIZE_TL_STATE = 4\nNUM_SIGNALIZED_CATS = 4\nNUM_INTERSECTION_CATS = 5\nNUM_AGE_CATS = 4\nNUM_GENDER_CATS = 3\nNUM_TRAFFIC_DIR_CATS = 2\nLANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7:4, 8:4}\nNUM_LANE_CATS = len(set(LANE_CATEGORIES.values()))\nINPUT_SIZE_STATIC = NUM_SIGNALIZED_CATS + NUM_INTERSECTION_CATS + NUM_AGE_CATS + NUM_GENDER_CATS + NUM_TRAFFIC_DIR_CATS + NUM_LANE_CATS\nGRID_SIZE = 3\nINPUT_SIZE_YOLOP = GRID_SIZE**2 * 2 + 2 # 20\n\nLSTM_HIDDEN_SIZE = 256\nNUM_LSTM_LAYERS = 2\nDROPOUT_RATE = 0.3\nNUM_CLASSES = 2\nATTENTION_DIM = 128\n\n# --- Training Hyperparameters ---\nLEARNING_RATE = 1e-4\nBATCH_SIZE = 32\nNUM_EPOCHS = 15\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\n# --- Dataset Splits ---\nTRAIN_SETS_STR = ['set01', 'set02', 'set04', 'set06']\nVAL_SETS_STR = ['set05']\n\n# --- Mappings ---\nTL_STATE_MAP = {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}\nNUM_TL_STATES = len(TL_STATE_MAP)\nSIGNALIZED_MAP = {'n/a': 0, 'C': 1, 'S': 2, 'CS': 3}\nINTERSECTION_MAP = {'midblock': 0, 'T': 1, 'T-left': 2, 'T-right': 3, 'four-way': 4}\nAGE_MAP = {'child': 0, 'young': 1, 'adult': 2, 'senior': 3}\nGENDER_MAP = {'n/a': 0, 'female': 1, 'male': 2}\nTRAFFIC_DIR_MAP = {'OW': 0, 'TW': 1}\n\n# --- Output Files (for intermediate balanced data/scalers) ---\nBALANCED_DATA_PKL_PATH = \"/kaggle/working/balanced_train_data_with_yolop.pkl\"\nSCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\"\n\n# --- Helper: One-Hot Encoding ---\ndef to_one_hot(index, num_classes):\n    vec = np.zeros(num_classes, dtype=np.float32)\n    safe_index = int(np.clip(index, 0, num_classes - 1))\n    vec[safe_index] = 1.0\n    return vec\n\n# --- Balancing Function ---\ndef balance_samples_count(seq_data, label_type, random_seed=42):\n    print('---------------------------------------------------------')\n    print(f\"Balancing samples based on '{label_type}' key\")\n    if label_type not in seq_data: raise KeyError(f\"Label type '{label_type}' not found.\")\n\n    gt_labels = [] # Initialize before try block\n    try:\n        gt_labels = [lbl[0] for lbl in seq_data[label_type]]\n    except (IndexError, TypeError) as e:\n        raise ValueError(f\"Labels under '{label_type}' not in expected format [[label_val]]. Original error: {e}\") from e\n\n    if not gt_labels: # Check if gt_labels was successfully populated\n        print(f\"Warning: No valid labels found under key '{label_type}'. Cannot balance.\")\n        return seq_data.copy() # Return original data if labels are missing/malformed\n\n    if not all(isinstance(l, (int, float)) and l in [0, 1] for l in gt_labels):\n        print(f\"Warning: Labels for balancing contain values other than 0 or 1 or are not numeric.\")\n        # Decide how to handle non 0/1 labels if necessary, here we proceed assuming they exist\n\n    num_pos_samples = np.count_nonzero(np.array(gt_labels)); num_neg_samples = len(gt_labels) - num_pos_samples\n    new_seq_data = {}\n    if num_neg_samples == num_pos_samples:\n        print('Samples already balanced.'); return seq_data.copy()\n    else:\n        print(f'Unbalanced: Positive (1): {num_pos_samples} | Negative (0): {num_neg_samples}')\n        majority_label = 0 if num_neg_samples > num_pos_samples else 1\n        minority_count = min(num_neg_samples, num_pos_samples); print(f\"Undersampling majority class ({majority_label}) to match count ({minority_count}).\")\n        majority_indices = np.where(np.array(gt_labels) == majority_label)[0]; minority_indices = np.where(np.array(gt_labels) != majority_label)[0]\n        np.random.seed(random_seed); keep_majority_indices = np.random.choice(majority_indices, size=minority_count, replace=False)\n        final_indices = np.concatenate((minority_indices, keep_majority_indices)); np.random.shuffle(final_indices)\n\n        for k, v_list in seq_data.items():\n            if isinstance(v_list, list) and len(v_list) == len(gt_labels):\n                 try:\n                     # Check if list contains numpy arrays before converting the whole list\n                     if v_list and isinstance(v_list[0], np.ndarray):\n                         # Ensure all elements are arrays of compatible shape if needed,\n                         # or handle potential errors during conversion\n                         try:\n                             v_array = np.array(v_list) # This might fail if arrays have different shapes\n                             new_seq_data[k] = list(v_array[final_indices])\n                         except ValueError as ve:\n                             print(f\"Warning: Could not convert list for key '{k}' to single NumPy array due to varying shapes/types. Processing element-wise. Error: {ve}\")\n                             # Fallback to list comprehension if conversion fails\n                             new_seq_data[k] = [v_list[i] for i in final_indices]\n                     else:\n                         # Simple list comprehension for non-array lists\n                         new_seq_data[k] = [v_list[i] for i in final_indices]\n                 except Exception as e:\n                     # Catch any other unexpected errors during processing\n                     print(f\"Error processing key '{k}' during balancing: {e}. Assigning empty list.\")\n                     new_seq_data[k] = []\n            else:\n                 # Keep lists that don't match the label length (e.g., metadata)\n                 print(f\"Warn: Skipping key '{k}' in balancing (length mismatch or not a list).\")\n                 new_seq_data[k] = v_list # Keep original\n\n        if label_type in new_seq_data:\n            # Recalculate counts from the balanced data\n            new_gt_labels = []\n            try:\n                new_gt_labels = [lbl[0] for lbl in new_seq_data[label_type]]\n                final_pos = np.count_nonzero(np.array(new_gt_labels)); final_neg = len(new_gt_labels) - final_pos;\n                print(f'Balanced:   Positive (1): {final_pos} | Negative (0): {final_neg}')\n            except (IndexError, TypeError, ValueError) as e:\n                 print(f\"Error verifying balanced labels for key '{label_type}': {e}\")\n        else:\n            print(\"Error: Label key lost during balancing process.\")\n        print('---------------------------------------------------------')\n        return new_seq_data\n\n# --- Dataset Class ---\nclass PIEDataset(Dataset):\n    def __init__(self, pie_database, set_names, pose_data_dir, yolop_data_dir, seq_len, pred_len, scalers=None, streams_to_generate=None):\n        self.pie_db = pie_database; self.set_names = set_names; self.pose_data_dir = pose_data_dir; self.yolop_data_dir = yolop_data_dir\n        self.seq_len = seq_len; self.pred_len = pred_len; self.scalers = scalers or {};\n        self.streams_to_generate = streams_to_generate or ALL_POSSIBLE_STREAMS\n        self.sequences = []; self.all_pose_data = {}; self.all_yolop_data = {}\n        self._input_sizes_for_error = self._get_input_sizes_dict()\n        if 'pose' in self.streams_to_generate: self._load_pose_data()\n        if 'yolop' in self.streams_to_generate: self._load_yolop_data()\n        self._generate_sequence_list()\n        if not self.sequences: raise ValueError(f\"Dataset init failed: No sequences generated for sets {self.set_names}\")\n\n    def _get_input_sizes_dict(self):\n        input_sizes = {}; special_cases = {'TRAFFIC_LIGHT': 'TL_STATE', 'STATIC_CONTEXT': 'STATIC', 'EGO_SPEED': 'EGO_SPEED', 'EGO_ACC': 'EGO_ACC', 'EGO_GYRO': 'EGO_GYRO', 'PED_ACTION': 'PED_ACTION', 'PED_LOOK': 'PED_LOOK', 'PED_OCCLUSION': 'PED_OCC', 'YOLOP':'YOLOP'}\n        for stream in ALL_POSSIBLE_STREAMS:\n            size_constant_name = f'INPUT_SIZE_{stream.upper()}'; stream_upper_key = stream.upper(); suffix = special_cases.get(stream_upper_key)\n            if suffix: size_constant_name = f'INPUT_SIZE_{suffix}'\n            elif stream == 'bbox': size_constant_name = 'INPUT_SIZE_BBOX'\n            elif stream == 'pose': size_constant_name = 'INPUT_SIZE_POSE'\n            if size_constant_name in globals(): input_sizes[stream] = globals()[size_constant_name]\n            else: input_sizes[stream] = 1 # Default size if constant not found\n        return input_sizes\n\n    def _load_pose_data(self):\n        print(f\"\\nLoading pose data for sets: {self.set_names} from {self.pose_data_dir}\"); sets_loaded_count = 0\n        for set_id in self.set_names:\n            self.all_pose_data[set_id] = {}; pose_set_path = os.path.join(self.pose_data_dir, set_id)\n            if not os.path.isdir(pose_set_path):\n                # print(f\"Warning: Pose directory not found for set {set_id}: {pose_set_path}\") # Optional warning\n                continue\n            pkl_files_in_set = [f for f in os.listdir(pose_set_path) if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")];\n            if not pkl_files_in_set:\n                # print(f\"Warning: No pose pkl files found in {pose_set_path}\") # Optional warning\n                continue;\n            loaded_video_count = 0\n            for pkl_filename in pkl_files_in_set:\n                pkl_file_path = os.path.join(pose_set_path, pkl_filename);\n                try:\n                    with open(pkl_file_path, 'rb') as f: loaded_pkl_content = pickle.load(f)\n                    if not isinstance(loaded_pkl_content, dict) or len(loaded_pkl_content) != 1:\n                         print(f\"Warning: Unexpected format in pose PKL {pkl_file_path}. Skipping.\")\n                         continue\n                    unique_video_key, video_data = list(loaded_pkl_content.items())[0]; video_id = \"_\".join(unique_video_key.split('_')[1:])\n                    # Check if this video is actually in our PIE database for this set\n                    if set_id in self.pie_db and video_id in self.pie_db[set_id]:\n                         self.all_pose_data[set_id][video_id] = video_data; loaded_video_count += 1\n                    # else: # Optional: Warn if pose data exists but video isn't in DB\n                    #    print(f\"Debug: Pose data found for {set_id}/{video_id} but video not in main pie_db. Skipping.\")\n                except FileNotFoundError:\n                    # This specific error is less likely now due to os.listdir, but keep for robustness\n                    print(f\"Warning: Pose file not found during loading (should not happen after listdir): {pkl_file_path}\")\n                except pickle.UnpicklingError as pe:\n                     print(f\"Error unpickling pose file {pkl_file_path}: {pe}. Skipping.\")\n                except Exception as e:\n                     print(f\"Error loading or processing pose PKL {pkl_file_path}: {e}\")\n            if loaded_video_count > 0: sets_loaded_count += 1\n        print(f\"Finished loading pose data for {sets_loaded_count} relevant sets.\")\n\n    def _load_yolop_data(self):\n        print(f\"\\nLoading YOLOP data for sets: {self.set_names} from {self.yolop_data_dir}\"); sets_loaded_count = 0\n        for set_id in self.set_names: self.all_yolop_data[set_id] = {} # Initialize dict for each set\n\n        if not os.path.isdir(self.yolop_data_dir):\n            print(f\"Error: YOLOP feature directory not found: {self.yolop_data_dir}\"); return\n\n        all_pkl_files = [] # Initialize before try block\n        try:\n            all_pkl_files = [f for f in os.listdir(self.yolop_data_dir) if f.endswith(\"_yolop_features.pkl\")]\n        except FileNotFoundError: # More specific than general Exception\n             print(f\"Error: YOLOP feature directory not found when listing files: {self.yolop_data_dir}\"); return\n        except PermissionError:\n             print(f\"Error: Permission denied when listing files in YOLOP directory {self.yolop_data_dir}\"); return\n        except Exception as e:\n             print(f\"Error listing files in YOLOP directory {self.yolop_data_dir}: {e}\"); return\n\n        if not all_pkl_files: print(f\"Warning: No '*_yolop_features.pkl' files found directly in {self.yolop_data_dir}\")\n\n        loaded_file_count = 0; files_for_needed_sets = 0\n        for pkl_filename in tqdm(all_pkl_files, desc=\"Loading YOLOP PKLs\"):\n            set_id_from_file = None # Initialize before try\n            video_id = None # Initialize before try\n            try:\n                parts = pkl_filename.replace(\"_yolop_features.pkl\", \"\").split('_');\n                if len(parts) < 2: # Need at least setX_videoY\n                     raise IndexError(\"Filename does not contain enough parts for set and video ID.\")\n                set_id_from_file = parts[0]; video_id = \"_\".join(parts[1:])\n            except IndexError as e: # Catch specific error\n                print(f\"Warning: Could not parse set/video ID from filename '{pkl_filename}': {e}. Skipping.\"); continue\n\n            if set_id_from_file in self.set_names:\n                files_for_needed_sets += 1; pkl_file_path = os.path.join(self.yolop_data_dir, pkl_filename)\n                try:\n                    with open(pkl_file_path, 'rb') as f: loaded_pkl_content = pickle.load(f)\n                    # Basic validation of loaded content structure\n                    if not isinstance(loaded_pkl_content, dict) or len(loaded_pkl_content) != 1:\n                        print(f\"Warn: PKL {pkl_filename} has unexpected format (expected dict with 1 item). Skip.\"); continue\n                    unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n                    # Key doesn't strictly need to match video_id if format is {internal_key: data}\n                    # Ensure the set_id exists in the outer dictionary\n                    if set_id_from_file not in self.all_yolop_data:\n                        self.all_yolop_data[set_id_from_file] = {}\n                    self.all_yolop_data[set_id_from_file][video_id] = video_data; loaded_file_count += 1\n                except FileNotFoundError:\n                    print(f\"Warn: YOLOP feature file not found during loading: {pkl_file_path}\")\n                except pickle.UnpicklingError as pe:\n                     print(f\"Error unpickling YOLOP file {pkl_file_path}: {pe}. Skipping.\")\n                except Exception as e:\n                    print(f\"Error loading or processing YOLOP PKL {pkl_file_path}: {e}\")\n\n        print(f\"Finished loading YOLOP data. Found {loaded_file_count} relevant files out of {files_for_needed_sets} possible for sets {self.set_names}.\")\n        if loaded_file_count > 0:\n            actual_sets_loaded = sum(1 for s in self.set_names if s in self.all_yolop_data and self.all_yolop_data[s])\n            print(f\"Data successfully loaded for {actual_sets_loaded} sets.\")\n\n    def _generate_sequence_list(self):\n        sequence_count = 0; ped_count = 0\n        for set_id in tqdm(self.set_names, desc=f\"Generating Sequences for {self.set_names}\"):\n            if set_id not in self.pie_db: continue\n            for video_id, video_data in self.pie_db[set_id].items():\n                if 'ped_annotations' not in video_data: continue\n                for ped_id, ped_data in video_data['ped_annotations'].items():\n                    ped_count += 1;\n                    if 'frames' not in ped_data or not isinstance(ped_data['frames'], list) or len(ped_data['frames']) < self.seq_len + self.pred_len: continue\n                    # Ensure frames are sorted and integers\n                    try:\n                        sorted_frames = sorted([int(f) for f in ped_data['frames']])\n                    except (ValueError, TypeError):\n                        print(f\"Warning: Non-integer or unsortable frame numbers for {set_id}/{video_id}/{ped_id}. Skipping ped.\")\n                        continue\n\n                    for i in range(len(sorted_frames) - self.seq_len - self.pred_len + 1):\n                        start_frame = sorted_frames[i]; end_frame_observe = sorted_frames[i + self.seq_len - 1]\n                        # Check for frame continuity in observation window\n                        if end_frame_observe - start_frame != self.seq_len - 1: continue\n\n                        target_frame_actual_idx = i + self.seq_len + self.pred_len - 1\n                        # Check index bounds (already implicitly handled by range end, but explicit check is fine)\n                        if target_frame_actual_idx >= len(sorted_frames): continue\n                        target_frame = sorted_frames[target_frame_actual_idx]\n\n                        # Check for frame continuity up to target frame\n                        if target_frame - end_frame_observe != self.pred_len: continue\n\n                        self.sequences.append((set_id, video_id, ped_id, start_frame)); sequence_count += 1\n        print(f\"Dataset initialized with {sequence_count} sequences from {ped_count} pedestrians for sets {self.set_names}.\")\n\n    def __len__(self): return len(self.sequences)\n\n    def __getitem__(self, idx):\n        set_id, video_id, ped_id, start_frame = self.sequences[idx];\n        frame_nums = list(range(start_frame, start_frame + self.seq_len));\n        target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n\n        # Safely access nested dictionary structure\n        video_db = self.pie_db.get(set_id, {}).get(video_id, {});\n        ped_db = video_db.get('ped_annotations', {}).get(ped_id, {});\n        ego_db = video_db.get('vehicle_annotations', {});\n        traffic_db = video_db.get('traffic_annotations', {});\n        ped_attributes = ped_db.get('attributes', {})\n\n        feature_sequences = {stream: [] for stream in self.streams_to_generate};\n        label = 0 # Default label\n\n        # --- Safely Extract Label ---\n        target_frame_db_idx = -1 # Initialize before try\n        if 'frames' in ped_db and isinstance(ped_db['frames'], list) and \\\n           'behavior' in ped_db and isinstance(ped_db['behavior'], dict) and \\\n           'cross' in ped_db['behavior'] and isinstance(ped_db['behavior']['cross'], list):\n            try:\n                # Ensure target_frame_num is comparable to elements in ped_db['frames']\n                target_frame_num_comp = int(target_frame_num) # Or str if frames are strings\n                target_frame_db_idx = ped_db['frames'].index(target_frame_num_comp)\n                if target_frame_db_idx < len(ped_db['behavior']['cross']):\n                    label_val = ped_db['behavior']['cross'][target_frame_db_idx];\n                    # Handle potential non-integer labels or placeholder -1\n                    if isinstance(label_val, (int, float)) and label_val == 1:\n                        label = 1\n                    else:\n                        label = 0 # Treat -1 or other values as 0 (not crossing)\n                # else: label remains 0 if index out of bounds for 'cross' list\n            except (ValueError, TypeError, IndexError) as e:\n                 # ValueError if target_frame_num not in list\n                 # TypeError if list contains non-comparable types\n                 # IndexError if list is empty or other issues (less likely here)\n                 # print(f\"Debug: Label not found for idx {idx}, frame {target_frame_num}. Error: {e}\") # Optional debug\n                 label = 0 # Keep default label if lookup fails\n\n        # --- Extract Static Context ---\n        static_vec = None # Initialize before conditional block\n        if 'static_context' in self.streams_to_generate:\n            # Use .get with defaults for safety\n            sig_idx = SIGNALIZED_MAP.get(ped_attributes.get('signalized', 'n/a'), 0)\n            int_idx = INTERSECTION_MAP.get(ped_attributes.get('intersection', 'midblock'), 0)\n            age_idx = AGE_MAP.get(ped_attributes.get('age', 'adult'), 2)\n            gen_idx = GENDER_MAP.get(ped_attributes.get('gender', 'n/a'), 0)\n            td_val = ped_attributes.get('traffic_direction', 'OW')\n            td_idx = TRAFFIC_DIR_MAP.get(td_val, 0)\n            nl_val = ped_attributes.get('num_lanes', 2)\n            # Ensure nl_val is usable as a key or default\n            nl_cat_idx = LANE_CATEGORIES.get(int(nl_val), LANE_CATEGORIES[max(LANE_CATEGORIES.keys())]) if isinstance(nl_val, (int, str)) and str(nl_val).isdigit() else LANE_CATEGORIES[max(LANE_CATEGORIES.keys())]\n\n            try:\n                static_features_list = [\n                    to_one_hot(sig_idx, NUM_SIGNALIZED_CATS), to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n                    to_one_hot(age_idx, NUM_AGE_CATS), to_one_hot(gen_idx, NUM_GENDER_CATS),\n                    to_one_hot(td_idx, NUM_TRAFFIC_DIR_CATS), to_one_hot(nl_cat_idx, NUM_LANE_CATS)\n                ]\n                static_vec = np.concatenate(static_features_list)\n                if static_vec.shape[0] != INPUT_SIZE_STATIC:\n                    print(f\"Warning: Static vector size mismatch for idx {idx}. Expected {INPUT_SIZE_STATIC}, got {static_vec.shape[0]}. Using zeros.\")\n                    static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n            except Exception as e:\n                 print(f\"Error creating static context vector for idx {idx}: {e}. Using zeros.\")\n                 static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n\n\n        # --- Extract Sequential Features ---\n        for frame_num in frame_nums:\n            frame_db_idx = -1 # Default index if frame not found for ped\n            if 'frames' in ped_db and isinstance(ped_db['frames'], list):\n                 try:\n                     frame_num_comp = int(frame_num) # Or str if frames are strings\n                     frame_db_idx = ped_db['frames'].index(frame_num_comp)\n                 except (ValueError, TypeError):\n                     pass # Keep frame_db_idx as -1 if frame not found or type mismatch\n\n            ego_frame_data = ego_db.get(frame_num, {}) # Use .get for safety\n\n            # --- Bbox ---\n            if 'bbox' in self.streams_to_generate:\n                bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32); # Default\n                if frame_db_idx != -1 and 'bbox' in ped_db and isinstance(ped_db['bbox'], list) and len(ped_db['bbox']) > frame_db_idx:\n                     try:\n                         bbox_coords = ped_db['bbox'][frame_db_idx]\n                         # Check if bbox_coords is a valid list/tuple of 4 numbers\n                         if isinstance(bbox_coords, (list, tuple)) and len(bbox_coords) == 4 and all(isinstance(n, (int, float)) for n in bbox_coords):\n                             x1, y1, x2, y2 = bbox_coords\n                             img_w = video_db.get('width', 1920); img_h = video_db.get('height', 1080)\n                             # Ensure width/height are valid numbers > 0\n                             if isinstance(img_w, (int, float)) and img_w > 0 and isinstance(img_h, (int, float)) and img_h > 0:\n                                 cx = ((x1 + x2) / 2) / img_w; cy = ((y1 + y2) / 2) / img_h;\n                                 w = (x2 - x1) / img_w; h = (y2 - y1) / img_h;\n                                 # Basic validation of normalized coordinates\n                                 if w>0 and h>0 and 0<=cx<=1 and 0<=cy<=1:\n                                     bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n                                 # else: print(f\"Debug: Invalid normalized bbox values idx {idx} frame {frame_num}\") # Optional debug\n                         # else: print(f\"Debug: Invalid bbox data format idx {idx} frame {frame_num}\") # Optional debug\n                     except (TypeError, ValueError, ZeroDivisionError) as e: # Catch specific errors\n                          # TypeError if coords are not numbers, ValueError if unpacking fails (less likely with check)\n                          # ZeroDivisionError if img_w/h is 0\n                          print(f\"Warn: Error processing bbox idx {idx} frame {frame_num}: {e}. Using zeros.\")\n                          bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32) # Ensure default on error\n                     except Exception as e: # Catch any other unexpected error\n                          print(f\"Warn: Unexpected error processing bbox idx {idx} frame {frame_num}: {e}. Using zeros.\")\n                          bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32)\n                feature_sequences['bbox'].append(bbox_norm)\n\n            # --- Pose ---\n            if 'pose' in self.streams_to_generate:\n                pose_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32); # Default\n                # Safely access nested pose data structure\n                vid_pose_data = self.all_pose_data.get(set_id, {}).get(video_id, {});\n                frame_pose_data = vid_pose_data.get(frame_num, {});\n                loaded_pose = frame_pose_data.get(ped_id)\n                if loaded_pose is not None and isinstance(loaded_pose, np.ndarray) and loaded_pose.shape == (INPUT_SIZE_POSE,):\n                    pose_vector = loaded_pose # Use loaded data if valid\n                feature_sequences['pose'].append(pose_vector)\n\n            # --- Ego Speed ---\n            if 'ego_speed' in self.streams_to_generate:\n                speed = 0.0 # Default\n                try:\n                    obd_speed = ego_frame_data.get('OBD_speed')\n                    gps_speed = ego_frame_data.get('GPS_speed')\n                    # Prioritize OBD, fallback to GPS, ensure numeric\n                    if isinstance(obd_speed, (int, float)) and obd_speed > 0: # Often 0 means unavailable\n                        speed = float(obd_speed)\n                    elif isinstance(gps_speed, (int, float)):\n                        speed = float(gps_speed)\n                    # Apply scaling safely\n                    mean = self.scalers.get('ego_speed_mean', 0.0)\n                    std = self.scalers.get('ego_speed_std', 1.0)\n                    speed_scaled = (speed - mean) / std if std != 0 else 0.0\n                except Exception as e:\n                     print(f\"Warn: Error processing ego_speed idx {idx} frame {frame_num}: {e}. Using 0.\")\n                     speed_scaled = 0.0\n                feature_sequences['ego_speed'].append([speed_scaled])\n\n            # --- Ego Acc ---\n            if 'ego_acc' in self.streams_to_generate:\n                accX_scaled, accY_scaled = 0.0, 0.0 # Defaults\n                try:\n                    accX = float(ego_frame_data.get('accX', 0.0)) # Ensure float\n                    accY = float(ego_frame_data.get('accY', 0.0)) # Ensure float\n                    accX_mean = self.scalers.get('accX_mean', 0.0); accX_std = self.scalers.get('accX_std', 1.0)\n                    accY_mean = self.scalers.get('accY_mean', 0.0); accY_std = self.scalers.get('accY_std', 1.0)\n                    accX_scaled = (accX - accX_mean) / accX_std if accX_std != 0 else 0.0\n                    accY_scaled = (accY - accY_mean) / accY_std if accY_std != 0 else 0.0\n                except Exception as e:\n                    print(f\"Warn: Error processing ego_acc idx {idx} frame {frame_num}: {e}. Using 0,0.\")\n                    accX_scaled, accY_scaled = 0.0, 0.0\n                feature_sequences['ego_acc'].append([accX_scaled, accY_scaled])\n\n            # --- Ego Gyro ---\n            if 'ego_gyro' in self.streams_to_generate:\n                gyroZ_scaled = 0.0 # Default\n                try:\n                    gyroZ = float(ego_frame_data.get('gyroZ', 0.0)) # Ensure float\n                    mean = self.scalers.get('gyroZ_mean', 0.0); std = self.scalers.get('gyroZ_std', 1.0)\n                    gyroZ_scaled = (gyroZ - mean) / std if std != 0 else 0.0\n                except Exception as e:\n                     print(f\"Warn: Error processing ego_gyro idx {idx} frame {frame_num}: {e}. Using 0.\")\n                     gyroZ_scaled = 0.0\n                feature_sequences['ego_gyro'].append([gyroZ_scaled])\n\n            # --- Ped Action ---\n            if 'ped_action' in self.streams_to_generate:\n                action = 0.0 # Default\n                if frame_db_idx != -1 and 'behavior' in ped_db and isinstance(ped_db['behavior'], dict) and \\\n                   'action' in ped_db['behavior'] and isinstance(ped_db['behavior']['action'], list) and \\\n                   len(ped_db['behavior']['action']) > frame_db_idx:\n                   try:\n                       action_val = ped_db['behavior']['action'][frame_db_idx]\n                       action = float(action_val) # Convert to float, handle potential errors\n                   except (ValueError, TypeError) as e:\n                       print(f\"Warn: Invalid ped_action value idx {idx} frame {frame_num}: {action_val}. Error: {e}. Using 0.\")\n                       action = 0.0\n                feature_sequences['ped_action'].append([action])\n\n            # --- Ped Look ---\n            if 'ped_look' in self.streams_to_generate:\n                look = 0.0 # Default\n                if frame_db_idx != -1 and 'behavior' in ped_db and isinstance(ped_db['behavior'], dict) and \\\n                   'look' in ped_db['behavior'] and isinstance(ped_db['behavior']['look'], list) and \\\n                   len(ped_db['behavior']['look']) > frame_db_idx:\n                   try:\n                       look_val = ped_db['behavior']['look'][frame_db_idx]\n                       look = float(look_val) # Convert to float\n                   except (ValueError, TypeError) as e:\n                       print(f\"Warn: Invalid ped_look value idx {idx} frame {frame_num}: {look_val}. Error: {e}. Using 0.\")\n                       look = 0.0\n                feature_sequences['ped_look'].append([look])\n\n            # --- Ped Occlusion ---\n            if 'ped_occlusion' in self.streams_to_generate:\n                occ = 0.0; # Default (no occlusion)\n                if frame_db_idx != -1 and 'occlusion' in ped_db and isinstance(ped_db['occlusion'], list) and \\\n                   len(ped_db['occlusion']) > frame_db_idx:\n                   try:\n                       occ_val = ped_db['occlusion'][frame_db_idx];\n                       # Assuming occlusion is 0, 1, 2 -> map to 0.0, 0.5, 1.0\n                       occ = float(occ_val) / 2.0 if isinstance(occ_val, (int, float)) else 0.0\n                   except (ValueError, TypeError) as e:\n                        print(f\"Warn: Invalid ped_occlusion value idx {idx} frame {frame_num}: {occ_val}. Error: {e}. Using 0.\")\n                        occ = 0.0\n                feature_sequences['ped_occlusion'].append([occ])\n\n            # --- Traffic Light ---\n            if 'traffic_light' in self.streams_to_generate:\n                state_int = 0 # Default state (__undefined__)\n                if isinstance(traffic_db, dict): # Check traffic_db is a dict\n                    for obj_id, obj_data in traffic_db.items():\n                         if isinstance(obj_data, dict) and obj_data.get('obj_class') == 'traffic_light' and \\\n                            'frames' in obj_data and isinstance(obj_data['frames'], list) and \\\n                            'state' in obj_data and isinstance(obj_data['state'], list):\n                              try:\n                                  frame_num_comp = int(frame_num) # Ensure comparable type\n                                  tl_frame_idx = obj_data['frames'].index(frame_num_comp);\n                                  if tl_frame_idx < len(obj_data['state']):\n                                      state_val = obj_data['state'][tl_frame_idx];\n                                      # Use the state if it's defined (non-zero)\n                                      if state_val in TL_STATE_MAP and state_val != 0:\n                                          state_int = TL_STATE_MAP[state_val] if isinstance(state_val, str) else int(state_val) # Handle both str/int keys if needed\n                                          break # Found relevant TL state for this frame\n                              except (ValueError, TypeError, IndexError):\n                                  continue # Frame not found for this TL or other error, check next TL\n                feature_sequences['traffic_light'].append(to_one_hot(state_int, NUM_TL_STATES))\n\n            # --- Static Context (Append per frame) ---\n            if 'static_context' in self.streams_to_generate:\n                # Use the static_vec calculated once outside the loop\n                feature_sequences['static_context'].append(static_vec if static_vec is not None else np.zeros(INPUT_SIZE_STATIC, dtype=np.float32))\n\n            # --- YOLOP Feature Extraction Logic ---\n            if 'yolop' in self.streams_to_generate:\n                yolop_vector = np.zeros(INPUT_SIZE_YOLOP, dtype=np.float32) # Default zeros\n                # Safely access nested yolop data\n                if set_id in self.all_yolop_data and video_id in self.all_yolop_data[set_id]:\n                    frame_yolop_data = self.all_yolop_data[set_id][video_id].get(frame_num, {})\n                    loaded_yolop = frame_yolop_data.get(ped_id)\n                    # Validate loaded data\n                    if loaded_yolop is not None and isinstance(loaded_yolop, np.ndarray) and loaded_yolop.shape == (INPUT_SIZE_YOLOP,):\n                        yolop_vector = loaded_yolop\n                feature_sequences['yolop'].append(yolop_vector)\n\n        # --- Convert sequences to Tensors ---\n        features = {};\n        try:\n            for stream_name in self.streams_to_generate:\n                 if stream_name in feature_sequences:\n                      # Convert list of numpy arrays/scalars to a single numpy array before tensor conversion\n                      np_array_feature = np.array(feature_sequences[stream_name], dtype=np.float32)\n                      # Check shape consistency (optional but good)\n                      expected_shape = (self.seq_len, self._input_sizes_for_error.get(stream_name, 1))\n                      if np_array_feature.shape != expected_shape:\n                           # Handle shape mismatch: print warning, pad/truncate, or raise error\n                           # Here, we'll print a warning and try to use it anyway if the first dim matches\n                           print(f\"Warning: Feature shape mismatch for '{stream_name}' idx {idx}. Expected {expected_shape}, got {np_array_feature.shape}. Trying to proceed.\")\n                           if np_array_feature.shape[0] != self.seq_len:\n                               # If sequence length is wrong, this is critical, create zeros\n                               print(f\"Error: Critical shape mismatch (sequence length) for '{stream_name}' idx {idx}. Using zeros.\")\n                               features[stream_name] = torch.zeros(expected_shape, dtype=torch.float32)\n                           else:\n                               # If only feature dim mismatches, maybe adaptable? Or force zeros?\n                               print(f\"Warning: Feature dimension mismatch for '{stream_name}' idx {idx}. Using zeros.\")\n                               features[stream_name] = torch.zeros(expected_shape, dtype=torch.float32)\n                               # Or try reshaping if possible? features[stream_name] = torch.tensor(np_array_feature.reshape(expected_shape), dtype=torch.float32)\n                      else:\n                           features[stream_name] = torch.tensor(np_array_feature, dtype=torch.float32)\n                 else:\n                      # This case should not happen if initialized correctly, but handle defensively\n                      print(f\"Warning: Stream '{stream_name}' requested but not found in feature_sequences for idx {idx}. Adding zeros.\")\n                      expected_shape = (self.seq_len, self._input_sizes_for_error.get(stream_name, 1))\n                      features[stream_name] = torch.zeros(expected_shape, dtype=torch.float32)\n\n        except (ValueError, TypeError) as e: # Catch specific numpy/tensor conversion errors\n             print(f\"Error converting features to tensor for idx {idx}: {e}. Returning dummy zeros.\")\n             # Ensure dummy features have correct shapes based on _input_sizes_for_error\n             features = { name: torch.zeros((self.seq_len, self._input_sizes_for_error.get(name, 1)), dtype=torch.float32)\n                          for name in self.streams_to_generate }\n             label = 0 # Return default label with dummy features\n\n        return features, torch.tensor(label, dtype=torch.long)\n\n\n# --- Wrapper Dataset for Balanced Data ---\nclass BalancedDataset(Dataset):\n    def __init__(self, data_dict, active_streams, label_key='label'):\n        self.active_streams = active_streams\n        self.label_key = label_key\n        if self.label_key not in data_dict or not data_dict[self.label_key]:\n             raise ValueError(f\"Label key '{self.label_key}' missing or empty in provided data_dict.\")\n\n        self.num_samples = len(data_dict[self.label_key])\n        if self.num_samples == 0:\n            print(\"Warning: BalancedDataset initialized with zero samples.\")\n            # Initialize features/labels as empty tensors to avoid errors later if len=0\n            self.features = {stream: torch.empty(0) for stream in self.active_streams}\n            self.labels = torch.empty(0, dtype=torch.long)\n            return # Exit early if no samples\n\n        self.features = {}\n        for stream in self.active_streams:\n             if stream in data_dict and data_dict[stream] is not None: # Check stream exists and is not None\n                 try:\n                     # Convert list of numpy arrays (expected from balancing) to a single tensor\n                     stream_data_np = np.array(data_dict[stream])\n                     self.features[stream] = torch.tensor(stream_data_np, dtype=torch.float32)\n                 except ValueError as e:\n                      # Handle cases where np.array fails (e.g., inconsistent shapes within the list)\n                      raise ValueError(f\"Error converting balanced data for stream '{stream}' to tensor. Inconsistent shapes? Error: {e}\") from e\n                 except TypeError as e:\n                     raise TypeError(f\"Error converting balanced data for stream '{stream}' to tensor. Invalid data types? Error: {e}\") from e\n                 except Exception as e: # Catch other unexpected errors\n                      raise RuntimeError(f\"Unexpected error creating tensor for balanced stream '{stream}': {e}\") from e\n\n                 # Check length consistency after successful tensor creation\n                 if len(self.features[stream]) != self.num_samples:\n                     raise ValueError(f\"Length mismatch after tensor creation: Stream '{stream}' ({len(self.features[stream])}) vs Labels ({self.num_samples})\")\n\n             else:\n                  # If an active stream is missing or None in the balanced data, it's an error\n                  raise KeyError(f\"Active stream '{stream}' requested but missing or None in the balanced data dictionary.\")\n\n        try:\n            # Ensure labels are correctly extracted (assuming list of lists like [[0], [1]])\n            self.labels = torch.tensor([lbl[0] for lbl in data_dict[self.label_key]], dtype=torch.long)\n            if len(self.labels) != self.num_samples:\n                 raise ValueError(f\"Length mismatch after label processing: Labels ({len(self.labels)}) vs Expected ({self.num_samples})\")\n        except (IndexError, TypeError) as e:\n             raise ValueError(f\"Error processing labels from balanced data. Expected list of single-element lists (e.g., [[0], [1]]). Error: {e}\") from e\n        except Exception as e:\n             raise RuntimeError(f\"Unexpected error creating label tensor from balanced data: {e}\") from e\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        if idx >= self.num_samples:\n             raise IndexError(\"Index out of bounds\")\n        # Return only features for streams active in this run\n        try:\n            feature_dict = {stream: self.features[stream][idx] for stream in self.active_streams if stream in self.features}\n        except KeyError as e:\n             raise KeyError(f\"Active stream '{e}' not found in self.features during __getitem__ for index {idx}. This should not happen if init was successful.\") from e\n        label = self.labels[idx]\n        return feature_dict, label\n\n\n# --- Model Architecture ---\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim, attention_dim):\n        super(Attention, self).__init__()\n        # Basic validation\n        if not isinstance(hidden_dim, int) or hidden_dim <= 0:\n            raise ValueError(\"hidden_dim must be a positive integer\")\n        if not isinstance(attention_dim, int) or attention_dim <= 0:\n            raise ValueError(\"attention_dim must be a positive integer\")\n\n        self.attention_net = nn.Sequential(\n            nn.Linear(hidden_dim, attention_dim),\n            nn.Tanh(),\n            nn.Linear(attention_dim, 1)\n        )\n\n    def forward(self, lstm_output):\n        # lstm_output shape: (batch, seq_len, hidden_dim * num_directions)\n        if lstm_output.ndim != 3:\n             raise ValueError(f\"Expected lstm_output to have 3 dimensions (batch, seq, hidden), got {lstm_output.ndim}\")\n        try:\n            att_scores = self.attention_net(lstm_output).squeeze(2) # (batch, seq_len)\n            if att_scores.shape[0] != lstm_output.shape[0] or att_scores.shape[1] != lstm_output.shape[1]:\n                 raise RuntimeError(\"Attention scores shape mismatch after squeeze.\")\n            att_weights = torch.softmax(att_scores, dim=1) # (batch, seq_len)\n            # Weighted sum: (batch, seq_len, hidden) * (batch, seq_len, 1) -> sum along seq_len dim\n            context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1) # (batch, hidden)\n        except Exception as e:\n             print(f\"Error during Attention forward pass: {e}\")\n             # Depending on desired robustness, could return zeros or re-raise\n             raise RuntimeError(\"Failed in Attention forward pass\") from e\n        return context_vector, att_weights\n\nclass MultiStreamAdaptiveLSTM(nn.Module):\n    def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes, attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n        super(MultiStreamAdaptiveLSTM, self).__init__()\n        if not stream_names:\n            raise ValueError(\"stream_names cannot be empty.\")\n        if not isinstance(input_sizes, dict):\n            raise TypeError(\"input_sizes must be a dictionary.\")\n        self.stream_names = stream_names\n        self.lstms = nn.ModuleDict()\n        self.attentions = nn.ModuleDict()\n        print(f\"Initializing model with streams: {self.stream_names}\")\n\n        num_active_streams = 0 # Count streams successfully added\n        for name in self.stream_names:\n            if name not in input_sizes:\n                # Option 1: Raise error if a requested stream is missing config\n                raise KeyError(f\"Input size for stream '{name}' not provided in input_sizes dictionary.\")\n                # Option 2: Warn and skip (might lead to downstream errors if logic expects it)\n                # print(f\"Warning: Input size for stream '{name}' not provided. Skipping this stream.\")\n                # continue\n\n            current_input_size = input_sizes[name]\n            if not isinstance(current_input_size, int) or current_input_size <= 0:\n                 raise ValueError(f\"Invalid input size ({current_input_size}) for stream '{name}'. Must be a positive integer.\")\n\n            print(f\"  - Adding stream '{name}' with input size {current_input_size}\")\n            try:\n                self.lstms[name] = nn.LSTM(\n                    current_input_size, lstm_hidden_size, num_lstm_layers,\n                    batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0,\n                    bidirectional=True # Assuming bidirectional based on Attention hidden_dim * 2\n                )\n                self.attentions[name] = Attention(lstm_hidden_size * 2 , attention_dim) # *2 for bidirectional\n                num_active_streams += 1\n            except Exception as e:\n                 print(f\"Error initializing LSTM/Attention for stream '{name}': {e}\")\n                 raise # Re-raise to stop model creation if a part fails\n\n        if num_active_streams == 0:\n             raise ValueError(\"No streams were successfully initialized in the model.\")\n\n        combined_feature_dim = lstm_hidden_size * 2 * num_active_streams # *2 for bidirectional\n        print(f\"  Combined feature dimension: {combined_feature_dim}\")\n\n        # Classifier layers\n        self.dropout = nn.Dropout(dropout_rate)\n        # Ensure intermediate_dim is reasonable\n        intermediate_dim = max(num_classes * 2, combined_feature_dim // 2, 16) # Added min size\n        self.fc1 = nn.Linear(combined_feature_dim, intermediate_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(intermediate_dim, num_classes)\n\n    def forward(self, x):\n        if not isinstance(x, dict):\n             raise TypeError(f\"Input 'x' must be a dictionary of stream tensors, got {type(x)}\")\n\n        stream_context_vectors = []\n        stream_att_weights = {} # Optional: if needed for analysis\n\n        processed_streams = 0\n        for name in self.stream_names: # Iterate through streams model was configured for\n            if name not in x:\n                # This indicates a problem upstream (DataLoader/Dataset didn't provide expected data)\n                raise KeyError(f\"Input dictionary 'x' is missing expected stream '{name}' during forward pass.\")\n                # Alternative (more robust, less strict): warn and skip\n                # print(f\"Warning: Stream '{name}' expected but not found in input batch. Skipping.\")\n                # continue\n\n            stream_input = x[name]\n            # Basic validation of input tensor for this stream\n            if not isinstance(stream_input, torch.Tensor):\n                 raise TypeError(f\"Input for stream '{name}' must be a torch.Tensor, got {type(stream_input)}\")\n            if stream_input.ndim != 3: # Expect (batch, seq_len, feature_dim)\n                 raise ValueError(f\"Input tensor for stream '{name}' has incorrect dimensions ({stream_input.ndim}). Expected 3 (batch, seq, feature).\")\n\n            try:\n                lstm_out, _ = self.lstms[name](stream_input) # (batch, seq_len, hidden*2)\n                context_vector, attention_weights = self.attentions[name](lstm_out) # (batch, hidden*2)\n                stream_context_vectors.append(context_vector)\n                # stream_att_weights[name] = attention_weights # Optional storage\n                processed_streams += 1\n            except Exception as e:\n                 print(f\"Error processing stream '{name}' in forward pass: {e}\")\n                 raise RuntimeError(f\"Failed during LSTM/Attention for stream '{name}'\") from e\n\n        # Check if any streams were actually processed (especially important if skipping was allowed)\n        if not stream_context_vectors:\n            raise RuntimeError(\"No stream outputs generated during forward pass. Input might be missing all expected streams.\")\n        if processed_streams != len(self.stream_names):\n             print(f\"Warning: Only processed {processed_streams}/{len(self.stream_names)} expected streams.\") # Should not happen if KeyErrors are raised\n\n        try:\n            fused_features = torch.cat(stream_context_vectors, dim=1) # Concatenate along feature dimension\n            # Check shape after concat: (batch, sum_of_context_vector_dims)\n            # Expected dim = num_active_streams * lstm_hidden_size * 2\n            expected_fused_dim = processed_streams * self.lstms[self.stream_names[0]].hidden_size * 2 # Get hidden size from one LSTM\n            if fused_features.shape[1] != expected_fused_dim:\n                 raise RuntimeError(f\"Fused feature dimension mismatch. Expected {expected_fused_dim}, got {fused_features.shape[1]}\")\n\n            # Pass through classifier\n            out = self.dropout(fused_features)\n            out = self.relu(self.fc1(out))\n            out = self.dropout(out)\n            logits = self.fc2(out)\n        except Exception as e:\n             print(f\"Error during final classification layers: {e}\")\n             raise RuntimeError(\"Failed in classifier part of forward pass\") from e\n\n        return logits\n\n# --- Training and Evaluation Functions ---\ndef train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n    active_streams = model.stream_names # Get streams the model expects\n\n    batch_num = 0\n    for features, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n        batch_num += 1\n        # Ensure features is a dict and move only expected streams to device\n        if not isinstance(features, dict):\n            print(f\"Error: Expected features to be a dict in batch {batch_num}, got {type(features)}. Skipping batch.\")\n            continue\n        input_features = {}\n        try:\n            for name in active_streams:\n                if name not in features:\n                    raise KeyError(f\"Required stream '{name}' missing from batch {batch_num}.\")\n                input_features[name] = features[name].to(device)\n            labels = labels.to(device)\n        except KeyError as e:\n            print(f\"Error preparing batch {batch_num}: {e}. Skipping.\")\n            continue\n        except Exception as e:\n            print(f\"Error moving batch {batch_num} to device {device}: {e}. Skipping.\")\n            continue\n\n        try:\n            optimizer.zero_grad()\n            outputs = model(input_features) # Shape: (batch_size, num_classes)\n            # Ensure labels have the correct shape for CrossEntropyLoss (batch_size)\n            if labels.ndim != 1:\n                 labels = labels.squeeze() # Attempt to fix if shape is (batch_size, 1)\n                 if labels.ndim != 1:\n                      raise ValueError(f\"Labels have incorrect shape {labels.shape} for CrossEntropyLoss in batch {batch_num}.\")\n\n            loss = criterion(outputs, labels)\n            loss.backward()\n            # Optional: Gradient clipping\n            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n\n            total_loss += loss.item()\n            with torch.no_grad(): # Ensure preds calculation doesn't track gradients\n                 preds = torch.argmax(outputs, dim=1)\n                 all_preds.extend(preds.cpu().numpy())\n                 all_labels.extend(labels.cpu().numpy())\n\n        except Exception as e:\n            print(f\"Error during training step for batch {batch_num}: {e}\")\n            # Decide whether to skip batch or re-raise\n            # Skipping might hide persistent issues\n            # raise e # Option: Stop training on error\n\n    avg_loss = total_loss / max(1, len(dataloader)) # Avoid division by zero if dataloader is empty\n    accuracy = 0.0\n    if all_labels: # Calculate accuracy only if some batches were processed\n       try:\n           accuracy = accuracy_score(all_labels, all_preds)\n       except ValueError as e:\n           print(f\"Error calculating training accuracy: {e}. Labels: {np.unique(all_labels)}, Preds: {np.unique(all_preds)}\")\n\n    return avg_loss, accuracy\n\ndef evaluate_epoch(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_labels = []\n    all_preds = []\n    all_probs = []\n    active_streams = model.stream_names\n\n    batch_num = 0\n    with torch.no_grad():\n        for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n            batch_num += 1\n            if not isinstance(features, dict):\n                print(f\"Error: Expected features to be a dict in eval batch {batch_num}, got {type(features)}. Skipping.\")\n                continue\n            input_features = {}\n            try:\n                for name in active_streams:\n                    if name not in features:\n                        raise KeyError(f\"Required stream '{name}' missing from eval batch {batch_num}.\")\n                    input_features[name] = features[name].to(device)\n                labels = labels.to(device)\n            except KeyError as e:\n                print(f\"Error preparing eval batch {batch_num}: {e}. Skipping.\")\n                continue\n            except Exception as e:\n                print(f\"Error moving eval batch {batch_num} to device {device}: {e}. Skipping.\")\n                continue\n\n            try:\n                outputs = model(input_features)\n                # Ensure labels have the correct shape\n                if labels.ndim != 1:\n                     labels = labels.squeeze()\n                     if labels.ndim != 1:\n                          raise ValueError(f\"Labels have incorrect shape {labels.shape} for loss calculation in eval batch {batch_num}.\")\n\n                loss = criterion(outputs, labels)\n                total_loss += loss.item()\n\n                probs = torch.softmax(outputs, dim=1)\n                preds = torch.argmax(probs, dim=1)\n\n                all_labels.extend(labels.cpu().numpy())\n                all_preds.extend(preds.cpu().numpy())\n                all_probs.extend(probs.cpu().numpy()) # Store probabilities for AUC\n\n            except Exception as e:\n                 print(f\"Error during evaluation step for batch {batch_num}: {e}\")\n                 # Continue evaluation if one batch fails? Or stop?\n                 # continue\n\n    avg_loss = total_loss / max(1, len(dataloader))\n    # Initialize metrics with default values\n    accuracy, precision, recall, f1, auc = 0.0, 0.0, 0.0, 0.0, float('nan')\n\n    if not all_labels: # Check if any data was processed\n        print(\"Warning: No labels collected during evaluation. Returning zero/NaN metrics.\")\n        return {'loss': avg_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}\n\n    try:\n        all_probs = np.array(all_probs)\n        all_labels = np.array(all_labels)\n        all_preds = np.array(all_preds)\n\n        accuracy = accuracy_score(all_labels, all_preds)\n        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', pos_label=1, zero_division=0)\n\n        # Calculate AUC only if both classes are present in true labels\n        if len(np.unique(all_labels)) > 1:\n            # Ensure probabilities correspond to the positive class (class 1)\n            if all_probs.shape[1] == 2:\n                 auc = roc_auc_score(all_labels, all_probs[:, 1])\n            else:\n                 print(f\"Warning: Probability array has unexpected shape {all_probs.shape}. Cannot calculate AUC.\")\n                 auc = float('nan')\n        else:\n            print(\"Warning: Only one class present in ground truth labels during evaluation. AUC is not defined.\")\n            auc = float('nan') # AUC is not defined for single-class data\n\n    except ValueError as e:\n        print(f\"Error calculating evaluation metrics: {e}\")\n        # This might happen if shapes mismatch or other sklearn issues\n    except Exception as e:\n        print(f\"Unexpected error calculating evaluation metrics: {e}\")\n\n\n    return {'loss': avg_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}\n\ndef get_predictions_and_labels(model, dataloader, device):\n    model.eval()\n    all_labels = []\n    all_preds = []\n    active_streams = model.stream_names\n\n    batch_num = 0\n    with torch.no_grad():\n        for features, labels in tqdm(dataloader, desc=\"Generating CM Data\", leave=False):\n             batch_num += 1\n             if not isinstance(features, dict):\n                 print(f\"Error: Expected features to be a dict in CM batch {batch_num}, got {type(features)}. Skipping.\")\n                 continue\n             input_features = {}\n             try:\n                 for name in active_streams:\n                     if name not in features:\n                         raise KeyError(f\"Required stream '{name}' missing from CM batch {batch_num}.\")\n                     input_features[name] = features[name].to(device)\n                 labels = labels.to(device)\n             except KeyError as e:\n                 print(f\"Error preparing CM batch {batch_num}: {e}. Skipping.\")\n                 continue\n             except Exception as e:\n                 print(f\"Error moving CM batch {batch_num} to device {device}: {e}. Skipping.\")\n                 continue\n\n             try:\n                 outputs = model(input_features)\n                 preds = torch.argmax(outputs, dim=1)\n                 all_labels.extend(labels.cpu().numpy())\n                 all_preds.extend(preds.cpu().numpy())\n             except Exception as e:\n                  print(f\"Error during prediction for CM data batch {batch_num}: {e}\")\n                  # continue\n\n    return np.array(all_labels), np.array(all_preds)\n\n# --- Main Execution Block ---\nif __name__ == '__main__':\n\n    # --- Step 1: Data Preparation ---\n    run_data_prep = not (os.path.exists(BALANCED_DATA_PKL_PATH) and os.path.exists(SCALERS_PKL_PATH))\n\n    # Declare variables used in both branches before the conditional block\n    pie_database = None\n    scalers = {}\n    balanced_train_data_dict = None\n\n    if run_data_prep:\n        print(\"--- Running Data Preparation ---\")\n        # --- Generate/Load PIE Database ---\n        print(f\"Checking for PIE database cache at: {PIE_DATABASE_CACHE_PATH}\")\n        if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n            if PIE is None: # Check if import failed earlier\n                 raise ImportError(\"PIE class could not be imported, and database cache does not exist. Cannot generate database.\")\n            print(\"PIE database cache not found. Generating...\");\n            try:\n                 # Assuming PIE class needs initialization parameters like data_path\n                 pie_dataset_interface = PIE(data_path=PIE_ROOT_PATH, regen_database=True) # Adjust params as needed\n                 pie_database = pie_dataset_interface.generate_database()\n                 if not pie_database: raise RuntimeError(\"PIE database generation returned empty.\")\n                 print(\"PIE database generated successfully.\")\n                 # Optional: Save the generated database\n                 # try:\n                 #     with open(PIE_DATABASE_CACHE_PATH.replace('.pkl', '_newly_generated.pkl'), 'wb') as f: # Save with different name\n                 #         pickle.dump(pie_database, f, pickle.HIGHEST_PROTOCOL)\n                 #     print(f\"Saved newly generated database.\")\n                 # except Exception as e:\n                 #     print(f\"Error saving newly generated PIE database: {e}\")\n\n            except Exception as e:\n                 raise RuntimeError(f\"Failed to initialize or run PIE database generation: {e}\") from e\n        else:\n            print(\"Loading PIE database from cache...\")\n            try:\n                with open(PIE_DATABASE_CACHE_PATH, 'rb') as f: pie_database = pickle.load(f)\n                print(\"PIE database loaded successfully.\")\n                if not isinstance(pie_database, dict) or not pie_database: # Basic validation\n                     raise ValueError(\"Loaded PIE database is not a valid dictionary or is empty.\")\n            except FileNotFoundError:\n                 raise FileNotFoundError(f\"PIE database cache file not found at {PIE_DATABASE_CACHE_PATH} despite os.path.exists being true initially.\")\n            except pickle.UnpicklingError as pe:\n                 raise RuntimeError(f\"Failed to unpickle PIE database from {PIE_DATABASE_CACHE_PATH}: {pe}\") from pe\n            except Exception as e:\n                 raise RuntimeError(f\"Failed to load PIE database from cache: {e}\") from e\n\n        if pie_database is None: # Should have been caught by exceptions, but double check\n             raise RuntimeError(\"PIE Database is None after generation/loading attempt.\")\n\n        # --- Calculate Standardization Parameters ---\n        print(\"\\nCalculating standardization parameters from training set...\")\n        all_train_ego_speeds = []; all_train_accX = []; all_train_accY = []; all_train_gyroZ = []\n        # Check if pie_database is a dictionary before iterating\n        if isinstance(pie_database, dict):\n            for set_id in TRAIN_SETS_STR:\n                 if set_id in pie_database and isinstance(pie_database[set_id], dict):\n                     for video_id, video_data in pie_database[set_id].items():\n                          if isinstance(video_data, dict) and 'vehicle_annotations' in video_data and isinstance(video_data['vehicle_annotations'], dict):\n                               for frame_num, ego_frame_data in video_data['vehicle_annotations'].items():\n                                   if isinstance(ego_frame_data, dict): # Check ego data is a dict\n                                        try: # Safely get and convert values\n                                             obd_speed = ego_frame_data.get('OBD_speed')\n                                             gps_speed = ego_frame_data.get('GPS_speed')\n                                             speed = 0.0\n                                             if isinstance(obd_speed, (int, float)) and obd_speed > 0:\n                                                  speed = float(obd_speed)\n                                             elif isinstance(gps_speed, (int, float)):\n                                                  speed = float(gps_speed)\n                                             all_train_ego_speeds.append(speed)\n\n                                             accX = float(ego_frame_data.get('accX', 0.0))\n                                             accY = float(ego_frame_data.get('accY', 0.0))\n                                             gyroZ = float(ego_frame_data.get('gyroZ', 0.0))\n                                             all_train_accX.append(accX); all_train_accY.append(accY); all_train_gyroZ.append(gyroZ)\n                                        except (TypeError, ValueError) as e:\n                                             print(f\"Warning: Invalid data type for ego stats in {set_id}/{video_id}/frame {frame_num}. Skipping frame. Error: {e}\")\n        else:\n             print(\"Warning: pie_database is not a dictionary. Cannot calculate scalers.\")\n\n\n        scalers = {} # Initialize scalers dict\n        try:\n            if all_train_ego_speeds:\n                 mean_speed = np.mean(all_train_ego_speeds); std_speed = np.std(all_train_ego_speeds)\n                 scalers['ego_speed_mean'] = mean_speed; scalers['ego_speed_std'] = std_speed if std_speed > 1e-6 else 1.0;\n                 print(f\"  Ego Speed: Mean={scalers['ego_speed_mean']:.2f}, Std={scalers['ego_speed_std']:.2f}\")\n            if all_train_accX: # Assume accY exists if accX does\n                 mean_accX = np.mean(all_train_accX); std_accX = np.std(all_train_accX)\n                 mean_accY = np.mean(all_train_accY); std_accY = np.std(all_train_accY)\n                 scalers['accX_mean'] = mean_accX; scalers['accX_std'] = std_accX if std_accX > 1e-6 else 1.0;\n                 scalers['accY_mean'] = mean_accY; scalers['accY_std'] = std_accY if std_accY > 1e-6 else 1.0;\n                 print(f\"  Ego AccX: Mean={scalers['accX_mean']:.2f}, Std={scalers['accX_std']:.2f}\"); print(f\"  Ego AccY: Mean={scalers['accY_mean']:.2f}, Std={scalers['accY_std']:.2f}\")\n            if all_train_gyroZ:\n                 mean_gyroZ = np.mean(all_train_gyroZ); std_gyroZ = np.std(all_train_gyroZ)\n                 scalers['gyroZ_mean'] = mean_gyroZ; scalers['gyroZ_std'] = std_gyroZ if std_gyroZ > 1e-6 else 1.0;\n                 print(f\"  Ego GyroZ: Mean={scalers['gyroZ_mean']:.2f}, Std={scalers['gyroZ_std']:.2f}\")\n        except Exception as e:\n             print(f\"Error calculating scaler statistics: {e}\") # Catch potential numpy errors\n        print(\"Standardization parameters calculation finished.\")\n\n        # --- Initialize FULL Training Dataset ---\n        print(\"\\nInitializing full training dataset (for extraction/balancing)...\")\n        full_train_dataset = None # Initialize before try block\n        try:\n            full_train_dataset = PIEDataset(pie_database, TRAIN_SETS_STR, POSE_DATA_DIR, YOLOP_FEATURE_DIR, SEQ_LEN, PRED_LEN, scalers, ALL_POSSIBLE_STREAMS)\n            if len(full_train_dataset) == 0:\n                 # This isn't necessarily an error, could be no valid sequences found\n                 print(\"Warning: Full Train Dataset initialization resulted in 0 sequences.\")\n                 # Depending on requirements, could raise ValueError here or allow proceeding\n                 # raise ValueError(\"Full Train Dataset loading resulted in 0 sequences.\")\n        except Exception as e:\n             print(f\"Error initializing full training dataset: {e}\")\n             raise # Re-raise critical error\n\n\n        # --- Prepare and Balance Training Data ---\n        print(\"\\nExtracting ALL stream data from training set for balancing...\")\n        training_data_dict = {stream: [] for stream in ALL_POSSIBLE_STREAMS}; training_data_dict['label'] = []\n        # Proceed only if full_train_dataset was created and has items\n        if full_train_dataset and len(full_train_dataset) > 0:\n            num_extraction_errors = 0\n            for i in tqdm(range(len(full_train_dataset)), desc=\"Extracting data\"):\n                 try:\n                     features, label = full_train_dataset[i] # __getitem__ might raise errors\n                     # Validate features dictionary\n                     if not isinstance(features, dict):\n                          raise TypeError(f\"__getitem__ returned features of type {type(features)}, expected dict.\")\n                     for stream_name in ALL_POSSIBLE_STREAMS:\n                         if stream_name in features and features[stream_name] is not None:\n                              # Convert tensor back to numpy for storage in dict (if not already numpy)\n                              feature_data = features[stream_name]\n                              if isinstance(feature_data, torch.Tensor):\n                                   training_data_dict[stream_name].append(feature_data.cpu().numpy())\n                              elif isinstance(feature_data, np.ndarray):\n                                   training_data_dict[stream_name].append(feature_data)\n                              else:\n                                   raise TypeError(f\"Feature data for stream '{stream_name}' is type {type(feature_data)}, expected Tensor or ndarray.\")\n                         else:\n                              # If a stream is missing from __getitem__ output, record it and add zeros\n                              print(f\"Warn: Stream '{stream_name}' missing from __getitem__ output for idx {i}. Appending zeros.\")\n                              # Determine expected feature size for the zeros array\n                              expected_size = full_train_dataset._input_sizes_for_error.get(stream_name, 1)\n                              training_data_dict[stream_name].append(np.zeros((SEQ_LEN, expected_size), dtype=np.float32))\n                     # Append label\n                     training_data_dict['label'].append([label.item()]) # Assuming label is a 0-dim tensor\n\n                 except Exception as e:\n                     num_extraction_errors += 1\n                     print(f\"Error extracting data for index {i}: {e}. Skipping item.\")\n                     # Optionally add placeholder data or just skip\n                     if num_extraction_errors > len(full_train_dataset) * 0.1: # Stop if too many errors\n                          print(\"Error: Excessive errors during data extraction. Aborting.\")\n                          raise RuntimeError(\"Too many errors during data extraction.\") from e\n\n            print(f\"Data extraction finished. Original training samples considered: {len(full_train_dataset)}, Errors encountered: {num_extraction_errors}\")\n            print(f\"Samples successfully extracted: {len(training_data_dict['label'])}\")\n\n            if not training_data_dict['label']: # Check if any data was actually extracted\n                 print(\"Error: No data was successfully extracted. Cannot proceed with balancing.\")\n                 # Handle this case: maybe exit or raise an error\n                 sys.exit(\"Exiting due to lack of extracted data.\")\n            else:\n                 # --- Balance Data ---\n                 label_key_for_balancing = 'label'\n                 try:\n                     balanced_train_data_dict = balance_samples_count(training_data_dict, label_type=label_key_for_balancing)\n                 except Exception as e:\n                      print(f\"Error during sample balancing: {e}\")\n                      raise # Balancing is critical, re-raise\n\n        else:\n             print(\"Skipping data extraction and balancing because full_train_dataset is empty or None.\")\n             balanced_train_data_dict = {} # Ensure it's an empty dict if skipped\n\n        # --- Cleanup ---\n        del training_data_dict # Free memory\n        if full_train_dataset: del full_train_dataset\n\n\n        # --- Save Balanced Data and Scalers ---\n        if balanced_train_data_dict: # Only save if balancing happened and produced data\n            print(f\"\\nSaving balanced training data to: {BALANCED_DATA_PKL_PATH}\")\n            try:\n                with open(BALANCED_DATA_PKL_PATH, 'wb') as f: pickle.dump(balanced_train_data_dict, f, pickle.HIGHEST_PROTOCOL)\n                print(\" -> Balanced data saved.\")\n            except pickle.PicklingError as pe: print(f\"  Error pickling balanced data: {pe}\")\n            except Exception as e: print(f\"  Error saving balanced data: {e}\")\n        else:\n             print(\"\\nSkipping saving balanced data (was not generated or is empty).\")\n\n        if scalers: # Only save if scalers were calculated\n            print(f\"\\nSaving scalers to: {SCALERS_PKL_PATH}\")\n            try:\n                with open(SCALERS_PKL_PATH, 'wb') as f: pickle.dump(scalers, f, pickle.HIGHEST_PROTOCOL)\n                print(\" -> Scalers saved.\")\n            except pickle.PicklingError as pe: print(f\"  Error pickling scalers: {pe}\")\n            except Exception as e: print(f\"  Error saving scalers: {e}\")\n        else:\n             print(\"\\nSkipping saving scalers (were not calculated or are empty).\")\n\n        # Delete PIE db only if it was loaded/generated in this block\n        if 'pie_database' in locals() and pie_database is not None: del pie_database\n        gc.collect()\n        print(\"\\n--- Data Preparation and Balancing Completed ---\")\n\n    else: # Data prep files should exist, load them\n        print(\"--- Skipping Data Preparation: Loading Pre-Saved Files ---\")\n        print(f\"\\nLoading balanced training data from: {BALANCED_DATA_PKL_PATH}\")\n        print(f\"Loading scalers from: {SCALERS_PKL_PATH}\")\n        try:\n            if not os.path.exists(BALANCED_DATA_PKL_PATH):\n                 raise FileNotFoundError(f\"Balanced data file not found: {BALANCED_DATA_PKL_PATH}\")\n            with open(BALANCED_DATA_PKL_PATH, 'rb') as f: balanced_train_data_dict = pickle.load(f)\n\n            if not os.path.exists(SCALERS_PKL_PATH):\n                 raise FileNotFoundError(f\"Scalers file not found: {SCALERS_PKL_PATH}\")\n            with open(SCALERS_PKL_PATH, 'rb') as f: scalers = pickle.load(f)\n\n            # Basic validation of loaded data\n            if not isinstance(balanced_train_data_dict, dict):\n                 raise TypeError(f\"Loaded balanced data is not a dictionary (type: {type(balanced_train_data_dict)}).\")\n            if not isinstance(scalers, dict):\n                 raise TypeError(f\"Loaded scalers data is not a dictionary (type: {type(scalers)}).\")\n\n            print(\" -> Pre-processed data loaded successfully.\")\n\n        except FileNotFoundError as e:\n             print(f\"ERROR: Required pre-processed file not found: {e}. Cannot continue without running the data preparation phase.\")\n             sys.exit(1) # Exit script\n        except pickle.UnpicklingError as pe:\n             print(f\"ERROR: Failed to unpickle pre-processed data: {pe}. Files might be corrupted. Re-run prep phase.\")\n             sys.exit(1)\n        except Exception as e:\n             print(f\"ERROR: Unexpected error loading pre-processed data: {e}\")\n             sys.exit(1)\n\n        # Ensure loaded variables are not None before proceeding\n        if balanced_train_data_dict is None or scalers is None:\n             print(\"ERROR: Loading pre-processed data resulted in None variables. Cannot continue.\")\n             sys.exit(1)\n\n\n    # --- Step 2: Model Training and Evaluation ---\n    print(\"\\n\" + \"=\"*70)\n    print(\"--- Running Model Training and Evaluation ---\")\n    print(f\"Active Streams for this run: {ACTIVE_STREAMS}\")\n    print(\"-\" * 70)\n\n    # --- Load Full PIE Database (needed for Validation Dataset) ---\n    # This needs to happen regardless of whether data prep ran or not\n    print(\"\\nLoading PIE database cache for Validation Dataset...\")\n    if pie_database is None: # Check if it needs loading (wasn't loaded/kept from prep phase)\n        if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n             raise FileNotFoundError(f\"PIE database cache not found at {PIE_DATABASE_CACHE_PATH}. Required for Validation Dataset.\")\n        try:\n            with open(PIE_DATABASE_CACHE_PATH, 'rb') as f: pie_database = pickle.load(f)\n            print(\" -> PIE database loaded successfully for validation.\")\n            if not isinstance(pie_database, dict) or not pie_database: # Basic validation\n                 raise ValueError(\"Loaded PIE database for validation is not a valid dictionary or is empty.\")\n        except FileNotFoundError: # Should be caught by os.path.exists, but defensive check\n             raise FileNotFoundError(f\"PIE database cache file not found at {PIE_DATABASE_CACHE_PATH} when loading for validation.\")\n        except pickle.UnpicklingError as pe:\n             raise RuntimeError(f\"Failed to unpickle PIE database for validation from {PIE_DATABASE_CACHE_PATH}: {pe}\") from pe\n        except Exception as e:\n             raise RuntimeError(f\"Failed to load PIE database for validation: {e}\") from e\n\n    if pie_database is None: # Final check\n         raise RuntimeError(\"PIE Database is still None before creating Validation Dataset.\")\n\n\n    # --- Create Datasets and DataLoaders ---\n    print(\"\\nCreating Datasets and DataLoaders...\")\n    train_dataset, val_dataset = None, None # Initialize before try\n    train_loader, val_loader = None, None # Initialize before try\n    try:\n        # Ensure balanced data dict is valid before creating dataset\n        if not balanced_train_data_dict or 'label' not in balanced_train_data_dict or not balanced_train_data_dict['label']:\n             raise ValueError(\"Balanced training data dictionary is invalid or missing labels. Cannot create training dataset.\")\n        train_dataset = BalancedDataset(balanced_train_data_dict, ACTIVE_STREAMS, label_key='label')\n\n        # Free memory associated with the large dictionary now\n        del balanced_train_data_dict\n        gc.collect()\n\n        # Validation dataset needs the PIE database and scalers\n        if not pie_database: raise ValueError(\"PIE database is None, cannot create validation dataset.\")\n        if not scalers: print(\"Warning: Scalers dictionary is empty when creating validation dataset.\") # Non-fatal?\n\n        val_dataset = PIEDataset(pie_database, VAL_SETS_STR, POSE_DATA_DIR, YOLOP_FEATURE_DIR, SEQ_LEN, PRED_LEN, scalers, ALL_POSSIBLE_STREAMS)\n\n        if len(train_dataset) == 0: print(\"Warning: Training dataset is empty after initialization.\") # Potentially problematic\n        if len(val_dataset) == 0: print(\"Warning: Validation dataset is empty after initialization.\") # Potentially problematic\n        # Decide if empty datasets should halt execution\n        if len(train_dataset) == 0 or len(val_dataset) == 0:\n            raise ValueError(\"Cannot proceed with empty Training or Validation dataset.\")\n\n        # Create DataLoaders\n        # Consider adding persistent_workers=True if num_workers > 0 for efficiency\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=min(2, os.cpu_count()), pin_memory=True, drop_last=True) # drop_last can help with batch norm stability\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=min(2, os.cpu_count()), pin_memory=True)\n        print(f\"DataLoaders created. Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n\n    except (ValueError, KeyError, TypeError, RuntimeError, Exception) as e: # Catch specific and general errors\n        print(f\"FATAL ERROR creating Datasets or DataLoaders: {e}\")\n        # Potentially print more context here if needed\n        raise # Re-raise to stop execution\n\n    # Free memory again - PIE database might be large\n    del pie_database\n    gc.collect()\n\n    # --- Initialize Model ---\n    print(\"\\nInitializing model...\")\n    model = None # Initialize before try block\n    try:\n        current_input_sizes = {}\n        for stream in ACTIVE_STREAMS: # Based on ACTIVE_STREAMS for this run\n            size_constant_name = f'INPUT_SIZE_{stream.upper()}'\n            special_cases = {'TRAFFIC_LIGHT': 'TL_STATE','STATIC_CONTEXT': 'STATIC','EGO_SPEED': 'EGO_SPEED','EGO_ACC': 'EGO_ACC','EGO_GYRO': 'EGO_GYRO','PED_ACTION': 'PED_ACTION','PED_LOOK': 'PED_LOOK','PED_OCCLUSION': 'PED_OCC', 'YOLOP':'YOLOP'}\n            stream_upper_key = stream.upper(); suffix = special_cases.get(stream_upper_key)\n            if suffix: size_constant_name = f'INPUT_SIZE_{suffix}'\n            elif stream == 'bbox': size_constant_name = 'INPUT_SIZE_BBOX'\n            elif stream == 'pose': size_constant_name = 'INPUT_SIZE_POSE'\n            # Add other stream-specific mappings if necessary\n\n            if size_constant_name in globals():\n                 input_size = globals()[size_constant_name]\n                 if not isinstance(input_size, int) or input_size <= 0:\n                      raise ValueError(f\"Invalid input size ({input_size}) retrieved for constant {size_constant_name} (stream {stream}).\")\n                 current_input_sizes[stream] = input_size\n            else:\n                 raise ValueError(f\"Input size constant '{size_constant_name}' not found in globals for active stream '{stream}'\")\n\n        # Ensure all active streams have a size\n        if len(current_input_sizes) != len(ACTIVE_STREAMS):\n             missing_streams = set(ACTIVE_STREAMS) - set(current_input_sizes.keys())\n             raise ValueError(f\"Input sizes could not be determined for all active streams. Missing: {missing_streams}\")\n\n        model = MultiStreamAdaptiveLSTM(\n            input_sizes=current_input_sizes,\n            lstm_hidden_size=LSTM_HIDDEN_SIZE,\n            num_lstm_layers=NUM_LSTM_LAYERS,\n            num_classes=NUM_CLASSES,\n            attention_dim=ATTENTION_DIM,\n            dropout_rate=DROPOUT_RATE,\n            stream_names=ACTIVE_STREAMS # Pass only active streams the model should use\n        ).to(DEVICE)\n\n        print(\"\\n--- Model Architecture ---\"); print(model);\n        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad);\n        print(f\"Total Trainable Parameters: {num_params:,}\"); print(\"-\" * 30)\n\n    except (KeyError, ValueError, TypeError, RuntimeError, Exception) as e:\n         print(f\"FATAL ERROR initializing model: {e}\")\n         raise # Re-raise to stop execution\n\n    # --- Class Weighting & Optimizer ---\n    print(\"\\nCalculating Class Weights for Loss Function...\")\n    class_weights = torch.tensor([1.0, 1.0], dtype=torch.float32).to(DEVICE) # Default weights\n    try:\n        # Ensure train_dataset exists and has labels\n        if train_dataset and hasattr(train_dataset, 'labels') and len(train_dataset.labels) > 0:\n            balanced_train_labels_list = train_dataset.labels.cpu().numpy() # Get labels from balanced dataset instance\n            count_0 = np.count_nonzero(balanced_train_labels_list == 0);\n            count_1 = np.count_nonzero(balanced_train_labels_list == 1);\n            total = len(balanced_train_labels_list)\n\n            if total > 0 and count_0 > 0 and count_1 > 0: # Ensure counts are valid\n                weight_0 = total / (2.0 * count_0); weight_1 = total / (2.0 * count_1)\n                class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float32).to(DEVICE)\n                print(f\"Using Calculated Class Weights for Loss: 0={weight_0:.2f}, 1={weight_1:.2f}\")\n            elif total > 0:\n                 print(f\"Warning: Only one class present in balanced training labels (0: {count_0}, 1: {count_1}). Using default weights [1.0, 1.0].\")\n            else:\n                 print(\"Warning: Training dataset has no labels. Using default weights [1.0, 1.0].\")\n        else:\n            print(\"Warning: Could not access labels from training dataset. Using default weights [1.0, 1.0].\")\n\n    except Exception as e:\n         print(f\"Error calculating class weights: {e}. Using default weights [1.0, 1.0].\")\n\n\n    # Initialize criterion and optimizer only if model exists\n    criterion = None\n    optimizer = None\n    if model:\n         try:\n             criterion = nn.CrossEntropyLoss(weight=class_weights)\n             optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n         except Exception as e:\n              print(f\"FATAL ERROR creating criterion or optimizer: {e}\")\n              raise # Stop execution\n    else:\n         raise RuntimeError(\"Model was not initialized. Cannot create criterion/optimizer.\")\n\n\n    best_val_f1 = -1.0; train_losses, val_losses = [], []; train_accs, val_accs = [], []; val_f1s = []\n    best_epoch_path = \"\" # Initialize before loop\n\n\n    # --- Training Loop ---\n    print(\"\\n--- Starting Training ---\")\n    if not train_loader or not val_loader:\n         raise RuntimeError(\"DataLoaders are not available. Cannot start training.\")\n\n    for epoch in range(NUM_EPOCHS):\n        epoch_start_time = time.time()\n        try:\n            train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n            val_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n            epoch_duration = time.time() - epoch_start_time\n\n            # Store metrics\n            train_losses.append(train_loss); val_losses.append(val_metrics['loss'])\n            train_accs.append(train_acc); val_accs.append(val_metrics['accuracy'])\n            val_f1s.append(val_metrics['f1'])\n\n            print(f\"--- Epoch {epoch+1}/{NUM_EPOCHS} ({epoch_duration:.2f} sec) ---\")\n            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n            print(f\"  Val Loss:   {val_metrics.get('loss', float('nan')):.4f}, Val Acc:  {val_metrics.get('accuracy', float('nan')):.4f}\") # Use .get for safety\n            print(f\"  Val Prec:   {val_metrics.get('precision', float('nan')):.4f}, Recall: {val_metrics.get('recall', float('nan')):.4f}, F1: {val_metrics.get('f1', float('nan')):.4f}\")\n            print(f\"  Val AUC:    {val_metrics.get('auc', float('nan')):.4f}\") # AUC might be nan\n\n            # Save best model based on validation F1\n            current_f1 = val_metrics.get('f1', -1.0) # Default to -1 if F1 is missing/NaN\n            if not np.isnan(current_f1) and current_f1 > best_val_f1:\n                best_val_f1 = current_f1\n                # Ensure active streams string is filesystem-safe\n                safe_streams_str = \"_\".join(sorted(ACTIVE_STREAMS)).replace('/', '_').replace('\\\\', '_')\n                current_best_epoch_path = f'best_model_{safe_streams_str}_ep{epoch+1}.pth'\n                try:\n                    torch.save(model.state_dict(), current_best_epoch_path)\n                    # Clean up previous best model file if it exists? Optional.\n                    # if best_epoch_path and os.path.exists(best_epoch_path):\n                    #     os.remove(best_epoch_path)\n                    best_epoch_path = current_best_epoch_path # Store path to the new best model\n                    print(f\"  >> Saved new best model to {best_epoch_path} (F1: {best_val_f1:.4f})\")\n                except Exception as e:\n                     print(f\"  >> Error saving model checkpoint {current_best_epoch_path}: {e}\")\n            print(\"-\" * 30)\n\n        except KeyboardInterrupt:\n             print(\"\\nTraining interrupted by user.\")\n             break # Exit training loop\n        except Exception as e:\n             print(f\"\\n--- ERROR DURING EPOCH {epoch+1} ---\")\n             print(f\"Error: {e}\")\n             # Option: break the loop, or try to continue?\n             # For stability, breaking might be safer if the error is persistent (e.g., OOM)\n             print(\"Stopping training due to error.\")\n             import traceback\n             traceback.print_exc() # Print stack trace for debugging\n             break\n\n    print(\"--- Training Finished ---\")\n\n    # --- Plotting ---\n    print(\"\\n--- Plotting Training History ---\")\n    try:\n        # Ensure metrics lists are not empty before plotting\n        if train_losses and val_losses and train_accs and val_accs and val_f1s:\n            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n            epochs_range = range(1, len(train_losses) + 1) # Use actual number of completed epochs\n\n            axes[0].plot(epochs_range, train_losses, label='Train Loss')\n            axes[0].plot(epochs_range, val_losses, label='Val Loss')\n            axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].set_title('Loss Curve'); axes[0].legend(); axes[0].grid(True)\n\n            axes[1].plot(epochs_range, train_accs, label='Train Accuracy')\n            axes[1].plot(epochs_range, val_accs, label='Val Accuracy')\n            axes[1].plot(epochs_range, val_f1s, label='Val F1-Score', linestyle='--')\n            axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Metric'); axes[1].set_title('Accuracy & F1-Score Curve'); axes[1].legend(); axes[1].grid(True)\n\n            plt.tight_layout(); plt.show()\n        else:\n             print(\"Skipping plotting - insufficient metric data collected.\")\n    except Exception as e:\n         print(f\"Error generating plots: {e}\")\n\n\n    # --- Final Evaluation ---\n    print(\"\\n--- Final Evaluation on Validation Set using Best Model ---\")\n    final_metrics = {} # Initialize before try\n    true_labels, pred_labels = np.array([]), np.array([]) # Initialize before try\n    try:\n        # Check if a best model was saved and exists\n        if best_epoch_path and os.path.exists(best_epoch_path):\n            print(f\"Loading best saved model '{best_epoch_path}'\")\n            try:\n                # Load state dict onto the correct device\n                state_dict = torch.load(best_epoch_path, map_location=DEVICE)\n                model.load_state_dict(state_dict)\n                print(\" -> Best model loaded successfully.\")\n            except FileNotFoundError:\n                 print(f\"Warn: Best model file '{best_epoch_path}' not found despite check. Evaluating final model state.\")\n            except Exception as e:\n                 print(f\"Warn: Error loading best model state_dict from {best_epoch_path}: {e}. Evaluating final model state.\")\n        else:\n             print(f\"Warning: No best model saved or found at '{best_epoch_path}'. Evaluating final model state from training end.\")\n\n        # Perform final evaluation\n        if model and val_loader and criterion:\n             final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n             true_labels, pred_labels = get_predictions_and_labels(model, val_loader, DEVICE)\n        else:\n             print(\"Error: Model, validation loader, or criterion not available for final evaluation.\")\n             # Set final metrics to NaN/empty if evaluation cannot run\n             final_metrics = {'accuracy': float('nan'), 'precision': float('nan'), 'recall': float('nan'), 'f1': float('nan'), 'auc': float('nan'), 'loss': float('nan')}\n             true_labels, pred_labels = np.array([]), np.array([])\n\n\n        # Display results\n        print(\"\\n--- Final Performance Metrics ---\")\n        print(f\"  Streams:   {', '.join(ACTIVE_STREAMS)}\")\n        print(f\"  Accuracy:  {final_metrics.get('accuracy', float('nan')):.4f}\")\n        print(f\"  Precision: {final_metrics.get('precision', float('nan')):.4f}\")\n        print(f\"  Recall:    {final_metrics.get('recall', float('nan')):.4f}\")\n        print(f\"  F1 Score:  {final_metrics.get('f1', float('nan')):.4f}\")\n        print(f\"  AUC:       {final_metrics.get('auc', float('nan')):.4f}\")\n        print(f\"  Loss:      {final_metrics.get('loss', float('nan')):.4f}\")\n        if best_epoch_path:\n             print(f\"  (Best Validation F1 during training: {best_val_f1:.4f} - from model '{os.path.basename(best_epoch_path)}')\")\n        else:\n             print(f\"  (Best Validation F1 during training: {best_val_f1:.4f} - model file not saved/found)\")\n\n\n        # Plot Confusion Matrix\n        if true_labels.size > 0 and pred_labels.size > 0: # Ensure labels were generated\n             print(\"\\n--- Confusion Matrix ---\")\n             try:\n                 cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1]) # Explicitly define labels\n                 labels_display = ['Not Crossing', 'Crossing']\n                 disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_display);\n                 disp.plot(cmap=plt.cm.Blues);\n                 plt.title(f'CM ({\", \".join(ACTIVE_STREAMS)})');\n                 plt.show()\n             except ValueError as e:\n                  print(f\"Error generating confusion matrix: {e}\")\n                  print(f\"True labels unique: {np.unique(true_labels)}, Pred labels unique: {np.unique(pred_labels)}\")\n             except Exception as e:\n                  print(f\"Unexpected error generating confusion matrix display: {e}\")\n        else:\n             print(\"Skipping confusion matrix - no prediction data available.\")\n\n    except Exception as e:\n         print(f\"\\n--- ERROR DURING FINAL EVALUATION ---\")\n         print(f\"Error: {e}\")\n         import traceback\n         traceback.print_exc()\n\n\n    print(\"\\n--- Script Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T14:22:52.907680Z","iopub.execute_input":"2025-05-05T14:22:52.908013Z"}},"outputs":[{"name":"stdout","text":"All possible streams: ['bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro', 'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context', 'yolop']\n--- Running Experiment With Active Streams: ['bbox', 'ped_action', 'ped_look', 'ego_speed', 'ego_acc', 'yolop'] ---\nUsing device: cuda\n--- Skipping Data Preparation: Loading Pre-Saved Files ---\n\nLoading balanced training data from: /kaggle/working/balanced_train_data_with_yolop.pkl\nLoading scalers from: /kaggle/working/scalers.pkl\n -> Pre-processed data loaded successfully.\n\n======================================================================\n--- Running Model Training and Evaluation ---\nActive Streams for this run: ['bbox', 'ped_action', 'ped_look', 'ego_speed', 'ego_acc', 'yolop']\n----------------------------------------------------------------------\n\nLoading PIE database cache for Validation Dataset...\n -> PIE database loaded successfully for validation.\n\nCreating Datasets and DataLoaders...\n\nLoading pose data for sets: ['set05', 'set06'] from /kaggle/input/pose-data/extracted_poses2\nFinished loading pose data for 2 relevant sets.\n\nLoading YOLOP data for sets: ['set05', 'set06'] from /kaggle/input/yolop-data/yolop features\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading YOLOP PKLs:   0%|          | 0/53 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98c64082c5d94a978307e840a6d3f2c2"}},"metadata":{}},{"name":"stdout","text":"Finished loading YOLOP data. Found 11 relevant files out of 11 possible for sets ['set05', 'set06'].\nData successfully loaded for 2 sets.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating Sequences for ['set05', 'set06']:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa6aca5beafe4f67b6a29ef48344f265"}},"metadata":{}},{"name":"stdout","text":"Dataset initialized with 77288 sequences from 243 pedestrians for sets ['set05', 'set06'].\nDataLoaders created. Train batches: 3435, Val batches: 2416\n\nInitializing model...\nInitializing model with streams: ['bbox', 'ped_action', 'ped_look', 'ego_speed', 'ego_acc', 'yolop']\n  - Adding stream 'bbox' with input size 4\n  - Adding stream 'ped_action' with input size 1\n  - Adding stream 'ped_look' with input size 1\n  - Adding stream 'ego_speed' with input size 1\n  - Adding stream 'ego_acc' with input size 2\n  - Adding stream 'yolop' with input size 20\n  Combined feature dimension: 3072\n\n--- Model Architecture ---\nMultiStreamAdaptiveLSTM(\n  (lstms): ModuleDict(\n    (bbox): LSTM(4, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ped_action): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ped_look): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ego_speed): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ego_acc): LSTM(2, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (yolop): LSTM(20, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n  )\n  (attentions): ModuleDict(\n    (bbox): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ped_action): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ped_look): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ego_speed): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ego_acc): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (yolop): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc1): Linear(in_features=3072, out_features=1536, bias=True)\n  (relu): ReLU()\n  (fc2): Linear(in_features=1536, out_features=2, bias=True)\n)\nTotal Trainable Parameters: 17,809,416\n------------------------------\n\nCalculating Class Weights for Loss Function...\nUsing Calculated Class Weights for Loss: 0=1.00, 1=1.00\n\n--- Starting Training ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3435 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 1/15 (358.55 sec) ---\n  Train Loss: 0.2978, Train Acc: 0.8710\n  Val Loss:   0.1758, Val Acc:  0.9114\n  Val Prec:   0.6534, Recall: 0.8952, F1: 0.7554\n  Val AUC:    0.9706\n  >> Saved new best model to best_model_bbox_ego_acc_ego_speed_ped_action_ped_look_yolop_ep1.pth (F1: 0.7554)\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3435 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 2/15 (361.98 sec) ---\n  Train Loss: 0.2437, Train Acc: 0.8957\n  Val Loss:   0.1992, Val Acc:  0.9002\n  Val Prec:   0.6207, Recall: 0.8919, F1: 0.7320\n  Val AUC:    0.9655\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3435 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68231a140cd94c7fb4e74ae9bbde6f19"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}