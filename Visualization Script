{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11201333,"sourceType":"datasetVersion","datasetId":6993690},{"sourceId":11201362,"sourceType":"datasetVersion","datasetId":6993708},{"sourceId":11201388,"sourceType":"datasetVersion","datasetId":6993722},{"sourceId":11201422,"sourceType":"datasetVersion","datasetId":6993740},{"sourceId":11201506,"sourceType":"datasetVersion","datasetId":6993794},{"sourceId":11201543,"sourceType":"datasetVersion","datasetId":6993809},{"sourceId":11382982,"sourceType":"datasetVersion","datasetId":7127490},{"sourceId":11402679,"sourceType":"datasetVersion","datasetId":7142036},{"sourceId":302300,"sourceType":"modelInstanceVersion","modelInstanceId":258142,"modelId":279383},{"sourceId":307831,"sourceType":"modelInstanceVersion","modelInstanceId":262207,"modelId":283333},{"sourceId":316944,"sourceType":"modelInstanceVersion","modelInstanceId":267476,"modelId":288527},{"sourceId":329886,"sourceType":"modelInstanceVersion","modelInstanceId":276781,"modelId":297682},{"sourceId":329908,"sourceType":"modelInstanceVersion","modelInstanceId":276800,"modelId":297702}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/aras62/PIE.git\n!unzip /content/PIE/annotations/annotations.zip -d /content/PIE\n!unzip /content/PIE/annotations/annotations_vehicle.zip -d /content/PIE\n# !git clone https://github.com/hustvl/YOLOP.git\n!mkdir /kaggle/working/PIE/content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:27:07.771729Z","iopub.execute_input":"2025-04-19T13:27:07.772029Z","iopub.status.idle":"2025-04-19T13:27:08.263278Z","shell.execute_reply.started":"2025-04-19T13:27:07.771999Z","shell.execute_reply":"2025-04-19T13:27:08.262097Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'PIE' already exists and is not an empty directory.\nunzip:  cannot find or open /content/PIE/annotations/annotations.zip, /content/PIE/annotations/annotations.zip.zip or /content/PIE/annotations/annotations.zip.ZIP.\nunzip:  cannot find or open /content/PIE/annotations/annotations_vehicle.zip, /content/PIE/annotations/annotations_vehicle.zip.zip or /content/PIE/annotations/annotations_vehicle.zip.ZIP.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q ultralytics opencv-python-headless ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T00:19:41.214375Z","iopub.execute_input":"2025-04-23T00:19:41.214596Z","iopub.status.idle":"2025-04-23T00:19:47.453977Z","shell.execute_reply.started":"2025-04-23T00:19:41.214574Z","shell.execute_reply":"2025-04-23T00:19:47.452881Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.5/983.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# import os\n# import pickle\n# import sys\n# import numpy as np\n# from tqdm.notebook import tqdm\n# import random\n\n# print(\"--- PIE Database Cache Verification Script ---\")\n\n# # --- Configuration ---\n# # <<< --- SET THIS TO THE LOCATION OF YOUR GENERATED CACHE --- >>>\n# PKL_FILE_PATH = \"/kaggle/working/PIE/data_cache/pie_database.pkl\"\n# # <<< --- END CONFIGURATION --- >>>\n\n# # Define expected top-level keys (these are the set IDs)\n# EXPECTED_SETS = {'set01', 'set02', 'set03', 'set04', 'set05', 'set06'}\n\n# # Define expected keys within each video dictionary\n# EXPECTED_VIDEO_KEYS = {'num_frames', 'width', 'height',\n#                        'ped_annotations', 'traffic_annotations', 'vehicle_annotations'}\n\n# # Define expected keys within each pedestrian annotation dictionary\n# EXPECTED_PED_KEYS = {'frames', 'bbox', 'occlusion', 'behavior', 'attributes'}\n# EXPECTED_PED_BEHAVIOR_KEYS = {'gesture', 'look', 'action', 'cross'}\n# # Attributes can vary, so we won't check all exhaustively here, but check presence\n\n# # Define expected keys within vehicle (ego) frame dictionary (sample)\n# EXPECTED_EGO_FRAME_KEYS = {'OBD_speed', 'GPS_speed', 'accX', 'accY', 'accZ',\n#                            'gyroX', 'gyroY', 'gyroZ', 'heading_angle', 'latitude',\n#                            'longitude', 'pitch', 'roll', 'yaw'}\n\n\n# # --- Verification Parameters ---\n# MAX_FRAMES_TO_CHECK_PER_VIDEO = 50 # Limit checks per video for speed\n# MAX_PEDS_TO_CHECK_PER_VIDEO = 20  # Limit checks per video for speed\n# MAX_TRAFFIC_OBJS_TO_CHECK_PER_VIDEO = 20 # Limit checks\n# PRINT_SAMPLE_COUNT = 3 # How many sample data lines to print\n\n# # --- Counters and Flags ---\n# errors_found = 0\n# warnings_found = 0\n# checked_sets = 0\n# checked_videos = 0\n# checked_peds = 0\n# checked_ped_frames = 0\n# checked_ego_frames = 0\n\n# # --- Helper Function for Reporting ---\n# def report_error(message):\n#     global errors_found\n#     print(f\"  ERROR: {message}\")\n#     errors_found += 1\n\n# def report_warning(message):\n#     global warnings_found\n#     print(f\"  Warning: {message}\")\n#     warnings_found += 1\n\n# # --- 1. Load the PKL File ---\n# print(f\"\\n[1] Loading PKL file: {PKL_FILE_PATH}\")\n# if not os.path.exists(PKL_FILE_PATH):\n#     print(f\"  ERROR: PKL file not found at the specified path.\")\n#     exit()\n\n# try:\n#     with open(PKL_FILE_PATH, 'rb') as f:\n#         pie_database = pickle.load(f)\n#     print(\"  -> PKL file loaded successfully.\")\n# except Exception as e:\n#     print(f\"  ERROR: Failed to load PKL file: {e}\")\n#     exit()\n\n# # --- 2. Basic Structure Checks ---\n# print(f\"\\n[2] Checking Top-Level Structure...\")\n# if not isinstance(pie_database, dict):\n#     report_error(f\"Loaded data is not a dictionary (Type: {type(pie_database)}).\")\n#     exit()\n# print(f\"  -> Top level is a dictionary: OK\")\n\n# found_sets = set(pie_database.keys())\n# if found_sets != EXPECTED_SETS:\n#     report_warning(f\"Set keys mismatch. Found: {found_sets}, Expected: {EXPECTED_SETS}\")\n# else:\n#     print(f\"  -> Found expected set keys: OK\")\n# checked_sets = len(found_sets)\n\n# # --- 3. Detailed Content Checks ---\n# print(f\"\\n[3] Checking Set/Video/Annotation Structures...\")\n\n# sample_data_to_print = []\n\n# for set_id in tqdm(found_sets, desc=\"Checking Sets\"):\n#     if not isinstance(pie_database[set_id], dict):\n#         report_error(f\"Data for set '{set_id}' is not a dictionary.\")\n#         continue\n\n#     video_ids = list(pie_database[set_id].keys())\n#     checked_videos += len(video_ids)\n\n#     for video_id in tqdm(video_ids, desc=f\"Videos in {set_id}\", leave=False):\n#         video_data = pie_database[set_id][video_id]\n#         if not isinstance(video_data, dict):\n#             report_error(f\"Data for video '{set_id}/{video_id}' is not a dictionary.\")\n#             continue\n\n#         # Check video-level keys\n#         missing_vid_keys = EXPECTED_VIDEO_KEYS - set(video_data.keys())\n#         if missing_vid_keys:\n#             report_warning(f\"Video '{set_id}/{video_id}' missing keys: {missing_vid_keys}\")\n\n#         # Basic type checks for video keys\n#         img_width = video_data.get('width', -1)\n#         img_height = video_data.get('height', -1)\n#         if not isinstance(video_data.get('num_frames'), int): report_warning(f\"num_frames type mismatch in {set_id}/{video_id}\")\n#         if not isinstance(img_width, int): report_warning(f\"width type mismatch in {set_id}/{video_id}\")\n#         if not isinstance(img_height, int): report_warning(f\"height type mismatch in {set_id}/{video_id}\")\n#         if not isinstance(video_data.get('ped_annotations'), dict): report_error(f\"ped_annotations not a dict in {set_id}/{video_id}\"); continue # Stop checking peds for this video if structure wrong\n#         if not isinstance(video_data.get('traffic_annotations'), dict): report_warning(f\"traffic_annotations not a dict in {set_id}/{video_id}\")\n#         if not isinstance(video_data.get('vehicle_annotations'), dict): report_warning(f\"vehicle_annotations not a dict in {set_id}/{video_id}\")\n\n\n#         # --- Check Pedestrian Annotations ---\n#         ped_annotations = video_data.get('ped_annotations', {})\n#         ped_ids_to_check = list(ped_annotations.keys())\n#         random.shuffle(ped_ids_to_check) # Check a random subset\n\n#         for i, ped_id in enumerate(ped_ids_to_check):\n#             if i >= MAX_PEDS_TO_CHECK_PER_VIDEO: break # Limit checks\n#             checked_peds += 1\n#             ped_data = ped_annotations[ped_id]\n#             if not isinstance(ped_data, dict): report_error(f\"Data for ped '{ped_id}' in {set_id}/{video_id} is not a dict.\"); continue\n\n#             missing_ped_keys = EXPECTED_PED_KEYS - set(ped_data.keys())\n#             if missing_ped_keys: report_warning(f\"Ped '{ped_id}' in {set_id}/{video_id} missing keys: {missing_ped_keys}\")\n\n#             # Check structure of essential lists/dicts\n#             frames = ped_data.get('frames', [])\n#             bboxes = ped_data.get('bbox', [])\n#             occlusions = ped_data.get('occlusion', [])\n#             behavior = ped_data.get('behavior', {})\n#             attributes = ped_data.get('attributes', {})\n\n#             if not isinstance(frames, list): report_error(f\"Ped '{ped_id}' frames not a list.\"); continue\n#             if not isinstance(bboxes, list): report_error(f\"Ped '{ped_id}' bbox not a list.\"); continue\n#             if not isinstance(occlusions, list): report_error(f\"Ped '{ped_id}' occlusion not a list.\"); continue\n#             if not isinstance(behavior, dict): report_error(f\"Ped '{ped_id}' behavior not a dict.\"); continue\n#             if not isinstance(attributes, dict): report_warning(f\"Ped '{ped_id}' attributes not a dict.\"); continue # Attributes might be empty\n\n#             # Check list lengths consistency\n#             n_frames = len(frames)\n#             if n_frames == 0 and (len(bboxes) > 0 or len(occlusions) > 0): report_warning(f\"Ped '{ped_id}' has bboxes/occlusions but 0 frames listed.\")\n#             if len(bboxes) != n_frames: report_error(f\"Ped '{ped_id}' bbox length ({len(bboxes)}) != frames length ({n_frames}).\")\n#             if len(occlusions) != n_frames: report_error(f\"Ped '{ped_id}' occlusion length ({len(occlusions)}) != frames length ({n_frames}).\")\n\n#             missing_beh_keys = EXPECTED_PED_BEHAVIOR_KEYS - set(behavior.keys())\n#             if missing_beh_keys: report_warning(f\"Ped '{ped_id}' behavior missing keys: {missing_beh_keys}\")\n\n#             for beh_key, beh_list in behavior.items():\n#                 if not isinstance(beh_list, list): report_error(f\"Ped '{ped_id}' behavior '{beh_key}' not a list.\"); continue\n#                 if len(beh_list) != n_frames: report_error(f\"Ped '{ped_id}' behavior '{beh_key}' length ({len(beh_list)}) != frames length ({n_frames}).\")\n\n#             # Check sample frame content\n#             frames_to_check_in_ped = list(range(n_frames))\n#             random.shuffle(frames_to_check_in_ped)\n#             for k, frame_idx in enumerate(frames_to_check_in_ped):\n#                  if k >= MAX_FRAMES_TO_CHECK_PER_VIDEO: break\n#                  checked_ped_frames +=1\n#                  # Check frame number type\n#                  if not isinstance(frames[frame_idx], int): report_warning(f\"Ped '{ped_id}' frame value at index {frame_idx} not int.\")\n#                  # Check bbox format and range\n#                  if len(bboxes) > frame_idx:\n#                       bbox = bboxes[frame_idx]\n#                       if not isinstance(bbox, list) or len(bbox) != 4: report_error(f\"Ped '{ped_id}' frame {frames[frame_idx]} bbox invalid format: {bbox}\"); continue\n#                       try:\n#                            x1,y1,x2,y2 = map(float, bbox)\n#                            if img_width>0 and img_height>0 and not (0 <= x1 < x2 <= img_width and 0 <= y1 < y2 <= img_height): report_warning(f\"Ped '{ped_id}' frame {frames[frame_idx]} bbox out of bounds: {[int(x) for x in bbox]} vs {img_width}x{img_height}\")\n#                       except (ValueError, TypeError): report_error(f\"Ped '{ped_id}' frame {frames[frame_idx]} bbox contains non-numeric values: {bbox}\")\n#                  # Check occlusion value\n#                  if len(occlusions) > frame_idx:\n#                       occ = occlusions[frame_idx]\n#                       if not isinstance(occ, int) or occ not in [0, 1, 2]: report_warning(f\"Ped '{ped_id}' frame {frames[frame_idx]} invalid occlusion value: {occ}\")\n\n#                  # Add sample for printing later\n#                  if len(sample_data_to_print) < PRINT_SAMPLE_COUNT:\n#                       sample_data_to_print.append(f\" Sample Ped Data: Set={set_id}, Vid={video_id}, Ped={ped_id}, Frame={frames[frame_idx]}, BBox={bboxes[frame_idx] if len(bboxes) > frame_idx else 'N/A'}, Occ={occlusions[frame_idx] if len(occlusions) > frame_idx else 'N/A'}\")\n\n#         # --- Check Vehicle Annotations (Ego Data) ---\n#         vehicle_annotations = video_data.get('vehicle_annotations', {})\n#         ego_frames_to_check = list(vehicle_annotations.keys())\n#         random.shuffle(ego_frames_to_check)\n\n#         for k, frame_num in enumerate(ego_frames_to_check):\n#             if k >= MAX_FRAMES_TO_CHECK_PER_VIDEO: break # Limit checks\n#             checked_ego_frames += 1\n#             if not isinstance(frame_num, int): report_warning(f\"Ego data frame key '{frame_num}' in {set_id}/{video_id} is not int.\") ; continue\n#             ego_frame_data = vehicle_annotations[frame_num]\n#             if not isinstance(ego_frame_data, dict): report_error(f\"Ego data for frame {frame_num} in {set_id}/{video_id} is not dict.\"); continue\n\n#             missing_ego_keys = EXPECTED_EGO_FRAME_KEYS - set(ego_frame_data.keys())\n#             # Don't warn about every missing key, just check a few critical ones\n#             if 'OBD_speed' not in ego_frame_data and 'GPS_speed' not in ego_frame_data: report_warning(f\"Ego frame {frame_num} in {set_id}/{video_id} missing speed data.\")\n#             for key in EXPECTED_EGO_FRAME_KEYS:\n#                 if key in ego_frame_data and not isinstance(ego_frame_data[key], (float, int)): report_warning(f\"Ego frame {frame_num} key '{key}' value is not float/int (type: {type(ego_frame_data[key])})\")\n\n#             # Add sample for printing later\n#             if len(sample_data_to_print) < PRINT_SAMPLE_COUNT * 2 and k < 5: # Print a few ego samples too\n#                   sample_data_to_print.append(f\" Sample Ego Data: Set={set_id}, Vid={video_id}, Frame={frame_num}, Speed={ego_frame_data.get('OBD_speed', 'N/A'):.2f}, AccX={ego_frame_data.get('accX', 'N/A'):.2f}\")\n\n# # --- 4. Print Summary ---\n# print(f\"\\n[4] Verification Summary ---\")\n# print(f\"  - Checked {checked_sets} sets.\")\n# print(f\"  - Checked {checked_videos} videos.\")\n# print(f\"  - Checked {checked_peds} pedestrian tracks (sampled max {MAX_PEDS_TO_CHECK_PER_VIDEO} per video).\")\n# print(f\"  - Checked {checked_ped_frames} pedestrian frame entries (sampled max {MAX_FRAMES_TO_CHECK_PER_VIDEO} per ped).\")\n# print(f\"  - Checked {checked_ego_frames} ego data frame entries (sampled max {MAX_FRAMES_TO_CHECK_PER_VIDEO} per video).\")\n# print(f\"  - Total Errors Found: {errors_found}\")\n# print(f\"  - Total Warnings Found: {warnings_found}\")\n\n# if errors_found == 0:\n#     print(\"\\n  >>> Structure and basic content checks PASSED (with potential warnings). <<<\")\n# else:\n#     print(\"\\n  >>> ERRORS FOUND during structural/content checks. Review messages above. <<<\")\n\n# # --- 5. Print Sample Data ---\n# if sample_data_to_print:\n#     print(\"\\n[5] Sample Data Points ---\")\n#     for line in sample_data_to_print:\n#         print(line)\n\n# print(\"\\n--- Verification Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:10.145506Z","iopub.execute_input":"2025-04-15T19:55:10.145861Z","iopub.status.idle":"2025-04-15T19:55:10.152835Z","shell.execute_reply.started":"2025-04-15T19:55:10.145834Z","shell.execute_reply":"2025-04-15T19:55:10.151785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport xml.etree.ElementTree as ET\nimport os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\nfrom tqdm.notebook import tqdm\nimport random\nimport math\nimport zipfile\nimport cv2\nfrom ultralytics import YOLO\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:10.154360Z","iopub.execute_input":"2025-04-15T19:55:10.154719Z","iopub.status.idle":"2025-04-15T19:55:16.590012Z","shell.execute_reply.started":"2025-04-15T19:55:10.154687Z","shell.execute_reply":"2025-04-15T19:55:16.589278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + 'annotations'):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.590843Z","iopub.execute_input":"2025-04-15T19:55:16.591210Z","iopub.status.idle":"2025-04-15T19:55:16.596284Z","shell.execute_reply.started":"2025-04-15T19:55:16.591187Z","shell.execute_reply":"2025-04-15T19:55:16.595261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations_vehicle.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + 'annotations_vehicle'):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.597390Z","iopub.execute_input":"2025-04-15T19:55:16.597839Z","iopub.status.idle":"2025-04-15T19:55:16.618074Z","shell.execute_reply.started":"2025-04-15T19:55:16.597803Z","shell.execute_reply":"2025-04-15T19:55:16.617129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations_attributes.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + \"annotations_attributes\"):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.619029Z","iopub.execute_input":"2025-04-15T19:55:16.619360Z","iopub.status.idle":"2025-04-15T19:55:16.635884Z","shell.execute_reply.started":"2025-04-15T19:55:16.619328Z","shell.execute_reply":"2025-04-15T19:55:16.634975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- IMPORTS ---\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import Dataset, DataLoader\n# import xml.etree.ElementTree as ET\n# import os\n# import numpy as np\n# from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n# from sklearn.preprocessing import StandardScaler # For standardizing ego features\n# from tqdm.notebook import tqdm\n# import random\n# import math\n# import zipfile\n# import cv2\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# import pickle\n# import time\n# import sys # For path manipulation if needed\n\n# # --- Add PIE utilities path if necessary (adjust path) ---\n# pie_utilities_path = '/kaggle/working/PIE/utilities'\n# if pie_utilities_path not in sys.path:\n#     sys.path.insert(0, pie_utilities_path)\n# try:\n#     # We only need PIE class to generate the database if needed\n#     from pie_data import PIE\n# except ImportError as e:\n#     print(f\"Warning: Could not import PIE class from {pie_utilities_path}. Database must already exist. Error: {e}\")\n#     PIE = None # Define PIE as None if import fails\n\n# # --- Configuration ---\n# PIE_ROOT_PATH = '/kaggle/working/PIE' # Path where PIE repo was cloned/unzipped\n# VIDEO_INPUT_DIR = '/kaggle/input' # Where pie-setXX video dataset folders are\n# POSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2' # Where setXX subdirs with PKLs are\n# PIE_DATABASE_CACHE_PATH = '/kaggle/input/pie-database/pie_database.pkl'\n\n# # --- Stream Control ---\n# ACTIVE_STREAMS = [\n#     'bbox',\n#     # 'pose',\n#     # 'ego_speed',\n#     # 'ego_acc',\n#     # 'ego_gyro',\n#     # 'ped_action',\n#     # 'ped_look',\n#     # 'ped_occlusion',\n#     # 'traffic_light',\n#     # 'static_context'\n# ]\n# print(f\"Active Streams: {ACTIVE_STREAMS}\")\n\n# # Model Hyperparameters\n# SEQ_LEN = 30\n# PRED_LEN = 1\n# # Input Sizes (Define for ALL potential streams)\n# INPUT_SIZE_BBOX = 4\n# INPUT_SIZE_POSE = 34\n# INPUT_SIZE_EGO_SPEED = 1\n# INPUT_SIZE_EGO_ACC = 2 # X, Y components\n# INPUT_SIZE_EGO_GYRO = 1 # Z component (Yaw rate)\n# INPUT_SIZE_PED_ACTION = 1 # 0:standing, 1:walking\n# INPUT_SIZE_PED_LOOK = 1 # 0:not-looking, 1:looking\n# INPUT_SIZE_PED_OCC = 1 # 0:none, 0.5:part, 1:full (normalized)\n# INPUT_SIZE_TL_STATE = 4 # 0:Undef, 1:Red, 2:Yellow, 3:Green (One-Hot)\n# # Static Feature Sizes (Matches pie_data.py mappings)\n# NUM_SIGNALIZED_CATS = 4\n# NUM_INTERSECTION_CATS = 5\n# NUM_AGE_CATS = 4\n# NUM_GENDER_CATS = 3\n# INPUT_SIZE_STATIC = NUM_SIGNALIZED_CATS + NUM_INTERSECTION_CATS + NUM_AGE_CATS + NUM_GENDER_CATS # = 16\n\n# LSTM_HIDDEN_SIZE = 256\n# NUM_LSTM_LAYERS = 2\n# DROPOUT_RATE = 0.3\n# NUM_CLASSES = 2\n# ATTENTION_DIM = 128\n\n# # Training Hyperparameters\n# LEARNING_RATE = 1e-4 # Possibly lower LR needed after balancing/more features\n# BATCH_SIZE = 32\n# NUM_EPOCHS = 10 # Increase epochs slightly for balanced data\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {DEVICE}\")\n\n# # Dataset Splits (Using PIE default)\n# TRAIN_SETS_STR = ['set01', 'set02', 'set04']\n# VAL_SETS_STR = ['set05', 'set06']\n# TEST_SETS_STR = ['set03'] # Although not used in training loop\n\n# # Mappings (Matches pie_data.py)\n# TL_STATE_MAP = {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}\n# NUM_TL_STATES = len(TL_STATE_MAP)\n# SIGNALIZED_MAP = {'n/a': 0, 'C': 1, 'S': 2, 'CS': 3}\n# INTERSECTION_MAP = {'midblock': 0, 'T': 1, 'T-left': 2, 'T-right': 3, 'four-way': 4}\n# AGE_MAP = {'child': 0, 'young': 1, 'adult': 2, 'senior': 3}\n# GENDER_MAP = {'n/a': 0, 'female': 1, 'male': 2}\n\n# # --- Helper: One-Hot Encoding ---\n# def to_one_hot(index, num_classes):\n#     vec = np.zeros(num_classes, dtype=np.float32)\n#     if 0 <= index < num_classes:\n#         vec[index] = 1.0\n#     else: # Handle unexpected index\n#         vec[0] = 1.0 # Default to first class\n#     return vec\n\n# # --- Balancing Function ---\n# def balance_samples_count(seq_data, label_type, random_seed=42):\n#     print('---------------------------------------------------------')\n#     print(f\"Balancing samples based on '{label_type}' key\")\n#     if label_type not in seq_data:\n#         raise KeyError(f\"Label type '{label_type}' not found.\")\n#     try:\n#         gt_labels = [lbl[0] for lbl in seq_data[label_type]]\n#     except (IndexError, TypeError):\n#         raise ValueError(f\"Labels under '{label_type}' not in expected format [[label_val]].\")\n\n#     if not all(l in [0, 1] for l in gt_labels):\n#         print(f\"Warning: Labels for balancing contain values other than 0 or 1.\")\n\n#     num_pos_samples = np.count_nonzero(np.array(gt_labels))\n#     num_neg_samples = len(gt_labels) - num_pos_samples\n#     new_seq_data = {}\n\n#     if num_neg_samples == num_pos_samples:\n#         print('Samples already balanced.')\n#         return seq_data.copy()\n#     else:\n#         print(f'Unbalanced: Positive (1): {num_pos_samples} | Negative (0): {num_neg_samples}')\n#         majority_label = 0 if num_neg_samples > num_pos_samples else 1\n#         minority_count = min(num_neg_samples, num_pos_samples)\n#         print(f\"Undersampling majority class ({majority_label}) to match count ({minority_count}).\")\n\n#         majority_indices = np.where(np.array(gt_labels) == majority_label)[0]\n#         minority_indices = np.where(np.array(gt_labels) != majority_label)[0]\n#         np.random.seed(random_seed)\n#         keep_majority_indices = np.random.choice(majority_indices, size=minority_count, replace=False)\n#         final_indices = np.concatenate((minority_indices, keep_majority_indices))\n#         np.random.shuffle(final_indices)\n\n#         for k, v_list in seq_data.items():\n#             if isinstance(v_list, list) and len(v_list) == len(gt_labels):\n#                  try:\n#                      # Check if the list contains numpy arrays before converting the whole list\n#                      if v_list and isinstance(v_list[0], np.ndarray):\n#                          v_array = np.array(v_list)\n#                          new_seq_data[k] = list(v_array[final_indices])\n#                      else: # Assume list of lists or list of scalars\n#                           new_seq_data[k] = [v_list[i] for i in final_indices]\n#                  except Exception as e:\n#                       print(f\"Error processing key '{k}' during balancing: {e}. Skip.\")\n#                       new_seq_data[k] = []\n#             else:\n#                  print(f\"Warn: Skipping key '{k}' in balancing (not list or len mismatch).\")\n#                  new_seq_data[k] = v_list\n\n#         # Check if label key still exists after potential errors\n#         if label_type in new_seq_data:\n#              new_gt_labels = [lbl[0] for lbl in new_seq_data[label_type]]\n#              final_pos = np.count_nonzero(np.array(new_gt_labels))\n#              final_neg = len(new_gt_labels) - final_pos\n#              print(f'Balanced:   Positive (1): {final_pos} | Negative (0): {final_neg}')\n#         else:\n#              print(\"Error: Label key was lost during balancing process.\")\n\n#         print('---------------------------------------------------------')\n#         return new_seq_data\n\n\n# # --- Dataset Class ---\n# class PIEDataset(Dataset):\n#     def __init__(self, pie_database, set_names, pose_data_dir, seq_len, pred_len, scalers=None, active_streams=None):\n#         self.pie_db = pie_database\n#         self.set_names = set_names\n#         self.pose_data_dir = pose_data_dir\n#         self.seq_len = seq_len\n#         self.pred_len = pred_len\n#         self.scalers = scalers or {}\n#         self.active_streams = active_streams or []\n#         self.sequences = []\n#         self.all_pose_data = {}\n#         # Store input sizes needed for error handling in __getitem__\n#         self._input_sizes_for_error = self._get_input_sizes_dict()\n#         self._load_pose_data()\n#         self._generate_sequence_list()\n#         if not self.sequences:\n#             raise ValueError(f\"Dataset init failed: No sequences for {self.set_names}\")\n\n#     def _get_input_sizes_dict(self):\n#         # Helper to create input sizes dict, needed for error fallback in __getitem__\n#         input_sizes = {}\n#         for stream in self.active_streams:\n#             size_constant_name = f'INPUT_SIZE_{stream.upper()}'\n#             special_cases = {'TRAFFIC_LIGHT': 'TL_STATE', 'STATIC_CONTEXT': 'STATIC', 'EGO_SPEED': 'EGO_SPEED', 'EGO_ACC': 'EGO_ACC', 'EGO_GYRO': 'EGO_GYRO', 'PED_ACTION': 'PED_ACTION', 'PED_LOOK': 'PED_LOOK', 'PED_OCCLUSION': 'PED_OCC'}\n#             stream_upper_key = stream.upper()\n#             suffix = special_cases.get(stream_upper_key)\n#             if suffix:\n#                  size_constant_name = f'INPUT_SIZE_{suffix}'\n#             elif stream == 'bbox': size_constant_name = 'INPUT_SIZE_BBOX'\n#             elif stream == 'pose': size_constant_name = 'INPUT_SIZE_POSE'\n#             if size_constant_name in globals(): input_sizes[stream] = globals()[size_constant_name]\n#             else: input_sizes[stream] = 1 # Default size 1 if not found (should not happen ideally)\n#         return input_sizes\n\n#     def _load_pose_data(self):\n#         print(f\"\\nLoading pose data for sets: {self.set_names} from {self.pose_data_dir}\")\n#         sets_loaded_count = 0\n#         for set_id in tqdm(self.set_names, desc=\"Loading Pose Sets\"):\n#             self.all_pose_data[set_id] = {}\n#             pose_set_path = os.path.join(self.pose_data_dir, set_id)\n#             if not os.path.isdir(pose_set_path):\n#                 print(f\"Warn: Pose dir missing for {set_id} at {pose_set_path}\")\n#                 continue\n#             pkl_files_in_set = [f for f in os.listdir(pose_set_path) if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")]\n#             if not pkl_files_in_set:\n#                 continue\n#             loaded_video_count = 0\n#             for pkl_filename in tqdm(pkl_files_in_set, desc=f\"Loading PKLs for {set_id}\", leave=False):\n#                 pkl_file_path = os.path.join(pose_set_path, pkl_filename)\n#                 try:\n#                     with open(pkl_file_path, 'rb') as f:\n#                         loaded_pkl_content = pickle.load(f)\n#                     if len(loaded_pkl_content) != 1:\n#                         continue\n#                     unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n#                     video_id = \"_\".join(unique_video_key.split('_')[1:])\n#                     self.all_pose_data[set_id][video_id] = video_data\n#                     loaded_video_count += 1\n#                 except FileNotFoundError:\n#                     pass # Expected if some videos failed extraction\n#                 except Exception as e:\n#                     print(f\"Error loading pose PKL {pkl_file_path}: {e}\")\n#             if loaded_video_count > 0:\n#                 sets_loaded_count += 1\n#         print(f\"Finished loading pose data for {sets_loaded_count} sets.\")\n\n#     def _generate_sequence_list(self):\n#         print(f\"Generating sequence list from PIE database for sets: {self.set_names}\")\n#         sequence_count = 0\n#         ped_count = 0\n#         for set_id in tqdm(self.set_names, desc=\"Generating Sequences\"):\n#             if set_id not in self.pie_db:\n#                 continue\n#             for video_id, video_data in self.pie_db[set_id].items():\n#                 if 'ped_annotations' not in video_data:\n#                     continue\n#                 for ped_id, ped_data in video_data['ped_annotations'].items():\n#                     ped_count += 1\n#                     if 'frames' not in ped_data or len(ped_data['frames']) < self.seq_len + self.pred_len:\n#                         continue\n#                     sorted_frames = sorted(ped_data['frames'])\n#                     for i in range(len(sorted_frames) - self.seq_len - self.pred_len + 1):\n#                         start_frame = sorted_frames[i]\n#                         end_frame_observe = sorted_frames[i + self.seq_len - 1]\n#                         if end_frame_observe - start_frame != self.seq_len - 1:\n#                             continue\n#                         target_frame_actual_idx = i + self.seq_len + self.pred_len - 1\n#                         if target_frame_actual_idx >= len(sorted_frames):\n#                              continue # Avoid index out of bounds\n#                         target_frame = sorted_frames[target_frame_actual_idx]\n#                         if target_frame - end_frame_observe != self.pred_len:\n#                              continue\n#                         self.sequences.append((set_id, video_id, ped_id, start_frame))\n#                         sequence_count += 1\n#         print(f\"Found {sequence_count} valid sequences from {ped_count} pedestrian tracks.\")\n\n#     def __len__(self):\n#         return len(self.sequences)\n\n#     def __getitem__(self, idx):\n#         set_id, video_id, ped_id, start_frame = self.sequences[idx]\n#         frame_nums = list(range(start_frame, start_frame + self.seq_len))\n#         target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n\n#         # Get data from the loaded PIE database\n#         video_db = self.pie_db.get(set_id, {}).get(video_id, {})\n#         ped_db = video_db.get('ped_annotations', {}).get(ped_id, {})\n#         ego_db = video_db.get('vehicle_annotations', {}) # Frame -> {sensor: value}\n#         traffic_db = video_db.get('traffic_annotations', {}) # ObjID -> {frames:[], state:[], ...}\n#         ped_attributes = ped_db.get('attributes', {})\n\n#         # Initialize feature sequences\n#         feature_sequences = {stream: [] for stream in self.active_streams}\n\n#         # Static Features (calculated once)\n#         static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32) # Default static\n#         if 'static_context' in self.active_streams:\n#             sig_idx = ped_attributes.get('signalized', 0)\n#             int_idx = ped_attributes.get('intersection', 0)\n#             age_idx = ped_attributes.get('age', 2) # Default to 'adult'\n#             gen_idx = ped_attributes.get('gender', 0)\n#             static_vec = np.concatenate([\n#                 to_one_hot(sig_idx, NUM_SIGNALIZED_CATS),\n#                 to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n#                 to_one_hot(age_idx, NUM_AGE_CATS),\n#                 to_one_hot(gen_idx, NUM_GENDER_CATS)\n#             ])\n#             if static_vec.shape[0] != INPUT_SIZE_STATIC: # Sanity check\n#                  static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n\n#         # Get Target Label\n#         label = 0 # Default to not-crossing\n#         if 'frames' in ped_db and 'behavior' in ped_db and 'cross' in ped_db['behavior']:\n#              try:\n#                  target_frame_db_idx = ped_db['frames'].index(target_frame_num)\n#                  label = ped_db['behavior']['cross'][target_frame_db_idx]\n#                  if label == -1: label = 0 # Map irrelevant to not-crossing\n#              except (ValueError, IndexError):\n#                  pass # Keep default label\n\n#         # Iterate through sequence frames\n#         for frame_num in frame_nums:\n#             frame_db_idx = -1\n#             if 'frames' in ped_db:\n#                  try:\n#                      frame_db_idx = ped_db['frames'].index(frame_num)\n#                  except ValueError:\n#                      pass # Frame not found for this pedestrian in this sequence part\n#             ego_frame_data = ego_db.get(frame_num, {})\n\n#             # --- Extract for ACTIVE streams ---\n#             if 'bbox' in self.active_streams:\n#                 bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32) # Default\n#                 if frame_db_idx != -1 and 'bbox' in ped_db and len(ped_db['bbox']) > frame_db_idx:\n#                     # --- Start Corrected Try/Except ---\n#                      try:\n#                           x1, y1, x2, y2 = ped_db['bbox'][frame_db_idx]\n#                           img_w = video_db.get('width', 1920) # Use default if missing\n#                           img_h = video_db.get('height', 1080)\n#                           if img_w > 0 and img_h > 0: # Check for valid image dimensions\n#                                cx = ((x1 + x2) / 2) / img_w\n#                                cy = ((y1 + y2) / 2) / img_h\n#                                w = (x2 - x1) / img_w\n#                                h = (y2 - y1) / img_h\n#                                # Check for valid normalized bbox dimensions\n#                                if w > 0 and h > 0 and 0 <= cx <= 1 and 0 <= cy <= 1:\n#                                     bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n#                      except Exception as e:\n#                           # Keep default zero vector if any error occurs during processing\n#                           # print(f\"Warning: Error processing bbox F:{frame_num} P:{ped_id} V:{video_id} - {e}\") # Optional warning\n#                           pass\n#                     # --- End Corrected Try/Except ---\n#                 feature_sequences['bbox'].append(bbox_norm)\n\n#             # --- (Rest of stream extractions - unchanged logic but ensure proper indentation) ---\n#             if 'pose' in self.active_streams:\n#                 pose_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n#                 vid_pose_data = self.all_pose_data.get(set_id, {}).get(video_id, {})\n#                 frame_pose_data = vid_pose_data.get(frame_num, {})\n#                 loaded_pose = frame_pose_data.get(ped_id)\n#                 if loaded_pose is not None and isinstance(loaded_pose, np.ndarray) and loaded_pose.shape == (INPUT_SIZE_POSE,):\n#                     pose_vector = loaded_pose\n#                 feature_sequences['pose'].append(pose_vector)\n\n#             if 'ego_speed' in self.active_streams:\n#                 speed = ego_frame_data.get('OBD_speed', 0.0)\n#                 if speed == 0.0: speed = ego_frame_data.get('GPS_speed', 0.0)\n#                 speed_scaled = (speed - self.scalers.get('ego_speed_mean', 0.0)) / self.scalers.get('ego_speed_std', 1.0)\n#                 feature_sequences['ego_speed'].append([speed_scaled])\n\n#             if 'ego_acc' in self.active_streams:\n#                 accX = ego_frame_data.get('accX', 0.0)\n#                 accY = ego_frame_data.get('accY', 0.0)\n#                 accX_scaled = (accX - self.scalers.get('accX_mean', 0.0)) / self.scalers.get('accX_std', 1.0)\n#                 accY_scaled = (accY - self.scalers.get('accY_mean', 0.0)) / self.scalers.get('accY_std', 1.0)\n#                 feature_sequences['ego_acc'].append([accX_scaled, accY_scaled])\n\n#             if 'ego_gyro' in self.active_streams:\n#                 gyroZ = ego_frame_data.get('gyroZ', 0.0)\n#                 gyroZ_scaled = (gyroZ - self.scalers.get('gyroZ_mean', 0.0)) / self.scalers.get('gyroZ_std', 1.0)\n#                 feature_sequences['ego_gyro'].append([gyroZ_scaled])\n\n#             if 'ped_action' in self.active_streams:\n#                 action = 0 # Default standing\n#                 if frame_db_idx != -1 and 'behavior' in ped_db and 'action' in ped_db['behavior'] and len(ped_db['behavior']['action']) > frame_db_idx:\n#                      action = ped_db['behavior']['action'][frame_db_idx]\n#                 feature_sequences['ped_action'].append([float(action)])\n\n#             if 'ped_look' in self.active_streams:\n#                 look = 0 # Default not-looking\n#                 if frame_db_idx != -1 and 'behavior' in ped_db and 'look' in ped_db['behavior'] and len(ped_db['behavior']['look']) > frame_db_idx:\n#                      look = ped_db['behavior']['look'][frame_db_idx]\n#                 feature_sequences['ped_look'].append([float(look)])\n\n#             if 'ped_occlusion' in self.active_streams:\n#                 occ = 0.0 # Default none\n#                 if frame_db_idx != -1 and 'occlusion' in ped_db and len(ped_db['occlusion']) > frame_db_idx:\n#                      occ_val = ped_db['occlusion'][frame_db_idx]\n#                      occ = float(occ_val) / 2.0 # Normalize\n#                 feature_sequences['ped_occlusion'].append([occ])\n\n#             if 'traffic_light' in self.active_streams:\n#                 state_int = 0\n#                 for obj_id, obj_data in traffic_db.items():\n#                      if obj_data.get('obj_class') == 'traffic_light' and 'frames' in obj_data and 'state' in obj_data:\n#                           try:\n#                               tl_frame_idx = obj_data['frames'].index(frame_num)\n#                               state_val = obj_data['state'][tl_frame_idx]\n#                               if state_val != 0:\n#                                   state_int = state_val\n#                                   break # Found first non-undefined state\n#                           except (ValueError, IndexError):\n#                               continue\n#                 feature_sequences['traffic_light'].append(to_one_hot(state_int, NUM_TL_STATES))\n\n#             if 'static_context' in self.active_streams:\n#                 feature_sequences['static_context'].append(static_vec)\n\n#         # --- Convert lists to Tensors ---\n#         features = {}\n#         try:\n#             for stream_name in self.active_streams:\n#                  features[stream_name] = torch.tensor(np.array(feature_sequences[stream_name], dtype=np.float32), dtype=torch.float32)\n\n#         except Exception as e:\n#              print(f\"Error converting features idx {idx}: {e}. Return dummy.\")\n#              # Use the pre-calculated sizes dictionary for fallback\n#              features = {\n#                  name: torch.zeros((self.seq_len, self._input_sizes_for_error.get(name, 1)), dtype=torch.float32)\n#                  for name in self.active_streams\n#              }\n\n#         return features, torch.tensor(label, dtype=torch.long)\n\n# # --- Wrapper Dataset for Balanced Data ---\n# class BalancedDataset(Dataset):\n#     def __init__(self, data_dict, active_streams, label_key='label'):\n#         self.active_streams = active_streams\n#         self.label_key = label_key\n#         if self.label_key not in data_dict or not data_dict[self.label_key]:\n#              raise ValueError(f\"Label key '{self.label_key}' missing/empty.\")\n#         self.num_samples = len(data_dict[self.label_key])\n#         if self.num_samples == 0:\n#             print(\"Warning: BalancedDataset initialized with zero samples.\")\n\n#         self.features = {}\n#         for stream in self.active_streams:\n#              if stream in data_dict and data_dict[stream]:\n#                  try:\n#                      self.features[stream] = torch.tensor(np.array(data_dict[stream]), dtype=torch.float32)\n#                  except ValueError as e:\n#                       raise ValueError(f\"Error converting stream '{stream}' data: {e}\")\n#              else:\n#                   raise KeyError(f\"Stream '{stream}' missing/empty in balanced data.\")\n#         try:\n#             self.labels = torch.tensor([lbl[0] for lbl in data_dict[self.label_key]], dtype=torch.long)\n#         except (IndexError, TypeError) as e:\n#              raise ValueError(f\"Error converting labels: {e}\")\n\n#         for stream in self.active_streams:\n#              if len(self.features[stream]) != self.num_samples:\n#                  raise ValueError(f\"Len mismatch: Stream '{stream}' ({len(self.features[stream])}) vs Labels ({self.num_samples})\")\n#     def __len__(self):\n#         return self.num_samples\n#     def __getitem__(self, idx):\n#         feature_dict = {stream: self.features[stream][idx] for stream in self.active_streams}\n#         label = self.labels[idx]\n#         return feature_dict, label\n\n# # --- Model Architecture ---\n# class Attention(nn.Module):\n#     def __init__(self, hidden_dim, attention_dim):\n#         super(Attention, self).__init__()\n#         self.attention_net = nn.Sequential(\n#             nn.Linear(hidden_dim, attention_dim),\n#             nn.Tanh(),\n#             nn.Linear(attention_dim, 1)\n#         )\n#     def forward(self, lstm_output):\n#         att_scores = self.attention_net(lstm_output).squeeze(2)\n#         att_weights = torch.softmax(att_scores, dim=1)\n#         context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1)\n#         return context_vector, att_weights\n# class MultiStreamAdaptiveLSTM(nn.Module):\n#     def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes, attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n#         super(MultiStreamAdaptiveLSTM, self).__init__()\n#         if not stream_names: raise ValueError(\"stream_names cannot be empty.\")\n#         self.stream_names = stream_names\n#         self.lstms = nn.ModuleDict()\n#         self.attentions = nn.ModuleDict()\n#         print(f\"Initializing model with streams: {self.stream_names}\")\n#         for name in self.stream_names:\n#             if name not in input_sizes: raise KeyError(f\"Input size for stream '{name}' not provided.\")\n#             current_input_size = input_sizes[name]\n#             print(f\"  - Adding stream '{name}' with input size {current_input_size}\")\n#             self.lstms[name] = nn.LSTM(current_input_size, lstm_hidden_size, num_lstm_layers,\n#                                        batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0,\n#                                        bidirectional=True)\n#             self.attentions[name] = Attention(lstm_hidden_size * 2 , attention_dim)\n#         num_active_streams = len(self.stream_names)\n#         combined_feature_dim = lstm_hidden_size * 2 * num_active_streams\n#         print(f\"  Combined feature dimension: {combined_feature_dim}\")\n#         self.dropout = nn.Dropout(dropout_rate)\n#         intermediate_dim = max(num_classes * 4, combined_feature_dim // 2)\n#         self.fc1 = nn.Linear(combined_feature_dim, intermediate_dim)\n#         self.relu = nn.ReLU()\n#         self.fc2 = nn.Linear(intermediate_dim, num_classes)\n#     def forward(self, x):\n#         stream_context_vectors = []\n#         stream_att_weights = {}\n#         for name in self.stream_names:\n#             if name not in x: print(f\"Warning: Stream '{name}' expected but not in input data.\"); continue\n#             lstm_out, _ = self.lstms[name](x[name])\n#             context_vector, attention_weights = self.attentions[name](lstm_out)\n#             stream_context_vectors.append(context_vector)\n#             stream_att_weights[name] = attention_weights\n#         if not stream_context_vectors: raise RuntimeError(\"No stream outputs generated.\")\n#         fused_features = torch.cat(stream_context_vectors, dim=1)\n#         out = self.dropout(fused_features)\n#         out = self.relu(self.fc1(out))\n#         out = self.dropout(out)\n#         logits = self.fc2(out)\n#         return logits\n\n# # --- Training and Evaluation Functions ---\n# def train_epoch(model, dataloader, optimizer, criterion, device):\n#     model.train()\n#     total_loss = 0.0\n#     all_preds = []\n#     all_labels = []\n#     active_streams = model.stream_names\n#     for features, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n#         input_features = {name: features[name].to(device) for name in active_streams if name in features}\n#         labels = labels.to(device)\n#         optimizer.zero_grad()\n#         outputs = model(input_features)\n#         loss = criterion(outputs, labels)\n#         loss.backward()\n#         optimizer.step()\n#         total_loss += loss.item()\n#         preds = torch.argmax(outputs, dim=1)\n#         all_preds.extend(preds.cpu().numpy())\n#         all_labels.extend(labels.cpu().numpy())\n#     avg_loss = total_loss / len(dataloader)\n#     accuracy = accuracy_score(all_labels, all_preds)\n#     return avg_loss, accuracy\n\n# def evaluate_epoch(model, dataloader, criterion, device):\n#     model.eval()\n#     total_loss = 0.0\n#     all_labels = []\n#     all_preds = []\n#     all_probs = []\n#     active_streams = model.stream_names\n#     with torch.no_grad():\n#         for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n#             input_features = {name: features[name].to(device) for name in active_streams if name in features}\n#             labels = labels.to(device)\n#             outputs = model(input_features)\n#             loss = criterion(outputs, labels)\n#             total_loss += loss.item()\n#             probs = torch.softmax(outputs, dim=1)\n#             preds = torch.argmax(probs, dim=1)\n#             all_labels.extend(labels.cpu().numpy())\n#             all_preds.extend(preds.cpu().numpy())\n#             all_probs.extend(probs.cpu().numpy())\n#     avg_loss = total_loss / len(dataloader)\n#     all_probs = np.array(all_probs); all_labels = np.array(all_labels); all_preds = np.array(all_preds)\n#     accuracy = accuracy_score(all_labels, all_preds)\n#     precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', pos_label=1, zero_division=0)\n#     auc = roc_auc_score(all_labels, all_probs[:, 1]) if len(np.unique(all_labels)) > 1 else float('nan')\n#     return {'loss': avg_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}\n\n# def get_predictions_and_labels(model, dataloader, device):\n#     model.eval(); all_labels = []; all_preds = []\n#     active_streams = model.stream_names\n#     with torch.no_grad():\n#         for features, labels in tqdm(dataloader, desc=\"Generating CM Data\", leave=False):\n#              input_features = {name: features[name].to(device) for name in active_streams if name in features}\n#              labels = labels.to(device); outputs = model(input_features); preds = torch.argmax(outputs, dim=1)\n#              all_labels.extend(labels.cpu().numpy()); all_preds.extend(preds.cpu().numpy())\n#     return np.array(all_labels), np.array(all_preds)\n\n# # --- Main Execution ---\n# if __name__ == '__main__':\n\n#     # --- Generate/Load PIE Database ---\n#     print(f\"Checking for PIE database cache at: {PIE_DATABASE_CACHE_PATH}\")\n#     if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n#         if PIE is None: raise ImportError(\"PIE class not imported, cannot generate database.\")\n#         print(\"PIE database cache not found. Generating...\");\n#         pie_dataset_interface = PIE(data_path=PIE_ROOT_PATH, regen_database=True)\n#         pie_database = pie_dataset_interface.generate_database()\n#         if not pie_database: raise RuntimeError(\"Failed to generate PIE database.\")\n#         print(\"PIE database generated successfully.\")\n#     else:\n#         print(\"Loading PIE database from cache...\")\n#         try:\n#             with open(PIE_DATABASE_CACHE_PATH, 'rb') as f: pie_database = pickle.load(f)\n#             print(\"PIE database loaded successfully.\")\n#         except Exception as e: raise RuntimeError(f\"Failed to load PIE database: {e}\")\n\n#     # --- Calculate Standardization Parameters ---\n#     print(\"\\nCalculating standardization parameters from training set...\")\n#     all_train_ego_speeds = []; all_train_accX = []; all_train_accY = []; all_train_gyroZ = []\n#     for set_id in TRAIN_SETS_STR:\n#          if set_id in pie_database:\n#              for video_id, video_data in pie_database[set_id].items():\n#                   if 'vehicle_annotations' in video_data:\n#                        for frame_num, ego_frame_data in video_data['vehicle_annotations'].items():\n#                            speed = ego_frame_data.get('OBD_speed', 0.0);\n#                            if speed == 0.0: speed = ego_frame_data.get('GPS_speed', 0.0)\n#                            all_train_ego_speeds.append(speed); all_train_accX.append(ego_frame_data.get('accX', 0.0));\n#                            all_train_accY.append(ego_frame_data.get('accY', 0.0)); all_train_gyroZ.append(ego_frame_data.get('gyroZ', 0.0))\n#     scalers = {}\n#     if all_train_ego_speeds: scalers['ego_speed_mean'] = np.mean(all_train_ego_speeds); scalers['ego_speed_std'] = np.std(all_train_ego_speeds) if np.std(all_train_ego_speeds) > 1e-6 else 1.0; print(f\"  Ego Speed: Mean={scalers['ego_speed_mean']:.2f}, Std={scalers['ego_speed_std']:.2f}\")\n#     if all_train_accX: scalers['accX_mean'] = np.mean(all_train_accX); scalers['accX_std'] = np.std(all_train_accX) if np.std(all_train_accX) > 1e-6 else 1.0; scalers['accY_mean'] = np.mean(all_train_accY); scalers['accY_std'] = np.std(all_train_accY) if np.std(all_train_accY) > 1e-6 else 1.0; print(f\"  Ego AccX: Mean={scalers['accX_mean']:.2f}, Std={scalers['accX_std']:.2f}\"); print(f\"  Ego AccY: Mean={scalers['accY_mean']:.2f}, Std={scalers['accY_std']:.2f}\")\n#     if all_train_gyroZ: scalers['gyroZ_mean'] = np.mean(all_train_gyroZ); scalers['gyroZ_std'] = np.std(all_train_gyroZ) if np.std(all_train_gyroZ) > 1e-6 else 1.0; print(f\"  Ego GyroZ: Mean={scalers['gyroZ_mean']:.2f}, Std={scalers['gyroZ_std']:.2f}\")\n#     print(\"Standardization parameters calculated.\")\n\n#     # --- Initialize FULL Datasets ---\n#     print(\"\\nInitializing full datasets...\")\n#     full_train_dataset = PIEDataset(pie_database, TRAIN_SETS_STR, POSE_DATA_DIR, SEQ_LEN, PRED_LEN, scalers, ACTIVE_STREAMS)\n#     val_dataset = PIEDataset(pie_database, VAL_SETS_STR, POSE_DATA_DIR, SEQ_LEN, PRED_LEN, scalers, ACTIVE_STREAMS)\n#     if len(full_train_dataset) == 0 or len(val_dataset) == 0: raise ValueError(\"Dataset loading failed.\")\n\n#     # --- Prepare and Balance Training Data ---\n#     print(\"\\nExtracting training data for balancing...\")\n#     training_data_dict = {stream: [] for stream in ACTIVE_STREAMS}; training_data_dict['label'] = []\n#     for i in tqdm(range(len(full_train_dataset)), desc=\"Extracting data\"):\n#          features, label = full_train_dataset[i]\n#          for stream_name in ACTIVE_STREAMS: training_data_dict[stream_name].append(features[stream_name].numpy())\n#          training_data_dict['label'].append([label.item()]) # Store label as list containing the item\n#     print(f\"Original training samples: {len(training_data_dict['label'])}\")\n#     del full_train_dataset # Free memory\n\n#     label_key_for_balancing = 'label' # Key used in training_data_dict\n#     balanced_train_data_dict = balance_samples_count(training_data_dict, label_type=label_key_for_balancing)\n#     del training_data_dict # Free up memory\n\n#     # --- Create Balanced Training Dataset and DataLoaders ---\n#     print(\"\\nCreating DataLoaders...\")\n#     try: balanced_train_dataset = BalancedDataset(balanced_train_data_dict, ACTIVE_STREAMS, label_key=label_key_for_balancing); del balanced_train_data_dict\n#     except Exception as e: print(f\"Error creating BalancedDataset: {e}\"); raise\n#     if len(balanced_train_dataset) == 0: raise ValueError(\"Balanced training dataset is empty!\")\n\n#     train_loader = DataLoader(balanced_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n#     val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n#     print(\"DataLoaders created.\")\n\n#     # --- Initialize Model ---\n#     input_sizes = {}\n#     for stream in ACTIVE_STREAMS:\n#         size_constant_name = f'INPUT_SIZE_{stream.upper()}'\n#         special_cases = {'TRAFFIC_LIGHT': 'TL_STATE', 'STATIC_CONTEXT': 'STATIC', 'EGO_SPEED': 'EGO_SPEED', 'EGO_ACC': 'EGO_ACC', 'EGO_GYRO': 'EGO_GYRO', 'PED_ACTION': 'PED_ACTION', 'PED_LOOK': 'PED_LOOK', 'PED_OCCLUSION': 'PED_OCC'}\n#         stream_upper_key = stream.upper()\n#         suffix = special_cases.get(stream_upper_key)\n#         if suffix: size_constant_name = f'INPUT_SIZE_{suffix}'\n#         elif stream == 'bbox': size_constant_name = 'INPUT_SIZE_BBOX'\n#         elif stream == 'pose': size_constant_name = 'INPUT_SIZE_POSE'\n#         if size_constant_name in globals(): input_sizes[stream] = globals()[size_constant_name]\n#         else: raise ValueError(f\"Input size constant {size_constant_name} not found for stream {stream}\")\n\n#     model = MultiStreamAdaptiveLSTM(\n#         input_sizes=input_sizes, lstm_hidden_size=LSTM_HIDDEN_SIZE,\n#         num_lstm_layers=NUM_LSTM_LAYERS, num_classes=NUM_CLASSES, attention_dim=ATTENTION_DIM,\n#         dropout_rate=DROPOUT_RATE, stream_names=ACTIVE_STREAMS ).to(DEVICE)\n\n#     print(\"\\n--- Model Architecture ---\"); print(model); num_params = sum(p.numel() for p in model.parameters() if p.requires_grad); print(f\"Total Trainable Parameters: {num_params:,}\"); print(\"-\" * 30)\n\n#     # --- Class Weighting & Optimizer ---\n#     print(\"\\nCalculating Class Weights for Loss Function...\")\n#     balanced_train_labels_list = balanced_train_dataset.labels.tolist() # Use balanced list\n#     count_0 = balanced_train_labels_list.count(0); count_1 = balanced_train_labels_list.count(1)\n#     total = len(balanced_train_labels_list)\n#     if total == 0: print(\"Warning: Balanced dataset empty. Use equal weights.\"); weight_0 = 1.0; weight_1 = 1.0\n#     elif count_0 == 0: print(\"Warning: Class 0 missing. Adjust weights.\"); weight_0 = 0.0; weight_1 = 1.0\n#     elif count_1 == 0: print(\"Warning: Class 1 missing. Adjust weights.\"); weight_0 = 1.0; weight_1 = 0.0\n#     else: weight_0 = total / (2.0 * count_0); weight_1 = total / (2.0 * count_1) # Inverse frequency\n#     class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float32).to(DEVICE)\n#     print(f\"Using Class Weights for Loss: 0={weight_0:.2f}, 1={weight_1:.2f}\")\n#     criterion = nn.CrossEntropyLoss(weight=class_weights)\n#     # criterion = nn.CrossEntropyLoss() # Uncomment to disable class weighting\n#     optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n#     best_val_f1 = -1.0; train_losses, val_losses = [], []; train_accs, val_accs = [], []; val_f1s = []\n\n#     # --- Training Loop ---\n#     print(\"\\n--- Starting Training on Balanced Data---\")\n#     for epoch in range(NUM_EPOCHS):\n#         epoch_start_time = time.time()\n#         train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n#         val_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n#         epoch_duration = time.time() - epoch_start_time\n#         train_losses.append(train_loss); val_losses.append(val_metrics['loss'])\n#         train_accs.append(train_acc); val_accs.append(val_metrics['accuracy'])\n#         val_f1s.append(val_metrics['f1'])\n#         print(f\"--- Epoch {epoch+1}/{NUM_EPOCHS} ({epoch_duration:.2f} sec) ---\")\n#         print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n#         print(f\"  Val Loss:   {val_metrics['loss']:.4f}, Val Acc:  {val_metrics['accuracy']:.4f}\")\n#         print(f\"  Val Prec:   {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n#         print(f\"  Val AUC:    {val_metrics['auc']:.4f}\")\n#         if val_metrics['f1'] > best_val_f1:\n#             best_val_f1 = val_metrics['f1']\n#             torch.save(model.state_dict(), 'best_model_balanced.pth')\n#             print(f\"  >> Saved new best model with F1: {best_val_f1:.4f}\")\n#         print(\"-\" * 30)\n#     print(\"--- Training Finished ---\")\n\n#     # --- Plotting ---\n#     print(\"\\n--- Plotting Training History ---\")\n#     fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n#     axes[0].plot(range(1, NUM_EPOCHS + 1), train_losses, label='Train Loss')\n#     axes[0].plot(range(1, NUM_EPOCHS + 1), val_losses, label='Val Loss')\n#     axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].set_title('Loss Curve'); axes[0].legend(); axes[0].grid(True)\n#     axes[1].plot(range(1, NUM_EPOCHS + 1), train_accs, label='Train Accuracy')\n#     axes[1].plot(range(1, NUM_EPOCHS + 1), val_accs, label='Val Accuracy')\n#     axes[1].plot(range(1, NUM_EPOCHS + 1), val_f1s, label='Val F1-Score', linestyle='--')\n#     axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Metric'); axes[1].set_title('Accuracy & F1-Score Curve'); axes[1].legend(); axes[1].grid(True)\n#     plt.tight_layout(); plt.show()\n\n#     # --- Final Evaluation ---\n#     print(\"\\n--- Final Evaluation on Validation Set using Best Model ---\")\n#     best_model_path = 'best_model_balanced.pth'\n#     if os.path.exists(best_model_path):\n#         print(f\"Loading best saved model '{best_model_path}'\")\n#         model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n#     else: print(\"Warning: No saved best model found. Evaluating final model.\")\n#     final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n#     true_labels, pred_labels = get_predictions_and_labels(model, val_loader, DEVICE)\n#     cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1])\n#     labels_display = ['Not Crossing', 'Crossing']\n#     print(\"\\n--- Final Performance Metrics ---\")\n#     print(f\"  Accuracy:  {final_metrics['accuracy']:.4f}\"); print(f\"  Precision: {final_metrics['precision']:.4f}\"); print(f\"  Recall:    {final_metrics['recall']:.4f}\"); print(f\"  F1 Score:  {final_metrics['f1']:.4f}\"); print(f\"  AUC:       {final_metrics['auc']:.4f}\"); print(f\"  Loss:      {final_metrics['loss']:.4f}\")\n#     print(\"\\n--- Confusion Matrix ---\")\n#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_display); disp.plot(cmap=plt.cm.Blues); plt.title('Confusion Matrix (Validation Set)'); plt.show()\n\n#     print(\"\\n--- Script Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.637207Z","iopub.execute_input":"2025-04-15T19:55:16.637572Z","iopub.status.idle":"2025-04-15T19:55:16.655468Z","shell.execute_reply.started":"2025-04-15T19:55:16.637506Z","shell.execute_reply":"2025-04-15T19:55:16.654641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- CELL 1: DATA PREPARATION AND BALANCING (RUN ONCE) ---\n\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import Dataset, DataLoader\n# import xml.etree.ElementTree as ET\n# import os\n# import numpy as np\n# from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n# from sklearn.preprocessing import StandardScaler  # For standardizing ego features\n# from tqdm.notebook import tqdm\n# import random\n# import math\n# import zipfile\n# import cv2\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# import pickle\n# import time\n# import sys\n\n# # --- Add PIE utilities path if necessary (adjust path) ---\n# pie_utilities_path = '/kaggle/working/PIE/utilities'\n# if pie_utilities_path not in sys.path:\n#     sys.path.insert(0, pie_utilities_path)\n# try:\n#     from pie_data import PIE\n# except ImportError as e:\n#     print(f\"Warning: Could not import PIE class from {pie_utilities_path}. Database must already exist. Error: {e}\")\n#     PIE = None\n\n# # --- Configuration (Copy relevant parts here) ---\n# PIE_ROOT_PATH = '/kaggle/working/PIE'\n# VIDEO_INPUT_DIR = '/kaggle/input'\n# POSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2'\n# PIE_DATABASE_CACHE_PATH = '/kaggle/input/pie-database/pie_database.pkl'\n\n# # --- Define ALL possible streams you might want to experiment with ---\n# # --- The data extraction will prepare ALL of these ---\n# ALL_POSSIBLE_STREAMS = [\n#     'bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro',\n#     'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context'\n# ]\n# print(f\"Data will be prepared for streams: {ALL_POSSIBLE_STREAMS}\")\n\n# # --- Model Hyperparameters (Needed for Dataset) ---\n# SEQ_LEN = 30\n# PRED_LEN = 1\n\n# # --- Input Sizes (Needed for Dataset Error Handling & Definitions) ---\n# INPUT_SIZE_BBOX = 4\n# INPUT_SIZE_POSE = 34\n# INPUT_SIZE_EGO_SPEED = 1\n# INPUT_SIZE_EGO_ACC = 2\n# INPUT_SIZE_EGO_GYRO = 1\n# INPUT_SIZE_PED_ACTION = 1\n# INPUT_SIZE_PED_LOOK = 1\n# INPUT_SIZE_PED_OCC = 1\n# INPUT_SIZE_TL_STATE = 4\n# NUM_SIGNALIZED_CATS = 4\n# NUM_INTERSECTION_CATS = 5\n# NUM_AGE_CATS = 4\n# NUM_GENDER_CATS = 3\n# INPUT_SIZE_STATIC = NUM_SIGNALIZED_CATS + NUM_INTERSECTION_CATS + NUM_AGE_CATS + NUM_GENDER_CATS  # = 16\n\n# # --- Dataset Splits ---\n# TRAIN_SETS_STR = ['set01', 'set02', 'set04']\n# VAL_SETS_STR = ['set05', 'set06']\n\n# # --- Mappings ---\n# TL_STATE_MAP = {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}\n# NUM_TL_STATES = len(TL_STATE_MAP)\n# SIGNALIZED_MAP = {'n/a': 0, 'C': 1, 'S': 2, 'CS': 3}\n# INTERSECTION_MAP = {'midblock': 0, 'T': 1, 'T-left': 2, 'T-right': 3, 'four-way': 4}\n# AGE_MAP = {'child': 0, 'young': 1, 'adult': 2, 'senior': 3}\n# GENDER_MAP = {'n/a': 0, 'female': 1, 'male': 2}\n\n# # --- Output Files from this Cell ---\n# BALANCED_DATA_PKL_PATH = \"/kaggle/working/balanced_train_data.pkl\"\n# SCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\"\n# VAL_SEQUENCES_PKL_PATH = \"/kaggle/working/val_sequences.pkl\"  # Save sequence info for val set\n\n# # --- Helper: One-Hot Encoding ---\n# def to_one_hot(index, num_classes):\n#     vec = np.zeros(num_classes, dtype=np.float32)\n#     if 0 <= index < num_classes:\n#         vec[index] = 1.0\n#     else:\n#         vec[0] = 1.0  # Default\n#     return vec\n\n# # --- Balancing Function ---\n# def balance_samples_count(seq_data, label_type, random_seed=42):\n#     print('---------------------------------------------------------')\n#     print(f\"Balancing samples based on '{label_type}' key\")\n#     if label_type not in seq_data:\n#         raise KeyError(f\"Label type '{label_type}' not found.\")\n#     try:\n#         gt_labels = [lbl[0] for lbl in seq_data[label_type]]\n#     except (IndexError, TypeError):\n#         raise ValueError(f\"Labels under '{label_type}' not in expected format [[label_val]].\")\n#     if not all(l in [0, 1] for l in gt_labels):\n#         print(f\"Warning: Labels for balancing contain values other than 0 or 1.\")\n#     num_pos_samples = np.count_nonzero(np.array(gt_labels))\n#     num_neg_samples = len(gt_labels) - num_pos_samples\n#     new_seq_data = {}\n#     if num_neg_samples == num_pos_samples:\n#         print('Samples already balanced.')\n#         return seq_data.copy()\n#     else:\n#         print(f'Unbalanced: Positive (1): {num_pos_samples} | Negative (0): {num_neg_samples}')\n#         majority_label = 0 if num_neg_samples > num_pos_samples else 1\n#         minority_count = min(num_neg_samples, num_pos_samples)\n#         print(f\"Undersampling majority class ({majority_label}) to match count ({minority_count}).\")\n#         majority_indices = np.where(np.array(gt_labels) == majority_label)[0]\n#         minority_indices = np.where(np.array(gt_labels) != majority_label)[0]\n#         np.random.seed(random_seed)\n#         keep_majority_indices = np.random.choice(majority_indices, size=minority_count, replace=False)\n#         final_indices = np.concatenate((minority_indices, keep_majority_indices))\n#         np.random.shuffle(final_indices)\n#         for k, v_list in seq_data.items():\n#             if isinstance(v_list, list) and len(v_list) == len(gt_labels):\n#                 try:\n#                     if v_list and isinstance(v_list[0], np.ndarray):\n#                         v_array = np.array(v_list)\n#                         new_seq_data[k] = list(v_array[final_indices])\n#                     else:\n#                         new_seq_data[k] = [v_list[i] for i in final_indices]\n#                 except Exception as e:\n#                     print(f\"Error processing key '{k}' during balancing: {e}. Skip.\")\n#                     new_seq_data[k] = []\n#             else:\n#                 print(f\"Warn: Skipping key '{k}' in balancing.\")\n#                 new_seq_data[k] = v_list\n#         if label_type in new_seq_data:\n#             new_gt_labels = [lbl[0] for lbl in new_seq_data[label_type]]\n#             final_pos = np.count_nonzero(np.array(new_gt_labels))\n#             final_neg = len(new_gt_labels) - final_pos\n#             print(f'Balanced:   Positive (1): {final_pos} | Negative (0): {final_neg}')\n#         else:\n#             print(\"Error: Label key lost during balancing.\")\n#         print('---------------------------------------------------------')\n#         return new_seq_data\n\n# # --- Dataset Class (Needed for initial loading) ---\n# class PIEDataset(Dataset):\n#     # --- (Dataset class definition - identical to the previous working version) ---\n#     # --- (Includes __init__, _load_pose_data, _generate_sequence_list, __len__, __getitem__) ---\n#     def __init__(self, pie_database, set_names, pose_data_dir, seq_len, pred_len, scalers=None, active_streams=None):\n#         self.pie_db = pie_database\n#         self.set_names = set_names\n#         self.pose_data_dir = pose_data_dir\n#         self.seq_len = seq_len\n#         self.pred_len = pred_len\n#         self.scalers = scalers or {}\n#         # Store ALL possible streams; __getitem__ will generate data for all streams.\n#         self.active_streams = ALL_POSSIBLE_STREAMS  # Generate all streams for potential use\n#         self.sequences = []\n#         self.all_pose_data = {}\n#         self._input_sizes_for_error = self._get_input_sizes_dict()  # Get sizes for all streams\n#         self._load_pose_data()\n#         self._generate_sequence_list()\n#         if not self.sequences:\n#             raise ValueError(f\"Dataset init failed: No sequences for {self.set_names}\")\n\n#     def _get_input_sizes_dict(self):\n#         input_sizes = {}\n#         special_cases = {\n#             'TRAFFIC_LIGHT': 'TL_STATE',\n#             'STATIC_CONTEXT': 'STATIC',\n#             'EGO_SPEED': 'EGO_SPEED',\n#             'EGO_ACC': 'EGO_ACC',\n#             'EGO_GYRO': 'EGO_GYRO',\n#             'PED_ACTION': 'PED_ACTION',\n#             'PED_LOOK': 'PED_LOOK',\n#             'PED_OCCLUSION': 'PED_OCC'\n#         }\n#         for stream in self.active_streams:\n#             stream_upper_key = stream.upper()\n#             if stream_upper_key in special_cases:\n#                 size_constant_name = f\"INPUT_SIZE_{special_cases[stream_upper_key]}\"\n#             elif stream == 'bbox':\n#                 size_constant_name = 'INPUT_SIZE_BBOX'\n#             elif stream == 'pose':\n#                 size_constant_name = 'INPUT_SIZE_POSE'\n#             else:\n#                 size_constant_name = f\"INPUT_SIZE_{stream.upper()}\"\n#             if size_constant_name in globals():\n#                 input_sizes[stream] = globals()[size_constant_name]\n#             else:\n#                 input_sizes[stream] = 1\n#         return input_sizes\n\n#     def _load_pose_data(self):\n#         print(f\"\\nLoading pose data for sets: {self.set_names} from {self.pose_data_dir}\")\n#         sets_loaded_count = 0\n#         for set_id in tqdm(self.set_names, desc=\"Loading Pose Sets\"):\n#             self.all_pose_data[set_id] = {}\n#             pose_set_path = os.path.join(self.pose_data_dir, set_id)\n#             if not os.path.isdir(pose_set_path):\n#                 print(f\"Warn: Pose dir missing for {set_id} at {pose_set_path}\")\n#                 continue\n#             pkl_files_in_set = [f for f in os.listdir(pose_set_path)\n#                                 if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")]\n#             if not pkl_files_in_set:\n#                 continue\n#             loaded_video_count = 0\n#             for pkl_filename in tqdm(pkl_files_in_set, desc=f\"Loading PKLs for {set_id}\", leave=False):\n#                 pkl_file_path = os.path.join(pose_set_path, pkl_filename)\n#                 try:\n#                     with open(pkl_file_path, 'rb') as f:\n#                         loaded_pkl_content = pickle.load(f)\n#                     if len(loaded_pkl_content) != 1:\n#                         continue\n#                     unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n#                     video_id = \"_\".join(unique_video_key.split('_')[1:])\n#                     self.all_pose_data[set_id][video_id] = video_data\n#                     loaded_video_count += 1\n#                 except FileNotFoundError:\n#                     pass\n#                 except Exception as e:\n#                     print(f\"Error loading pose PKL {pkl_file_path}: {e}\")\n#             if loaded_video_count > 0:\n#                 sets_loaded_count += 1\n#         print(f\"Finished loading pose data for {sets_loaded_count} sets.\")\n\n#     def _generate_sequence_list(self):\n#         print(f\"Generating sequence list from PIE database for sets: {self.set_names}\")\n#         sequence_count = 0\n#         ped_count = 0\n#         for set_id in tqdm(self.set_names, desc=\"Generating Sequences\"):\n#             if set_id not in self.pie_db:\n#                 continue\n#             for video_id, video_data in self.pie_db[set_id].items():\n#                 if 'ped_annotations' not in video_data:\n#                     continue\n#                 for ped_id, ped_data in video_data['ped_annotations'].items():\n#                     ped_count += 1\n#                     if 'frames' not in ped_data or len(ped_data['frames']) < self.seq_len + self.pred_len:\n#                         continue\n#                     sorted_frames = sorted(ped_data['frames'])\n#                     for i in range(len(sorted_frames) - self.seq_len - self.pred_len + 1):\n#                         start_frame = sorted_frames[i]\n#                         end_frame_observe = sorted_frames[i + self.seq_len - 1]\n#                         if end_frame_observe - start_frame != self.seq_len - 1:\n#                             continue\n#                         target_frame_actual_idx = i + self.seq_len + self.pred_len - 1\n#                         if target_frame_actual_idx >= len(sorted_frames):\n#                             continue\n#                         target_frame = sorted_frames[target_frame_actual_idx]\n#                         if target_frame - end_frame_observe != self.pred_len:\n#                             continue\n#                         self.sequences.append((set_id, video_id, ped_id, start_frame))\n#                         sequence_count += 1\n#         print(f\"Found {sequence_count} valid sequences from {ped_count} pedestrian tracks.\")\n\n#     def __len__(self):\n#         return len(self.sequences)\n\n#     def __getitem__(self, idx):\n#         # Get sequence identifiers\n#         set_id, video_id, ped_id, start_frame = self.sequences[idx]\n#         frame_nums = list(range(start_frame, start_frame + self.seq_len))\n#         target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n\n#         video_db = self.pie_db.get(set_id, {}).get(video_id, {})\n#         ped_db = video_db.get('ped_annotations', {}).get(ped_id, {})\n#         ego_db = video_db.get('vehicle_annotations', {})\n#         traffic_db = video_db.get('traffic_annotations', {})\n#         ped_attributes = ped_db.get('attributes', {})\n\n#         # Initialize feature sequences and label\n#         feature_sequences = {stream: [] for stream in self.active_streams}\n#         label = 0\n\n#         # Determine target label (default is 0 if not found)\n#         if 'frames' in ped_db and 'behavior' in ped_db and 'cross' in ped_db['behavior']:\n#             try:\n#                 target_frame_db_idx = ped_db['frames'].index(target_frame_num)\n#                 label = ped_db['behavior']['cross'][target_frame_db_idx]\n#                 if label == -1:\n#                     label = 0\n#             except (ValueError, IndexError):\n#                 pass\n\n#         # Static features\n#         static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n#         if 'static_context' in self.active_streams:\n#             sig_idx = ped_attributes.get('signalized', 0)\n#             int_idx = ped_attributes.get('intersection', 0)\n#             age_idx = ped_attributes.get('age', 2)\n#             gen_idx = ped_attributes.get('gender', 0)\n#             static_vec = np.concatenate([\n#                 to_one_hot(sig_idx, NUM_SIGNALIZED_CATS),\n#                 to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n#                 to_one_hot(age_idx, NUM_AGE_CATS),\n#                 to_one_hot(gen_idx, NUM_GENDER_CATS)\n#             ])\n#             if static_vec.shape[0] != INPUT_SIZE_STATIC:\n#                 static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n\n#         # Loop over each frame in the sequence\n#         for frame_num in frame_nums:\n#             frame_db_idx = -1\n#             if 'frames' in ped_db:\n#                 try:\n#                     frame_db_idx = ped_db['frames'].index(frame_num)\n#                 except ValueError:\n#                     pass\n#             ego_frame_data = ego_db.get(frame_num, {})\n\n#             # 'bbox' stream\n#             if 'bbox' in self.active_streams:\n#                 bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32)\n#                 if frame_db_idx != -1 and 'bbox' in ped_db and len(ped_db['bbox']) > frame_db_idx:\n#                     try:\n#                         x1, y1, x2, y2 = ped_db['bbox'][frame_db_idx]\n#                         img_w = video_db.get('width', 1920)\n#                         img_h = video_db.get('height', 1080)\n#                         if img_w > 0 and img_h > 0:\n#                             cx = ((x1 + x2) / 2) / img_w\n#                             cy = ((y1 + y2) / 2) / img_h\n#                             w = (x2 - x1) / img_w\n#                             h = (y2 - y1) / img_h\n#                             if w > 0 and h > 0 and 0 <= cx <= 1 and 0 <= cy <= 1:\n#                                 bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n#                     except Exception:\n#                         pass\n#                 feature_sequences['bbox'].append(bbox_norm)\n\n#             # 'pose' stream\n#             if 'pose' in self.active_streams:\n#                 pose_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n#                 vid_pose_data = self.all_pose_data.get(set_id, {}).get(video_id, {})\n#                 frame_pose_data = vid_pose_data.get(frame_num, {})\n#                 loaded_pose = frame_pose_data.get(ped_id)\n#                 if loaded_pose is not None and isinstance(loaded_pose, np.ndarray) and loaded_pose.shape == (INPUT_SIZE_POSE,):\n#                     pose_vector = loaded_pose\n#                 feature_sequences['pose'].append(pose_vector)\n\n#             # 'ego_speed' stream\n#             if 'ego_speed' in self.active_streams:\n#                 speed = ego_frame_data.get('OBD_speed', 0.0)\n#                 if speed == 0.0:\n#                     speed = ego_frame_data.get('GPS_speed', 0.0)\n#                 speed_scaled = (speed - self.scalers.get('ego_speed_mean', 0.0)) / self.scalers.get('ego_speed_std', 1.0)\n#                 feature_sequences['ego_speed'].append([speed_scaled])\n\n#             # 'ego_acc' stream\n#             if 'ego_acc' in self.active_streams:\n#                 accX = ego_frame_data.get('accX', 0.0)\n#                 accY = ego_frame_data.get('accY', 0.0)\n#                 accX_scaled = (accX - self.scalers.get('accX_mean', 0.0)) / self.scalers.get('accX_std', 1.0)\n#                 accY_scaled = (accY - self.scalers.get('accY_mean', 0.0)) / self.scalers.get('accY_std', 1.0)\n#                 feature_sequences['ego_acc'].append([accX_scaled, accY_scaled])\n\n#             # 'ego_gyro' stream\n#             if 'ego_gyro' in self.active_streams:\n#                 gyroZ = ego_frame_data.get('gyroZ', 0.0)\n#                 gyroZ_scaled = (gyroZ - self.scalers.get('gyroZ_mean', 0.0)) / self.scalers.get('gyroZ_std', 1.0)\n#                 feature_sequences['ego_gyro'].append([gyroZ_scaled])\n\n#             # 'ped_action' stream\n#             if 'ped_action' in self.active_streams:\n#                 action = 0\n#                 if frame_db_idx != -1 and 'behavior' in ped_db and 'action' in ped_db['behavior'] and len(ped_db['behavior']['action']) > frame_db_idx:\n#                     action = ped_db['behavior']['action'][frame_db_idx]\n#                 feature_sequences['ped_action'].append([float(action)])\n\n#             # 'ped_look' stream\n#             if 'ped_look' in self.active_streams:\n#                 look = 0\n#                 if frame_db_idx != -1 and 'behavior' in ped_db and 'look' in ped_db['behavior'] and len(ped_db['behavior']['look']) > frame_db_idx:\n#                     look = ped_db['behavior']['look'][frame_db_idx]\n#                 feature_sequences['ped_look'].append([float(look)])\n\n#             # 'ped_occlusion' stream\n#             if 'ped_occlusion' in self.active_streams:\n#                 occ = 0.0\n#                 if frame_db_idx != -1 and 'occlusion' in ped_db and len(ped_db['occlusion']) > frame_db_idx:\n#                     occ_val = ped_db['occlusion'][frame_db_idx]\n#                     occ = float(occ_val) / 2.0\n#                 feature_sequences['ped_occlusion'].append([occ])\n\n#             # 'traffic_light' stream\n#             if 'traffic_light' in self.active_streams:\n#                 state_int = 0\n#                 for obj_id, obj_data in traffic_db.items():\n#                     if obj_data.get('obj_class') == 'traffic_light' and 'frames' in obj_data and 'state' in obj_data:\n#                         try:\n#                             tl_frame_idx = obj_data['frames'].index(frame_num)\n#                             state_val = obj_data['state'][tl_frame_idx]\n#                             if state_val != 0:\n#                                 state_int = state_val\n#                                 break\n#                         except (ValueError, IndexError):\n#                             continue\n#                 feature_sequences['traffic_light'].append(to_one_hot(state_int, NUM_TL_STATES))\n\n#             # 'static_context' stream\n#             if 'static_context' in self.active_streams:\n#                 feature_sequences['static_context'].append(static_vec)\n\n#         features = {}\n#         try:\n#             for stream_name in self.active_streams:\n#                 features[stream_name] = torch.tensor(\n#                     np.array(feature_sequences[stream_name], dtype=np.float32),\n#                     dtype=torch.float32\n#                 )\n#         except Exception as e:\n#             print(f\"Error converting features idx {idx}: {e}. Return dummy.\")\n#             features = {\n#                 name: torch.zeros((self.seq_len, self._input_sizes_for_error.get(name, 1)), dtype=torch.float32)\n#                 for name in self.active_streams\n#             }\n#         return features, torch.tensor(label, dtype=torch.long)\n\n# # --- Main Data Prep Execution Block ---\n# if __name__ == '__main__':\n\n#     # --- Generate/Load PIE Database ---\n#     print(f\"Checking for PIE database cache at: {PIE_DATABASE_CACHE_PATH}\")\n#     if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n#         if PIE is None:\n#             raise ImportError(\"PIE class not imported, cannot generate database.\")\n#         print(\"PIE database cache not found. Generating...\")\n#         pie_dataset_interface = PIE(data_path=PIE_ROOT_PATH, regen_database=True)\n#         pie_database = pie_dataset_interface.generate_database()\n#         if not pie_database:\n#             raise RuntimeError(\"Failed to generate PIE database.\")\n#         print(\"PIE database generated successfully.\")\n#     else:\n#         print(\"Loading PIE database from cache...\")\n#         try:\n#             with open(PIE_DATABASE_CACHE_PATH, 'rb') as f:\n#                 pie_database = pickle.load(f)\n#             print(\"PIE database loaded successfully.\")\n#         except Exception as e:\n#             raise RuntimeError(f\"Failed to load PIE database: {e}\")\n\n#     # --- Calculate Standardization Parameters ---\n#     print(\"\\nCalculating standardization parameters from training set...\")\n#     all_train_ego_speeds = []\n#     all_train_accX = []\n#     all_train_accY = []\n#     all_train_gyroZ = []\n#     for set_id in TRAIN_SETS_STR:\n#         if set_id in pie_database:\n#             for video_id, video_data in pie_database[set_id].items():\n#                 if 'vehicle_annotations' in video_data:\n#                     for frame_num, ego_frame_data in video_data['vehicle_annotations'].items():\n#                         speed = ego_frame_data.get('OBD_speed', 0.0)\n#                         if speed == 0.0:\n#                             speed = ego_frame_data.get('GPS_speed', 0.0)\n#                         all_train_ego_speeds.append(speed)\n#                         all_train_accX.append(ego_frame_data.get('accX', 0.0))\n#                         all_train_accY.append(ego_frame_data.get('accY', 0.0))\n#                         all_train_gyroZ.append(ego_frame_data.get('gyroZ', 0.0))\n#     scalers = {}\n#     if all_train_ego_speeds:\n#         scalers['ego_speed_mean'] = np.mean(all_train_ego_speeds)\n#         scalers['ego_speed_std'] = np.std(all_train_ego_speeds) if np.std(all_train_ego_speeds) > 1e-6 else 1.0\n#         print(f\"  Ego Speed: Mean={scalers['ego_speed_mean']:.2f}, Std={scalers['ego_speed_std']:.2f}\")\n#     if all_train_accX:\n#         scalers['accX_mean'] = np.mean(all_train_accX)\n#         scalers['accX_std'] = np.std(all_train_accX) if np.std(all_train_accX) > 1e-6 else 1.0\n#         scalers['accY_mean'] = np.mean(all_train_accY)\n#         scalers['accY_std'] = np.std(all_train_accY) if np.std(all_train_accY) > 1e-6 else 1.0\n#         print(f\"  Ego AccX: Mean={scalers['accX_mean']:.2f}, Std={scalers['accX_std']:.2f}\")\n#         print(f\"  Ego AccY: Mean={scalers['accY_mean']:.2f}, Std={scalers['accY_std']:.2f}\")\n#     if all_train_gyroZ:\n#         scalers['gyroZ_mean'] = np.mean(all_train_gyroZ)\n#         scalers['gyroZ_std'] = np.std(all_train_gyroZ) if np.std(all_train_gyroZ) > 1e-6 else 1.0\n#         print(f\"  Ego GyroZ: Mean={scalers['gyroZ_mean']:.2f}, Std={scalers['gyroZ_std']:.2f}\")\n#     print(\"Standardization parameters calculated.\")\n\n#     # --- Initialize FULL Training Dataset ---\n#     print(\"\\nInitializing full training dataset (for extraction)...\")\n#     full_train_dataset = PIEDataset(\n#         pie_database,\n#         TRAIN_SETS_STR,\n#         POSE_DATA_DIR,\n#         SEQ_LEN,\n#         PRED_LEN,\n#         scalers,\n#         ALL_POSSIBLE_STREAMS\n#     )\n#     if len(full_train_dataset) == 0:\n#         raise ValueError(\"Full Train Dataset loading failed.\")\n\n#     # --- Prepare and Balance Training Data ---\n#     print(\"\\nExtracting ALL stream data from training set for balancing...\")\n#     training_data_dict = {stream: [] for stream in ALL_POSSIBLE_STREAMS}\n#     training_data_dict['label'] = []\n#     for i in tqdm(range(len(full_train_dataset)), desc=\"Extracting data\"):\n#         features, label = full_train_dataset[i]\n#         for stream_name in ALL_POSSIBLE_STREAMS:\n#             if stream_name in features:\n#                 training_data_dict[stream_name].append(features[stream_name].numpy())\n#             else:\n#                 print(f\"Warning: Stream {stream_name} missing from dataset output index {i}.\")\n#                 # Append zeros of correct shape as placeholder\n#                 size_const = f\"INPUT_SIZE_{stream_name.upper()}\"\n#                 special_cases = {\n#                     'TRAFFIC_LIGHT': 'TL_STATE',\n#                     'STATIC_CONTEXT': 'STATIC',\n#                     'EGO_SPEED': 'EGO_SPEED',\n#                     'EGO_ACC': 'EGO_ACC',\n#                     'EGO_GYRO': 'EGO_GYRO',\n#                     'PED_ACTION': 'PED_ACTION',\n#                     'PED_LOOK': 'PED_LOOK',\n#                     'PED_OCCLUSION': 'PED_OCC'\n#                 }\n#                 if stream_name.upper() in special_cases:\n#                     size_const = f\"INPUT_SIZE_{special_cases[stream_name.upper()]}\"\n#                 elif stream_name == 'bbox':\n#                     size_const = 'INPUT_SIZE_BBOX'\n#                 elif stream_name == 'pose':\n#                     size_const = 'INPUT_SIZE_POSE'\n#                 stream_size = globals().get(size_const, 1)\n#                 training_data_dict[stream_name].append(np.zeros((SEQ_LEN, stream_size), dtype=np.float32))\n#         training_data_dict['label'].append([label.item()])\n#     print(f\"Original training samples: {len(training_data_dict['label'])}\")\n#     del full_train_dataset  # Free memory\n\n#     label_key_for_balancing = 'label'\n#     balanced_train_data_dict = balance_samples_count(training_data_dict, label_type=label_key_for_balancing)\n#     del training_data_dict  # Free up memory\n\n#     # --- Save Balanced Data and Scalers ---\n#     print(f\"\\nSaving balanced training data to: {BALANCED_DATA_PKL_PATH}\")\n#     try:\n#         with open(BALANCED_DATA_PKL_PATH, 'wb') as f:\n#             pickle.dump(balanced_train_data_dict, f, pickle.HIGHEST_PROTOCOL)\n#         print(\" -> Balanced data saved.\")\n#     except Exception as e:\n#         print(f\"  Error saving balanced data: {e}\")\n\n#     print(f\"\\nSaving scalers to: {SCALERS_PKL_PATH}\")\n#     try:\n#         with open(SCALERS_PKL_PATH, 'wb') as f:\n#             pickle.dump(scalers, f, pickle.HIGHEST_PROTOCOL)\n#         print(\" -> Scalers saved.\")\n#     except Exception as e:\n#         print(f\"  Error saving scalers: {e}\")\n\n#     # --- Prepare and Save Validation Sequence Info ---\n#     print(\"\\nInitializing validation dataset (for sequence info)...\")\n#     # Pass an empty list for streams if only sequence identifiers are needed.\n#     val_dataset = PIEDataset(pie_database, VAL_SETS_STR, POSE_DATA_DIR, SEQ_LEN, PRED_LEN, scalers, [])\n#     val_sequences_info = val_dataset.sequences\n#     del val_dataset\n\n#     print(f\"Saving validation sequence info ({len(val_sequences_info)} sequences) to: {VAL_SEQUENCES_PKL_PATH}\")\n#     try:\n#         with open(VAL_SEQUENCES_PKL_PATH, 'wb') as f:\n#             pickle.dump(val_sequences_info, f, pickle.HIGHEST_PROTOCOL)\n#         print(\" -> Validation sequence info saved.\")\n#     except Exception as e:\n#         print(f\" Error saving validation sequence info: {e}\")\n\n#     print(\"\\n--- Data Preparation and Balancing Cell Finished ---\")\n#     print(\"You can now run the next cell for training and evaluation.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T14:52:10.240242Z","iopub.execute_input":"2025-04-19T14:52:10.240560Z","iopub.status.idle":"2025-04-19T15:04:28.628183Z","shell.execute_reply.started":"2025-04-19T14:52:10.240534Z","shell.execute_reply":"2025-04-19T15:04:28.627487Z"}},"outputs":[{"name":"stdout","text":"Data will be prepared for streams: ['bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro', 'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context']\nChecking for PIE database cache at: /kaggle/input/pie-database/pie_database.pkl\nLoading PIE database from cache...\nPIE database loaded successfully.\n\nCalculating standardization parameters from training set...\n  Ego Speed: Mean=13.43, Std=13.31\n  Ego AccX: Mean=-0.03, Std=0.08\n  Ego AccY: Mean=-0.52, Std=0.85\n  Ego GyroZ: Mean=-0.04, Std=4.48\nStandardization parameters calculated.\n\nInitializing full training dataset (for extraction)...\n\nLoading pose data for sets: ['set01', 'set02', 'set04'] from /kaggle/input/pose-data/extracted_poses2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading Pose Sets:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10a9249bc44c4636b88ed06878bf7ed5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set01:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set02:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set04:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Finished loading pose data for 3 sets.\nGenerating sequence list from PIE database for sets: ['set01', 'set02', 'set04']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating Sequences:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a435dd122f49460793e0756a8b36f6b7"}},"metadata":{}},{"name":"stdout","text":"Found 333454 valid sequences from 880 pedestrian tracks.\n\nExtracting ALL stream data from training set for balancing...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting data:   0%|          | 0/333454 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b0e3c8f6404433db464ee8c3d6604f3"}},"metadata":{}},{"name":"stdout","text":"Original training samples: 333454\n---------------------------------------------------------\nBalancing samples based on 'label' key\nUnbalanced: Positive (1): 54967 | Negative (0): 278487\nUndersampling majority class (0) to match count (54967).\nBalanced:   Positive (1): 54967 | Negative (0): 54967\n---------------------------------------------------------\n\nSaving balanced training data to: /kaggle/working/balanced_train_data.pkl\n -> Balanced data saved.\n\nSaving scalers to: /kaggle/working/scalers.pkl\n -> Scalers saved.\n\nInitializing validation dataset (for sequence info)...\n\nLoading pose data for sets: ['set05', 'set06'] from /kaggle/input/pose-data/extracted_poses2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading Pose Sets:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f9df5c2edc4423197ae33200d35d06a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set05:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set06:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Finished loading pose data for 2 sets.\nGenerating sequence list from PIE database for sets: ['set05', 'set06']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating Sequences:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acf119597cd14c2bba5cfb82afd91916"}},"metadata":{}},{"name":"stdout","text":"Found 77288 valid sequences from 243 pedestrian tracks.\nSaving validation sequence info (77288 sequences) to: /kaggle/working/val_sequences.pkl\n -> Validation sequence info saved.\n\n--- Data Preparation and Balancing Cell Finished ---\nYou can now run the next cell for training and evaluation.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# # --- CELL 2: ABLATION STUDY - MODEL TRAINING AND EVALUATION ---\n\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import Dataset, DataLoader\n# import os\n# import sys\n# import numpy as np\n# import pandas as pd\n# import pickle\n# import time\n# from tqdm.notebook import tqdm\n# from sklearn.metrics import (\n#     accuracy_score,\n#     precision_recall_fscore_support,\n#     roc_auc_score,\n#     confusion_matrix,\n#     ConfusionMatrixDisplay\n# )\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# # --- Add PIE utilities path if necessary ---\n# pie_utilities_path = '/kaggle/working/PIE/utilities'\n# if pie_utilities_path not in sys.path:\n#     sys.path.insert(0, pie_utilities_path)\n\n# try:\n#     from pie_data import PIE\n# except ImportError as e:\n#     print(f\"Warn: Could not import PIE class: {e}\")\n#     PIE = None\n\n# # --- Configuration ---\n# PIE_DATABASE_CACHE_PATH = '/kaggle/input/pie-database/pie_database.pkl'\n# POSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2'\n# VAL_SETS_STR = ['set05', 'set06']\n\n# # --- Experiment settings ---\n# ALL_POSSIBLE_STREAMS = [\n#     'bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro',\n#     'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context'\n# ]\n\n# STREAM_COMBINATIONS_TO_TEST = [\n#     ['bbox'],\n#     ['bbox', 'ped_action'],\n#     ['bbox', 'ped_look'],\n#     ['bbox', 'ego_speed'],\n#     ['bbox', 'ego_acc'],\n#     ['bbox', 'ped_action', 'ped_look'],\n#     ['bbox', 'ego_speed', 'ego_acc'],\n#     ['bbox', 'ped_action', 'ped_look', 'ego_speed', 'ego_acc'],\n#     ['bbox', 'ped_action', 'ped_look', 'ego_speed', 'ego_acc', 'traffic_light', 'static_context']\n# ]\n\n# # --- Hyperparameters ---\n# SEQ_LEN = 30\n# PRED_LEN = 1\n\n# INPUT_SIZE_BBOX = 4\n# INPUT_SIZE_POSE = 34\n# INPUT_SIZE_EGO_SPEED = 1\n# INPUT_SIZE_EGO_ACC = 2\n# INPUT_SIZE_EGO_GYRO = 1\n# INPUT_SIZE_PED_ACTION = 1\n# INPUT_SIZE_PED_LOOK = 1\n# INPUT_SIZE_PED_OCC = 1\n# INPUT_SIZE_TL_STATE = 4\n\n# NUM_SIGNALIZED_CATS = 4\n# NUM_INTERSECTION_CATS = 5\n# NUM_AGE_CATS = 4\n# NUM_GENDER_CATS = 3\n# NUM_TRAFFIC_DIR_CATS = 2\n# LANE_CATEGORIES = {1:0,2:1,3:2,4:3,5:4,6:4,7:4,8:4}\n# NUM_LANE_CATS = len(set(LANE_CATEGORIES.values()))\n\n# INPUT_SIZE_STATIC = (\n#     NUM_SIGNALIZED_CATS +\n#     NUM_INTERSECTION_CATS +\n#     NUM_AGE_CATS +\n#     NUM_GENDER_CATS +\n#     NUM_TRAFFIC_DIR_CATS +\n#     NUM_LANE_CATS\n# )\n\n# LSTM_HIDDEN_SIZE = 256\n# NUM_LSTM_LAYERS = 2\n# DROPOUT_RATE = 0.3\n# NUM_CLASSES = 2\n# ATTENTION_DIM = 128\n\n# LEARNING_RATE = 1e-4\n# BATCH_SIZE = 32\n# NUM_EPOCHS = 10\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# print(f\"Using device: {DEVICE}\")\n\n# # --- Paths for data ---\n# BALANCED_DATA_PKL_PATH = \"/kaggle/working/balanced_train_data.pkl\"\n# SCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\"\n\n# # --- Helpers, Dataset & Model definitions ---\n# # (Use the fully corrected versions from Cell 1 and Cell 2 above,\n# # ensuring no inline semicolons remain and each block is properly indented.)\n\n# def to_one_hot(index, num_classes):\n#     vec = np.zeros(num_classes, dtype=np.float32)\n#     idx = int(np.clip(index, 0, num_classes - 1))\n#     vec[idx] = 1.0\n#     return vec\n\n# # ... Paste full corrected definitions for:\n# #   PIEDataset, BalancedDataset,\n# #   Attention, MultiStreamAdaptiveLSTM,\n# #   train_epoch, evaluate_epoch, get_predictions_and_labels ...\n\n# # --- Main Ablation Loop ---\n# if __name__ == '__main__':\n#     # Load balanced training data and scalers\n#     try:\n#         with open(BALANCED_DATA_PKL_PATH, 'rb') as f:\n#             balanced_train_data_dict = pickle.load(f)\n#         with open(SCALERS_PKL_PATH, 'rb') as f:\n#             scalers = pickle.load(f)\n#     except Exception as e:\n#         print(f\"Error loading pre-processed data: {e}\")\n#         sys.exit(1)\n\n#     # Load PIE database\n#     if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n#         raise FileNotFoundError(\"PIE database cache not found.\")\n#     with open(PIE_DATABASE_CACHE_PATH, 'rb') as f:\n#         pie_database = pickle.load(f)\n\n#     # Prepare validation dataset once\n#     val_dataset = PIEDataset(\n#         pie_database, VAL_SETS_STR, POSE_DATA_DIR,\n#         SEQ_LEN, PRED_LEN, scalers,\n#         ALL_POSSIBLE_STREAMS\n#     )\n#     val_loader = DataLoader(\n#         val_dataset, batch_size=BATCH_SIZE,\n#         shuffle=False, num_workers=2, pin_memory=True\n#     )\n\n#     results_summary = {}\n\n#     for streams in STREAM_COMBINATIONS_TO_TEST:\n#         print(f\"\\n=== Streams: {streams} ===\")\n#         # Create train dataset for this combo\n#         train_ds = BalancedDataset(\n#             balanced_train_data_dict, streams, label_key='label'\n#         )\n#         train_loader = DataLoader(\n#             train_ds, batch_size=BATCH_SIZE,\n#             shuffle=True, num_workers=2, pin_memory=True\n#         )\n\n#         # Model, optimizer, loss\n#         input_sizes = {\n#             s: globals()[f\"INPUT_SIZE_{s.upper()}\"] for s in streams\n#         }\n#         model = MultiStreamAdaptiveLSTM(\n#             input_sizes, LSTM_HIDDEN_SIZE, NUM_LSTM_LAYERS,\n#             NUM_CLASSES, ATTENTION_DIM, DROPOUT_RATE, streams\n#         ).to(DEVICE)\n\n#         # Class weights\n#         labels = train_ds.labels.tolist()\n#         c0 = labels.count(0)\n#         c1 = labels.count(1)\n#         tot = len(labels)\n#         if tot > 0 and c0 and c1:\n#             w0 = tot / (2*c0)\n#             w1 = tot / (2*c1)\n#         else:\n#             w0 = w1 = 1.0\n#         criterion = nn.CrossEntropyLoss(weight=torch.tensor([w0,w1]).to(DEVICE))\n#         optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n#         best_val_f1 = -1.0\n#         best_model_path = None\n\n#         for ep in range(NUM_EPOCHS):\n#             train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n#             m = evaluate_epoch(model, val_loader, criterion, DEVICE)\n#             if m['f1'] > best_val_f1:\n#                 best_val_f1 = m['f1']\n#                 best_model_path = f\"best_{'_'.join(streams)}_ep{ep+1}.pth\"\n#                 torch.save(model.state_dict(), best_model_path)\n\n#         if best_model_path:\n#             model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n#         final_m = evaluate_epoch(model, val_loader, criterion, DEVICE)\n#         results_summary[tuple(streams)] = final_m\n\n#     # Print summary\n#     df = pd.DataFrame.from_dict(results_summary, orient='index')\n#     df.index = ['+'.join(k) for k in df.index]\n#     df = df.sort_values('f1', ascending=False)\n#     print(\"\\nAblation Results:\")\n#     print(df.to_markdown(floatfmt=\".4f\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T16:03:33.499749Z","iopub.execute_input":"2025-04-19T16:03:33.500117Z","iopub.status.idle":"2025-04-19T16:03:44.831455Z","shell.execute_reply.started":"2025-04-19T16:03:33.500087Z","shell.execute_reply":"2025-04-19T16:03:44.830341Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nLoading pose data for sets: ['set05', 'set06'] from /kaggle/input/pose-data/extracted_poses2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading Pose Sets:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d16a6bbd6e4e41ccbbe55f4e4c04cdb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set05:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set06:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Finished loading pose data for 2 sets.\nGenerating sequence list from PIE database for sets: ['set05', 'set06']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating Sequences:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d4fb65afa3e46768a7e39192952a776"}},"metadata":{}},{"name":"stdout","text":"Found 77288 valid sequences from 243 pedestrian tracks.\n\n=== Streams: ['bbox'] ===\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-768ec72862b4>\u001b[0m in \u001b[0;36m<cell line: 122>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"INPUT_SIZE_{s.upper()}\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         }\n\u001b[0;32m--> 167\u001b[0;31m         model = MultiStreamAdaptiveLSTM(\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0minput_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM_HIDDEN_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_LSTM_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mATTENTION_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROPOUT_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'MultiStreamAdaptiveLSTM' is not defined"],"ename":"NameError","evalue":"name 'MultiStreamAdaptiveLSTM' is not defined","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"# --- IMPORTS ---\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport xml.etree.ElementTree as ET\nimport os\nimport numpy as np\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_recall_fscore_support,\n    roc_auc_score,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n)\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm.notebook import tqdm\nimport random\nimport math\nimport zipfile\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport time\nimport sys\nimport gc  # garbage collector for cleanup calls\n\n# --- Add PIE utilities path if necessary (adjust path) ---\npie_utilities_path = '/kaggle/working/PIE/utilities'\nif pie_utilities_path not in sys.path:\n    sys.path.insert(0, pie_utilities_path)\ntry:\n    from pie_data import PIE\nexcept ImportError as e:\n    print(f\"Warning: Could not import PIE class from {pie_utilities_path}. Database must already exist. Error: {e}\")\n    PIE = None\n\n# --- Configuration ---\nPIE_ROOT_PATH = '/kaggle/working/PIE'\nVIDEO_INPUT_DIR = '/kaggle/input'\nPOSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2'\nPIE_DATABASE_CACHE_PATH = '/kaggle/input/pie-database/pie_database.pkl'\n\n# --- Define ALL possible streams (used by Dataset class) ---\nALL_POSSIBLE_STREAMS = [\n    'bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro',\n    'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context'\n]\nprint(f\"All possible streams: {ALL_POSSIBLE_STREAMS}\")\n\n# --- STREAM COMBINATIONS TO TEST FOR ABLATION STUDY ---\n\nSTREAM_COMBINATIONS_TO_TEST = [\n    ['bbox', 'ped_action', 'ped_look', 'ego_speed', 'ego_acc'], \n    ['traffic_light', 'bbox', 'pose', 'ped_action'],\n    ['bbox', 'ped_action', 'ped_look', 'ego_speed', 'ego_acc', 'traffic_light'],\n    ['bbox', 'ped_action', 'ped_look', 'ego_speed', 'ego_acc', 'static_context'],\n    ['bbox', 'ped_action', 'ped_look', 'ego_speed', 'ego_acc', 'ego_gyro'],\n    ['bbox', 'ped_action', 'ped_look', 'ego_speed', 'ego_acc', 'ped_occlusion'],\n    ['bbox', 'ped_action', 'ped_look', 'ego_speed', 'ego_acc', 'pose'],\n    ['bbox', 'ped_action', 'ped_look', 'ego_speed', 'ego_acc', 'traffic_light', 'static_context'],\n    ['bbox', 'ped_action', 'ped_look'], \n]\n\nprint(f\"Stream combinations to test: {STREAM_COMBINATIONS_TO_TEST}\")\n\n# --- Model Hyperparameters ---\nSEQ_LEN = 30\nPRED_LEN = 1\n\n# --- Input Sizes ---\nINPUT_SIZE_BBOX = 4\nINPUT_SIZE_POSE = 34\nINPUT_SIZE_EGO_SPEED = 1\nINPUT_SIZE_EGO_ACC = 2\nINPUT_SIZE_EGO_GYRO = 1\nINPUT_SIZE_PED_ACTION = 1\nINPUT_SIZE_PED_LOOK = 1\nINPUT_SIZE_PED_OCC = 1\nINPUT_SIZE_TL_STATE = 4\nNUM_SIGNALIZED_CATS = 4\nNUM_INTERSECTION_CATS = 5\nNUM_AGE_CATS = 4\nNUM_GENDER_CATS = 3\nNUM_TRAFFIC_DIR_CATS = 2\nLANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7: 4, 8: 4}\nNUM_LANE_CATS = len(set(LANE_CATEGORIES.values()))\nINPUT_SIZE_STATIC = (\n    NUM_SIGNALIZED_CATS\n    + NUM_INTERSECTION_CATS\n    + NUM_AGE_CATS\n    + NUM_GENDER_CATS\n    + NUM_TRAFFIC_DIR_CATS\n    + NUM_LANE_CATS\n)\n\nLSTM_HIDDEN_SIZE = 256\nNUM_LSTM_LAYERS = 2\nDROPOUT_RATE = 0.3\nNUM_CLASSES = 2\nATTENTION_DIM = 128\n\n# --- Training Hyperparameters ---\nLEARNING_RATE = 1e-4\nBATCH_SIZE = 32\nNUM_EPOCHS = 10\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\n# --- Dataset Splits ---\nTRAIN_SETS_STR = ['set01', 'set02', 'set04']\nVAL_SETS_STR = ['set05', 'set06']\n\n# --- Mappings ---\nTL_STATE_MAP = {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}\nNUM_TL_STATES = len(TL_STATE_MAP)\nSIGNALIZED_MAP = {'n/a': 0, 'C': 1, 'S': 2, 'CS': 3}\nINTERSECTION_MAP = {'midblock': 0, 'T': 1, 'T-left': 2, 'T-right': 3, 'four-way': 4}\nAGE_MAP = {'child': 0, 'young': 1, 'adult': 2, 'senior': 3}\nGENDER_MAP = {'n/a': 0, 'female': 1, 'male': 2}\nTRAFFIC_DIR_MAP = {'OW': 0, 'TW': 1}\n\n# --- Output Files (for intermediate balanced data/scalers) ---\nBALANCED_DATA_PKL_PATH = \"/kaggle/working/balanced_train_data.pkl\"\nSCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\"\n\n\n# --- Helper: One-Hot Encoding ---\ndef to_one_hot(index, num_classes):\n    vec = np.zeros(num_classes, dtype=np.float32)\n    safe_index = int(np.clip(index, 0, num_classes - 1))\n    vec[safe_index] = 1.0\n    return vec\n\n\n# --- Balancing Function ---\ndef balance_samples_count(seq_data, label_type, random_seed=42):\n    print('---------------------------------------------------------')\n    print(f\"Balancing samples based on '{label_type}' key\")\n    if label_type not in seq_data:\n        raise KeyError(f\"Label type '{label_type}' not found.\")\n    try:\n        gt_labels = [lbl[0] for lbl in seq_data[label_type]]\n    except (IndexError, TypeError):\n        raise ValueError(f\"Labels under '{label_type}' not in expected format [[label_val]].\")\n\n    if not all(l in [0, 1] for l in gt_labels):\n        print(f\"Warning: Labels for balancing contain values other than 0 or 1.\")\n\n    num_pos_samples = np.count_nonzero(np.array(gt_labels))\n    num_neg_samples = len(gt_labels) - num_pos_samples\n    new_seq_data = {}\n\n    if num_neg_samples == num_pos_samples:\n        print('Samples already balanced.')\n        return seq_data.copy()\n    else:\n        print(f'Unbalanced: Positive (1): {num_pos_samples} | Negative (0): {num_neg_samples}')\n        majority_label = 0 if num_neg_samples > num_pos_samples else 1\n        minority_count = min(num_neg_samples, num_pos_samples)\n        print(f\"Undersampling majority class ({majority_label}) to match count ({minority_count}).\")\n\n        majority_indices = np.where(np.array(gt_labels) == majority_label)[0]\n        minority_indices = np.where(np.array(gt_labels) != majority_label)[0]\n        np.random.seed(random_seed)\n        keep_majority_indices = np.random.choice(\n            majority_indices, size=minority_count, replace=False\n        )\n        final_indices = np.concatenate((minority_indices, keep_majority_indices))\n        np.random.shuffle(final_indices)\n\n        for k, v_list in seq_data.items():\n            if isinstance(v_list, list) and len(v_list) == len(gt_labels):\n                try:\n                    if v_list and isinstance(v_list[0], np.ndarray):\n                        v_array = np.array(v_list)\n                        new_seq_data[k] = list(v_array[final_indices])\n                    else:\n                        new_seq_data[k] = [v_list[i] for i in final_indices]\n                except Exception as e:\n                    print(f\"Error processing key '{k}' during balancing: {e}. Skip.\")\n                    new_seq_data[k] = []\n            else:\n                print(f\"Warn: Skipping key '{k}' in balancing (not list or len mismatch).\")\n                new_seq_data[k] = v_list\n\n        if label_type in new_seq_data:\n            new_gt_labels = [lbl[0] for lbl in new_seq_data[label_type]]\n            final_pos = np.count_nonzero(np.array(new_gt_labels))\n            final_neg = len(new_gt_labels) - final_pos\n            print(f'Balanced:   Positive (1): {final_pos} | Negative (0): {final_neg}')\n        else:\n            print(\"Error: Label key was lost during balancing process.\")\n\n        print('---------------------------------------------------------')\n        return new_seq_data\n\n\n# --- Dataset Class ---\nclass PIEDataset(Dataset):\n    def __init__(\n        self,\n        pie_database,\n        set_names,\n        pose_data_dir,\n        seq_len,\n        pred_len,\n        scalers=None,\n        streams_to_generate=None,\n    ):\n        self.pie_db = pie_database\n        self.set_names = set_names\n        self.pose_data_dir = pose_data_dir\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.scalers = scalers or {}\n        self.streams_to_generate = (\n            streams_to_generate or ALL_POSSIBLE_STREAMS\n        )  # Streams this instance CAN generate\n        self.sequences = []\n        self.all_pose_data = {}\n        self._input_sizes_for_error = self._get_input_sizes_dict()\n        if 'pose' in self.streams_to_generate:\n            self._load_pose_data()\n        self._generate_sequence_list()\n        if not self.sequences:\n            raise ValueError(f\"Dataset init failed: No sequences for {self.set_names}\")\n\n    def _get_input_sizes_dict(self):\n        input_sizes = {}\n        special_cases = {\n            'TRAFFIC_LIGHT': 'TL_STATE',\n            'STATIC_CONTEXT': 'STATIC',\n            'EGO_SPEED': 'EGO_SPEED',\n            'EGO_ACC': 'EGO_ACC',\n            'EGO_GYRO': 'EGO_GYRO',\n            'PED_ACTION': 'PED_ACTION',\n            'PED_LOOK': 'PED_LOOK',\n            'PED_OCCLUSION': 'PED_OCC',\n        }\n        for stream in ALL_POSSIBLE_STREAMS:\n            size_constant_name = f'INPUT_SIZE_{stream.upper()}'\n            stream_upper_key = stream.upper()\n            suffix = special_cases.get(stream_upper_key)\n            if suffix:\n                size_constant_name = f'INPUT_SIZE_{suffix}'\n            elif stream == 'bbox':\n                size_constant_name = 'INPUT_SIZE_BBOX'\n            elif stream == 'pose':\n                size_constant_name = 'INPUT_SIZE_POSE'\n\n            input_sizes[stream] = globals().get(size_constant_name, 1)\n        return input_sizes\n\n    def _load_pose_data(self):\n        print(f\"\\nLoading pose data for sets: {self.set_names} from {self.pose_data_dir}\")\n        sets_loaded_count = 0\n        for set_id in self.set_names:\n            self.all_pose_data[set_id] = {}\n            pose_set_path = os.path.join(self.pose_data_dir, set_id)\n            if not os.path.isdir(pose_set_path):\n                continue\n            pkl_files_in_set = [\n                f\n                for f in os.listdir(pose_set_path)\n                if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")\n            ]\n            if not pkl_files_in_set:\n                continue\n            loaded_video_count = 0\n            for pkl_filename in pkl_files_in_set:\n                pkl_file_path = os.path.join(pose_set_path, pkl_filename)\n                try:\n                    with open(pkl_file_path, 'rb') as f:\n                        loaded_pkl_content = pickle.load(f)\n                    if len(loaded_pkl_content) != 1:\n                        continue\n                    unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n                    video_id = \"_\".join(unique_video_key.split(\"_\")[1:])\n                    if video_id in self.pie_db.get(set_id, {}):\n                        self.all_pose_data[set_id][video_id] = video_data\n                        loaded_video_count += 1\n                except FileNotFoundError:\n                    pass\n                except Exception as e:\n                    print(f\"Error loading pose PKL {pkl_file_path}: {e}\")\n            if loaded_video_count > 0:\n                sets_loaded_count += 1\n        print(f\"Finished loading pose data for {sets_loaded_count} sets for this Dataset instance.\")\n\n    def _generate_sequence_list(self):\n        sequence_count = 0\n        for set_id in self.set_names:\n            if set_id not in self.pie_db:\n                continue\n            for video_id, video_data in self.pie_db[set_id].items():\n                if \"ped_annotations\" not in video_data:\n                    continue\n                for ped_id, ped_data in video_data[\"ped_annotations\"].items():\n                    if \"frames\" not in ped_data or len(ped_data[\"frames\"]) < self.seq_len + self.pred_len:\n                        continue\n                    sorted_frames = sorted(ped_data[\"frames\"])\n                    for i in range(len(sorted_frames) - self.seq_len - self.pred_len + 1):\n                        start_frame = sorted_frames[i]\n                        end_frame_observe = sorted_frames[i + self.seq_len - 1]\n                        if end_frame_observe - start_frame != self.seq_len - 1:\n                            continue\n                        target_frame_actual_idx = i + self.seq_len + self.pred_len - 1\n                        if target_frame_actual_idx >= len(sorted_frames):\n                            continue\n                        target_frame = sorted_frames[target_frame_actual_idx]\n                        if target_frame - end_frame_observe != self.pred_len:\n                            continue\n                        self.sequences.append((set_id, video_id, ped_id, start_frame))\n                        sequence_count += 1\n        print(f\"Dataset initialized with {sequence_count} sequences for sets {self.set_names}.\")\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        set_id, video_id, ped_id, start_frame = self.sequences[idx]\n        frame_nums = list(range(start_frame, start_frame + self.seq_len))\n        target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n\n        video_db = self.pie_db.get(set_id, {}).get(video_id, {})\n        ped_db = video_db.get(\"ped_annotations\", {}).get(ped_id, {})\n        ego_db = video_db.get(\"vehicle_annotations\", {})\n        traffic_db = video_db.get(\"traffic_annotations\", {})\n        ped_attributes = ped_db.get(\"attributes\", {})\n\n        feature_sequences = {s: [] for s in self.streams_to_generate}\n        label = 0\n        if \"frames\" in ped_db and \"behavior\" in ped_db and \"cross\" in ped_db[\"behavior\"]:\n            try:\n                tf_idx = ped_db[\"frames\"].index(target_frame_num)\n                label = ped_db[\"behavior\"][\"cross\"][tf_idx]\n                if label == -1:\n                    label = 0\n            except (ValueError, IndexError):\n                pass\n\n        static_vec = None\n        if \"static_context\" in self.streams_to_generate:\n            sig = ped_attributes.get(\"signalized\", 0)\n            inter = ped_attributes.get(\"intersection\", 0)\n            age = ped_attributes.get(\"age\", 2)\n            gen = ped_attributes.get(\"gender\", 0)\n            td = int(ped_attributes.get(\"traffic_direction\", 0))\n            nl = ped_attributes.get(\"num_lanes\", 2)\n            nl_idx = LANE_CATEGORIES.get(nl, list(LANE_CATEGORIES.values())[-1])\n            static_features = [\n                to_one_hot(sig, NUM_SIGNALIZED_CATS),\n                to_one_hot(inter, NUM_INTERSECTION_CATS),\n                to_one_hot(age, NUM_AGE_CATS),\n                to_one_hot(gen, NUM_GENDER_CATS),\n                to_one_hot(td, NUM_TRAFFIC_DIR_CATS),\n                to_one_hot(nl_idx, NUM_LANE_CATS),\n            ]\n            static_vec = np.concatenate(static_features)\n            if static_vec.shape[0] != INPUT_SIZE_STATIC:\n                static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n\n        for frame_num in frame_nums:\n            idx_in = -1\n            if \"frames\" in ped_db:\n                try:\n                    idx_in = ped_db[\"frames\"].index(frame_num)\n                except ValueError:\n                    pass\n            ego_f = ego_db.get(frame_num, {})\n\n            if \"bbox\" in self.streams_to_generate:\n                bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32)\n                if idx_in != -1 and \"bbox\" in ped_db and len(ped_db[\"bbox\"]) > idx_in:\n                    try:\n                        x1, y1, x2, y2 = ped_db[\"bbox\"][idx_in]\n                        w_img = video_db.get(\"width\", 1920)\n                        h_img = video_db.get(\"height\", 1080)\n                        if w_img > 0 and h_img > 0:\n                            cx = ((x1 + x2) / 2) / w_img\n                            cy = ((y1 + y2) / 2) / h_img\n                            w = (x2 - x1) / w_img\n                            h = (y2 - y1) / h_img\n                            if 0 <= cx <= 1 and 0 <= cy <= 1 and w > 0 and h > 0:\n                                bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n                    except Exception:\n                        pass\n                feature_sequences[\"bbox\"].append(bbox_norm)\n\n            if \"pose\" in self.streams_to_generate:\n                pose_v = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n                if set_id in self.all_pose_data and video_id in self.all_pose_data[set_id]:\n                    pdict = self.all_pose_data[set_id][video_id].get(frame_num, {})\n                    lp = pdict.get(ped_id)\n                    if isinstance(lp, np.ndarray) and lp.shape == (INPUT_SIZE_POSE,):\n                        pose_v = lp\n                feature_sequences[\"pose\"].append(pose_v)\n\n            if \"ego_speed\" in self.streams_to_generate:\n                sp = ego_f.get(\"OBD_speed\", 0.0) or ego_f.get(\"GPS_speed\", 0.0) or 0.0\n                sp_s = (sp - self.scalers.get(\"ego_speed_mean\", 0.0)) / self.scalers.get(\"ego_speed_std\", 1.0)\n                feature_sequences[\"ego_speed\"].append([sp_s])\n\n            if \"ego_acc\" in self.streams_to_generate:\n                ax = ego_f.get(\"accX\", 0.0)\n                ay = ego_f.get(\"accY\", 0.0)\n                ax_s = (ax - self.scalers.get(\"accX_mean\", 0.0)) / self.scalers.get(\"accX_std\", 1.0)\n                ay_s = (ay - self.scalers.get(\"accY_mean\", 0.0)) / self.scalers.get(\"accY_std\", 1.0)\n                feature_sequences[\"ego_acc\"].append([ax_s, ay_s])\n\n            if \"ego_gyro\" in self.streams_to_generate:\n                gz = ego_f.get(\"gyroZ\", 0.0)\n                gz_s = (gz - self.scalers.get(\"gyroZ_mean\", 0.0)) / self.scalers.get(\"gyroZ_std\", 1.0)\n                feature_sequences[\"ego_gyro\"].append([gz_s])\n\n            if \"ped_action\" in self.streams_to_generate:\n                ac = 0\n                if idx_in != -1 and \"behavior\" in ped_db and \"action\" in ped_db[\"behavior\"]:\n                    arr = ped_db[\"behavior\"][\"action\"]\n                    if len(arr) > idx_in:\n                        ac = arr[idx_in]\n                feature_sequences[\"ped_action\"].append([float(ac)])\n\n            if \"ped_look\" in self.streams_to_generate:\n                lk = 0\n                if idx_in != -1 and \"behavior\" in ped_db and \"look\" in ped_db[\"behavior\"]:\n                    arr = ped_db[\"behavior\"][\"look\"]\n                    if len(arr) > idx_in:\n                        lk = arr[idx_in]\n                feature_sequences[\"ped_look\"].append([float(lk)])\n\n            if \"ped_occlusion\" in self.streams_to_generate:\n                oc = 0.0\n                if idx_in != -1 and \"occlusion\" in ped_db:\n                    arr = ped_db[\"occlusion\"]\n                    if len(arr) > idx_in:\n                        oc = float(arr[idx_in]) / 2.0\n                feature_sequences[\"ped_occlusion\"].append([oc])\n\n            if \"traffic_light\" in self.streams_to_generate:\n                st = 0\n                for obj_id, od in traffic_db.items():\n                    if od.get(\"obj_class\") == \"traffic_light\" and \"frames\" in od and \"state\" in od:\n                        try:\n                            fi = od[\"frames\"].index(frame_num)\n                            sv = od[\"state\"][fi]\n                            if sv != 0:\n                                st = sv\n                                break\n                        except (ValueError, IndexError):\n                            continue\n                feature_sequences[\"traffic_light\"].append(to_one_hot(st, NUM_TL_STATES))\n\n            if \"static_context\" in self.streams_to_generate:\n                feature_sequences[\"static_context\"].append(\n                    static_vec\n                    if static_vec is not None\n                    else np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n                )\n\n        # Convert lists to tensors\n        features = {}\n        try:\n            for sn in self.streams_to_generate:\n                arr = np.array(feature_sequences[sn], dtype=np.float32)\n                features[sn] = torch.tensor(arr, dtype=torch.float32)\n        except Exception as e:\n            print(f\"Error converting features idx {idx}: {e}. Return dummy.\")\n            features = {\n                name: torch.zeros(\n                    (self.seq_len, self._input_sizes_for_error.get(name, 1)),\n                    dtype=torch.float32,\n                )\n                for name in self.streams_to_generate\n            }\n\n        return features, torch.tensor(label, dtype=torch.long)\n\n\n# --- Wrapper Dataset for Balanced Data ---\nclass BalancedDataset(Dataset):\n    def __init__(self, data_dict, active_streams, label_key='label'):\n        self.active_streams = active_streams\n        self.label_key = label_key\n        if label_key not in data_dict or not data_dict[label_key]:\n            raise ValueError(f\"Label key '{label_key}' missing/empty.\")\n        self.num_samples = len(data_dict[label_key])\n        if self.num_samples == 0:\n            print(\"Warning: BalancedDataset initialized with zero samples.\")\n        self.features = {}\n        for s in active_streams:\n            if s in data_dict and data_dict[s]:\n                self.features[s] = torch.tensor(\n                    np.array(data_dict[s]), dtype=torch.float32\n                )\n            else:\n                raise KeyError(f\"Stream '{s}' requested but missing/empty in balanced data.\")\n        try:\n            self.labels = torch.tensor(\n                [lbl[0] for lbl in data_dict[label_key]], dtype=torch.long\n            )\n        except (IndexError, TypeError) as e:\n            raise ValueError(f\"Error converting labels: {e}\")\n        for s in active_streams:\n            if len(self.features[s]) != self.num_samples:\n                raise ValueError(f\"Len mismatch: Stream '{s}' vs Labels\")\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        fd = {s: self.features[s][idx] for s in self.active_streams}\n        return fd, self.labels[idx]\n\n\n# --- Model Architecture ---\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.attention_net = nn.Sequential(\n            nn.Linear(hidden_dim, attention_dim),\n            nn.Tanh(),\n            nn.Linear(attention_dim, 1),\n        )\n\n    def forward(self, lstm_output):\n        scores = self.attention_net(lstm_output).squeeze(2)\n        weights = torch.softmax(scores, dim=1)\n        context = torch.sum(lstm_output * weights.unsqueeze(2), dim=1)\n        return context, weights\n\n\nclass MultiStreamAdaptiveLSTM(nn.Module):\n    def __init__(\n        self,\n        input_sizes,\n        lstm_hidden_size,\n        num_lstm_layers,\n        num_classes,\n        attention_dim,\n        dropout_rate,\n        stream_names=['bbox', 'pose'],\n    ):\n        super(MultiStreamAdaptiveLSTM, self).__init__()\n        if not stream_names:\n            raise ValueError(\"stream_names cannot be empty.\")\n        self.stream_names = stream_names\n        self.lstms = nn.ModuleDict()\n        self.attentions = nn.ModuleDict()\n        print(f\"Initializing model with streams: {self.stream_names}\")\n        for name in stream_names:\n            if name not in input_sizes:\n                raise KeyError(f\"Input size for stream '{name}' not provided.\")\n            insize = input_sizes[name]\n            print(f\"  - Adding stream '{name}' with input size {insize}\")\n            self.lstms[name] = nn.LSTM(\n                insize,\n                lstm_hidden_size,\n                num_lstm_layers,\n                batch_first=True,\n                dropout=dropout_rate if num_lstm_layers > 1 else 0,\n                bidirectional=True,\n            )\n            self.attentions[name] = Attention(lstm_hidden_size * 2, attention_dim)\n        nf = lstm_hidden_size * 2 * len(stream_names)\n        print(f\"  Combined feature dimension: {nf}\")\n        self.dropout = nn.Dropout(dropout_rate)\n        interm = max(num_classes * 4, nf // 2)\n        self.fc1 = nn.Linear(nf, interm)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(interm, num_classes)\n\n    def forward(self, x):\n        ctxs = []\n        for name in self.stream_names:\n            if name not in x:\n                print(f\"Warning: Stream '{name}' expected but missing.\")\n                continue\n            out, _ = self.lstms[name](x[name])\n            ctx, _ = self.attentions[name](out)\n            ctxs.append(ctx)\n        if not ctxs:\n            raise RuntimeError(\"No stream outputs generated.\")\n        f = torch.cat(ctxs, dim=1)\n        y = self.dropout(f)\n        y = self.relu(self.fc1(y))\n        y = self.dropout(y)\n        return self.fc2(y)\n\n\n# --- Training and Evaluation Functions ---\ndef train_epoch(model, loader, optim, crit, device):\n    model.train()\n    total, allp, allg = 0.0, [], []\n    for feats, labs in tqdm(loader, desc=\"Training\", leave=False):\n        inp = {n: feats[n].to(device) for n in model.stream_names if n in feats}\n        labs = labs.to(device)\n        optim.zero_grad()\n        out = model(inp)\n        loss = crit(out, labs)\n        loss.backward()\n        optim.step()\n        total += loss.item()\n        preds = out.argmax(1).cpu().numpy()\n        allp.extend(preds)\n        allg.extend(labs.cpu().numpy())\n    return total / max(1, len(loader)), accuracy_score(allg, allp)\n\n\ndef evaluate_epoch(model, loader, crit, device):\n    model.eval()\n    total, allg, allp, allpr = 0.0, [], [], []\n    with torch.no_grad():\n        for feats, labs in tqdm(loader, desc=\"Evaluating\", leave=False):\n            inp = {n: feats[n].to(device) for n in model.stream_names if n in feats}\n            labs = labs.to(device)\n            out = model(inp)\n            loss = crit(out, labs)\n            total += loss.item()\n            probs = torch.softmax(out, 1).cpu().numpy()\n            preds = probs.argmax(1)\n            allg.extend(labs.cpu().numpy())\n            allp.extend(preds)\n            allpr.extend(probs)\n    allg = np.array(allg)\n    allp = np.array(allp)\n    allpr = np.array(allpr)\n    acc = accuracy_score(allg, allp)\n    prec, rec, f1, _ = precision_recall_fscore_support(\n        allg, allp, average='binary', pos_label=1, zero_division=0\n    )\n    auc = roc_auc_score(allg, allpr[:, 1]) if len(np.unique(allg)) > 1 else float('nan')\n    return {'loss': total / max(1, len(loader)), 'accuracy': acc,\n            'precision': prec, 'recall': rec, 'f1': f1, 'auc': auc}\n\n\ndef get_predictions_and_labels(model, loader, device):\n    model.eval()\n    allg, allp = [], []\n    with torch.no_grad():\n        for feats, labs in tqdm(loader, desc=\"Generating CM Data\", leave=False):\n            inp = {n: feats[n].to(device) for n in model.stream_names if n in feats}\n            labs = labs.to(device)\n            out = model(inp)\n            preds = out.argmax(1)\n            allg.extend(labs.cpu().numpy())\n            allp.extend(preds.cpu().numpy())\n    return np.array(allg), np.array(allp)\n\n\n# --- Main Execution Block ---\nif __name__ == '__main__':\n    # --- Step 1: Data Preparation ---\n    print(\"--- Running Data Preparation ---\")\n    print(f\"Checking for PIE database cache at: {PIE_DATABASE_CACHE_PATH}\")\n    if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n        if PIE is None:\n            raise ImportError(\"PIE class not imported, cannot generate database.\")\n        print(\"PIE database cache not found. Generating...\")\n        pie_interface = PIE(data_path=PIE_ROOT_PATH, regen_database=True)\n        pie_database = pie_interface.generate_database()\n        if not pie_database:\n            raise RuntimeError(\"Failed to generate PIE database.\")\n        print(\"PIE database generated successfully.\")\n    else:\n        print(\"Loading PIE database from cache...\")\n        try:\n            with open(PIE_DATABASE_CACHE_PATH, 'rb') as f:\n                pie_database = pickle.load(f)\n            print(\"PIE database loaded successfully.\")\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load PIE database: {e}\")\n\n    # --- Calculate Standardization Parameters ---\n    print(\"\\nCalculating standardization parameters from training set...\")\n    all_train_ego_speeds, all_train_accX, all_train_accY, all_train_gyroZ = [], [], [], []\n    for sid in TRAIN_SETS_STR:\n        if sid in pie_database:\n            for vid, vdata in pie_database[sid].items():\n                va = vdata.get('vehicle_annotations', {})\n                for fn, ed in va.items():\n                    sp = ed.get('OBD_speed', 0.0) or ed.get('GPS_speed', 0.0) or 0.0\n                    all_train_ego_speeds.append(sp)\n                    all_train_accX.append(ed.get('accX', 0.0))\n                    all_train_accY.append(ed.get('accY', 0.0))\n                    all_train_gyroZ.append(ed.get('gyroZ', 0.0))\n    scalers = {}\n    if all_train_ego_speeds:\n        scalers['ego_speed_mean'] = np.mean(all_train_ego_speeds)\n        scalers['ego_speed_std'] = np.std(all_train_ego_speeds) or 1.0\n        print(f\"  Ego Speed: Mean={scalers['ego_speed_mean']:.2f}, Std={scalers['ego_speed_std']:.2f}\")\n    if all_train_accX:\n        scalers['accX_mean'] = np.mean(all_train_accX)\n        scalers['accX_std'] = np.std(all_train_accX) or 1.0\n        scalers['accY_mean'] = np.mean(all_train_accY)\n        scalers['accY_std'] = np.std(all_train_accY) or 1.0\n        print(f\"  Ego AccX: Mean={scalers['accX_mean']:.2f}, Std={scalers['accX_std']:.2f}\")\n        print(f\"  Ego AccY: Mean={scalers['accY_mean']:.2f}, Std={scalers['accY_std']:.2f}\")\n    if all_train_gyroZ:\n        scalers['gyroZ_mean'] = np.mean(all_train_gyroZ)\n        scalers['gyroZ_std'] = np.std(all_train_gyroZ) or 1.0\n        print(f\"  Ego GyroZ: Mean={scalers['gyroZ_mean']:.2f}, Std={scalers['gyroZ_std']:.2f}\")\n    print(\"Standardization parameters calculated.\")\n\n    # --- Initialize FULL Training Dataset ---\n    print(\"\\nInitializing full training dataset (for extraction)...\")\n    full_train_dataset = PIEDataset(\n        pie_database, TRAIN_SETS_STR, POSE_DATA_DIR, SEQ_LEN, PRED_LEN, scalers, ALL_POSSIBLE_STREAMS\n    )\n    if len(full_train_dataset) == 0:\n        raise ValueError(\"Full Train Dataset loading failed.\")\n\n    # --- Prepare and Balance Training Data ---\n    print(\"\\nExtracting ALL stream data from training set for balancing...\")\n    training_data_dict = {s: [] for s in ALL_POSSIBLE_STREAMS}\n    training_data_dict['label'] = []\n    for i in tqdm(range(len(full_train_dataset)), desc=\"Extracting data\"):\n        feats, lab = full_train_dataset[i]\n        for s in ALL_POSSIBLE_STREAMS:\n            arr = feats.get(s)\n            if arr is not None:\n                training_data_dict[s].append(arr.numpy())\n            else:\n                size = full_train_dataset._input_sizes_for_error.get(s, 1)\n                training_data_dict[s].append(np.zeros((SEQ_LEN, size), dtype=np.float32))\n        training_data_dict['label'].append([lab.item()])\n    print(f\"Original training samples: {len(training_data_dict['label'])}\")\n    del full_train_dataset\n    gc.collect()\n\n    balanced_train_data_dict = balance_samples_count(\n        training_data_dict, label_type='label'\n    )\n    del training_data_dict\n    gc.collect()\n\n    print(f\"\\nSaving balanced training data to: {BALANCED_DATA_PKL_PATH}\")\n    with open(BALANCED_DATA_PKL_PATH, 'wb') as f:\n        pickle.dump(balanced_train_data_dict, f, pickle.HIGHEST_PROTOCOL)\n    print(\" -> Balanced data saved.\")\n\n    print(f\"\\nSaving scalers to: {SCALERS_PKL_PATH}\")\n    with open(SCALERS_PKL_PATH, 'wb') as f:\n        pickle.dump(scalers, f, pickle.HIGHEST_PROTOCOL)\n    print(\" -> Scalers saved.\")\n\n    # --- Clean up and start Step 2 ---\n    del pie_database\n    gc.collect()\n\n    print(\"\\n--- Data Preparation and Balancing Completed ---\")\n    print(\"--- Ablation Study Starting Below ---\")\n    print(\"-\" * 70)\n\n    # --- Step 2: Ablation Study ---\n    print(f\"\\nLoading balanced training data from: {BALANCED_DATA_PKL_PATH}\")\n    print(f\"Loading scalers from: {SCALERS_PKL_PATH}\")\n    with open(BALANCED_DATA_PKL_PATH, 'rb') as f:\n        balanced_train_data_dict = pickle.load(f)\n    with open(SCALERS_PKL_PATH, 'rb') as f:\n        scalers = pickle.load(f)\n    print(\" -> Pre-processed data loaded successfully.\")\n\n    print(\"\\nLoading PIE database cache for Validation...\")\n    with open(PIE_DATABASE_CACHE_PATH, 'rb') as f:\n        pie_database = pickle.load(f)\n    print(\" -> PIE database loaded successfully.\")\n\n    print(\"\\nInitializing validation dataset...\")\n    val_dataset = PIEDataset(\n        pie_database, VAL_SETS_STR, POSE_DATA_DIR, SEQ_LEN, PRED_LEN, scalers, ALL_POSSIBLE_STREAMS\n    )\n    val_loader = DataLoader(\n        val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True\n    )\n    print(\"Validation dataset and loader created.\")\n    del pie_database\n    gc.collect()\n\n    # --- Ablation Study Loop ---\n    results_summary = {}\n\n    for streams in STREAM_COMBINATIONS_TO_TEST:\n        key = tuple(sorted(streams))\n        print(f\"\\n===== Experiment: Active Streams: {streams} =====\")\n\n        try:\n            curr_ds = BalancedDataset(balanced_train_data_dict, streams, label_key='label')\n        except Exception as e:\n            print(f\"Error creating BalancedDataset: {e}. Skipping.\")\n            continue\n        train_loader = DataLoader(\n            curr_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True\n        )\n\n        print(\"Initializing model...\")\n        curr_ins = {}\n        special = {\n            'TRAFFIC_LIGHT': 'TL_STATE',\n            'STATIC_CONTEXT': 'STATIC',\n            'EGO_SPEED': 'EGO_SPEED',\n            'EGO_ACC': 'EGO_ACC',\n            'EGO_GYRO': 'EGO_GYRO',\n            'PED_ACTION': 'PED_ACTION',\n            'PED_LOOK': 'PED_LOOK',\n            'PED_OCCLUSION': 'PED_OCC',\n        }\n        for s in streams:\n            name = f'INPUT_SIZE_{s.upper()}'\n            suf = special.get(s.upper(), None)\n            if suf:\n                name = f'INPUT_SIZE_{suf}'\n            elif s == 'bbox':\n                name = 'INPUT_SIZE_BBOX'\n            elif s == 'pose':\n                name = 'INPUT_SIZE_POSE'\n            curr_ins[s] = globals()[name]\n\n        model = MultiStreamAdaptiveLSTM(\n            input_sizes=curr_ins,\n            lstm_hidden_size=LSTM_HIDDEN_SIZE,\n            num_lstm_layers=NUM_LSTM_LAYERS,\n            num_classes=NUM_CLASSES,\n            attention_dim=ATTENTION_DIM,\n            dropout_rate=DROPOUT_RATE,\n            stream_names=streams,\n        ).to(DEVICE)\n        print(\n            f\"Model Initialized. Trainable Parameters: \"\n            f\"{sum(p.numel() for p in model.parameters() if p.requires_grad):,}\"\n        )\n\n        labs = curr_ds.labels.tolist()\n        c0, c1 = labs.count(0), labs.count(1)\n        tot = len(labs)\n        if tot == 0:\n            w0 = w1 = 1.0\n        elif c0 == 0:\n            w0, w1 = 0.0, 1.0\n        elif c1 == 0:\n            w0, w1 = 1.0, 0.0\n        else:\n            w0, w1 = tot / (2 * c0), tot / (2 * c1)\n        cw = torch.tensor([w0, w1], dtype=torch.float32).to(DEVICE)\n        crit = nn.CrossEntropyLoss(weight=cw)\n        opt = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n        best_f1 = -1.0\n        best_path = None\n\n        for ep in range(NUM_EPOCHS):\n            t_loss, t_acc = train_epoch(model, train_loader, opt, crit, DEVICE)\n            vmet = evaluate_epoch(model, val_loader, crit, DEVICE)\n            print(\n                f\"Epoch {ep+1}/{NUM_EPOCHS}: \"\n                f\"Train Loss={t_loss:.4f}, Train Acc={t_acc:.4f} | \"\n                f\"Val Loss={vmet['loss']:.4f}, Val Acc={vmet['accuracy']:.4f}, F1={vmet['f1']:.4f}\"\n            )\n            if vmet['f1'] > best_f1:\n                best_f1 = vmet['f1']\n                path = f\"best_model_{'_'.join(sorted(streams))}_ep{ep+1}.pth\"\n                torch.save(model.state_dict(), path)\n                best_path = path\n                print(f\"  → New best F1, saved to {path}\")\n\n        # final eval\n        if best_path:\n            model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n            final_met = evaluate_epoch(model, val_loader, crit, DEVICE)\n        else:\n            final_met = vmet\n\n        results_summary[key] = {\n            'Best Val F1': best_f1,\n            'Final F1': final_met['f1'],\n            'Final Precision': final_met['precision'],\n            'Final Recall': final_met['recall'],\n            'Final Accuracy': final_met['accuracy'],\n            'Final AUC': final_met['auc'],\n            'Final Loss': final_met['loss'],\n        }\n        print(f\"Results for {streams}: {results_summary[key]}\")\n\n        del model, opt, crit, train_loader, curr_ds\n        gc.collect()\n        torch.cuda.empty_cache()\n        time.sleep(1)\n\n    # --- Print Summary Table ---\n    print(\"\\n--- Ablation Study Summary ---\")\n    if results_summary:\n        # Filter out keys that are not tuples before creating DataFrame\n        valid_results = {k: v for k, v in results_summary.items() if isinstance(k, tuple)}\n        if not valid_results:\n             print(\"No valid experiment results found in summary dictionary.\")\n        else:\n            summary_df = pd.DataFrame.from_dict(valid_results, orient='index')\n            # Format index - ensure keys are tuples of strings before joining\n            summary_df.index = [\", \".join(map(str, k)) for k in summary_df.index]\n            cols_order = ['Best Val F1', 'Final F1', 'Final Recall', 'Final Precision', 'Final Accuracy', 'Final AUC', 'Final Loss']\n            summary_df = summary_df[[col for col in cols_order if col in summary_df.columns]]\n            summary_df = summary_df.sort_values(by='Best Val F1', ascending=False)\n            print(summary_df.to_markdown(floatfmt=\".4f\"))\n    else:\n        print(\"No experiments were completed successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T22:16:45.043447Z","iopub.execute_input":"2025-04-19T22:16:45.043800Z","execution_failed":"2025-04-19T23:54:25.964Z"}},"outputs":[{"name":"stdout","text":"All possible streams: ['bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro', 'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context']\nStream combinations to test: [['ped_occlusion'], ['traffic_light'], ['static_context'], ['bbox'], ['pose'], ['ego_speed'], ['ego_acc'], ['ego_gyro'], ['ped_action'], ['ped_look'], ['ped_action', 'ped_look'], ['ego_speed', 'ego_acc'], ['ped_action', 'ped_look', 'ego_speed', 'ego_acc'], ['ped_action', 'ped_look', 'ego_speed', 'ego_acc', 'traffic_light', 'static_context']]\nUsing device: cuda\n--- Running Data Preparation ---\nChecking for PIE database cache at: /kaggle/input/pie-database/pie_database.pkl\nLoading PIE database from cache...\nPIE database loaded successfully.\n\nCalculating standardization parameters from training set...\n  Ego Speed: Mean=13.43, Std=13.31\n  Ego AccX: Mean=-0.03, Std=0.08\n  Ego AccY: Mean=-0.52, Std=0.85\n  Ego GyroZ: Mean=-0.04, Std=4.48\nStandardization parameters calculated.\n\nInitializing full training dataset (for extraction)...\n\nLoading pose data for sets: ['set01', 'set02', 'set04'] from /kaggle/input/pose-data/extracted_poses2\nFinished loading pose data for 3 sets for this Dataset instance.\nDataset initialized with 333454 sequences for sets ['set01', 'set02', 'set04'].\n\nExtracting ALL stream data from training set for balancing...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting data:   0%|          | 0/333454 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce755fe76a8e4599ba55b81b8dbe05aa"}},"metadata":{}},{"name":"stdout","text":"Original training samples: 333454\n---------------------------------------------------------\nBalancing samples based on 'label' key\nUnbalanced: Positive (1): 54967 | Negative (0): 278487\nUndersampling majority class (0) to match count (54967).\nBalanced:   Positive (1): 54967 | Negative (0): 54967\n---------------------------------------------------------\n\nSaving balanced training data to: /kaggle/working/balanced_train_data.pkl\n -> Balanced data saved.\n\nSaving scalers to: /kaggle/working/scalers.pkl\n -> Scalers saved.\n\n--- Data Preparation and Balancing Completed ---\n--- Ablation Study Starting Below ---\n----------------------------------------------------------------------\n\nLoading balanced training data from: /kaggle/working/balanced_train_data.pkl\nLoading scalers from: /kaggle/working/scalers.pkl\n -> Pre-processed data loaded successfully.\n\nLoading PIE database cache for Validation...\n -> PIE database loaded successfully.\n\nInitializing validation dataset...\n\nLoading pose data for sets: ['set05', 'set06'] from /kaggle/input/pose-data/extracted_poses2\nFinished loading pose data for 2 sets for this Dataset instance.\nDataset initialized with 77288 sequences for sets ['set05', 'set06'].\nValidation dataset and loader created.\n\n===== Experiment: Active Streams: ['ped_occlusion'] =====\nInitializing model...\nInitializing model with streams: ['ped_occlusion']\n  - Adding stream 'ped_occlusion' with input size 1\n  Combined feature dimension: 512\nModel Initialized. Trainable Parameters: 2,305,027\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1/10: Train Loss=0.6827, Train Acc=0.5581 | Val Loss=0.6948, Val Acc=0.3958, F1=0.3081\n  → New best F1, saved to best_model_ped_occlusion_ep1.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2/10: Train Loss=0.6807, Train Acc=0.5636 | Val Loss=0.6475, Val Acc=0.4618, F1=0.3236\n  → New best F1, saved to best_model_ped_occlusion_ep2.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3/10: Train Loss=0.6748, Train Acc=0.5714 | Val Loss=0.6446, Val Acc=0.4850, F1=0.3286\n  → New best F1, saved to best_model_ped_occlusion_ep3.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4/10: Train Loss=0.6738, Train Acc=0.5715 | Val Loss=0.6378, Val Acc=0.4854, F1=0.3273\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5/10: Train Loss=0.6727, Train Acc=0.5731 | Val Loss=0.6486, Val Acc=0.4852, F1=0.3280\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6/10: Train Loss=0.6722, Train Acc=0.5739 | Val Loss=0.6520, Val Acc=0.4659, F1=0.3238\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7/10: Train Loss=0.6717, Train Acc=0.5742 | Val Loss=0.6531, Val Acc=0.4769, F1=0.3261\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8/10: Train Loss=0.6714, Train Acc=0.5744 | Val Loss=0.6460, Val Acc=0.4759, F1=0.3250\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9/10: Train Loss=0.6709, Train Acc=0.5755 | Val Loss=0.6322, Val Acc=0.4881, F1=0.3256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10/10: Train Loss=0.6706, Train Acc=0.5748 | Val Loss=0.6490, Val Acc=0.4784, F1=0.3263\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-2-a3659cfa0bfe>:875: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Results for ['ped_occlusion']: {'Best Val F1': 0.32855913760817856, 'Final F1': 0.32855913760817856, 'Final Precision': 0.20516169809333193, 'Final Recall': 0.8244158482898747, 'Final Accuracy': 0.4850300175965221, 'Final AUC': 0.6442339470963601, 'Final Loss': 0.6446443637031198}\n\n===== Experiment: Active Streams: ['traffic_light'] =====\nInitializing model...\nInitializing model with streams: ['traffic_light']\n  - Adding stream 'traffic_light' with input size 4\n  Combined feature dimension: 512\nModel Initialized. Trainable Parameters: 2,311,171\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1/10: Train Loss=0.6810, Train Acc=0.5703 | Val Loss=0.6240, Val Acc=0.8360, F1=0.4851\n  → New best F1, saved to best_model_traffic_light_ep1.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2/10: Train Loss=0.6796, Train Acc=0.5727 | Val Loss=0.6464, Val Acc=0.8363, F1=0.4856\n  → New best F1, saved to best_model_traffic_light_ep2.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1dff38521df44959c9c0ed93de8a5c0"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# --- IMPORTS ---------------------------------------------------------------\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader        # keep DataLoader around\nimport xml.etree.ElementTree as ET\nimport os\nimport numpy as np\nfrom tqdm.notebook import tqdm                         # progress-bar\nimport random, math, zipfile, cv2, pandas as pd\nimport matplotlib.pyplot as plt, seaborn as sns\nimport pickle, time, sys, gc\n\n# ---------------------------------------------------------------------------\n# Optional PIE utilities path (not required for visualisation-only script)\n# pie_utilities_path = '/kaggle/working/PIE/utilities'\n# if pie_utilities_path not in sys.path:\n#     sys.path.insert(0, pie_utilities_path)\n# try:\n#     from pie_data import PIE\n# except ImportError:\n#     PIE = None\n# ---------------------------------------------------------------------------\n\n# ------------------------------- Configuration -----------------------------\nVIDEO_ID_TO_PROCESS = \"video_0001\"\nSET_ID_TO_PROCESS   = \"set01\"\n\n# ------------------------------- Paths ------------------------------------\nIMAGE_FRAME_DIR         = \"/kaggle/input/sample-images2/Frames\"\nPIE_DATABASE_CACHE_PATH = \"/kaggle/input/pie-database/pie_database.pkl\"\nSCALERS_PKL_PATH        = \"/kaggle/working/scalers.pkl\"\nMODEL_PATH              = (\"/kaggle/input/best_model_ablation/pytorch/default/1/\"\n                           \"best_model_bbox_ego_acc_ego_speed_ped_action_ped_look_static_context_ep1.pth\")\nOUTPUT_VIDEO_PATH       = f\"/kaggle/working/{SET_ID_TO_PROCESS}_{VIDEO_ID_TO_PROCESS}_predictions.mp4\"\n\nPOSE_DATA_DIR = \"/kaggle/input/sample-poses\"   # <-- NEW: required for pose loading\n# ---------------------------------------------------------------------------\n\n# *** Define the streams this specific model was trained with ***\nMODEL_ACTIVE_STREAMS = [\n    \"bbox\",\n    \"ego_speed\",\n    \"ego_acc\",\n    \"ped_action\",\n    \"ped_look\",\n    \"static_context\",\n]\nprint(f\"Model uses Active Streams: {MODEL_ACTIVE_STREAMS}\")\n\n# -------------------- Model & dataset hyper-parameters --------------------\nSEQ_LEN = 30\nPRED_LEN = 1          # prediction at end of sequence\n\nINPUT_SIZE_BBOX       = 4\nINPUT_SIZE_POSE       = 34\nINPUT_SIZE_EGO_SPEED  = 1\nINPUT_SIZE_EGO_ACC    = 2\nINPUT_SIZE_EGO_GYRO   = 1\nINPUT_SIZE_PED_ACTION = 1\nINPUT_SIZE_PED_LOOK   = 1\nINPUT_SIZE_PED_OCC    = 1\nINPUT_SIZE_TL_STATE   = 4\n\nNUM_SIGNALIZED_CATS   = 4\nNUM_INTERSECTION_CATS = 5\nNUM_AGE_CATS          = 4\nNUM_GENDER_CATS       = 3\nNUM_TRAFFIC_DIR_CATS  = 2\nLANE_CATEGORIES       = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7: 4, 8: 4}\nNUM_LANE_CATS         = len(set(LANE_CATEGORIES.values()))\nINPUT_SIZE_STATIC     = (NUM_SIGNALIZED_CATS + NUM_INTERSECTION_CATS + NUM_AGE_CATS\n                         + NUM_GENDER_CATS + NUM_TRAFFIC_DIR_CATS + NUM_LANE_CATS)\n\nLSTM_HIDDEN_SIZE = 256\nNUM_LSTM_LAYERS  = 2\nDROPOUT_RATE     = 0.3\nNUM_CLASSES      = 2\nATTENTION_DIM    = 128\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\n# ----------------------------- label maps ---------------------------------\nTL_STATE_MAP    = {\"__undefined__\": 0, \"red\": 1, \"yellow\": 2, \"green\": 3}\nNUM_TL_STATES   = len(TL_STATE_MAP)\nSIGNALIZED_MAP  = {\"n/a\": 0, \"C\": 1, \"S\": 2, \"CS\": 3}\nINTERSECTION_MAP= {\"midblock\": 0, \"T\": 1, \"T-left\": 2, \"T-right\": 3, \"four-way\": 4}\nAGE_MAP         = {\"child\": 0, \"young\": 1, \"adult\": 2, \"senior\": 3}\nGENDER_MAP      = {\"n/a\": 0, \"female\": 1, \"male\": 2}\nTRAFFIC_DIR_MAP = {\"OW\": 0, \"TW\": 1}\n\n# ----------------------------- utilities ----------------------------------\ndef to_one_hot(index: int, num_classes: int) -> np.ndarray:\n    vec = np.zeros(num_classes, dtype=np.float32)\n    vec[int(np.clip(index, 0, num_classes - 1))] = 1.0\n    return vec\n\nALL_POSSIBLE_STREAMS = [\n    \"bbox\", \"pose\", \"ego_speed\", \"ego_acc\", \"ego_gyro\",\n    \"ped_action\", \"ped_look\", \"ped_occlusion\", \"traffic_light\", \"static_context\",\n]\n\n# ==========================================================================#\n#                                DATASET                                    #\n# ==========================================================================#\nclass PIEDatasetInference:\n    def __init__(self, pie_database: dict, set_names: list[str],\n                 pose_data_dir: str, scalers: dict | None = None):\n        self.pie_db    = pie_database\n        self.set_names = set_names\n        self.pose_data_dir = pose_data_dir\n        self.scalers   = scalers or {}\n        self.all_pose_data = {}\n        self._input_sizes_for_error = self._get_input_sizes_dict()\n        self._load_pose_data()\n\n    # ----------------------------------------------------------------------\n    def _get_input_sizes_dict(self) -> dict[str, int]:\n        input_sizes, special = {}, {\n            \"TRAFFIC_LIGHT\": \"TL_STATE\",\n            \"STATIC_CONTEXT\": \"STATIC\",\n            \"EGO_SPEED\": \"EGO_SPEED\",\n            \"EGO_ACC\": \"EGO_ACC\",\n            \"EGO_GYRO\": \"EGO_GYRO\",\n            \"PED_ACTION\": \"PED_ACTION\",\n            \"PED_LOOK\": \"PED_LOOK\",\n            \"PED_OCCLUSION\": \"PED_OCC\",\n        }\n        for stream in ALL_POSSIBLE_STREAMS:\n            const_name = f\"INPUT_SIZE_{stream.upper()}\"\n            suffix     = special.get(stream.upper())\n            if suffix:\n                const_name = f\"INPUT_SIZE_{suffix}\"\n            elif stream == \"bbox\":\n                const_name = \"INPUT_SIZE_BBOX\"\n            elif stream == \"pose\":\n                const_name = \"INPUT_SIZE_POSE\"\n            input_sizes[stream] = globals().get(const_name, 1)\n        return input_sizes\n\n    # ----------------------------------------------------------------------\n    def _load_pose_data(self):\n        print(\"Loading pose data (if available)...\")\n        sets_loaded = 0\n        for set_id in self.set_names:\n            self.all_pose_data[set_id] = {}\n            pose_set_path = os.path.join(self.pose_data_dir, set_id)\n            if not os.path.isdir(pose_set_path):\n                continue\n\n            pkl_files = [f for f in os.listdir(pose_set_path)\n                         if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")]\n\n            loaded = 0\n            for pkl_name in pkl_files:\n                pkl_path = os.path.join(pose_set_path, pkl_name)\n                try:\n                    with open(pkl_path, \"rb\") as f:\n                        loaded_pkl = pickle.load(f)\n                    if len(loaded_pkl) != 1:\n                        continue\n                    video_key, video_data = list(loaded_pkl.items())[0]\n                    video_id = \"_\".join(video_key.split(\"_\")[1:])\n                    if video_id in self.pie_db.get(set_id, {}):\n                        self.all_pose_data[set_id][video_id] = video_data\n                        loaded += 1\n                except Exception as e:\n                    print(f\"Pose load error {pkl_path}: {e}\")\n            if loaded:\n                sets_loaded += 1\n        print(f\"Finished loading pose data for {sets_loaded} set(s).\")\n\n    # ----------------------------------------------------------------------\n    def get_feature_sequence(\n        self,\n        set_id: str,\n        video_id: str,\n        ped_id: str,\n        end_frame_num: int,\n        seq_len: int,\n        streams_needed: list[str],\n    ):\n        \"\"\"Return {stream: tensor[1, seq_len, dim]} or None if invalid.\"\"\"\n        frames = list(range(end_frame_num - seq_len + 1, end_frame_num + 1))\n        if frames[0] < 0:\n            return None\n\n        video_db   = self.pie_db.get(set_id, {}).get(video_id, {})\n        ped_db     = video_db.get(\"ped_annotations\", {}).get(ped_id, {})\n        ego_db     = video_db.get(\"vehicle_annotations\", {})\n        traffic_db = video_db.get(\"traffic_annotations\", {})\n        ped_attr   = ped_db.get(\"attributes\", {})\n\n        feats = {s: [] for s in streams_needed}\n        valid = True\n\n        # static-context vector (once per sequence)\n        static_vec = None\n        if \"static_context\" in streams_needed:\n            sig_idx = ped_attr.get(\"signalized\", 0)\n            int_idx = ped_attr.get(\"intersection\", 0)\n            age_idx = ped_attr.get(\"age\", 2)\n            gen_idx = ped_attr.get(\"gender\", 0)\n            td_idx  = int(ped_attr.get(\"traffic_direction\", 0))\n            nl_cat  = LANE_CATEGORIES.get(\n                ped_attr.get(\"num_lanes\", 2),\n                LANE_CATEGORIES[max(LANE_CATEGORIES)],\n            )\n            static_vec = np.concatenate([\n                to_one_hot(sig_idx, NUM_SIGNALIZED_CATS),\n                to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n                to_one_hot(age_idx, NUM_AGE_CATS),\n                to_one_hot(gen_idx, NUM_GENDER_CATS),\n                to_one_hot(td_idx, NUM_TRAFFIC_DIR_CATS),\n                to_one_hot(nl_cat, NUM_LANE_CATS),\n            ])\n            if static_vec.size != INPUT_SIZE_STATIC:\n                static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n\n        # ----------------------- per-frame extraction ----------------------\n        for f in frames:\n            # index of this frame in pedestrian’s annotation list\n            if \"frames\" in ped_db:\n                try:\n                    frame_db_idx = ped_db[\"frames\"].index(f)\n                except ValueError:\n                    valid = False\n                    break\n            else:\n                valid = False\n                break\n\n            ego_frame = ego_db.get(f)\n\n            if ego_frame is None and any(s in streams_needed\n                                         for s in (\"ego_speed\", \"ego_acc\", \"ego_gyro\")):\n                valid = False\n                break\n\n            # ---------- bbox ----------\n            if \"bbox\" in streams_needed:\n                bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32)\n                if \"bbox\" in ped_db and len(ped_db[\"bbox\"]) > frame_db_idx:\n                    try:\n                        x1, y1, x2, y2 = ped_db[\"bbox\"][frame_db_idx]\n                        img_w = video_db.get(\"width\", 1920)\n                        img_h = video_db.get(\"height\", 1080)\n                        if img_w > 0 and img_h > 0:\n                            cx = (x1 + x2) / 2 / img_w\n                            cy = (y1 + y2) / 2 / img_h\n                            w  = (x2 - x1) / img_w\n                            h  = (y2 - y1) / img_h\n                            if (\n                                w > 0\n                                and h > 0\n                                and 0 <= cx <= 1\n                                and 0 <= cy <= 1\n                            ):\n                                bbox_norm = np.array([cx, cy, w, h], np.float32)\n                    except Exception:\n                        pass\n                feats[\"bbox\"].append(bbox_norm)\n\n            # ---------- pose ----------\n            if \"pose\" in streams_needed:\n                pose_vec = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n                vid_pose = self.all_pose_data.get(set_id, {}).get(video_id, {})\n                frame_pose = vid_pose.get(f, {})\n                loaded_pose = frame_pose.get(ped_id)\n                if (loaded_pose is not None\n                        and isinstance(loaded_pose, np.ndarray)\n                        and loaded_pose.shape == (INPUT_SIZE_POSE,)):\n                    pose_vec = loaded_pose\n                feats[\"pose\"].append(pose_vec)\n\n            # ---------- ego streams ----------\n            if \"ego_speed\" in streams_needed:\n                speed = ego_frame.get(\"OBD_speed\", 0.0)\n                if speed == 0.0:\n                    speed = ego_frame.get(\"GPS_speed\", 0.0)\n                speed_scaled = ((speed - self.scalers.get(\"ego_speed_mean\", 0.0))\n                                / self.scalers.get(\"ego_speed_std\", 1.0))\n                feats[\"ego_speed\"].append([speed_scaled])\n\n            if \"ego_acc\" in streams_needed:\n                acc_x = ego_frame.get(\"accX\", 0.0)\n                acc_y = ego_frame.get(\"accY\", 0.0)\n                feats[\"ego_acc\"].append([\n                    (acc_x - self.scalers.get(\"accX_mean\", 0.0))\n                    / self.scalers.get(\"accX_std\", 1.0),\n                    (acc_y - self.scalers.get(\"accY_mean\", 0.0))\n                    / self.scalers.get(\"accY_std\", 1.0),\n                ])\n\n            if \"ego_gyro\" in streams_needed:\n                gyro_z = ego_frame.get(\"gyroZ\", 0.0)\n                gyro_z_scaled = ((gyro_z - self.scalers.get(\"gyroZ_mean\", 0.0))\n                                 / self.scalers.get(\"gyroZ_std\", 1.0))\n                feats[\"ego_gyro\"].append([gyro_z_scaled])\n\n            # ---------- pedestrian behaviour ----------\n            if \"ped_action\" in streams_needed:\n                action_val = 0\n                if (\n                    \"behavior\" in ped_db\n                    and \"action\" in ped_db[\"behavior\"]\n                    and len(ped_db[\"behavior\"][\"action\"]) > frame_db_idx\n                ):\n                    action_val = ped_db[\"behavior\"][\"action\"][frame_db_idx]\n                feats[\"ped_action\"].append([float(action_val)])\n\n            if \"ped_look\" in streams_needed:\n                look_val = 0\n                if (\n                    \"behavior\" in ped_db\n                    and \"look\" in ped_db[\"behavior\"]\n                    and len(ped_db[\"behavior\"][\"look\"]) > frame_db_idx\n                ):\n                    look_val = ped_db[\"behavior\"][\"look\"][frame_db_idx]\n                feats[\"ped_look\"].append([float(look_val)])\n\n            if \"ped_occlusion\" in streams_needed:\n                occ = 0.0\n                if \"occlusion\" in ped_db and len(ped_db[\"occlusion\"]) > frame_db_idx:\n                    occ_val = ped_db[\"occlusion\"][frame_db_idx]\n                    occ = float(occ_val) / 2.0\n                feats[\"ped_occlusion\"].append([occ])\n\n            # ---------- traffic light ----------\n            if \"traffic_light\" in streams_needed:\n                state_int = 0\n                for obj_id, obj_data in traffic_db.items():\n                    if (\n                        obj_data.get(\"obj_class\") == \"traffic_light\"\n                        and \"frames\" in obj_data\n                        and \"state\" in obj_data\n                    ):\n                        try:\n                            tl_idx   = obj_data[\"frames\"].index(f)\n                            state_val = obj_data[\"state\"][tl_idx]\n                            if state_val != 0:\n                                state_int = state_val\n                                break\n                        except (ValueError, IndexError):\n                            continue\n                feats[\"traffic_light\"].append(to_one_hot(state_int, NUM_TL_STATES))\n\n            # ---------- static context ----------\n            if \"static_context\" in streams_needed:\n                feats[\"static_context\"].append(\n                    static_vec if static_vec is not None\n                    else np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n                )\n\n        # ------------------------------------------------------------------\n        if not valid:\n            return None\n\n        try:\n            out = {}\n            for name in streams_needed:\n                tensor = torch.tensor(\n                    np.array(feats[name], dtype=np.float32),\n                    dtype=torch.float32,\n                ).unsqueeze(0)                           # add batch dim\n                exp_shape = (1, seq_len,\n                             self._input_sizes_for_error.get(name, 1))\n                if tensor.shape != exp_shape:\n                    print(f\"Shape mismatch {name}: {tensor.shape} vs {exp_shape}\")\n                    return None\n                out[name] = tensor\n            return out\n        except Exception as e:\n            print(f\"Sequence→tensor error: {e}\")\n            return None\n\n# ==========================================================================#\n#                          MODEL ARCHITECTURE                               #\n# ==========================================================================#\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim: int, att_dim: int):\n        super().__init__()\n        # must be called attention_net so the saved keys match:\n        self.attention_net = nn.Sequential(\n            nn.Linear(hidden_dim, att_dim),\n            nn.Tanh(),\n            nn.Linear(att_dim, 1),\n        )\n\n    def forward(self, lstm_out: torch.Tensor):\n        # lstm_out: [B, T, 2H]\n        att_scores  = self.attention_net(lstm_out).squeeze(2)   # [B, T]\n        att_weights = torch.softmax(att_scores, dim=1)          # [B, T]\n        ctx         = torch.sum(lstm_out * att_weights.unsqueeze(2), dim=1)\n        return ctx, att_weights\n\n\nclass MultiStreamAdaptiveLSTM(nn.Module):\n    def __init__(\n        self,\n        input_sizes: dict[str, int],\n        lstm_hidden_size: int,\n        num_lstm_layers: int,\n        num_classes: int,\n        attention_dim: int,\n        dropout_rate: float,\n        stream_names: list[str] | None = None,\n    ):\n        super().__init__()\n        if not stream_names:\n            raise ValueError(\"stream_names cannot be empty\")\n\n        self.stream_names = list(stream_names)\n        self.lstms       = nn.ModuleDict()\n        self.attentions  = nn.ModuleDict()\n\n        for name in self.stream_names:\n            if name not in input_sizes:\n                raise KeyError(f\"Missing input size for stream '{name}'\")\n            self.lstms[name] = nn.LSTM(\n                input_sizes[name],\n                lstm_hidden_size,\n                num_lstm_layers,\n                batch_first=True,\n                dropout=dropout_rate if num_lstm_layers > 1 else 0.0,\n                bidirectional=True,\n            )\n            self.attentions[name] = Attention(lstm_hidden_size * 2, attention_dim)\n\n        combined_dim = lstm_hidden_size * 2 * len(self.stream_names)\n        inter_dim    = max(num_classes * 4, combined_dim // 2)\n\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc1     = nn.Linear(combined_dim, inter_dim)\n        self.relu    = nn.ReLU()\n        self.fc2     = nn.Linear(inter_dim, num_classes)\n\n    def forward(self, x_dict: dict[str, torch.Tensor]):\n        ctx_list = []\n        for name in self.stream_names:\n            if name not in x_dict:\n                print(f\"Warning: missing stream '{name}' in forward()\")\n                continue\n            lstm_out, _ = self.lstms[name](x_dict[name])\n            ctx, _      = self.attentions[name](lstm_out)\n            ctx_list.append(ctx)\n\n        if not ctx_list:\n            raise RuntimeError(\"No stream outputs generated\")\n\n        fused = torch.cat(ctx_list, dim=1)\n        out   = self.dropout(fused)\n        out   = self.relu(self.fc1(out))\n        out   = self.dropout(out)\n        logits= self.fc2(out)\n        return logits\n\n# ==========================================================================#\n#                              MAIN SCRIPT                                  #\n# ==========================================================================#\nif __name__ == \"__main__\":\n\n    # --------------------- load PIE database ------------------------------\n    print(\"\\nLoading PIE database cache …\")\n    if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n        raise FileNotFoundError(\"PIE database cache not found.\")\n    with open(PIE_DATABASE_CACHE_PATH, \"rb\") as f:\n        pie_database = pickle.load(f)\n    print(\" -> database loaded.\")\n\n    # --------------------- load scalers -----------------------------------\n    print(f\"\\nLoading scalers from {SCALERS_PKL_PATH}\")\n    try:\n        with open(SCALERS_PKL_PATH, \"rb\") as f:\n            scalers = pickle.load(f)\n        print(\" -> scalers loaded.\")\n    except FileNotFoundError:\n        sys.exit(f\"ERROR: Scalers file not found at {SCALERS_PKL_PATH}\")\n\n    # --------------------- helper dataset ---------------------------------\n    print(\"\\nInitialising dataset helper …\")\n    dataset_helper = PIEDatasetInference(\n        pie_database, [SET_ID_TO_PROCESS], POSE_DATA_DIR, scalers\n    )\n    print(\"Helper dataset ready.\")\n\n    # --------------------- build model ------------------------------------\n    print(\"\\nBuilding model …\")\n    input_sizes = {}\n    special = {\n        \"TRAFFIC_LIGHT\": \"TL_STATE\",\n        \"STATIC_CONTEXT\": \"STATIC\",\n        \"EGO_SPEED\": \"EGO_SPEED\",\n        \"EGO_ACC\": \"EGO_ACC\",\n        \"EGO_GYRO\": \"EGO_GYRO\",\n        \"PED_ACTION\": \"PED_ACTION\",\n        \"PED_LOOK\": \"PED_LOOK\",\n        \"PED_OCCLUSION\": \"PED_OCC\",\n    }\n    for s in MODEL_ACTIVE_STREAMS:\n        cname = f\"INPUT_SIZE_{s.upper()}\"\n        suff  = special.get(s.upper())\n        if suff:\n            cname = f\"INPUT_SIZE_{suff}\"\n        elif s == \"bbox\":\n            cname = \"INPUT_SIZE_BBOX\"\n        elif s == \"pose\":\n            cname = \"INPUT_SIZE_POSE\"\n        if cname not in globals():\n            raise ValueError(f\"Missing constant {cname}\")\n        input_sizes[s] = globals()[cname]\n\n    model = MultiStreamAdaptiveLSTM(\n        input_sizes,\n        LSTM_HIDDEN_SIZE,\n        NUM_LSTM_LAYERS,\n        NUM_CLASSES,\n        ATTENTION_DIM,\n        DROPOUT_RATE,\n        MODEL_ACTIVE_STREAMS,\n    ).to(DEVICE)\n\n    # --------------------- load weights -----------------------------------\n    print(f\"\\nLoading weights from {MODEL_PATH}\")\n    if not os.path.exists(MODEL_PATH):\n        raise FileNotFoundError(f\"Model file not found: {MODEL_PATH}\")\n    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n    model.eval()\n    print(\" -> model ready.\")\n\n    # --------------------- collect annotated frames -----------------------\n    print(f\"\\nScanning annotations for {SET_ID_TO_PROCESS}/{VIDEO_ID_TO_PROCESS} …\")\n    ped_ann = (pie_database\n               .get(SET_ID_TO_PROCESS, {})\n               .get(VIDEO_ID_TO_PROCESS, {})\n               .get(\"ped_annotations\", {}))\n    if not ped_ann:\n        raise ValueError(\"No pedestrian annotations found.\")\n\n    annotated = set()\n    for ped_data in ped_ann.values():\n        annotated.update(ped_data.get(\"frames\", []))\n    frames_sorted = sorted(annotated)\n    if not frames_sorted:\n        raise ValueError(\"No annotated frames found.\")\n    print(f\" -> found {len(frames_sorted)} frames.\")\n\n    # --------------------- inference --------------------------------------\n    print(\"\\nRunning inference on annotated frames …\")\n    preds_per_frame, error_frames = {}, 0\n\n    with torch.no_grad():\n        for fnum in tqdm(frames_sorted, desc=\"Predicting\"):\n            preds_per_frame[fnum] = {}\n            peds_here = [\n                pid for pid, pdata in ped_ann.items()\n                if fnum in pdata.get(\"frames\", [])\n            ]\n            for pid in peds_here:\n                seq = dataset_helper.get_feature_sequence(\n                    SET_ID_TO_PROCESS,\n                    VIDEO_ID_TO_PROCESS,\n                    pid,\n                    fnum,\n                    SEQ_LEN,\n                    MODEL_ACTIVE_STREAMS,\n                )\n                if seq is None:\n                    preds_per_frame[fnum][pid] = -1\n                    continue\n                try:\n                    seq = {k: v.to(DEVICE) for k, v in seq.items()}\n                    logits = model(seq)\n                    prob   = torch.softmax(logits, dim=1)\n                    pred   = torch.argmax(prob, dim=1).item()\n                    preds_per_frame[fnum][pid] = pred\n                except Exception as e:\n                    print(f\"Inference error F{fnum} P{pid}: {e}\")\n                    preds_per_frame[fnum][pid] = -1\n                    error_frames += 1\n\n    print(f\"Predictions complete. Errors on {error_frames} instances.\")\n\n# -------------------------------------------------------------------------\n# ------- Create & write the *annotated-only* video  -----------------------\nprint(f\"\\nCreating video with annotated frames only → {OUTPUT_VIDEO_PATH}\")\n\n# Use the first annotated frame to get size\nfirst_annotated_idx = frames_sorted[0] - 1        # convert 1-based → 0-based file index\nfirst_frame_path    = os.path.join(\n    IMAGE_FRAME_DIR, f\"frame_{first_annotated_idx:05d}.jpg\"\n)\nsample_img = cv2.imread(first_frame_path)\nif sample_img is None:\n    raise IOError(f\"Could not read first annotated frame: {first_frame_path}\")\nheight, width, _ = sample_img.shape\nprint(f\" -> video size {width}×{height}, total annotated frames {len(frames_sorted)}\")\n\nfourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\nfps    = 30                                   # keep whatever FPS you prefer\nwriter = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\nif not writer.isOpened():\n    raise IOError(\"Could not open cv2 VideoWriter\")\n\n# colours\ncol_nc, col_cr, col_un = (0,255,0), (0,0,255), (255,255,0)\n\nfor lookup in tqdm(frames_sorted, desc=\"Writing video\"):\n    idx = lookup - 1                          # image file index (0-based)\n    img_path = os.path.join(IMAGE_FRAME_DIR, f\"frame_{idx:05d}.jpg\")\n\n    frame = cv2.imread(img_path)\n    if frame is None:\n        frame = np.zeros((height, width, 3), np.uint8)\n\n    # draw predictions\n    preds_here = preds_per_frame.get(lookup, {})\n    for pid, bbox in [\n        (pid, pdata[\"bbox\"][pdata[\"frames\"].index(lookup)])\n        for pid, pdata in ped_ann.items()\n        if lookup in pdata.get(\"frames\", [])\n    ]:\n        pred = preds_here.get(pid, -1)\n        colour, label = (col_un, \"Unknown\")\n        if pred == 0:\n            colour, label = (col_nc, \"Not Crossing\")\n        elif pred == 1:\n            colour, label = (col_cr, \"Crossing\")\n\n        try:\n            x1, y1, x2, y2 = map(int, bbox)\n            cv2.rectangle(frame, (x1, y1), (x2, y2), colour, 2)\n            cv2.putText(frame, label, (x1, y1 - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, colour, 2)\n        except Exception:\n            pass\n\n    writer.write(frame)\n\nwriter.release()\nprint(f\"\\nAnnotated-only video saved to: {OUTPUT_VIDEO_PATH}\")\n# -------------------------------------------------------------------------\n\nprint(\"\\n--- Visualisation script finished ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T00:50:28.881287Z","iopub.execute_input":"2025-04-23T00:50:28.881641Z","iopub.status.idle":"2025-04-23T00:52:21.478344Z","shell.execute_reply.started":"2025-04-23T00:50:28.881617Z","shell.execute_reply":"2025-04-23T00:52:21.477516Z"}},"outputs":[{"name":"stdout","text":"Model uses Active Streams: ['bbox', 'ego_speed', 'ego_acc', 'ped_action', 'ped_look', 'static_context']\nUsing device: cuda\n\nLoading PIE database cache …\n -> database loaded.\n\nLoading scalers from /kaggle/working/scalers.pkl\n -> scalers loaded.\n\nInitialising dataset helper …\nLoading pose data (if available)...\nFinished loading pose data for 0 set(s).\nHelper dataset ready.\n\nBuilding model …\n\nLoading weights from /kaggle/input/best_model_ablation/pytorch/default/1/best_model_bbox_ego_acc_ego_speed_ped_action_ped_look_static_context_ep1.pth\n -> model ready.\n\nScanning annotations for set01/video_0001 …\n -> found 2006 frames.\n\nRunning inference on annotated frames …\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-6-97459c8a4849>:528: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting:   0%|          | 0/2006 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae0c937d6e554fd4a34555a66a547acf"}},"metadata":{}},{"name":"stdout","text":"Predictions complete. Errors on 0 instances.\n\nCreating video with annotated frames only → /kaggle/working/set01_video_0001_predictions.mp4\n -> video size 1920×1080, total annotated frames 2006\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Writing video:   0%|          | 0/2006 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1214434cdaa045e9a1440dd4808963d9"}},"metadata":{}},{"name":"stdout","text":"\nAnnotated-only video saved to: /kaggle/working/set01_video_0001_predictions.mp4\n\n--- Visualisation script finished ---\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}