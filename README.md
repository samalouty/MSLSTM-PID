# Multi-Stream LSTM for Pedestrian Intention Prediction

Predict whether a pedestrian will **cross the street in the near future** by fusing multiple information streams from the **Pedestrian Intention Estimation (PIE)** dataset.

<div align="center">
  <img src="https://github.com/user-attachments/assets/9c10d389-6996-471d-9279-e483a23a80c4" alt="Model overview" width="650">
</div>

---

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository hosts the code for a multi-stream LSTMâ€‘based framework designed to predict pedestrian crossing behavior. It utilizes the Pedestrian Intention Estimation (PIE) dataset and integrates diverse data modalities, including boundingâ€‘box trajectories, skeletal pose, panoptic scene features, pedestrian behaviors, egoâ€‘vehicle dynamics, and static environmental context.

## ğŸŒŸ Key Features

* **Multiâ€‘Modal Integration** â€“ Synchronized streams from bounding boxes, pose, panoptic segmentation (**YOLOP**), pedestrian actions, egoâ€‘vehicle data, and static environment.  
* **Robust Data Pipeline** â€“ Consolidates PIE annotations and standardises features.  
* **Advanced Feature Extraction**  
  * **YOLOv8xâ€‘poseâ€‘p6** for pedestrian pose.  
  * **YOLOP** for drivable area, lane lines, and nearbyâ€‘object context.  
  * PIEâ€‘native streams: bounding boxes, pedestrian behaviours (action, look, occlusion), egoâ€‘vehicle OBD, trafficâ€‘light state, static attributes.  
* **Data Balancing** â€“ Oversamples the minority *crossing* class with Gaussianâ€‘noise augmentation.  
* **Flexible Model Architecture** â€“ Each stream processed by a Bidirectional LSTM followed by an Attention layer.  
* **Fusion Techniques** â€“ Simple concatenation **or** learnable weightedâ€‘average fusion of stream context vectors.  
* **Systematic Evaluation** â€“ Supports ablations; metrics reported include F1, precision, recall, and AUC.

## ğŸ“‚ Repository Structure
```text
â”œâ”€â”€ JAAD/                  # Inference notebook for the JAAD dataset
â”‚
â”œâ”€â”€ Offline Extraction/    # Notebooks for pose & panoptic feature extraction
â”‚   â”œâ”€â”€ yolo_pose_extraction.ipynb
â”‚   â””â”€â”€ yolop-extraction.ipynb
â”‚
â”œâ”€â”€ Previous Versions/     # Early experiments & ablation studies
â”‚
â”œâ”€â”€ Final Version/         # Notebook that trains the best model
â”‚
â””â”€â”€ README.md
```

## âš™ï¸â€¯Setup & Installation
> **Tip:** Running on **Kaggle** (GPU P100) is often easier than a local setup.

### 1&nbsp;Â·â€¯Prerequisites
* Git
* PythonÂ â‰¥â€¯3.8
* **conda/miniconda** (recommended)

### 2&nbsp;Â·â€¯Clone Repositories
```bash
# Main project
git clone https://github.com/samalouty/MSLSTM-PID.git
cd MSLSTM-PID

# PIE dataset utilities (XML parsing helpers)
git clone https://github.com/aras62/PIE.git PIE

# YOLOP (only if you need to reâ€‘extract scene features)
git clone https://github.com/hustvl/YOLOP.git YOLOP
```

### 3&nbsp;Â·â€¯Download Datasets & Preâ€‘Trained Weights

#### PIE videos  
Download *set01*â€¯â€¦â€¯*set06* clips and place them, e.g.
```
data/pie_videos/set01/video_0001.mp4
```
Update `VIDEO_INPUT_DIR` in **Finalâ€¯Version.ipynb**.

#### PIE annotations  
Unzip `annotations.zip`, `annotations_attributes.zip`, `annotations_vehicle.zip` into:
```
PIE/annotations/annotations/set01/...
```

#### Model weights
| Component | Where to get it | Destination |
|-----------|-----------------|-------------|
| **YOLOv8xâ€‘poseâ€‘p6** | `ultralytics` hub â€“ downloaded automatically by *yolo_pose_extraction.ipynb* | â€” |
| **YOLOP** (`End-to-end.pth`) | YOLOP repo releases | `YOLOP/weights/` |

#### (Optional) Preâ€‘extracted features
If you already have `.pkl` files for pose or YOLOP, place them where `POSE_DATA_DIR` and `YOLOP_FEATURE_DIR` point.  
A cached `pie_database.pkl` can also be dropped at `PIE_DATABASE_CACHE_PATH`.

### 4&nbsp;Â·â€¯Python Environment
```bash
# Conda example
conda create -n mslstm_pid_env python=3.9 -y
conda activate mslstm_pid_env

# PyTorch (CUDAÂ 11.8 example)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Remaining deps
pip install numpy pandas scikit-learn tqdm matplotlib seaborn opencv-python ultralytics pyyaml jupyterlab yacs
```

---

## ğŸš€â€¯Running the Code
The workflow is notebookâ€‘driven:

1. **Offline Feature Extraction** *(optional but recommended)*  
   *Run once to create pose and YOLOP `.pkl` files.*
   * `Offlineâ€¯Extraction/yolo_pose_extraction.ipynb`  
   * `Offlineâ€¯Extraction/yolop-extraction.ipynb`

2. **Training & Evaluation â€“Â `Finalâ€¯Version.ipynb`**  
   *PhaseÂ A â€“ Data preparation* (autoâ€‘runs if caches missing):  
   parses XML, builds `pie_database.pkl`, computes scalers, and balances classes.  
   *PhaseÂ B â€“ Model*  
   set `ACTIVE_STREAMS` to choose modalities:
   ```python
   ACTIVE_STREAMS = [
       "bbox",
       "ped_action",
       "ped_look",
       # "pose",          # â† enable to add pose
       "ego_speed",
       "ego_acc",
       "yolop",    # YOLOP drivable/lane
   ]
   ```
   Train for `NUM_EPOCHS`; best model (by F1) is saved automatically.

3. **Ablation Studies**  
   Repeat stepâ€¯2 with different `ACTIVE_STREAMS` lists and compare results.

---

## ğŸ“Šâ€¯Expected Outputs
| Artifact | Purpose |
|----------|---------|
| `scalers.pkl` | Meanâ€¯/â€¯std params for numeric features |
| `aug_balanced_train_data_with_features.pkl` | Augmented training split |
| `best_model_[streams]_ep[X].pth` | Model checkpoint |
| Notebook stdout | Epochâ€‘wise metrics |
| Notebook figures | Loss curves, confusion matrix |

---

## ğŸ™â€¯Acknowledgements & Citation
If you use the PIE dataset, please cite:
```bibtex
@inproceedings{rasouli2019pie,
  title      = {{PIE}: A Large-Scale Dataset and Models for Pedestrian Intention Estimation and Trajectory Prediction},
  author     = {Rasouli, Amir and Kotseruba, Iuliia and Kunic, Toni and Tsotsos, John K},
  booktitle  = {Proc. IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages      = {6262--6271},
  year       = {2019}
}
```

If you use YOLOP, please cite:
```bibtex
@article{wu2020yolop,
  title   = {{YOLOP}: You Only Look Once for Panoptic Driving Perception},
  author  = {Wu, Dong and Liao, Manwen and Zhang, Weitian and Wang, Xinggang and Bai, Xiang and Cheng, Wenqing and Liu, Wenyu},
  journal = {arXiv preprint arXiv:2108.11250},
  year    = {2021}
}
```

---
