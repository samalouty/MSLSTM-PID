{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389efbbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T12:35:23.021986Z",
     "iopub.status.busy": "2025-05-26T12:35:23.021632Z",
     "iopub.status.idle": "2025-05-26T12:35:23.026349Z",
     "shell.execute_reply": "2025-05-26T12:35:23.025673Z"
    },
    "papermill": {
     "duration": 0.01096,
     "end_time": "2025-05-26T12:35:23.027710",
     "exception": false,
     "start_time": "2025-05-26T12:35:23.016750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/aras62/PIE.git\n",
    "# !unzip /content/PIE/annotations/annotations.zip -d /content/PIE\n",
    "# !unzip /content/PIE/annotations/annotations_vehicle.zip -d /content/PIE\n",
    "# !mkdir /kaggle/working/PIE/content\n",
    "# !git clone https://github.com/hustvl/YOLOP.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20be2aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T12:35:23.035367Z",
     "iopub.status.busy": "2025-05-26T12:35:23.035065Z",
     "iopub.status.idle": "2025-05-26T12:35:30.785514Z",
     "shell.execute_reply": "2025-05-26T12:35:30.784397Z"
    },
    "papermill": {
     "duration": 7.756367,
     "end_time": "2025-05-26T12:35:30.787799",
     "exception": false,
     "start_time": "2025-05-26T12:35:23.031432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'JAAD'...\r\n",
      "remote: Enumerating objects: 6155, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (724/724), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (72/72), done.\u001b[K\r\n",
      "remote: Total 6155 (delta 672), reused 695 (delta 652), pack-reused 5431 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (6155/6155), 42.16 MiB | 18.94 MiB/s, done.\r\n",
      "Resolving deltas: 100% (5491/5491), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ykotseruba/JAAD.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "598031cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T12:35:30.805238Z",
     "iopub.status.busy": "2025-05-26T12:35:30.804877Z",
     "iopub.status.idle": "2025-05-26T12:35:36.546289Z",
     "shell.execute_reply": "2025-05-26T12:35:36.545263Z"
    },
    "papermill": {
     "duration": 5.751361,
     "end_time": "2025-05-26T12:35:36.547903",
     "exception": false,
     "start_time": "2025-05-26T12:35:30.796542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q ultralytics opencv-python-headless "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe4b3d98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T12:35:36.563471Z",
     "iopub.status.busy": "2025-05-26T12:35:36.563235Z",
     "iopub.status.idle": "2025-05-26T12:35:42.635208Z",
     "shell.execute_reply": "2025-05-26T12:35:42.634438Z"
    },
    "papermill": {
     "duration": 6.081478,
     "end_time": "2025-05-26T12:35:42.636958",
     "exception": false,
     "start_time": "2025-05-26T12:35:36.555480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import math\n",
    "import zipfile\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98b3bf41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T12:35:42.653547Z",
     "iopub.status.busy": "2025-05-26T12:35:42.653053Z",
     "iopub.status.idle": "2025-05-26T12:35:42.656564Z",
     "shell.execute_reply": "2025-05-26T12:35:42.655842Z"
    },
    "papermill": {
     "duration": 0.013071,
     "end_time": "2025-05-26T12:35:42.657804",
     "exception": false,
     "start_time": "2025-05-26T12:35:42.644733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# zip_path = \"/kaggle/working/PIE/annotations/annotations.zip\"\n",
    "# extract_to = \"/kaggle/working/PIE/annotations/\"\n",
    "\n",
    "# if os.path.exists(extract_to + 'annotations'):\n",
    "#     print(\"Exists already. Not unzipping.\")\n",
    "# else:\n",
    "#     with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#         zip_ref.extractall(extract_to)\n",
    "#     print(\"Unzipped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae0d9fad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T12:35:42.673679Z",
     "iopub.status.busy": "2025-05-26T12:35:42.673410Z",
     "iopub.status.idle": "2025-05-26T12:35:42.676489Z",
     "shell.execute_reply": "2025-05-26T12:35:42.675794Z"
    },
    "papermill": {
     "duration": 0.012194,
     "end_time": "2025-05-26T12:35:42.677639",
     "exception": false,
     "start_time": "2025-05-26T12:35:42.665445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# zip_path = \"/kaggle/working/PIE/annotations/annotations_vehicle.zip\"\n",
    "# extract_to = \"/kaggle/working/PIE/annotations/\"\n",
    "\n",
    "# if os.path.exists(extract_to + 'annotations_vehicle'):\n",
    "#     print(\"Exists already. Not unzipping.\")\n",
    "# else:\n",
    "#     with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#         zip_ref.extractall(extract_to)\n",
    "#     print(\"Unzipped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99db61e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T12:35:42.693418Z",
     "iopub.status.busy": "2025-05-26T12:35:42.693089Z",
     "iopub.status.idle": "2025-05-26T12:35:42.696281Z",
     "shell.execute_reply": "2025-05-26T12:35:42.695596Z"
    },
    "papermill": {
     "duration": 0.012624,
     "end_time": "2025-05-26T12:35:42.697580",
     "exception": false,
     "start_time": "2025-05-26T12:35:42.684956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# zip_path = \"/kaggle/working/PIE/annotations/annotations_attributes.zip\"\n",
    "# extract_to = \"/kaggle/working/PIE/annotations/\"\n",
    "\n",
    "# if os.path.exists(extract_to + \"annotations_attributes\"):\n",
    "#     print(\"Exists already. Not unzipping.\")\n",
    "# else:\n",
    "#     with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#         zip_ref.extractall(extract_to)\n",
    "#     print(\"Unzipped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47befdce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T12:35:42.714300Z",
     "iopub.status.busy": "2025-05-26T12:35:42.713962Z",
     "iopub.status.idle": "2025-05-26T16:12:37.104652Z",
     "shell.execute_reply": "2025-05-26T16:12:37.103723Z"
    },
    "papermill": {
     "duration": 13014.401177,
     "end_time": "2025-05-26T16:12:37.106108",
     "exception": false,
     "start_time": "2025-05-26T12:35:42.704931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing jaad_data for the first time...\n",
      "✓ jaad_data imported.\n",
      "--- Active streams for Model: ['bbox', 'ego_acc', 'ego_speed', 'ped_action', 'ped_look', 'static_context'] ---\n",
      "Using device: cuda\n",
      "--- JAAD DATASET INFERENCE (Hybrid: GT for some, Inferred Beh from Pose) ---\n",
      "--- Processing specified videos: ['video_0006', 'video_0021', 'video_0040', 'video_0044', 'video_0072', 'video_0073', 'video_0082', 'video_0089', 'video_0099', 'video_0102', 'video_0123', 'video_0156', 'video_0160', 'video_0170', 'video_0172', 'video_0181', 'video_0193', 'video_0199', 'video_0217', 'video_0226', 'video_0252', 'video_0263', 'video_0273', 'video_0274', 'video_0291', 'video_0303', 'video_0306', 'video_0340', 'video_0343'] ---\n",
      "\n",
      "Initializing JAAD object...\n",
      "Created JAAD images dir: /kaggle/working/JAAD/images_extracted_custom_subset\n",
      "Videos to be processed in this run: ['video_0006', 'video_0021', 'video_0040', 'video_0044', 'video_0072', 'video_0073', 'video_0082', 'video_0089', 'video_0099', 'video_0102', 'video_0123', 'video_0156', 'video_0160', 'video_0170', 'video_0172', 'video_0181', 'video_0193', 'video_0199', 'video_0217', 'video_0226', 'video_0252', 'video_0263', 'video_0273', 'video_0274', 'video_0291', 'video_0303', 'video_0306', 'video_0340', 'video_0343']\n",
      "Images missing for video_0006. Will extract.\n",
      "Extracting images for 29 JAAD videos to /kaggle/working/JAAD/images_extracted_custom_subset...\n",
      "Processing specified subset of 29 videos: ['video_0006', 'video_0021', 'video_0040', 'video_0044', 'video_0072', 'video_0073', 'video_0082', 'video_0089', 'video_0099', 'video_0102', 'video_0123', 'video_0156', 'video_0160', 'video_0170', 'video_0172', 'video_0181', 'video_0193', 'video_0199', 'video_0217', 'video_0226', 'video_0252', 'video_0263', 'video_0273', 'video_0274', 'video_0291', 'video_0303', 'video_0306', 'video_0340', 'video_0343']\n",
      "Starting image extraction for video: video_0006\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0006\n",
      "[####################] 100.00% \n",
      "Video video_0006: Extracted 330 new images. Total frames iterated: 329. XML frames: 330.\n",
      "Starting image extraction for video: video_0021\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0021\n",
      "[####################] 100.00% \n",
      "Video video_0021: Extracted 180 new images. Total frames iterated: 179. XML frames: 180.\n",
      "Starting image extraction for video: video_0040\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0040\n",
      "[####################] 100.00% \n",
      "Video video_0040: Extracted 240 new images. Total frames iterated: 239. XML frames: 240.\n",
      "Starting image extraction for video: video_0044\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0044\n",
      "[####################] 100.00% \n",
      "Video video_0044: Extracted 210 new images. Total frames iterated: 209. XML frames: 210.\n",
      "Starting image extraction for video: video_0072\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0072\n",
      "[####################] 100.00% \n",
      "Video video_0072: Extracted 210 new images. Total frames iterated: 209. XML frames: 210.\n",
      "Starting image extraction for video: video_0073\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0073\n",
      "[####################] 100.00% \n",
      "Video video_0073: Extracted 120 new images. Total frames iterated: 119. XML frames: 120.\n",
      "Starting image extraction for video: video_0082\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0082\n",
      "[####################] 100.00% \n",
      "Video video_0082: Extracted 180 new images. Total frames iterated: 179. XML frames: 180.\n",
      "Starting image extraction for video: video_0089\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0089\n",
      "[####################] 100.00% \n",
      "Video video_0089: Extracted 360 new images. Total frames iterated: 359. XML frames: 360.\n",
      "Starting image extraction for video: video_0099\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0099\n",
      "[####################] 100.00% \n",
      "Video video_0099: Extracted 180 new images. Total frames iterated: 179. XML frames: 180.\n",
      "Starting image extraction for video: video_0102\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0102\n",
      "[####################] 100.00% \n",
      "Video video_0102: Extracted 360 new images. Total frames iterated: 359. XML frames: 360.\n",
      "Starting image extraction for video: video_0123\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0123\n",
      "[####################] 100.00% \n",
      "Video video_0123: Extracted 150 new images. Total frames iterated: 149. XML frames: 150.\n",
      "Starting image extraction for video: video_0156\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0156\n",
      "[####################] 100.00% \n",
      "Video video_0156: Extracted 150 new images. Total frames iterated: 149. XML frames: 150.\n",
      "Starting image extraction for video: video_0160\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0160\n",
      "[####################] 100.00% \n",
      "Video video_0160: Extracted 210 new images. Total frames iterated: 209. XML frames: 210.\n",
      "Starting image extraction for video: video_0170\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0170\n",
      "[####################] 100.00% \n",
      "Video video_0170: Extracted 120 new images. Total frames iterated: 119. XML frames: 120.\n",
      "Starting image extraction for video: video_0172\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0172\n",
      "[####################] 100.00% \n",
      "Video video_0172: Extracted 180 new images. Total frames iterated: 179. XML frames: 180.\n",
      "Starting image extraction for video: video_0181\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0181\n",
      "[####################] 100.00% \n",
      "Video video_0181: Extracted 90 new images. Total frames iterated: 89. XML frames: 90.\n",
      "Starting image extraction for video: video_0193\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0193\n",
      "[####################] 100.00% \n",
      "Video video_0193: Extracted 240 new images. Total frames iterated: 239. XML frames: 240.\n",
      "Starting image extraction for video: video_0199\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0199\n",
      "[####################] 100.00% \n",
      "Video video_0199: Extracted 930 new images. Total frames iterated: 929. XML frames: 930.\n",
      "Starting image extraction for video: video_0217\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0217\n",
      "[####################] 100.00% \n",
      "Video video_0217: Extracted 240 new images. Total frames iterated: 239. XML frames: 240.\n",
      "Starting image extraction for video: video_0226\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0226\n",
      "[####################] 100.00% \n",
      "Video video_0226: Extracted 330 new images. Total frames iterated: 329. XML frames: 330.\n",
      "Starting image extraction for video: video_0252\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0252\n",
      "[####################] 100.00% \n",
      "Video video_0252: Extracted 150 new images. Total frames iterated: 149. XML frames: 150.\n",
      "Starting image extraction for video: video_0263\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0263\n",
      "[####################] 100.00% \n",
      "Video video_0263: Extracted 120 new images. Total frames iterated: 119. XML frames: 120.\n",
      "Starting image extraction for video: video_0273\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0273\n",
      "[####################] 100.00% \n",
      "Video video_0273: Extracted 120 new images. Total frames iterated: 119. XML frames: 120.\n",
      "Starting image extraction for video: video_0274\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0274\n",
      "[####################] 100.00% \n",
      "Video video_0274: Extracted 270 new images. Total frames iterated: 269. XML frames: 270.\n",
      "Starting image extraction for video: video_0291\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0291\n",
      "[####################] 100.00% \n",
      "Video video_0291: Extracted 210 new images. Total frames iterated: 209. XML frames: 210.\n",
      "Starting image extraction for video: video_0303\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0303\n",
      "[####################] 100.00% \n",
      "Video video_0303: Extracted 180 new images. Total frames iterated: 179. XML frames: 180.\n",
      "Starting image extraction for video: video_0306\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0306\n",
      "[####################] 100.00% \n",
      "Video video_0306: Extracted 240 new images. Total frames iterated: 239. XML frames: 240.\n",
      "Starting image extraction for video: video_0340\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0340\n",
      "[####################] 100.00% \n",
      "Video video_0340: Extracted 150 new images. Total frames iterated: 149. XML frames: 150.\n",
      "Starting image extraction for video: video_0343\n",
      "Created directory: /kaggle/working/JAAD/images_extracted_custom_subset/video_0343\n",
      "[####################] 100.00% \n",
      "Video video_0343: Extracted 720 new images. Total frames iterated: 719. XML frames: 720.\n",
      "Image extraction process finished for selected videos.\n",
      "✓ JAAD images extracted for the specified subset.\n",
      "\n",
      "Loading/Generating JAAD database (annotations)...\n",
      "---------------------------------------------------------\n",
      "Generating database for jaad\n",
      "Getting annotations for video_0001\n",
      "Getting annotations for video_0002\n",
      "Getting annotations for video_0003\n",
      "Getting annotations for video_0004\n",
      "Getting annotations for video_0005\n",
      "Getting annotations for video_0006\n",
      "Getting annotations for video_0007\n",
      "Getting annotations for video_0008\n",
      "Getting annotations for video_0009\n",
      "Getting annotations for video_0010\n",
      "Getting annotations for video_0011\n",
      "Getting annotations for video_0012\n",
      "Getting annotations for video_0013\n",
      "Getting annotations for video_0014\n",
      "Getting annotations for video_0015\n",
      "Getting annotations for video_0016\n",
      "Getting annotations for video_0017\n",
      "Getting annotations for video_0018\n",
      "Getting annotations for video_0019\n",
      "Getting annotations for video_0020\n",
      "Getting annotations for video_0021\n",
      "Getting annotations for video_0022\n",
      "Getting annotations for video_0023\n",
      "Getting annotations for video_0024\n",
      "Getting annotations for video_0025\n",
      "Getting annotations for video_0026\n",
      "Getting annotations for video_0027\n",
      "Getting annotations for video_0028\n",
      "Getting annotations for video_0029\n",
      "Getting annotations for video_0030\n",
      "Getting annotations for video_0031\n",
      "Getting annotations for video_0032\n",
      "Getting annotations for video_0033\n",
      "Getting annotations for video_0034\n",
      "Getting annotations for video_0035\n",
      "Getting annotations for video_0036\n",
      "Getting annotations for video_0037\n",
      "Getting annotations for video_0038\n",
      "Getting annotations for video_0039\n",
      "Getting annotations for video_0040\n",
      "Getting annotations for video_0041\n",
      "Getting annotations for video_0042\n",
      "Getting annotations for video_0043\n",
      "Getting annotations for video_0044\n",
      "Getting annotations for video_0045\n",
      "Getting annotations for video_0046\n",
      "Getting annotations for video_0047\n",
      "Getting annotations for video_0048\n",
      "Getting annotations for video_0049\n",
      "Getting annotations for video_0050\n",
      "Getting annotations for video_0051\n",
      "Getting annotations for video_0052\n",
      "Getting annotations for video_0053\n",
      "Getting annotations for video_0054\n",
      "Getting annotations for video_0055\n",
      "Getting annotations for video_0056\n",
      "Getting annotations for video_0057\n",
      "Getting annotations for video_0058\n",
      "Getting annotations for video_0059\n",
      "Getting annotations for video_0060\n",
      "Getting annotations for video_0061\n",
      "Getting annotations for video_0062\n",
      "Getting annotations for video_0063\n",
      "Getting annotations for video_0064\n",
      "Getting annotations for video_0065\n",
      "Getting annotations for video_0066\n",
      "Getting annotations for video_0067\n",
      "Getting annotations for video_0068\n",
      "Getting annotations for video_0069\n",
      "Getting annotations for video_0070\n",
      "Getting annotations for video_0071\n",
      "Getting annotations for video_0072\n",
      "Getting annotations for video_0073\n",
      "Getting annotations for video_0074\n",
      "Getting annotations for video_0075\n",
      "Getting annotations for video_0076\n",
      "Getting annotations for video_0077\n",
      "Getting annotations for video_0078\n",
      "Getting annotations for video_0079\n",
      "Getting annotations for video_0080\n",
      "Getting annotations for video_0081\n",
      "Getting annotations for video_0082\n",
      "Getting annotations for video_0083\n",
      "Getting annotations for video_0084\n",
      "Getting annotations for video_0085\n",
      "Getting annotations for video_0086\n",
      "Getting annotations for video_0087\n",
      "Getting annotations for video_0088\n",
      "Getting annotations for video_0089\n",
      "Getting annotations for video_0090\n",
      "Getting annotations for video_0091\n",
      "Getting annotations for video_0092\n",
      "Getting annotations for video_0093\n",
      "Getting annotations for video_0094\n",
      "Getting annotations for video_0095\n",
      "Getting annotations for video_0096\n",
      "Getting annotations for video_0097\n",
      "Getting annotations for video_0098\n",
      "Getting annotations for video_0099\n",
      "Getting annotations for video_0100\n",
      "Getting annotations for video_0101\n",
      "Getting annotations for video_0102\n",
      "Getting annotations for video_0103\n",
      "Getting annotations for video_0104\n",
      "Getting annotations for video_0105\n",
      "Getting annotations for video_0106\n",
      "Getting annotations for video_0107\n",
      "Getting annotations for video_0108\n",
      "Getting annotations for video_0109\n",
      "Getting annotations for video_0110\n",
      "Getting annotations for video_0111\n",
      "Getting annotations for video_0112\n",
      "Getting annotations for video_0113\n",
      "Getting annotations for video_0114\n",
      "Getting annotations for video_0115\n",
      "Getting annotations for video_0116\n",
      "Getting annotations for video_0117\n",
      "Getting annotations for video_0118\n",
      "Getting annotations for video_0119\n",
      "Getting annotations for video_0120\n",
      "Getting annotations for video_0121\n",
      "Getting annotations for video_0122\n",
      "Getting annotations for video_0123\n",
      "Getting annotations for video_0124\n",
      "Getting annotations for video_0125\n",
      "Getting annotations for video_0126\n",
      "Getting annotations for video_0127\n",
      "Getting annotations for video_0128\n",
      "Getting annotations for video_0129\n",
      "Getting annotations for video_0130\n",
      "Getting annotations for video_0131\n",
      "Getting annotations for video_0132\n",
      "Getting annotations for video_0133\n",
      "Getting annotations for video_0134\n",
      "Getting annotations for video_0135\n",
      "Getting annotations for video_0136\n",
      "Getting annotations for video_0137\n",
      "Getting annotations for video_0138\n",
      "Getting annotations for video_0139\n",
      "Getting annotations for video_0140\n",
      "Getting annotations for video_0141\n",
      "Getting annotations for video_0142\n",
      "Getting annotations for video_0143\n",
      "Getting annotations for video_0144\n",
      "Getting annotations for video_0145\n",
      "Getting annotations for video_0146\n",
      "Getting annotations for video_0147\n",
      "Getting annotations for video_0148\n",
      "Getting annotations for video_0149\n",
      "Getting annotations for video_0150\n",
      "Getting annotations for video_0151\n",
      "Getting annotations for video_0152\n",
      "Getting annotations for video_0153\n",
      "Getting annotations for video_0154\n",
      "Getting annotations for video_0155\n",
      "Getting annotations for video_0156\n",
      "Getting annotations for video_0157\n",
      "Getting annotations for video_0158\n",
      "Getting annotations for video_0159\n",
      "Getting annotations for video_0160\n",
      "Getting annotations for video_0161\n",
      "Getting annotations for video_0162\n",
      "Getting annotations for video_0163\n",
      "Getting annotations for video_0164\n",
      "Getting annotations for video_0165\n",
      "Getting annotations for video_0166\n",
      "Getting annotations for video_0167\n",
      "Getting annotations for video_0168\n",
      "Getting annotations for video_0169\n",
      "Getting annotations for video_0170\n",
      "Getting annotations for video_0171\n",
      "Getting annotations for video_0172\n",
      "Getting annotations for video_0173\n",
      "Getting annotations for video_0174\n",
      "Getting annotations for video_0175\n",
      "Getting annotations for video_0176\n",
      "Getting annotations for video_0177\n",
      "Getting annotations for video_0178\n",
      "Getting annotations for video_0179\n",
      "Getting annotations for video_0180\n",
      "Getting annotations for video_0181\n",
      "Getting annotations for video_0182\n",
      "Getting annotations for video_0183\n",
      "Getting annotations for video_0184\n",
      "Getting annotations for video_0185\n",
      "Getting annotations for video_0186\n",
      "Getting annotations for video_0187\n",
      "Getting annotations for video_0188\n",
      "Getting annotations for video_0189\n",
      "Getting annotations for video_0190\n",
      "Getting annotations for video_0191\n",
      "Getting annotations for video_0192\n",
      "Getting annotations for video_0193\n",
      "Getting annotations for video_0194\n",
      "Getting annotations for video_0195\n",
      "Getting annotations for video_0196\n",
      "Getting annotations for video_0197\n",
      "Getting annotations for video_0198\n",
      "Getting annotations for video_0199\n",
      "Getting annotations for video_0200\n",
      "Getting annotations for video_0201\n",
      "Getting annotations for video_0202\n",
      "Getting annotations for video_0203\n",
      "Getting annotations for video_0204\n",
      "Getting annotations for video_0205\n",
      "Getting annotations for video_0206\n",
      "Getting annotations for video_0207\n",
      "Getting annotations for video_0208\n",
      "Getting annotations for video_0209\n",
      "Getting annotations for video_0210\n",
      "Getting annotations for video_0211\n",
      "Getting annotations for video_0212\n",
      "Getting annotations for video_0213\n",
      "Getting annotations for video_0214\n",
      "Getting annotations for video_0215\n",
      "Getting annotations for video_0216\n",
      "Getting annotations for video_0217\n",
      "Getting annotations for video_0218\n",
      "Getting annotations for video_0219\n",
      "Getting annotations for video_0220\n",
      "Getting annotations for video_0221\n",
      "Getting annotations for video_0222\n",
      "Getting annotations for video_0223\n",
      "Getting annotations for video_0224\n",
      "Getting annotations for video_0225\n",
      "Getting annotations for video_0226\n",
      "Getting annotations for video_0227\n",
      "Getting annotations for video_0228\n",
      "Getting annotations for video_0229\n",
      "Getting annotations for video_0230\n",
      "Getting annotations for video_0231\n",
      "Getting annotations for video_0232\n",
      "Getting annotations for video_0233\n",
      "Getting annotations for video_0234\n",
      "Getting annotations for video_0235\n",
      "Getting annotations for video_0236\n",
      "Getting annotations for video_0237\n",
      "Getting annotations for video_0238\n",
      "Getting annotations for video_0239\n",
      "Getting annotations for video_0240\n",
      "Getting annotations for video_0241\n",
      "Getting annotations for video_0242\n",
      "Getting annotations for video_0243\n",
      "Getting annotations for video_0244\n",
      "Getting annotations for video_0245\n",
      "Getting annotations for video_0246\n",
      "Getting annotations for video_0247\n",
      "Getting annotations for video_0248\n",
      "Getting annotations for video_0249\n",
      "Getting annotations for video_0250\n",
      "Getting annotations for video_0251\n",
      "Getting annotations for video_0252\n",
      "Getting annotations for video_0253\n",
      "Getting annotations for video_0254\n",
      "Getting annotations for video_0255\n",
      "Getting annotations for video_0256\n",
      "Getting annotations for video_0257\n",
      "Getting annotations for video_0258\n",
      "Getting annotations for video_0259\n",
      "Getting annotations for video_0260\n",
      "Getting annotations for video_0261\n",
      "Getting annotations for video_0262\n",
      "Getting annotations for video_0263\n",
      "Getting annotations for video_0264\n",
      "Getting annotations for video_0265\n",
      "Getting annotations for video_0266\n",
      "Getting annotations for video_0267\n",
      "Getting annotations for video_0268\n",
      "Getting annotations for video_0269\n",
      "Getting annotations for video_0270\n",
      "Getting annotations for video_0271\n",
      "Getting annotations for video_0272\n",
      "Getting annotations for video_0273\n",
      "Getting annotations for video_0274\n",
      "Getting annotations for video_0275\n",
      "Getting annotations for video_0276\n",
      "Getting annotations for video_0277\n",
      "Getting annotations for video_0278\n",
      "Getting annotations for video_0279\n",
      "Getting annotations for video_0280\n",
      "Getting annotations for video_0281\n",
      "Getting annotations for video_0282\n",
      "Getting annotations for video_0283\n",
      "Getting annotations for video_0284\n",
      "Getting annotations for video_0285\n",
      "Getting annotations for video_0286\n",
      "Getting annotations for video_0287\n",
      "Getting annotations for video_0288\n",
      "Getting annotations for video_0289\n",
      "Getting annotations for video_0290\n",
      "Getting annotations for video_0291\n",
      "Getting annotations for video_0292\n",
      "Getting annotations for video_0293\n",
      "Getting annotations for video_0294\n",
      "Getting annotations for video_0295\n",
      "Getting annotations for video_0296\n",
      "Getting annotations for video_0297\n",
      "Getting annotations for video_0298\n",
      "Getting annotations for video_0299\n",
      "Getting annotations for video_0300\n",
      "Getting annotations for video_0301\n",
      "Getting annotations for video_0302\n",
      "Getting annotations for video_0303\n",
      "Getting annotations for video_0304\n",
      "Getting annotations for video_0305\n",
      "Getting annotations for video_0306\n",
      "Getting annotations for video_0307\n",
      "Getting annotations for video_0308\n",
      "Getting annotations for video_0309\n",
      "Getting annotations for video_0310\n",
      "Getting annotations for video_0311\n",
      "Getting annotations for video_0312\n",
      "Getting annotations for video_0313\n",
      "Getting annotations for video_0314\n",
      "Getting annotations for video_0315\n",
      "Getting annotations for video_0316\n",
      "Getting annotations for video_0317\n",
      "Getting annotations for video_0318\n",
      "Getting annotations for video_0319\n",
      "Getting annotations for video_0320\n",
      "Getting annotations for video_0321\n",
      "Getting annotations for video_0322\n",
      "Getting annotations for video_0323\n",
      "Getting annotations for video_0324\n",
      "Getting annotations for video_0325\n",
      "Getting annotations for video_0326\n",
      "Getting annotations for video_0327\n",
      "Getting annotations for video_0328\n",
      "Getting annotations for video_0329\n",
      "Getting annotations for video_0330\n",
      "Getting annotations for video_0331\n",
      "Getting annotations for video_0332\n",
      "Getting annotations for video_0333\n",
      "Getting annotations for video_0334\n",
      "Getting annotations for video_0335\n",
      "Getting annotations for video_0336\n",
      "Getting annotations for video_0337\n",
      "Getting annotations for video_0338\n",
      "Getting annotations for video_0339\n",
      "Getting annotations for video_0340\n",
      "Getting annotations for video_0341\n",
      "Getting annotations for video_0342\n",
      "Getting annotations for video_0343\n",
      "Getting annotations for video_0344\n",
      "Getting annotations for video_0345\n",
      "Getting annotations for video_0346\n",
      "The database is written to /kaggle/working/JAAD/data_cache/jaad_database.pkl\n",
      "✓ JAAD DB loaded/generated.\n",
      "\n",
      "Enumerating JAAD validation sequences for specified videos...\n",
      "DEBUG: Effective val_video_ids for enumeration: ['video_0006', 'video_0021', 'video_0040', 'video_0044', 'video_0072', 'video_0073', 'video_0082', 'video_0089', 'video_0099', 'video_0102', 'video_0123', 'video_0156', 'video_0160', 'video_0170', 'video_0172', 'video_0181', 'video_0193', 'video_0199', 'video_0217', 'video_0226', 'video_0252', 'video_0263', 'video_0273', 'video_0274', 'video_0291', 'video_0303', 'video_0306', 'video_0340', 'video_0343']\n",
      "Enumerating for custom_val (29 videos)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe316b8b316a420b873b2a6c64ff6864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Videos(custom_val):   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  custom_val generated 8157 sequences.\n",
      "✓ JAAD val sequence list for custom subset saved. Found 8157 sequences.\n",
      "\n",
      "Loading YOLOv8-pose model...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-pose.pt to 'yolov8n-pose.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.52M/6.52M [00:00<00:00, 20.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ YOLO loaded.\n",
      "\n",
      "Creating JAAD Validation DataLoader for custom subset...\n",
      "✓ JAAD Validation DataLoader ready (Val:8157).\n",
      "\n",
      "Initializing model & loading pre-trained weights...\n",
      "Init Model: ['bbox', 'ego_acc', 'ego_speed', 'ped_action', 'ped_look', 'static_context']\n",
      "  – Str 'bbox'(in 4)\n",
      "  – Str 'ego_acc'(in 2)\n",
      "  – Str 'ego_speed'(in 1)\n",
      "  – Str 'ped_action'(in 1)\n",
      "  – Str 'ped_look'(in 1)\n",
      "  – Str 'static_context'(in 23)\n",
      "✓ Pre-trained loaded: /kaggle/input/mslstm-pid/pytorch/default/1/best_model_weighted_bbox_ego_acc_ego_speed_ped_action_ped_look_static_context_ep2.pth\n",
      "\n",
      "---Starting Inference & Eval on JAAD custom subset (Hybrid)---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7dd2756c7c450ba8844df921dad0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model Inference on Val Set:   0%|          | 0/1020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model inf pass complete. Processed 8157 samples in 1020 batches.\n",
      "\n",
      "---Inf Time Breakdown(Val Set)---\n",
      "Avg YOLO Pose/frame with ped:10.58ms(244710 total pose est.)\n",
      "Avg Heuristic Beh Inf/sequence:0.007ms(8157 seqs processed)\n",
      "Avg Main Model Fwd Pass/batch:10.84ms\n",
      "Avg Main Model Fwd Pass/sample:1.36ms\n",
      "\n",
      "DEBUG: Unique ground truth labels in validation set: (array([0, 1]), array([1969, 6188]))\n",
      "DEBUG: final_val_probs_pos min: 5.763202182151872e-08, max: 0.04490255191922188, mean: 0.00556185282766819\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4016a8f9e5304335b6beb1b5fbbf5696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TuningThresh(F1):   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DEBUG BEFORE ACCURACY SCORE ---\n",
      "final_val_labels type:<class 'numpy.ndarray'>,len:8157\n",
      "final_val_labels shape:(8157,)\n",
      "final_val_preds_tuned type:<class 'numpy.ndarray'>,is None:False\n",
      "final_val_preds_tuned shape:(8157,)\n",
      "--- END DEBUG ---\n",
      "\n",
      "---Final JAAD Metrics(Tuned Thresh,Inferred Beh)---\n",
      "F1                       :0.8627\n",
      "\n",
      "---Model Fusion Weights---\n",
      "bbox           :0.1692\n",
      "ego_acc        :0.1690\n",
      "ego_speed      :0.1628\n",
      "ped_action     :0.1657\n",
      "ped_look       :0.1644\n",
      "static_context :0.1689\n",
      "\n",
      "---JAAD CUSTOM SUBSET INFERENCE & EVALUATION SCRIPT COMPLETE---\n",
      "\n",
      "Cleaning up extracted images at /kaggle/working/JAAD/images_extracted_custom_subset...\n",
      "✓ Image Cleanup successful.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import random\n",
    "import importlib\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# -----------------------------------------------------------------------------#\n",
    "#                                JAAD utilities                                #\n",
    "# -----------------------------------------------------------------------------#\n",
    "MODIFIED_JAAD_DATA_PY_DIR = \"/kaggle/input/jaad-data-py\"\n",
    "if MODIFIED_JAAD_DATA_PY_DIR not in sys.path:\n",
    "    sys.path.insert(0, MODIFIED_JAAD_DATA_PY_DIR)\n",
    "\n",
    "if 'jaad_data' in sys.modules:\n",
    "    print(\"Attempting to reload jaad_data module...\")\n",
    "    try:\n",
    "        jaad_data_module = sys.modules['jaad_data']\n",
    "        importlib.reload(jaad_data_module)\n",
    "        from jaad_data import JAAD\n",
    "        print(\"✓ jaad_data reloaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not reload jaad_data, attempting fresh import: {e}\")\n",
    "        from jaad_data import JAAD\n",
    "        print(\"✓ jaad_data imported after path set.\")\n",
    "else:\n",
    "    print(\"Importing jaad_data for the first time...\")\n",
    "    from jaad_data import JAAD\n",
    "    print(\"✓ jaad_data imported.\")\n",
    "\n",
    "if JAAD is None:\n",
    "    print(f\"[ERROR] Could not import JAAD class from {MODIFIED_JAAD_DATA_PY_DIR}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# -----------------------------------------------------------------------------#\n",
    "#                              Configuration                                   #\n",
    "# -----------------------------------------------------------------------------#\n",
    "JAAD_DATASET_ROOT = \"/kaggle/working/JAAD\"\n",
    "JAAD_CLIPS_PATH = \"/kaggle/input/jaad-clips/JAAD_clips\"\n",
    "JAAD_IMAGES_PATH = os.path.join(JAAD_DATASET_ROOT, \"images_extracted_custom_subset\") # New name for clarity\n",
    "\n",
    "# --- SPECIFY VIDEO IDs for Validation ---\n",
    "# These videos will be used for image extraction (if needed) and sequence enumeration for validation.\n",
    "# Ensure these video IDs exist in JAAD_CLIPS_PATH and have corresponding annotation XMLs.\n",
    "VIDEOS_TO_PROCESS_FOR_VALIDATION = [\n",
    "    'video_0006', 'video_0021', 'video_0040', 'video_0044', 'video_0072', 'video_0073',\n",
    "    'video_0082', 'video_0089', 'video_0099', 'video_0102', 'video_0123', 'video_0156',\n",
    "    'video_0160', 'video_0170', 'video_0172', 'video_0181', 'video_0193', 'video_0199',\n",
    "    'video_0217', 'video_0226', 'video_0252', 'video_0263', 'video_0273', 'video_0274',\n",
    "    'video_0291', 'video_0303', 'video_0306', 'video_0340', 'video_0343'\n",
    "]# You can get video IDs from jaad_obj._get_video_ids_split('val', subset='default') to pick from actual val set\n",
    "# Or pick any videos you want to test on.\n",
    "\n",
    "FORCE_REEXTRACT_IMAGES_FOR_SPECIFIED_VIDEOS = False # If True, re-extracts images for the videos listed above\n",
    "FORCE_REENUMERATE_VAL_SEQUENCES = True # Set to True to always regenerate val sequences for the specified videos\n",
    "\n",
    "# Pickle file for validation sequences (name is generic, content depends on VIDEOS_TO_PROCESS_FOR_VALIDATION)\n",
    "JAAD_VAL_SEQUENCES_PKL_PATH = f\"/kaggle/working/jaad_val_seq_custom_subset.pkl\"\n",
    "\n",
    "PRETRAINED_MODEL_PATH = '/kaggle/input/mslstm-pid/pytorch/default/1/best_model_weighted_bbox_ego_acc_ego_speed_ped_action_ped_look_static_context_ep2.pth'\n",
    "\n",
    "ACTIVE_STREAMS_JAAD_MODEL = [\"bbox\", \"ego_acc\", \"ego_speed\", \"ped_action\", \"ped_look\", \"static_context\"]\n",
    "print(f\"--- Active streams for Model: {ACTIVE_STREAMS_JAAD_MODEL} ---\")\n",
    "\n",
    "SEQ_LEN, PRED_LEN = 30, 1\n",
    "INPUT_SIZE_BBOX_JAAD = 4\n",
    "INPUT_SIZE_POSE_JAAD = 34\n",
    "INPUT_SIZE_EGO_SPEED_JAAD = 1; INPUT_SIZE_EGO_ACC_JAAD = 2\n",
    "INPUT_SIZE_PED_ACTION_JAAD = 1; INPUT_SIZE_PED_LOOK_JAAD = 1\n",
    "\n",
    "NUM_SIGNALIZED_CATS_JAAD_PADDED = 4; NUM_INTERSECTION_CATS_JAAD_PADDED = 5\n",
    "NUM_AGE_CATS_JAAD = 4; NUM_GENDER_CATS_JAAD = 3\n",
    "NUM_TRAFFIC_DIR_CATS_JAAD = 2\n",
    "LANE_CATEGORIES_JAAD = {1:0, 2:1, 3:2, 4:3, 5:4, 6:4, 7:4, 8:4, -1:1}\n",
    "NUM_LANE_CATS_JAAD = 5\n",
    "INPUT_SIZE_STATIC_JAAD_PADDED = (\n",
    "    NUM_SIGNALIZED_CATS_JAAD_PADDED + NUM_INTERSECTION_CATS_JAAD_PADDED +\n",
    "    NUM_AGE_CATS_JAAD + NUM_GENDER_CATS_JAAD + NUM_TRAFFIC_DIR_CATS_JAAD + NUM_LANE_CATS_JAAD\n",
    ")\n",
    "if INPUT_SIZE_STATIC_JAAD_PADDED != 23:\n",
    "    print(f\"[WARNING] JAAD Static context size is {INPUT_SIZE_STATIC_JAAD_PADDED}, PIE model might expect 23.\")\n",
    "\n",
    "BATCH_SIZE    = 8\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "YOLO_POSE_MODEL_PATH = 'yolov8n-pose.pt'\n",
    "\n",
    "\n",
    "def to_one_hot_jaad(index: int, num_classes: int) -> np.ndarray: # Unchanged\n",
    "    vec = np.zeros(num_classes, dtype=np.float32); safe_index = int(np.clip(index, 0, num_classes - 1)); vec[safe_index] = 1.0; return vec\n",
    "def infer_ped_action_from_pose_sequence(pose_sequence_np: np.ndarray): # Unchanged (heuristic)\n",
    "    if pose_sequence_np.ndim == 2: num_kpts = pose_sequence_np.shape[1]//2;\n",
    "    if num_kpts==0: return [0.0]*pose_sequence_np.shape[0]; pose_sequence_np=pose_sequence_np.reshape(-1,num_kpts,2)\n",
    "    if pose_sequence_np.shape[1]!=17: return [0.0]*pose_sequence_np.shape[0]\n",
    "    la,ra=15,16; acts=[]; thm=0.01\n",
    "    for i in range(pose_sequence_np.shape[0]):\n",
    "        if i==0:acts.append(0.0);continue\n",
    "        lyp,lyc=pose_sequence_np[i-1,la,1],pose_sequence_np[i,la,1];ryp,ryc=pose_sequence_np[i-1,ra,1],pose_sequence_np[i,ra,1]\n",
    "        lv,rv=lyp>1e-3 and lyc>1e-3,ryp>1e-3 and ryc>1e-3; mov,vm=0.0,0\n",
    "        if lv:mov+=abs(lyc-lyp);vm+=1;\n",
    "        if rv:mov+=abs(ryc-ryp);vm+=1\n",
    "        avg_mov=(mov/vm) if vm>0 else 0.0; acts.append(1.0 if avg_mov>thm else 0.0)\n",
    "    return acts\n",
    "def infer_ped_look_from_pose_sequence(pose_sequence_np: np.ndarray): # Unchanged (heuristic)\n",
    "    if pose_sequence_np.ndim==2:num_kpts=pose_sequence_np.shape[1]//2\n",
    "    if num_kpts==0:return [0.0]*pose_sequence_np.shape[0];pose_sequence_np=pose_sequence_np.reshape(-1,num_kpts,2)\n",
    "    if pose_sequence_np.shape[1]!=17:return [0.0]*pose_sequence_np.shape[0]\n",
    "    ni=0;lks=[];ltl,lth=0.4,0.6\n",
    "    for i in range(pose_sequence_np.shape[0]):nx=pose_sequence_np[i,ni,0];lks.append(1.0 if nx>1e-3 and ltl<nx<lth else 0.0)\n",
    "    return lks\n",
    "\n",
    "class JAADDataset_Cell2(Dataset): # Definition mostly unchanged, yolo_model is passed\n",
    "    def __init__( self, jaad_db_obj: JAAD, jaad_sequences: list, seq_len: int, pred_len: int,\n",
    "                 active_streams_for_model: list[str], yolo_pose_model, all_jaad_annotations ):\n",
    "        self.jaad_obj=jaad_db_obj; self.sequences=jaad_sequences; self.seq_len=seq_len; self.pred_len=pred_len\n",
    "        self.model_active_streams=active_streams_for_model; self.yolo_pose_model=yolo_pose_model\n",
    "        self.total_yolo_pose_inference_time=0.0; self.num_yolo_pose_inferences=0\n",
    "        self.total_heuristic_inference_time=0.0; self.num_heuristic_inferences=0\n",
    "        self.jaad_full_db=all_jaad_annotations\n",
    "        if not self.sequences: print(f\"Warning: JAADDataset_Cell2 init with 0 sequences.\")\n",
    "    def _get_input_sizes_dict(self):\n",
    "        s={\"bbox\":INPUT_SIZE_BBOX_JAAD,\"ego_speed\":INPUT_SIZE_EGO_SPEED_JAAD,\"ego_acc\":INPUT_SIZE_EGO_ACC_JAAD,\n",
    "           \"ped_action\":INPUT_SIZE_PED_ACTION_JAAD,\"ped_look\":INPUT_SIZE_PED_LOOK_JAAD,\"static_context\":INPUT_SIZE_STATIC_JAAD_PADDED}\n",
    "        if \"pose\" in self.model_active_streams:s[\"pose\"]=INPUT_SIZE_POSE_JAAD # Not used by current model config\n",
    "        return s\n",
    "    def __len__(self): return len(self.sequences)\n",
    "    def __getitem__(self, idx: int): # Logic for providing dummy/inferred action/look\n",
    "        vid_id,ped_id,start_f,lbl=self.sequences[idx];vid_db=self.jaad_full_db.get(vid_id);ped_db=vid_db.get(\"ped_annotations\",{}).get(ped_id)\n",
    "        ped_attrs=ped_db.get(\"attributes\",{});veh_annots=vid_db.get(\"vehicle_annotations\",{})\n",
    "        ped_fs=ped_db.get(\"frames\",[]);\n",
    "        try:sf_idx=ped_fs.index(start_f)\n",
    "        except ValueError:raise ValueError(f\"F{start_f} for {ped_id}/{vid_id} not in frames.\")\n",
    "        seq_f_nums=[ped_fs[sf_idx+i] for i in range(self.seq_len) if sf_idx+i < len(ped_fs)]\n",
    "        if len(seq_f_nums)<self.seq_len:seq_f_nums.extend([seq_f_nums[-1]]*(self.seq_len-len(seq_f_nums)))\n",
    "        st_v_p,st_v=False,np.zeros(INPUT_SIZE_STATIC_JAAD_PADDED,dtype=np.float32)\n",
    "        fts={s:[] for s in ACTIVE_STREAMS_JAAD_MODEL};pose_4_heur=[]\n",
    "        for f_num in seq_f_nums:\n",
    "            try:fidx=ped_fs.index(f_num)\n",
    "            except ValueError:fidx=-1\n",
    "            if not st_v_p and \"static_context\" in ACTIVE_STREAMS_JAAD_MODEL:\n",
    "                s,it=ped_attrs.get(\"signalized\",0),ped_attrs.get(\"intersection\",0);a,g=ped_attrs.get(\"age\",2),ped_attrs.get(\"gender\",0)\n",
    "                td,nl=int(ped_attrs.get(\"traffic_direction\",0)),ped_attrs.get(\"num_lanes\",2);nlc=LANE_CATEGORIES_JAAD.get(nl,LANE_CATEGORIES_JAAD.get(2))\n",
    "                st_v=np.concatenate([to_one_hot_jaad(s,NUM_SIGNALIZED_CATS_JAAD_PADDED),to_one_hot_jaad(it,NUM_INTERSECTION_CATS_JAAD_PADDED),\n",
    "                                     to_one_hot_jaad(a,NUM_AGE_CATS_JAAD),to_one_hot_jaad(g,NUM_GENDER_CATS_JAAD),\n",
    "                                     to_one_hot_jaad(td,NUM_TRAFFIC_DIR_CATS_JAAD),to_one_hot_jaad(nlc,NUM_LANE_CATS_JAAD)]).astype(np.float32);st_v_p=True\n",
    "            bbox_d=ped_db[\"bbox\"][fidx] if fidx!=-1 else [0,0,0,0]\n",
    "            if \"bbox\" in ACTIVE_STREAMS_JAAD_MODEL:\n",
    "                x1,y1,x2,y2=bbox_d;iw,ih=vid_db[\"width\"],vid_db[\"height\"]\n",
    "                cx,cy,w,h=((x1+x2)/2)/iw if iw>0 else 0,((y1+y2)/2)/ih if ih>0 else 0,(x2-x1)/iw if iw>0 else 0,(y2-y1)/ih if ih>0 else 0\n",
    "                fts[\"bbox\"].append(np.array([cx,cy,w,h],dtype=np.float32))\n",
    "            curr_f_p=np.zeros(INPUT_SIZE_POSE_JAAD,dtype=np.float32)\n",
    "            if self.yolo_pose_model and fidx!=-1:\n",
    "                img_p=self.jaad_obj._get_image_path(vid_id,f_num)\n",
    "                if os.path.exists(img_p):\n",
    "                    img=cv2.imread(img_p)\n",
    "                    if img is not None:\n",
    "                        x1r,y1r,x2r,y2r=bbox_d;x1,y1,x2,y2=int(x1r),int(y1r),int(x2r),int(y2r)\n",
    "                        x1c,y1c=max(0,x1),max(0,y1);x2c,y2c=min(img.shape[1],x2),min(img.shape[0],y2)\n",
    "                        if x1c<x2c and y1c<y2c:\n",
    "                            crop=img[y1c:y2c,x1c:x2c]\n",
    "                            if crop.size>0:\n",
    "                                ts=time.time();res=self.yolo_pose_model(crop,verbose=False,device=DEVICE)\n",
    "                                self.total_yolo_pose_inference_time+=(time.time()-ts);self.num_yolo_pose_inferences+=1\n",
    "                                if res[0].keypoints and res[0].keypoints.data.numel()>0:\n",
    "                                    kpts_n=res[0].keypoints.xyn[0].cpu().numpy()\n",
    "                                    if kpts_n.size>0:flat_k=kpts_n.flatten();cl=len(flat_k)\n",
    "                                    if cl<INPUT_SIZE_POSE_JAAD:curr_f_p[:cl]=flat_k\n",
    "                                    else:curr_f_p=flat_k[:INPUT_SIZE_POSE_JAAD]\n",
    "            pose_4_heur.append(curr_f_p)\n",
    "            ego_a=veh_annots.get(f_num,0)\n",
    "            if \"ego_speed\" in ACTIVE_STREAMS_JAAD_MODEL:fts[\"ego_speed\"].append([float(ego_a)/4.0])\n",
    "            if \"ego_acc\" in ACTIVE_STREAMS_JAAD_MODEL:fts[\"ego_acc\"].append([0.,0.])\n",
    "            if \"static_context\" in ACTIVE_STREAMS_JAAD_MODEL:fts[\"static_context\"].append(st_v)\n",
    "        pose_seq_np=np.array(pose_4_heur);ths=time.time()\n",
    "        inf_acts=infer_ped_action_from_pose_sequence(pose_seq_np);inf_looks=infer_ped_look_from_pose_sequence(pose_seq_np)\n",
    "        self.total_heuristic_inference_time+=(time.time()-ths);self.num_heuristic_inferences+=1\n",
    "        if \"ped_action\" in ACTIVE_STREAMS_JAAD_MODEL:fts[\"ped_action\"]=[[a] for a in inf_acts]\n",
    "        if \"ped_look\" in ACTIVE_STREAMS_JAAD_MODEL:fts[\"ped_look\"]=[[l] for l in inf_looks]\n",
    "        final_fts={};\n",
    "        for sn in ACTIVE_STREAMS_JAAD_MODEL:\n",
    "            fa=np.asarray(fts[sn],dtype=np.float32);ed=self._get_input_sizes_dict().get(sn,1)\n",
    "            if fa.ndim==1 and ed==1:fa=fa.reshape(-1,1)\n",
    "            if fa.shape[0]<self.seq_len:pad_r=self.seq_len-fa.shape[0];pad_sh=(pad_r,)+fa.shape[1:] if fa.ndim>1 else (pad_r,ed);fa=np.vstack((fa,np.zeros(pad_sh,dtype=np.float32)))\n",
    "            if fa.shape[0]!=self.seq_len or (fa.ndim>1 and fa.shape[1]!=ed):fa=np.zeros((self.seq_len,ed),dtype=np.float32)\n",
    "            final_fts[sn]=torch.tensor(fa,dtype=torch.float32)\n",
    "        return final_fts,torch.tensor(lbl,dtype=torch.long)\n",
    "\n",
    "class Attention(nn.Module): # Unchanged\n",
    "    def __init__(self, hidden_dim, attention_dim):super().__init__(); self.attention_net = nn.Sequential(nn.Linear(hidden_dim,attention_dim),nn.Tanh(),nn.Linear(attention_dim,1))\n",
    "    def forward(self, lstm_output):att_scores=self.attention_net(lstm_output).squeeze(-1); att_weights=torch.softmax(att_scores,dim=1);ctx_vec=torch.sum(lstm_output*att_weights.unsqueeze(-1),dim=1); return ctx_vec,att_weights\n",
    "class MultiStreamWeightedAvgLSTM(nn.Module): # Unchanged\n",
    "    def __init__(self,input_sizes,lstm_hidden_size,num_lstm_layers,num_classes,attention_dim,dropout_rate,stream_names=None):\n",
    "        super().__init__(); assert stream_names,\"s_names empty.\"; self.stream_names=stream_names; self.num_active_streams=len(stream_names)\n",
    "        self.lstm_output_dim=lstm_hidden_size*2; self.lstms,self.attentions=nn.ModuleDict(),nn.ModuleDict(); print(f\"Init Model: {self.stream_names}\")\n",
    "        for name in self.stream_names:\n",
    "            assert name in input_sizes,f\"Input size for {name} missing.\";in_s=input_sizes[name];print(f\"  – Str '{name}'(in {in_s})\")\n",
    "            self.lstms[name]=nn.LSTM(in_s,lstm_hidden_size,num_lstm_layers,batch_first=True,dropout=dropout_rate if num_lstm_layers>1 else 0,bidirectional=True)\n",
    "            self.attentions[name]=Attention(self.lstm_output_dim,attention_dim)\n",
    "        self.fusion_weights=nn.Parameter(torch.ones(self.num_active_streams));fused_d=self.lstm_output_dim;self.dropout=nn.Dropout(dropout_rate)\n",
    "        inter_d=max(num_classes*4,fused_d//2);self.fc1=nn.Linear(fused_d,inter_d);self.relu=nn.ReLU();self.fc2=nn.Linear(inter_d,num_classes)\n",
    "    def forward(self, x):\n",
    "        ctx_vs,bsz,dev=[],-1,None;\n",
    "        for n in self.stream_names:\n",
    "            if n in x and x[n].numel()>0:bsz,dev=x[n].shape[0],x[n].device;break\n",
    "        assert bsz!=-1,\"No valid input batch.\"\n",
    "        for n in self.stream_names:\n",
    "            if n not in x or x[n].numel()==0:ctx_vs.append(torch.zeros(bsz,self.lstm_output_dim,device=dev));continue\n",
    "            lstm_o,_=self.lstms[n](x[n]);ctx_v,_=self.attentions[n](lstm_o);ctx_vs.append(ctx_v)\n",
    "        assert len(ctx_vs)==self.num_active_streams;stacked_ctx=torch.stack(ctx_vs,dim=1);norm_w=torch.softmax(self.fusion_weights,dim=0)\n",
    "        re_w=norm_w.view(1,-1,1);fused_r=torch.sum(stacked_ctx*re_w,dim=1);out=self.dropout(fused_r);out=self.relu(self.fc1(out))\n",
    "        out=self.dropout(out);logits=self.fc2(out);return logits\n",
    "\n",
    "def get_all_probabilities_and_labels(m,dl,d): # Unchanged\n",
    "    m.eval();ls,ps=[],[];asts=m.stream_names\n",
    "    with torch.no_grad():\n",
    "        for fts,lbls in tqdm(dl,desc=\"Getting Probs&Labels\",leave=False):\n",
    "            ins={n:fts[n].to(d) for n in asts if n in fts and fts[n].numel()>0};\n",
    "            if not ins: continue\n",
    "            o=m(ins);p=torch.softmax(o,dim=1);ls.extend(lbls.cpu().numpy());ps.extend(p[:,1].cpu().numpy())\n",
    "    return np.asarray(ls),np.asarray(ps)\n",
    "def find_optimal_threshold(yt,yp,metric='f1',steps=100): # Unchanged\n",
    "    bt,bm=0.5,-1.;mf=lambda t,p:precision_recall_fscore_support(t,p,average='binary',pos_label=1,zero_division=0)[2]\n",
    "    ths=np.linspace(0.,1.,steps+1)\n",
    "    for th in tqdm(ths,desc=f\"TuningThresh({metric.upper()})\",leave=False):\n",
    "        if len(yt)==0:continue\n",
    "        # Ensure yp is not empty for prd calculation, though get_all_probabilities_and_labels should ensure len(yp) == len(yt)\n",
    "        if len(yp) == 0: prd = np.array([], dtype=int) \n",
    "        else: prd=(yp>=th).astype(int)\n",
    "        if len(yt) != len(prd) and len(yt) > 0 : # Mismatch, can happen if yp was empty but yt not (should not occur with current get_all_prob)\n",
    "            # print(f\"Warning: Mismatch len(yt)={len(yt)}, len(prd)={len(prd)} for threshold {th}. Skipping this threshold.\")\n",
    "            continue # Or handle as appropriate, e.g. by setting cur_m to 0\n",
    "        cur_m=mf(yt,prd)\n",
    "        if cur_m>bm:bm,bt=cur_m,th\n",
    "        elif cur_m==bm and abs(th-0.5)<abs(bt-0.5):bt=th\n",
    "    return bt,bm\n",
    "def get_predictions_at_threshold(yp,th): # Unchanged, added debug prints earlier\n",
    "    # print(f\"    DEBUG (get_predictions_at_threshold): yp type: {type(yp)}, yp is None: {yp is None}, th: {th}\")\n",
    "    # if isinstance(yp, np.ndarray): print(f\"    DEBUG (get_predictions_at_threshold): yp shape: {yp.shape}\")\n",
    "    result = (yp >= th).astype(int)\n",
    "    # print(f\"    DEBUG (get_predictions_at_threshold): result type: {type(result)}, result is None: {result is None}\")\n",
    "    # if isinstance(result, np.ndarray): print(f\"    DEBUG (get_predictions_at_threshold): result shape: {result.shape}\")\n",
    "    return result\n",
    "# -----------------------------------------------------------------------------#\n",
    "#                                Main Execution Block                          #\n",
    "# -----------------------------------------------------------------------------#\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(JAAD_DATASET_ROOT):\n",
    "        print(f\"Error: JAAD_DATASET_ROOT ({JAAD_DATASET_ROOT}) does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"--- JAAD DATASET INFERENCE (Hybrid: GT for some, Inferred Beh from Pose) ---\")\n",
    "    print(f\"--- Processing specified videos: {VIDEOS_TO_PROCESS_FOR_VALIDATION} ---\")\n",
    "\n",
    "    print(\"\\nInitializing JAAD object...\")\n",
    "    if not os.path.exists(JAAD_IMAGES_PATH):\n",
    "        os.makedirs(JAAD_IMAGES_PATH); print(f\"Created JAAD images dir: {JAAD_IMAGES_PATH}\")\n",
    "\n",
    "    jaad_obj = JAAD(data_path=JAAD_DATASET_ROOT)\n",
    "    jaad_obj._clips_path = JAAD_CLIPS_PATH\n",
    "    jaad_obj._images_path = JAAD_IMAGES_PATH\n",
    "\n",
    "    # Use the user-specified list of videos for extraction and processing\n",
    "    videos_subset_for_run = VIDEOS_TO_PROCESS_FOR_VALIDATION\n",
    "    print(f\"Videos to be processed in this run: {videos_subset_for_run}\")\n",
    "\n",
    "    images_exist = True\n",
    "    if FORCE_REEXTRACT_IMAGES_FOR_SPECIFIED_VIDEOS: images_exist = False\n",
    "    else:\n",
    "        for vid in videos_subset_for_run:\n",
    "            vid_dir = os.path.join(JAAD_IMAGES_PATH, vid)\n",
    "            if not os.path.exists(vid_dir) or not os.listdir(vid_dir):\n",
    "                images_exist = False; print(f\"Images missing for {vid}. Will extract.\"); break\n",
    "    if not images_exist or FORCE_REEXTRACT_IMAGES_FOR_SPECIFIED_VIDEOS:\n",
    "        print(f\"Extracting images for {len(videos_subset_for_run)} JAAD videos to {JAAD_IMAGES_PATH}...\")\n",
    "        if FORCE_REEXTRACT_IMAGES_FOR_SPECIFIED_VIDEOS:\n",
    "            for vid_clean in videos_subset_for_run:\n",
    "                dir_to_clean = os.path.join(JAAD_IMAGES_PATH, vid_clean)\n",
    "                if os.path.exists(dir_to_clean): shutil.rmtree(dir_to_clean)\n",
    "        jaad_obj.extract_and_save_images(videos_to_process=videos_subset_for_run) # Pass the list\n",
    "        print(\"✓ JAAD images extracted for the specified subset.\")\n",
    "    else: print(\"JAAD images for the specified subset already exist.\")\n",
    "\n",
    "    print(\"\\nLoading/Generating JAAD database (annotations)...\")\n",
    "    jaad_full_db = jaad_obj.generate_database(); print(\"✓ JAAD DB loaded/generated.\")\n",
    "\n",
    "    print(f\"\\nEnumerating JAAD validation sequences for specified videos...\")\n",
    "    if not os.path.exists(JAAD_VAL_SEQUENCES_PKL_PATH) or FORCE_REENUMERATE_VAL_SEQUENCES:\n",
    "        # Enumerate sequences *only* from the videos specified in VIDEOS_TO_PROCESS_FOR_VALIDATION\n",
    "        # We treat these specified videos as our \"validation set\" for this run.\n",
    "        val_video_ids_for_enum = videos_subset_for_run\n",
    "        print(f\"DEBUG: Effective val_video_ids for enumeration: {val_video_ids_for_enum}\")\n",
    "\n",
    "        def enumerate_sequences(vids,split_name,full_db):\n",
    "            seqs=[]; print(f\"Enumerating for {split_name} ({len(vids)} videos)...\")\n",
    "            for v_id in tqdm(vids,desc=f\"Videos({split_name})\"):\n",
    "                vid_data=full_db.get(v_id);\n",
    "                if not vid_data:continue\n",
    "                for p_id,p_data in vid_data.get(\"ped_annotations\",{}).items():\n",
    "                    if 'b' not in p_id:continue\n",
    "                    frames=p_data.get(\"frames\",[]);\n",
    "                    if len(frames)<SEQ_LEN+PRED_LEN:continue\n",
    "                    cross_val=p_data.get(\"attributes\",{}).get(\"crossing\",0);lbl=1 if cross_val==1 else 0\n",
    "                    for i in range(len(frames)-SEQ_LEN-PRED_LEN+1):\n",
    "                        seqs.append((v_id,p_id,frames[i],lbl))\n",
    "            print(f\"  {split_name} generated {len(seqs)} sequences.\")\n",
    "            return seqs\n",
    "        all_val_sequences_tuples=enumerate_sequences(val_video_ids_for_enum,\"custom_val\",jaad_full_db)\n",
    "        with open(JAAD_VAL_SEQUENCES_PKL_PATH,\"wb\") as f:pickle.dump(all_val_sequences_tuples,f)\n",
    "        print(f\"✓ JAAD val sequence list for custom subset saved. Found {len(all_val_sequences_tuples)} sequences.\")\n",
    "    else:\n",
    "        print(\"Loading pre-prepared JAAD val sequence list for custom subset...\");\n",
    "        with open(JAAD_VAL_SEQUENCES_PKL_PATH,\"rb\") as f:all_val_sequences_tuples=pickle.load(f)\n",
    "        print(f\"✓ JAAD val sequence list for custom subset loaded. Found {len(all_val_sequences_tuples)} sequences.\")\n",
    "\n",
    "    print(\"\\nLoading YOLOv8-pose model...\");\n",
    "    yolo_pose_model=YOLO(YOLO_POSE_MODEL_PATH).to(DEVICE);\n",
    "    print(\"✓ YOLO loaded.\")\n",
    "    \n",
    "    print(\"\\nCreating JAAD Validation DataLoader for custom subset...\")\n",
    "    if not all_val_sequences_tuples:print(\"Warning: No val seqs for custom JAAD subset. Evaluation will be skipped.\")\n",
    "    \n",
    "    val_jaad_dataset=JAADDataset_Cell2(jaad_obj,all_val_sequences_tuples,SEQ_LEN,PRED_LEN,\n",
    "                                     ACTIVE_STREAMS_JAAD_MODEL,\n",
    "                                     yolo_pose_model,\n",
    "                                     jaad_full_db)\n",
    "    val_loader_jaad=DataLoader(val_jaad_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=0)\n",
    "    print(f\"✓ JAAD Validation DataLoader ready (Val:{len(val_jaad_dataset)}).\")\n",
    "\n",
    "    print(\"\\nInitializing model & loading pre-trained weights...\")\n",
    "    jaad_input_sizes = {}\n",
    "    for s_name_model in ACTIVE_STREAMS_JAAD_MODEL:\n",
    "        if s_name_model == \"static_context\": jaad_input_sizes[s_name_model] = INPUT_SIZE_STATIC_JAAD_PADDED\n",
    "        elif f\"INPUT_SIZE_{s_name_model.upper()}_JAAD\" in globals(): jaad_input_sizes[s_name_model] = globals()[f\"INPUT_SIZE_{s_name_model.upper()}_JAAD\"]\n",
    "        else: raise KeyError(f\"Input size constant not found for model stream: {s_name_model}\")\n",
    "\n",
    "    model=MultiStreamWeightedAvgLSTM(jaad_input_sizes,256,2,2,128,0.3,ACTIVE_STREAMS_JAAD_MODEL).to(DEVICE)\n",
    "    if os.path.exists(PRETRAINED_MODEL_PATH):\n",
    "        try:model.load_state_dict(torch.load(PRETRAINED_MODEL_PATH,map_location=DEVICE));print(f\"✓ Pre-trained loaded: {PRETRAINED_MODEL_PATH}\")\n",
    "        except Exception as e:print(f\"[ERROR]Load pre-trained:{e}. Exit.\");sys.exit(1)\n",
    "    else:print(f\"Pre-trained {PRETRAINED_MODEL_PATH} not found. Exit.\");sys.exit(1)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"\\n---Starting Inference & Eval on JAAD custom subset (Hybrid)---\")\n",
    "    val_jaad_dataset.total_yolo_pose_inference_time=0.;val_jaad_dataset.num_yolo_pose_inferences=0\n",
    "    val_jaad_dataset.total_heuristic_inference_time=0.;val_jaad_dataset.num_heuristic_inferences=0\n",
    "    total_model_inf_time,num_model_inf_batches=0.,0\n",
    "    final_val_lbls,final_val_probs_p=[],[]\n",
    "\n",
    "    if len(val_loader_jaad.dataset)>0:\n",
    "        tmp_lbls,tmp_probs=[],[]\n",
    "        with torch.no_grad():\n",
    "            for fts,lbls in tqdm(val_loader_jaad,desc=\"Model Inference on Val Set\",leave=False):\n",
    "                ins={n:fts[n].to(DEVICE) for n in ACTIVE_STREAMS_JAAD_MODEL if n in fts and fts[n].numel()>0}\n",
    "                if not ins:continue\n",
    "                st_m=time.time();outs=model(ins);total_model_inf_time+=(time.time()-st_m);num_model_inf_batches+=1\n",
    "                prbs=torch.softmax(outs,dim=1);tmp_lbls.extend(lbls.cpu().numpy());tmp_probs.extend(prbs[:,1].cpu().numpy())\n",
    "        final_val_lbls,final_val_probs_p=np.asarray(tmp_lbls),np.asarray(tmp_probs)\n",
    "        print(f\"Model inf pass complete. Processed {len(final_val_lbls)} samples in {num_model_inf_batches} batches.\")\n",
    "    else:print(\"Val dataset empty. Skipping eval.\")\n",
    "\n",
    "    avg_yolo_t=(val_jaad_dataset.total_yolo_pose_inference_time/val_jaad_dataset.num_yolo_pose_inferences*1000) if val_jaad_dataset.num_yolo_pose_inferences>0 else 0\n",
    "    avg_heur_t=(val_jaad_dataset.total_heuristic_inference_time/val_jaad_dataset.num_heuristic_inferences*1000) if val_jaad_dataset.num_heuristic_inferences>0 else 0\n",
    "    avg_model_t_batch=(total_model_inf_time/num_model_inf_batches*1000) if num_model_inf_batches>0 else 0\n",
    "    avg_model_t_sample=(total_model_inf_time/len(final_val_lbls)*1000) if len(final_val_lbls)>0 else 0\n",
    "    print(f\"\\n---Inf Time Breakdown(Val Set)---\")\n",
    "    print(f\"Avg YOLO Pose/frame with ped:{avg_yolo_t:.2f}ms({val_jaad_dataset.num_yolo_pose_inferences} total pose est.)\")\n",
    "    print(f\"Avg Heuristic Beh Inf/sequence:{avg_heur_t:.3f}ms({val_jaad_dataset.num_heuristic_inferences} seqs processed)\")\n",
    "    print(f\"Avg Main Model Fwd Pass/batch:{avg_model_t_batch:.2f}ms\")\n",
    "    if len(final_val_lbls)>0:print(f\"Avg Main Model Fwd Pass/sample:{avg_model_t_sample:.2f}ms\")\n",
    "    \n",
    "    print(f\"\\nDEBUG: Unique ground truth labels in validation set: {np.unique(final_val_lbls, return_counts=True) if len(final_val_lbls) > 0 else 'Empty'}\")\n",
    "    if len(final_val_lbls) > 0 and len(final_val_probs_p) > 0 : # Ensure probs is also populated\n",
    "        print(f\"DEBUG: final_val_probs_pos min: {np.min(final_val_probs_p)}, max: {np.max(final_val_probs_p)}, mean: {np.mean(final_val_probs_p)}\")\n",
    "        plt.figure(figsize=(6,3)); plt.hist(final_val_probs_p, bins=20); plt.title(\"Hist of Pred Probs (Class 1)\"); plt.xlabel(\"Prob\"); plt.ylabel(\"Freq\"); plt.show()\n",
    "\n",
    "        final_opt_th, _ = find_optimal_threshold(final_val_lbls,final_val_probs_p,metric='f1',steps=100)\n",
    "        final_val_preds_t=get_predictions_at_threshold(final_val_probs_p,final_opt_th)\n",
    "        \n",
    "        print(f\"--- DEBUG BEFORE ACCURACY SCORE ---\")\n",
    "        print(f\"final_val_labels type:{type(final_val_lbls)},len:{len(final_val_lbls) if isinstance(final_val_lbls,(list,np.ndarray))else 'N/A'}\")\n",
    "        if isinstance(final_val_lbls,np.ndarray):print(f\"final_val_labels shape:{final_val_lbls.shape}\")\n",
    "        print(f\"final_val_preds_tuned type:{type(final_val_preds_t)},is None:{final_val_preds_t is None}\")\n",
    "        if isinstance(final_val_preds_t,np.ndarray):print(f\"final_val_preds_tuned shape:{final_val_preds_t.shape}\")\n",
    "        print(f\"--- END DEBUG ---\")\n",
    "\n",
    "        if final_val_preds_t is None:print(\"ERROR:final_val_preds_tuned is None. Aborting.\")\n",
    "        else:\n",
    "            final_acc_t=accuracy_score(final_val_lbls,final_val_preds_t)\n",
    "            final_prec_t,final_rec_t,final_f1_t,_=precision_recall_fscore_support(final_val_lbls,final_val_preds_t,average=\"binary\",pos_label=1,zero_division=0)\n",
    "            final_auc_v=roc_auc_score(final_val_lbls,final_val_probs_p) if len(np.unique(final_val_lbls))>1 and len(final_val_probs_p)>0 else float('nan')\n",
    "            print(\"\\n---Final JAAD Metrics(Tuned Thresh,Inferred Beh)---\")\n",
    "            print(f\"{'F1':<25}:{final_f1_t:.4f}\")            \n",
    "    else:print(\"No val samples processed, cannot calc final metrics.\")\n",
    "    if hasattr(model,\"fusion_weights\"):w=torch.softmax(model.fusion_weights,0).detach().cpu().numpy();print(\"\\n---Model Fusion Weights---\");[print(f\"{s:<15}:{wg:.4f}\")for s,wg in zip(model.stream_names,w)]\n",
    "    print(\"\\n---JAAD CUSTOM SUBSET INFERENCE & EVALUATION SCRIPT COMPLETE---\")\n",
    "    if JAAD_IMAGES_PATH.startswith(\"/kaggle/working/\") and os.path.exists(JAAD_IMAGES_PATH):\n",
    "        print(f\"\\nCleaning up extracted images at {JAAD_IMAGES_PATH}...\")\n",
    "        try:shutil.rmtree(JAAD_IMAGES_PATH);print(\"✓ Image Cleanup successful.\")\n",
    "        except Exception as e:print(f\"Error image cleanup:{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb05ef",
   "metadata": {
    "papermill": {
     "duration": 0.314384,
     "end_time": "2025-05-26T16:12:37.739733",
     "exception": false,
     "start_time": "2025-05-26T16:12:37.425349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7411543,
     "sourceId": 11801892,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7496034,
     "sourceId": 11922980,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 279383,
     "modelInstanceId": 258142,
     "sourceId": 302300,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 283333,
     "modelInstanceId": 262207,
     "sourceId": 307831,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 288527,
     "modelInstanceId": 267476,
     "sourceId": 316944,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 297682,
     "modelInstanceId": 276781,
     "sourceId": 329886,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 297702,
     "modelInstanceId": 276800,
     "sourceId": 329908,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 314775,
     "modelInstanceId": 294156,
     "sourceId": 352620,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 342770,
     "modelInstanceId": 322103,
     "sourceId": 391172,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 342793,
     "modelInstanceId": 322120,
     "sourceId": 391205,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13040.546937,
   "end_time": "2025-05-26T16:12:40.737861",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-26T12:35:20.190924",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "029d31eee77b4b4bb88c3959134fde31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_220de51a99864d15bb1e2ebc13fbceac",
       "max": 101.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a5f2f0ef7f40405395b3c3d6cb0fee69",
       "tabbable": null,
       "tooltip": null,
       "value": 101.0
      }
     },
     "1e0f65f71f534b1d9261db52d257b2df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "220de51a99864d15bb1e2ebc13fbceac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "246546a6c7c8442c9fbee64fb17c3b18": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2ccc35182891496bae1aabaf5dbfc086": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "34c82acf0ba448928d10adf7b479d421": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3f873d1ba7c14ffeb28da81a942b904d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e55e4cbd67e84fe0928b8e709a706e46",
       "max": 29.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cc5e50b4dba444248499677a32b9ee14",
       "tabbable": null,
       "tooltip": null,
       "value": 29.0
      }
     },
     "4016a8f9e5304335b6beb1b5fbbf5696": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_79caf9d717954397b8d8d1665404241a",
        "IPY_MODEL_029d31eee77b4b4bb88c3959134fde31",
        "IPY_MODEL_d18150512e4d41c993743f15a577e68c"
       ],
       "layout": "IPY_MODEL_a2895e0558fc4e11b6203cf0bed551d6",
       "tabbable": null,
       "tooltip": null
      }
     },
     "415948007630487385fdf0a11dcd49da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c53e865a65d44a888db0008d6ada225a",
       "placeholder": "​",
       "style": "IPY_MODEL_1e0f65f71f534b1d9261db52d257b2df",
       "tabbable": null,
       "tooltip": null,
       "value": " 1020/1020 [3:25:38&lt;00:00, 10.37s/it]"
      }
     },
     "490d8ab63023427ba821b39419f237f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "530657662c244b699931d790eab48734": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "57b9ff1783c1467c95b691c39eda4b63": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "6bacde70a45f4d49a16b024ed0707830": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "79caf9d717954397b8d8d1665404241a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a439e4ded20b423ca1a2552b3f06db1b",
       "placeholder": "​",
       "style": "IPY_MODEL_c5e57d48f06d4e4bab2b5228e064ade2",
       "tabbable": null,
       "tooltip": null,
       "value": "TuningThresh(F1):  71%"
      }
     },
     "8e740bf06aca4e3aa9fbae31c49bc4e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9c7dd2756c7c450ba8844df921dad0d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d34dbc037a35436baebebdd49b5dd515",
        "IPY_MODEL_cc26dd79de2140bdaecafbb33f039c48",
        "IPY_MODEL_415948007630487385fdf0a11dcd49da"
       ],
       "layout": "IPY_MODEL_57b9ff1783c1467c95b691c39eda4b63",
       "tabbable": null,
       "tooltip": null
      }
     },
     "9e740a9f8c5546c9ad64741cc2fc2ddf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a2895e0558fc4e11b6203cf0bed551d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "a439e4ded20b423ca1a2552b3f06db1b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a5f2f0ef7f40405395b3c3d6cb0fee69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ac27c9275e834d00972920b3e605db25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bae0465928d947ba86d999ec7e51f2c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c53e865a65d44a888db0008d6ada225a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c5e57d48f06d4e4bab2b5228e064ade2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cc26dd79de2140bdaecafbb33f039c48": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8e740bf06aca4e3aa9fbae31c49bc4e6",
       "max": 1020.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_246546a6c7c8442c9fbee64fb17c3b18",
       "tabbable": null,
       "tooltip": null,
       "value": 1020.0
      }
     },
     "cc5e50b4dba444248499677a32b9ee14": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d18150512e4d41c993743f15a577e68c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bae0465928d947ba86d999ec7e51f2c3",
       "placeholder": "​",
       "style": "IPY_MODEL_530657662c244b699931d790eab48734",
       "tabbable": null,
       "tooltip": null,
       "value": " 72/101 [00:00&lt;00:00, 361.32it/s]"
      }
     },
     "d34dbc037a35436baebebdd49b5dd515": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e5550af8bd6c4f61bfd51ea505ce513c",
       "placeholder": "​",
       "style": "IPY_MODEL_ac27c9275e834d00972920b3e605db25",
       "tabbable": null,
       "tooltip": null,
       "value": "Model Inference on Val Set: 100%"
      }
     },
     "e5550af8bd6c4f61bfd51ea505ce513c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e55e4cbd67e84fe0928b8e709a706e46": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f52ae35c2b4b4c6fa313a4c4870fe006": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6bacde70a45f4d49a16b024ed0707830",
       "placeholder": "​",
       "style": "IPY_MODEL_34c82acf0ba448928d10adf7b479d421",
       "tabbable": null,
       "tooltip": null,
       "value": "Videos(custom_val): 100%"
      }
     },
     "fe316b8b316a420b873b2a6c64ff6864": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f52ae35c2b4b4c6fa313a4c4870fe006",
        "IPY_MODEL_3f873d1ba7c14ffeb28da81a942b904d",
        "IPY_MODEL_fe437cf5aa30454fbc4df1b85db373a6"
       ],
       "layout": "IPY_MODEL_490d8ab63023427ba821b39419f237f2",
       "tabbable": null,
       "tooltip": null
      }
     },
     "fe437cf5aa30454fbc4df1b85db373a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2ccc35182891496bae1aabaf5dbfc086",
       "placeholder": "​",
       "style": "IPY_MODEL_9e740a9f8c5546c9ad64741cc2fc2ddf",
       "tabbable": null,
       "tooltip": null,
       "value": " 29/29 [00:00&lt;00:00, 1801.76it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
