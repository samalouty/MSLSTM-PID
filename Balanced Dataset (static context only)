{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11201333,"sourceType":"datasetVersion","datasetId":6993690},{"sourceId":11201362,"sourceType":"datasetVersion","datasetId":6993708},{"sourceId":11201388,"sourceType":"datasetVersion","datasetId":6993722},{"sourceId":11201422,"sourceType":"datasetVersion","datasetId":6993740},{"sourceId":11201506,"sourceType":"datasetVersion","datasetId":6993794},{"sourceId":11201543,"sourceType":"datasetVersion","datasetId":6993809},{"sourceId":11382982,"sourceType":"datasetVersion","datasetId":7127490},{"sourceId":11402679,"sourceType":"datasetVersion","datasetId":7142036},{"sourceId":302300,"sourceType":"modelInstanceVersion","modelInstanceId":258142,"modelId":279383},{"sourceId":307831,"sourceType":"modelInstanceVersion","modelInstanceId":262207,"modelId":283333},{"sourceId":316944,"sourceType":"modelInstanceVersion","modelInstanceId":267476,"modelId":288527},{"sourceId":329886,"sourceType":"modelInstanceVersion","modelInstanceId":276781,"modelId":297682},{"sourceId":329908,"sourceType":"modelInstanceVersion","modelInstanceId":276800,"modelId":297702}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/aras62/PIE.git\n!unzip /content/PIE/annotations/annotations.zip -d /content/PIE\n!unzip /content/PIE/annotations/annotations_vehicle.zip -d /content/PIE\n# !git clone https://github.com/hustvl/YOLOP.git\n!mkdir /kaggle/working/PIE/content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:27:07.771729Z","iopub.execute_input":"2025-04-19T13:27:07.772029Z","iopub.status.idle":"2025-04-19T13:27:08.263278Z","shell.execute_reply.started":"2025-04-19T13:27:07.771999Z","shell.execute_reply":"2025-04-19T13:27:08.262097Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'PIE' already exists and is not an empty directory.\nunzip:  cannot find or open /content/PIE/annotations/annotations.zip, /content/PIE/annotations/annotations.zip.zip or /content/PIE/annotations/annotations.zip.ZIP.\nunzip:  cannot find or open /content/PIE/annotations/annotations_vehicle.zip, /content/PIE/annotations/annotations_vehicle.zip.zip or /content/PIE/annotations/annotations_vehicle.zip.ZIP.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q ultralytics opencv-python-headless ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:27:08.264587Z","iopub.execute_input":"2025-04-19T13:27:08.264917Z","iopub.status.idle":"2025-04-19T13:27:13.959591Z","shell.execute_reply.started":"2025-04-19T13:27:08.264874Z","shell.execute_reply":"2025-04-19T13:27:13.958494Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.8/978.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# import os\n# import pickle\n# import sys\n# import numpy as np\n# from tqdm.notebook import tqdm\n# import random\n\n# print(\"--- PIE Database Cache Verification Script ---\")\n\n# # --- Configuration ---\n# # <<< --- SET THIS TO THE LOCATION OF YOUR GENERATED CACHE --- >>>\n# PKL_FILE_PATH = \"/kaggle/working/PIE/data_cache/pie_database.pkl\"\n# # <<< --- END CONFIGURATION --- >>>\n\n# # Define expected top-level keys (these are the set IDs)\n# EXPECTED_SETS = {'set01', 'set02', 'set03', 'set04', 'set05', 'set06'}\n\n# # Define expected keys within each video dictionary\n# EXPECTED_VIDEO_KEYS = {'num_frames', 'width', 'height',\n#                        'ped_annotations', 'traffic_annotations', 'vehicle_annotations'}\n\n# # Define expected keys within each pedestrian annotation dictionary\n# EXPECTED_PED_KEYS = {'frames', 'bbox', 'occlusion', 'behavior', 'attributes'}\n# EXPECTED_PED_BEHAVIOR_KEYS = {'gesture', 'look', 'action', 'cross'}\n# # Attributes can vary, so we won't check all exhaustively here, but check presence\n\n# # Define expected keys within vehicle (ego) frame dictionary (sample)\n# EXPECTED_EGO_FRAME_KEYS = {'OBD_speed', 'GPS_speed', 'accX', 'accY', 'accZ',\n#                            'gyroX', 'gyroY', 'gyroZ', 'heading_angle', 'latitude',\n#                            'longitude', 'pitch', 'roll', 'yaw'}\n\n\n# # --- Verification Parameters ---\n# MAX_FRAMES_TO_CHECK_PER_VIDEO = 50 # Limit checks per video for speed\n# MAX_PEDS_TO_CHECK_PER_VIDEO = 20  # Limit checks per video for speed\n# MAX_TRAFFIC_OBJS_TO_CHECK_PER_VIDEO = 20 # Limit checks\n# PRINT_SAMPLE_COUNT = 3 # How many sample data lines to print\n\n# # --- Counters and Flags ---\n# errors_found = 0\n# warnings_found = 0\n# checked_sets = 0\n# checked_videos = 0\n# checked_peds = 0\n# checked_ped_frames = 0\n# checked_ego_frames = 0\n\n# # --- Helper Function for Reporting ---\n# def report_error(message):\n#     global errors_found\n#     print(f\"  ERROR: {message}\")\n#     errors_found += 1\n\n# def report_warning(message):\n#     global warnings_found\n#     print(f\"  Warning: {message}\")\n#     warnings_found += 1\n\n# # --- 1. Load the PKL File ---\n# print(f\"\\n[1] Loading PKL file: {PKL_FILE_PATH}\")\n# if not os.path.exists(PKL_FILE_PATH):\n#     print(f\"  ERROR: PKL file not found at the specified path.\")\n#     exit()\n\n# try:\n#     with open(PKL_FILE_PATH, 'rb') as f:\n#         pie_database = pickle.load(f)\n#     print(\"  -> PKL file loaded successfully.\")\n# except Exception as e:\n#     print(f\"  ERROR: Failed to load PKL file: {e}\")\n#     exit()\n\n# # --- 2. Basic Structure Checks ---\n# print(f\"\\n[2] Checking Top-Level Structure...\")\n# if not isinstance(pie_database, dict):\n#     report_error(f\"Loaded data is not a dictionary (Type: {type(pie_database)}).\")\n#     exit()\n# print(f\"  -> Top level is a dictionary: OK\")\n\n# found_sets = set(pie_database.keys())\n# if found_sets != EXPECTED_SETS:\n#     report_warning(f\"Set keys mismatch. Found: {found_sets}, Expected: {EXPECTED_SETS}\")\n# else:\n#     print(f\"  -> Found expected set keys: OK\")\n# checked_sets = len(found_sets)\n\n# # --- 3. Detailed Content Checks ---\n# print(f\"\\n[3] Checking Set/Video/Annotation Structures...\")\n\n# sample_data_to_print = []\n\n# for set_id in tqdm(found_sets, desc=\"Checking Sets\"):\n#     if not isinstance(pie_database[set_id], dict):\n#         report_error(f\"Data for set '{set_id}' is not a dictionary.\")\n#         continue\n\n#     video_ids = list(pie_database[set_id].keys())\n#     checked_videos += len(video_ids)\n\n#     for video_id in tqdm(video_ids, desc=f\"Videos in {set_id}\", leave=False):\n#         video_data = pie_database[set_id][video_id]\n#         if not isinstance(video_data, dict):\n#             report_error(f\"Data for video '{set_id}/{video_id}' is not a dictionary.\")\n#             continue\n\n#         # Check video-level keys\n#         missing_vid_keys = EXPECTED_VIDEO_KEYS - set(video_data.keys())\n#         if missing_vid_keys:\n#             report_warning(f\"Video '{set_id}/{video_id}' missing keys: {missing_vid_keys}\")\n\n#         # Basic type checks for video keys\n#         img_width = video_data.get('width', -1)\n#         img_height = video_data.get('height', -1)\n#         if not isinstance(video_data.get('num_frames'), int): report_warning(f\"num_frames type mismatch in {set_id}/{video_id}\")\n#         if not isinstance(img_width, int): report_warning(f\"width type mismatch in {set_id}/{video_id}\")\n#         if not isinstance(img_height, int): report_warning(f\"height type mismatch in {set_id}/{video_id}\")\n#         if not isinstance(video_data.get('ped_annotations'), dict): report_error(f\"ped_annotations not a dict in {set_id}/{video_id}\"); continue # Stop checking peds for this video if structure wrong\n#         if not isinstance(video_data.get('traffic_annotations'), dict): report_warning(f\"traffic_annotations not a dict in {set_id}/{video_id}\")\n#         if not isinstance(video_data.get('vehicle_annotations'), dict): report_warning(f\"vehicle_annotations not a dict in {set_id}/{video_id}\")\n\n\n#         # --- Check Pedestrian Annotations ---\n#         ped_annotations = video_data.get('ped_annotations', {})\n#         ped_ids_to_check = list(ped_annotations.keys())\n#         random.shuffle(ped_ids_to_check) # Check a random subset\n\n#         for i, ped_id in enumerate(ped_ids_to_check):\n#             if i >= MAX_PEDS_TO_CHECK_PER_VIDEO: break # Limit checks\n#             checked_peds += 1\n#             ped_data = ped_annotations[ped_id]\n#             if not isinstance(ped_data, dict): report_error(f\"Data for ped '{ped_id}' in {set_id}/{video_id} is not a dict.\"); continue\n\n#             missing_ped_keys = EXPECTED_PED_KEYS - set(ped_data.keys())\n#             if missing_ped_keys: report_warning(f\"Ped '{ped_id}' in {set_id}/{video_id} missing keys: {missing_ped_keys}\")\n\n#             # Check structure of essential lists/dicts\n#             frames = ped_data.get('frames', [])\n#             bboxes = ped_data.get('bbox', [])\n#             occlusions = ped_data.get('occlusion', [])\n#             behavior = ped_data.get('behavior', {})\n#             attributes = ped_data.get('attributes', {})\n\n#             if not isinstance(frames, list): report_error(f\"Ped '{ped_id}' frames not a list.\"); continue\n#             if not isinstance(bboxes, list): report_error(f\"Ped '{ped_id}' bbox not a list.\"); continue\n#             if not isinstance(occlusions, list): report_error(f\"Ped '{ped_id}' occlusion not a list.\"); continue\n#             if not isinstance(behavior, dict): report_error(f\"Ped '{ped_id}' behavior not a dict.\"); continue\n#             if not isinstance(attributes, dict): report_warning(f\"Ped '{ped_id}' attributes not a dict.\"); continue # Attributes might be empty\n\n#             # Check list lengths consistency\n#             n_frames = len(frames)\n#             if n_frames == 0 and (len(bboxes) > 0 or len(occlusions) > 0): report_warning(f\"Ped '{ped_id}' has bboxes/occlusions but 0 frames listed.\")\n#             if len(bboxes) != n_frames: report_error(f\"Ped '{ped_id}' bbox length ({len(bboxes)}) != frames length ({n_frames}).\")\n#             if len(occlusions) != n_frames: report_error(f\"Ped '{ped_id}' occlusion length ({len(occlusions)}) != frames length ({n_frames}).\")\n\n#             missing_beh_keys = EXPECTED_PED_BEHAVIOR_KEYS - set(behavior.keys())\n#             if missing_beh_keys: report_warning(f\"Ped '{ped_id}' behavior missing keys: {missing_beh_keys}\")\n\n#             for beh_key, beh_list in behavior.items():\n#                 if not isinstance(beh_list, list): report_error(f\"Ped '{ped_id}' behavior '{beh_key}' not a list.\"); continue\n#                 if len(beh_list) != n_frames: report_error(f\"Ped '{ped_id}' behavior '{beh_key}' length ({len(beh_list)}) != frames length ({n_frames}).\")\n\n#             # Check sample frame content\n#             frames_to_check_in_ped = list(range(n_frames))\n#             random.shuffle(frames_to_check_in_ped)\n#             for k, frame_idx in enumerate(frames_to_check_in_ped):\n#                  if k >= MAX_FRAMES_TO_CHECK_PER_VIDEO: break\n#                  checked_ped_frames +=1\n#                  # Check frame number type\n#                  if not isinstance(frames[frame_idx], int): report_warning(f\"Ped '{ped_id}' frame value at index {frame_idx} not int.\")\n#                  # Check bbox format and range\n#                  if len(bboxes) > frame_idx:\n#                       bbox = bboxes[frame_idx]\n#                       if not isinstance(bbox, list) or len(bbox) != 4: report_error(f\"Ped '{ped_id}' frame {frames[frame_idx]} bbox invalid format: {bbox}\"); continue\n#                       try:\n#                            x1,y1,x2,y2 = map(float, bbox)\n#                            if img_width>0 and img_height>0 and not (0 <= x1 < x2 <= img_width and 0 <= y1 < y2 <= img_height): report_warning(f\"Ped '{ped_id}' frame {frames[frame_idx]} bbox out of bounds: {[int(x) for x in bbox]} vs {img_width}x{img_height}\")\n#                       except (ValueError, TypeError): report_error(f\"Ped '{ped_id}' frame {frames[frame_idx]} bbox contains non-numeric values: {bbox}\")\n#                  # Check occlusion value\n#                  if len(occlusions) > frame_idx:\n#                       occ = occlusions[frame_idx]\n#                       if not isinstance(occ, int) or occ not in [0, 1, 2]: report_warning(f\"Ped '{ped_id}' frame {frames[frame_idx]} invalid occlusion value: {occ}\")\n\n#                  # Add sample for printing later\n#                  if len(sample_data_to_print) < PRINT_SAMPLE_COUNT:\n#                       sample_data_to_print.append(f\" Sample Ped Data: Set={set_id}, Vid={video_id}, Ped={ped_id}, Frame={frames[frame_idx]}, BBox={bboxes[frame_idx] if len(bboxes) > frame_idx else 'N/A'}, Occ={occlusions[frame_idx] if len(occlusions) > frame_idx else 'N/A'}\")\n\n#         # --- Check Vehicle Annotations (Ego Data) ---\n#         vehicle_annotations = video_data.get('vehicle_annotations', {})\n#         ego_frames_to_check = list(vehicle_annotations.keys())\n#         random.shuffle(ego_frames_to_check)\n\n#         for k, frame_num in enumerate(ego_frames_to_check):\n#             if k >= MAX_FRAMES_TO_CHECK_PER_VIDEO: break # Limit checks\n#             checked_ego_frames += 1\n#             if not isinstance(frame_num, int): report_warning(f\"Ego data frame key '{frame_num}' in {set_id}/{video_id} is not int.\") ; continue\n#             ego_frame_data = vehicle_annotations[frame_num]\n#             if not isinstance(ego_frame_data, dict): report_error(f\"Ego data for frame {frame_num} in {set_id}/{video_id} is not dict.\"); continue\n\n#             missing_ego_keys = EXPECTED_EGO_FRAME_KEYS - set(ego_frame_data.keys())\n#             # Don't warn about every missing key, just check a few critical ones\n#             if 'OBD_speed' not in ego_frame_data and 'GPS_speed' not in ego_frame_data: report_warning(f\"Ego frame {frame_num} in {set_id}/{video_id} missing speed data.\")\n#             for key in EXPECTED_EGO_FRAME_KEYS:\n#                 if key in ego_frame_data and not isinstance(ego_frame_data[key], (float, int)): report_warning(f\"Ego frame {frame_num} key '{key}' value is not float/int (type: {type(ego_frame_data[key])})\")\n\n#             # Add sample for printing later\n#             if len(sample_data_to_print) < PRINT_SAMPLE_COUNT * 2 and k < 5: # Print a few ego samples too\n#                   sample_data_to_print.append(f\" Sample Ego Data: Set={set_id}, Vid={video_id}, Frame={frame_num}, Speed={ego_frame_data.get('OBD_speed', 'N/A'):.2f}, AccX={ego_frame_data.get('accX', 'N/A'):.2f}\")\n\n# # --- 4. Print Summary ---\n# print(f\"\\n[4] Verification Summary ---\")\n# print(f\"  - Checked {checked_sets} sets.\")\n# print(f\"  - Checked {checked_videos} videos.\")\n# print(f\"  - Checked {checked_peds} pedestrian tracks (sampled max {MAX_PEDS_TO_CHECK_PER_VIDEO} per video).\")\n# print(f\"  - Checked {checked_ped_frames} pedestrian frame entries (sampled max {MAX_FRAMES_TO_CHECK_PER_VIDEO} per ped).\")\n# print(f\"  - Checked {checked_ego_frames} ego data frame entries (sampled max {MAX_FRAMES_TO_CHECK_PER_VIDEO} per video).\")\n# print(f\"  - Total Errors Found: {errors_found}\")\n# print(f\"  - Total Warnings Found: {warnings_found}\")\n\n# if errors_found == 0:\n#     print(\"\\n  >>> Structure and basic content checks PASSED (with potential warnings). <<<\")\n# else:\n#     print(\"\\n  >>> ERRORS FOUND during structural/content checks. Review messages above. <<<\")\n\n# # --- 5. Print Sample Data ---\n# if sample_data_to_print:\n#     print(\"\\n[5] Sample Data Points ---\")\n#     for line in sample_data_to_print:\n#         print(line)\n\n# print(\"\\n--- Verification Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:10.145506Z","iopub.execute_input":"2025-04-15T19:55:10.145861Z","iopub.status.idle":"2025-04-15T19:55:10.152835Z","shell.execute_reply.started":"2025-04-15T19:55:10.145834Z","shell.execute_reply":"2025-04-15T19:55:10.151785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport xml.etree.ElementTree as ET\nimport os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\nfrom tqdm.notebook import tqdm\nimport random\nimport math\nimport zipfile\nimport cv2\nfrom ultralytics import YOLO\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:10.154360Z","iopub.execute_input":"2025-04-15T19:55:10.154719Z","iopub.status.idle":"2025-04-15T19:55:16.590012Z","shell.execute_reply.started":"2025-04-15T19:55:10.154687Z","shell.execute_reply":"2025-04-15T19:55:16.589278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + 'annotations'):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.590843Z","iopub.execute_input":"2025-04-15T19:55:16.591210Z","iopub.status.idle":"2025-04-15T19:55:16.596284Z","shell.execute_reply.started":"2025-04-15T19:55:16.591187Z","shell.execute_reply":"2025-04-15T19:55:16.595261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations_vehicle.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + 'annotations_vehicle'):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.597390Z","iopub.execute_input":"2025-04-15T19:55:16.597839Z","iopub.status.idle":"2025-04-15T19:55:16.618074Z","shell.execute_reply.started":"2025-04-15T19:55:16.597803Z","shell.execute_reply":"2025-04-15T19:55:16.617129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations_attributes.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + \"annotations_attributes\"):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.619029Z","iopub.execute_input":"2025-04-15T19:55:16.619360Z","iopub.status.idle":"2025-04-15T19:55:16.635884Z","shell.execute_reply.started":"2025-04-15T19:55:16.619328Z","shell.execute_reply":"2025-04-15T19:55:16.634975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- IMPORTS ---\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import Dataset, DataLoader\n# import xml.etree.ElementTree as ET\n# import os\n# import numpy as np\n# from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n# from sklearn.preprocessing import StandardScaler # For standardizing ego features\n# from tqdm.notebook import tqdm\n# import random\n# import math\n# import zipfile\n# import cv2\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# import pickle\n# import time\n# import sys # For path manipulation if needed\n\n# # --- Add PIE utilities path if necessary (adjust path) ---\n# pie_utilities_path = '/kaggle/working/PIE/utilities'\n# if pie_utilities_path not in sys.path:\n#     sys.path.insert(0, pie_utilities_path)\n# try:\n#     # We only need PIE class to generate the database if needed\n#     from pie_data import PIE\n# except ImportError as e:\n#     print(f\"Warning: Could not import PIE class from {pie_utilities_path}. Database must already exist. Error: {e}\")\n#     PIE = None # Define PIE as None if import fails\n\n# # --- Configuration ---\n# PIE_ROOT_PATH = '/kaggle/working/PIE' # Path where PIE repo was cloned/unzipped\n# VIDEO_INPUT_DIR = '/kaggle/input' # Where pie-setXX video dataset folders are\n# POSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2' # Where setXX subdirs with PKLs are\n# PIE_DATABASE_CACHE_PATH = '/kaggle/input/pie-database/pie_database.pkl'\n\n# # --- Stream Control ---\n# ACTIVE_STREAMS = [\n#     'bbox',\n#     # 'pose',\n#     # 'ego_speed',\n#     # 'ego_acc',\n#     # 'ego_gyro',\n#     # 'ped_action',\n#     # 'ped_look',\n#     # 'ped_occlusion',\n#     # 'traffic_light',\n#     # 'static_context'\n# ]\n# print(f\"Active Streams: {ACTIVE_STREAMS}\")\n\n# # Model Hyperparameters\n# SEQ_LEN = 30\n# PRED_LEN = 1\n# # Input Sizes (Define for ALL potential streams)\n# INPUT_SIZE_BBOX = 4\n# INPUT_SIZE_POSE = 34\n# INPUT_SIZE_EGO_SPEED = 1\n# INPUT_SIZE_EGO_ACC = 2 # X, Y components\n# INPUT_SIZE_EGO_GYRO = 1 # Z component (Yaw rate)\n# INPUT_SIZE_PED_ACTION = 1 # 0:standing, 1:walking\n# INPUT_SIZE_PED_LOOK = 1 # 0:not-looking, 1:looking\n# INPUT_SIZE_PED_OCC = 1 # 0:none, 0.5:part, 1:full (normalized)\n# INPUT_SIZE_TL_STATE = 4 # 0:Undef, 1:Red, 2:Yellow, 3:Green (One-Hot)\n# # Static Feature Sizes (Matches pie_data.py mappings)\n# NUM_SIGNALIZED_CATS = 4\n# NUM_INTERSECTION_CATS = 5\n# NUM_AGE_CATS = 4\n# NUM_GENDER_CATS = 3\n# INPUT_SIZE_STATIC = NUM_SIGNALIZED_CATS + NUM_INTERSECTION_CATS + NUM_AGE_CATS + NUM_GENDER_CATS # = 16\n\n# LSTM_HIDDEN_SIZE = 256\n# NUM_LSTM_LAYERS = 2\n# DROPOUT_RATE = 0.3\n# NUM_CLASSES = 2\n# ATTENTION_DIM = 128\n\n# # Training Hyperparameters\n# LEARNING_RATE = 1e-4 # Possibly lower LR needed after balancing/more features\n# BATCH_SIZE = 32\n# NUM_EPOCHS = 10 # Increase epochs slightly for balanced data\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {DEVICE}\")\n\n# # Dataset Splits (Using PIE default)\n# TRAIN_SETS_STR = ['set01', 'set02', 'set04']\n# VAL_SETS_STR = ['set05', 'set06']\n# TEST_SETS_STR = ['set03'] # Although not used in training loop\n\n# # Mappings (Matches pie_data.py)\n# TL_STATE_MAP = {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}\n# NUM_TL_STATES = len(TL_STATE_MAP)\n# SIGNALIZED_MAP = {'n/a': 0, 'C': 1, 'S': 2, 'CS': 3}\n# INTERSECTION_MAP = {'midblock': 0, 'T': 1, 'T-left': 2, 'T-right': 3, 'four-way': 4}\n# AGE_MAP = {'child': 0, 'young': 1, 'adult': 2, 'senior': 3}\n# GENDER_MAP = {'n/a': 0, 'female': 1, 'male': 2}\n\n# # --- Helper: One-Hot Encoding ---\n# def to_one_hot(index, num_classes):\n#     vec = np.zeros(num_classes, dtype=np.float32)\n#     if 0 <= index < num_classes:\n#         vec[index] = 1.0\n#     else: # Handle unexpected index\n#         vec[0] = 1.0 # Default to first class\n#     return vec\n\n# # --- Balancing Function ---\n# def balance_samples_count(seq_data, label_type, random_seed=42):\n#     print('---------------------------------------------------------')\n#     print(f\"Balancing samples based on '{label_type}' key\")\n#     if label_type not in seq_data:\n#         raise KeyError(f\"Label type '{label_type}' not found.\")\n#     try:\n#         gt_labels = [lbl[0] for lbl in seq_data[label_type]]\n#     except (IndexError, TypeError):\n#         raise ValueError(f\"Labels under '{label_type}' not in expected format [[label_val]].\")\n\n#     if not all(l in [0, 1] for l in gt_labels):\n#         print(f\"Warning: Labels for balancing contain values other than 0 or 1.\")\n\n#     num_pos_samples = np.count_nonzero(np.array(gt_labels))\n#     num_neg_samples = len(gt_labels) - num_pos_samples\n#     new_seq_data = {}\n\n#     if num_neg_samples == num_pos_samples:\n#         print('Samples already balanced.')\n#         return seq_data.copy()\n#     else:\n#         print(f'Unbalanced: Positive (1): {num_pos_samples} | Negative (0): {num_neg_samples}')\n#         majority_label = 0 if num_neg_samples > num_pos_samples else 1\n#         minority_count = min(num_neg_samples, num_pos_samples)\n#         print(f\"Undersampling majority class ({majority_label}) to match count ({minority_count}).\")\n\n#         majority_indices = np.where(np.array(gt_labels) == majority_label)[0]\n#         minority_indices = np.where(np.array(gt_labels) != majority_label)[0]\n#         np.random.seed(random_seed)\n#         keep_majority_indices = np.random.choice(majority_indices, size=minority_count, replace=False)\n#         final_indices = np.concatenate((minority_indices, keep_majority_indices))\n#         np.random.shuffle(final_indices)\n\n#         for k, v_list in seq_data.items():\n#             if isinstance(v_list, list) and len(v_list) == len(gt_labels):\n#                  try:\n#                      # Check if the list contains numpy arrays before converting the whole list\n#                      if v_list and isinstance(v_list[0], np.ndarray):\n#                          v_array = np.array(v_list)\n#                          new_seq_data[k] = list(v_array[final_indices])\n#                      else: # Assume list of lists or list of scalars\n#                           new_seq_data[k] = [v_list[i] for i in final_indices]\n#                  except Exception as e:\n#                       print(f\"Error processing key '{k}' during balancing: {e}. Skip.\")\n#                       new_seq_data[k] = []\n#             else:\n#                  print(f\"Warn: Skipping key '{k}' in balancing (not list or len mismatch).\")\n#                  new_seq_data[k] = v_list\n\n#         # Check if label key still exists after potential errors\n#         if label_type in new_seq_data:\n#              new_gt_labels = [lbl[0] for lbl in new_seq_data[label_type]]\n#              final_pos = np.count_nonzero(np.array(new_gt_labels))\n#              final_neg = len(new_gt_labels) - final_pos\n#              print(f'Balanced:   Positive (1): {final_pos} | Negative (0): {final_neg}')\n#         else:\n#              print(\"Error: Label key was lost during balancing process.\")\n\n#         print('---------------------------------------------------------')\n#         return new_seq_data\n\n\n# # --- Dataset Class ---\n# class PIEDataset(Dataset):\n#     def __init__(self, pie_database, set_names, pose_data_dir, seq_len, pred_len, scalers=None, active_streams=None):\n#         self.pie_db = pie_database\n#         self.set_names = set_names\n#         self.pose_data_dir = pose_data_dir\n#         self.seq_len = seq_len\n#         self.pred_len = pred_len\n#         self.scalers = scalers or {}\n#         self.active_streams = active_streams or []\n#         self.sequences = []\n#         self.all_pose_data = {}\n#         # Store input sizes needed for error handling in __getitem__\n#         self._input_sizes_for_error = self._get_input_sizes_dict()\n#         self._load_pose_data()\n#         self._generate_sequence_list()\n#         if not self.sequences:\n#             raise ValueError(f\"Dataset init failed: No sequences for {self.set_names}\")\n\n#     def _get_input_sizes_dict(self):\n#         # Helper to create input sizes dict, needed for error fallback in __getitem__\n#         input_sizes = {}\n#         for stream in self.active_streams:\n#             size_constant_name = f'INPUT_SIZE_{stream.upper()}'\n#             special_cases = {'TRAFFIC_LIGHT': 'TL_STATE', 'STATIC_CONTEXT': 'STATIC', 'EGO_SPEED': 'EGO_SPEED', 'EGO_ACC': 'EGO_ACC', 'EGO_GYRO': 'EGO_GYRO', 'PED_ACTION': 'PED_ACTION', 'PED_LOOK': 'PED_LOOK', 'PED_OCCLUSION': 'PED_OCC'}\n#             stream_upper_key = stream.upper()\n#             suffix = special_cases.get(stream_upper_key)\n#             if suffix:\n#                  size_constant_name = f'INPUT_SIZE_{suffix}'\n#             elif stream == 'bbox': size_constant_name = 'INPUT_SIZE_BBOX'\n#             elif stream == 'pose': size_constant_name = 'INPUT_SIZE_POSE'\n#             if size_constant_name in globals(): input_sizes[stream] = globals()[size_constant_name]\n#             else: input_sizes[stream] = 1 # Default size 1 if not found (should not happen ideally)\n#         return input_sizes\n\n#     def _load_pose_data(self):\n#         print(f\"\\nLoading pose data for sets: {self.set_names} from {self.pose_data_dir}\")\n#         sets_loaded_count = 0\n#         for set_id in tqdm(self.set_names, desc=\"Loading Pose Sets\"):\n#             self.all_pose_data[set_id] = {}\n#             pose_set_path = os.path.join(self.pose_data_dir, set_id)\n#             if not os.path.isdir(pose_set_path):\n#                 print(f\"Warn: Pose dir missing for {set_id} at {pose_set_path}\")\n#                 continue\n#             pkl_files_in_set = [f for f in os.listdir(pose_set_path) if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")]\n#             if not pkl_files_in_set:\n#                 continue\n#             loaded_video_count = 0\n#             for pkl_filename in tqdm(pkl_files_in_set, desc=f\"Loading PKLs for {set_id}\", leave=False):\n#                 pkl_file_path = os.path.join(pose_set_path, pkl_filename)\n#                 try:\n#                     with open(pkl_file_path, 'rb') as f:\n#                         loaded_pkl_content = pickle.load(f)\n#                     if len(loaded_pkl_content) != 1:\n#                         continue\n#                     unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n#                     video_id = \"_\".join(unique_video_key.split('_')[1:])\n#                     self.all_pose_data[set_id][video_id] = video_data\n#                     loaded_video_count += 1\n#                 except FileNotFoundError:\n#                     pass # Expected if some videos failed extraction\n#                 except Exception as e:\n#                     print(f\"Error loading pose PKL {pkl_file_path}: {e}\")\n#             if loaded_video_count > 0:\n#                 sets_loaded_count += 1\n#         print(f\"Finished loading pose data for {sets_loaded_count} sets.\")\n\n#     def _generate_sequence_list(self):\n#         print(f\"Generating sequence list from PIE database for sets: {self.set_names}\")\n#         sequence_count = 0\n#         ped_count = 0\n#         for set_id in tqdm(self.set_names, desc=\"Generating Sequences\"):\n#             if set_id not in self.pie_db:\n#                 continue\n#             for video_id, video_data in self.pie_db[set_id].items():\n#                 if 'ped_annotations' not in video_data:\n#                     continue\n#                 for ped_id, ped_data in video_data['ped_annotations'].items():\n#                     ped_count += 1\n#                     if 'frames' not in ped_data or len(ped_data['frames']) < self.seq_len + self.pred_len:\n#                         continue\n#                     sorted_frames = sorted(ped_data['frames'])\n#                     for i in range(len(sorted_frames) - self.seq_len - self.pred_len + 1):\n#                         start_frame = sorted_frames[i]\n#                         end_frame_observe = sorted_frames[i + self.seq_len - 1]\n#                         if end_frame_observe - start_frame != self.seq_len - 1:\n#                             continue\n#                         target_frame_actual_idx = i + self.seq_len + self.pred_len - 1\n#                         if target_frame_actual_idx >= len(sorted_frames):\n#                              continue # Avoid index out of bounds\n#                         target_frame = sorted_frames[target_frame_actual_idx]\n#                         if target_frame - end_frame_observe != self.pred_len:\n#                              continue\n#                         self.sequences.append((set_id, video_id, ped_id, start_frame))\n#                         sequence_count += 1\n#         print(f\"Found {sequence_count} valid sequences from {ped_count} pedestrian tracks.\")\n\n#     def __len__(self):\n#         return len(self.sequences)\n\n#     def __getitem__(self, idx):\n#         set_id, video_id, ped_id, start_frame = self.sequences[idx]\n#         frame_nums = list(range(start_frame, start_frame + self.seq_len))\n#         target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n\n#         # Get data from the loaded PIE database\n#         video_db = self.pie_db.get(set_id, {}).get(video_id, {})\n#         ped_db = video_db.get('ped_annotations', {}).get(ped_id, {})\n#         ego_db = video_db.get('vehicle_annotations', {}) # Frame -> {sensor: value}\n#         traffic_db = video_db.get('traffic_annotations', {}) # ObjID -> {frames:[], state:[], ...}\n#         ped_attributes = ped_db.get('attributes', {})\n\n#         # Initialize feature sequences\n#         feature_sequences = {stream: [] for stream in self.active_streams}\n\n#         # Static Features (calculated once)\n#         static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32) # Default static\n#         if 'static_context' in self.active_streams:\n#             sig_idx = ped_attributes.get('signalized', 0)\n#             int_idx = ped_attributes.get('intersection', 0)\n#             age_idx = ped_attributes.get('age', 2) # Default to 'adult'\n#             gen_idx = ped_attributes.get('gender', 0)\n#             static_vec = np.concatenate([\n#                 to_one_hot(sig_idx, NUM_SIGNALIZED_CATS),\n#                 to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n#                 to_one_hot(age_idx, NUM_AGE_CATS),\n#                 to_one_hot(gen_idx, NUM_GENDER_CATS)\n#             ])\n#             if static_vec.shape[0] != INPUT_SIZE_STATIC: # Sanity check\n#                  static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n\n#         # Get Target Label\n#         label = 0 # Default to not-crossing\n#         if 'frames' in ped_db and 'behavior' in ped_db and 'cross' in ped_db['behavior']:\n#              try:\n#                  target_frame_db_idx = ped_db['frames'].index(target_frame_num)\n#                  label = ped_db['behavior']['cross'][target_frame_db_idx]\n#                  if label == -1: label = 0 # Map irrelevant to not-crossing\n#              except (ValueError, IndexError):\n#                  pass # Keep default label\n\n#         # Iterate through sequence frames\n#         for frame_num in frame_nums:\n#             frame_db_idx = -1\n#             if 'frames' in ped_db:\n#                  try:\n#                      frame_db_idx = ped_db['frames'].index(frame_num)\n#                  except ValueError:\n#                      pass # Frame not found for this pedestrian in this sequence part\n#             ego_frame_data = ego_db.get(frame_num, {})\n\n#             # --- Extract for ACTIVE streams ---\n#             if 'bbox' in self.active_streams:\n#                 bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32) # Default\n#                 if frame_db_idx != -1 and 'bbox' in ped_db and len(ped_db['bbox']) > frame_db_idx:\n#                     # --- Start Corrected Try/Except ---\n#                      try:\n#                           x1, y1, x2, y2 = ped_db['bbox'][frame_db_idx]\n#                           img_w = video_db.get('width', 1920) # Use default if missing\n#                           img_h = video_db.get('height', 1080)\n#                           if img_w > 0 and img_h > 0: # Check for valid image dimensions\n#                                cx = ((x1 + x2) / 2) / img_w\n#                                cy = ((y1 + y2) / 2) / img_h\n#                                w = (x2 - x1) / img_w\n#                                h = (y2 - y1) / img_h\n#                                # Check for valid normalized bbox dimensions\n#                                if w > 0 and h > 0 and 0 <= cx <= 1 and 0 <= cy <= 1:\n#                                     bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n#                      except Exception as e:\n#                           # Keep default zero vector if any error occurs during processing\n#                           # print(f\"Warning: Error processing bbox F:{frame_num} P:{ped_id} V:{video_id} - {e}\") # Optional warning\n#                           pass\n#                     # --- End Corrected Try/Except ---\n#                 feature_sequences['bbox'].append(bbox_norm)\n\n#             # --- (Rest of stream extractions - unchanged logic but ensure proper indentation) ---\n#             if 'pose' in self.active_streams:\n#                 pose_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n#                 vid_pose_data = self.all_pose_data.get(set_id, {}).get(video_id, {})\n#                 frame_pose_data = vid_pose_data.get(frame_num, {})\n#                 loaded_pose = frame_pose_data.get(ped_id)\n#                 if loaded_pose is not None and isinstance(loaded_pose, np.ndarray) and loaded_pose.shape == (INPUT_SIZE_POSE,):\n#                     pose_vector = loaded_pose\n#                 feature_sequences['pose'].append(pose_vector)\n\n#             if 'ego_speed' in self.active_streams:\n#                 speed = ego_frame_data.get('OBD_speed', 0.0)\n#                 if speed == 0.0: speed = ego_frame_data.get('GPS_speed', 0.0)\n#                 speed_scaled = (speed - self.scalers.get('ego_speed_mean', 0.0)) / self.scalers.get('ego_speed_std', 1.0)\n#                 feature_sequences['ego_speed'].append([speed_scaled])\n\n#             if 'ego_acc' in self.active_streams:\n#                 accX = ego_frame_data.get('accX', 0.0)\n#                 accY = ego_frame_data.get('accY', 0.0)\n#                 accX_scaled = (accX - self.scalers.get('accX_mean', 0.0)) / self.scalers.get('accX_std', 1.0)\n#                 accY_scaled = (accY - self.scalers.get('accY_mean', 0.0)) / self.scalers.get('accY_std', 1.0)\n#                 feature_sequences['ego_acc'].append([accX_scaled, accY_scaled])\n\n#             if 'ego_gyro' in self.active_streams:\n#                 gyroZ = ego_frame_data.get('gyroZ', 0.0)\n#                 gyroZ_scaled = (gyroZ - self.scalers.get('gyroZ_mean', 0.0)) / self.scalers.get('gyroZ_std', 1.0)\n#                 feature_sequences['ego_gyro'].append([gyroZ_scaled])\n\n#             if 'ped_action' in self.active_streams:\n#                 action = 0 # Default standing\n#                 if frame_db_idx != -1 and 'behavior' in ped_db and 'action' in ped_db['behavior'] and len(ped_db['behavior']['action']) > frame_db_idx:\n#                      action = ped_db['behavior']['action'][frame_db_idx]\n#                 feature_sequences['ped_action'].append([float(action)])\n\n#             if 'ped_look' in self.active_streams:\n#                 look = 0 # Default not-looking\n#                 if frame_db_idx != -1 and 'behavior' in ped_db and 'look' in ped_db['behavior'] and len(ped_db['behavior']['look']) > frame_db_idx:\n#                      look = ped_db['behavior']['look'][frame_db_idx]\n#                 feature_sequences['ped_look'].append([float(look)])\n\n#             if 'ped_occlusion' in self.active_streams:\n#                 occ = 0.0 # Default none\n#                 if frame_db_idx != -1 and 'occlusion' in ped_db and len(ped_db['occlusion']) > frame_db_idx:\n#                      occ_val = ped_db['occlusion'][frame_db_idx]\n#                      occ = float(occ_val) / 2.0 # Normalize\n#                 feature_sequences['ped_occlusion'].append([occ])\n\n#             if 'traffic_light' in self.active_streams:\n#                 state_int = 0\n#                 for obj_id, obj_data in traffic_db.items():\n#                      if obj_data.get('obj_class') == 'traffic_light' and 'frames' in obj_data and 'state' in obj_data:\n#                           try:\n#                               tl_frame_idx = obj_data['frames'].index(frame_num)\n#                               state_val = obj_data['state'][tl_frame_idx]\n#                               if state_val != 0:\n#                                   state_int = state_val\n#                                   break # Found first non-undefined state\n#                           except (ValueError, IndexError):\n#                               continue\n#                 feature_sequences['traffic_light'].append(to_one_hot(state_int, NUM_TL_STATES))\n\n#             if 'static_context' in self.active_streams:\n#                 feature_sequences['static_context'].append(static_vec)\n\n#         # --- Convert lists to Tensors ---\n#         features = {}\n#         try:\n#             for stream_name in self.active_streams:\n#                  features[stream_name] = torch.tensor(np.array(feature_sequences[stream_name], dtype=np.float32), dtype=torch.float32)\n\n#         except Exception as e:\n#              print(f\"Error converting features idx {idx}: {e}. Return dummy.\")\n#              # Use the pre-calculated sizes dictionary for fallback\n#              features = {\n#                  name: torch.zeros((self.seq_len, self._input_sizes_for_error.get(name, 1)), dtype=torch.float32)\n#                  for name in self.active_streams\n#              }\n\n#         return features, torch.tensor(label, dtype=torch.long)\n\n# # --- Wrapper Dataset for Balanced Data ---\n# class BalancedDataset(Dataset):\n#     def __init__(self, data_dict, active_streams, label_key='label'):\n#         self.active_streams = active_streams\n#         self.label_key = label_key\n#         if self.label_key not in data_dict or not data_dict[self.label_key]:\n#              raise ValueError(f\"Label key '{self.label_key}' missing/empty.\")\n#         self.num_samples = len(data_dict[self.label_key])\n#         if self.num_samples == 0:\n#             print(\"Warning: BalancedDataset initialized with zero samples.\")\n\n#         self.features = {}\n#         for stream in self.active_streams:\n#              if stream in data_dict and data_dict[stream]:\n#                  try:\n#                      self.features[stream] = torch.tensor(np.array(data_dict[stream]), dtype=torch.float32)\n#                  except ValueError as e:\n#                       raise ValueError(f\"Error converting stream '{stream}' data: {e}\")\n#              else:\n#                   raise KeyError(f\"Stream '{stream}' missing/empty in balanced data.\")\n#         try:\n#             self.labels = torch.tensor([lbl[0] for lbl in data_dict[self.label_key]], dtype=torch.long)\n#         except (IndexError, TypeError) as e:\n#              raise ValueError(f\"Error converting labels: {e}\")\n\n#         for stream in self.active_streams:\n#              if len(self.features[stream]) != self.num_samples:\n#                  raise ValueError(f\"Len mismatch: Stream '{stream}' ({len(self.features[stream])}) vs Labels ({self.num_samples})\")\n#     def __len__(self):\n#         return self.num_samples\n#     def __getitem__(self, idx):\n#         feature_dict = {stream: self.features[stream][idx] for stream in self.active_streams}\n#         label = self.labels[idx]\n#         return feature_dict, label\n\n# # --- Model Architecture ---\n# class Attention(nn.Module):\n#     def __init__(self, hidden_dim, attention_dim):\n#         super(Attention, self).__init__()\n#         self.attention_net = nn.Sequential(\n#             nn.Linear(hidden_dim, attention_dim),\n#             nn.Tanh(),\n#             nn.Linear(attention_dim, 1)\n#         )\n#     def forward(self, lstm_output):\n#         att_scores = self.attention_net(lstm_output).squeeze(2)\n#         att_weights = torch.softmax(att_scores, dim=1)\n#         context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1)\n#         return context_vector, att_weights\n# class MultiStreamAdaptiveLSTM(nn.Module):\n#     def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes, attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n#         super(MultiStreamAdaptiveLSTM, self).__init__()\n#         if not stream_names: raise ValueError(\"stream_names cannot be empty.\")\n#         self.stream_names = stream_names\n#         self.lstms = nn.ModuleDict()\n#         self.attentions = nn.ModuleDict()\n#         print(f\"Initializing model with streams: {self.stream_names}\")\n#         for name in self.stream_names:\n#             if name not in input_sizes: raise KeyError(f\"Input size for stream '{name}' not provided.\")\n#             current_input_size = input_sizes[name]\n#             print(f\"  - Adding stream '{name}' with input size {current_input_size}\")\n#             self.lstms[name] = nn.LSTM(current_input_size, lstm_hidden_size, num_lstm_layers,\n#                                        batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0,\n#                                        bidirectional=True)\n#             self.attentions[name] = Attention(lstm_hidden_size * 2 , attention_dim)\n#         num_active_streams = len(self.stream_names)\n#         combined_feature_dim = lstm_hidden_size * 2 * num_active_streams\n#         print(f\"  Combined feature dimension: {combined_feature_dim}\")\n#         self.dropout = nn.Dropout(dropout_rate)\n#         intermediate_dim = max(num_classes * 4, combined_feature_dim // 2)\n#         self.fc1 = nn.Linear(combined_feature_dim, intermediate_dim)\n#         self.relu = nn.ReLU()\n#         self.fc2 = nn.Linear(intermediate_dim, num_classes)\n#     def forward(self, x):\n#         stream_context_vectors = []\n#         stream_att_weights = {}\n#         for name in self.stream_names:\n#             if name not in x: print(f\"Warning: Stream '{name}' expected but not in input data.\"); continue\n#             lstm_out, _ = self.lstms[name](x[name])\n#             context_vector, attention_weights = self.attentions[name](lstm_out)\n#             stream_context_vectors.append(context_vector)\n#             stream_att_weights[name] = attention_weights\n#         if not stream_context_vectors: raise RuntimeError(\"No stream outputs generated.\")\n#         fused_features = torch.cat(stream_context_vectors, dim=1)\n#         out = self.dropout(fused_features)\n#         out = self.relu(self.fc1(out))\n#         out = self.dropout(out)\n#         logits = self.fc2(out)\n#         return logits\n\n# # --- Training and Evaluation Functions ---\n# def train_epoch(model, dataloader, optimizer, criterion, device):\n#     model.train()\n#     total_loss = 0.0\n#     all_preds = []\n#     all_labels = []\n#     active_streams = model.stream_names\n#     for features, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n#         input_features = {name: features[name].to(device) for name in active_streams if name in features}\n#         labels = labels.to(device)\n#         optimizer.zero_grad()\n#         outputs = model(input_features)\n#         loss = criterion(outputs, labels)\n#         loss.backward()\n#         optimizer.step()\n#         total_loss += loss.item()\n#         preds = torch.argmax(outputs, dim=1)\n#         all_preds.extend(preds.cpu().numpy())\n#         all_labels.extend(labels.cpu().numpy())\n#     avg_loss = total_loss / len(dataloader)\n#     accuracy = accuracy_score(all_labels, all_preds)\n#     return avg_loss, accuracy\n\n# def evaluate_epoch(model, dataloader, criterion, device):\n#     model.eval()\n#     total_loss = 0.0\n#     all_labels = []\n#     all_preds = []\n#     all_probs = []\n#     active_streams = model.stream_names\n#     with torch.no_grad():\n#         for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n#             input_features = {name: features[name].to(device) for name in active_streams if name in features}\n#             labels = labels.to(device)\n#             outputs = model(input_features)\n#             loss = criterion(outputs, labels)\n#             total_loss += loss.item()\n#             probs = torch.softmax(outputs, dim=1)\n#             preds = torch.argmax(probs, dim=1)\n#             all_labels.extend(labels.cpu().numpy())\n#             all_preds.extend(preds.cpu().numpy())\n#             all_probs.extend(probs.cpu().numpy())\n#     avg_loss = total_loss / len(dataloader)\n#     all_probs = np.array(all_probs); all_labels = np.array(all_labels); all_preds = np.array(all_preds)\n#     accuracy = accuracy_score(all_labels, all_preds)\n#     precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', pos_label=1, zero_division=0)\n#     auc = roc_auc_score(all_labels, all_probs[:, 1]) if len(np.unique(all_labels)) > 1 else float('nan')\n#     return {'loss': avg_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}\n\n# def get_predictions_and_labels(model, dataloader, device):\n#     model.eval(); all_labels = []; all_preds = []\n#     active_streams = model.stream_names\n#     with torch.no_grad():\n#         for features, labels in tqdm(dataloader, desc=\"Generating CM Data\", leave=False):\n#              input_features = {name: features[name].to(device) for name in active_streams if name in features}\n#              labels = labels.to(device); outputs = model(input_features); preds = torch.argmax(outputs, dim=1)\n#              all_labels.extend(labels.cpu().numpy()); all_preds.extend(preds.cpu().numpy())\n#     return np.array(all_labels), np.array(all_preds)\n\n# # --- Main Execution ---\n# if __name__ == '__main__':\n\n#     # --- Generate/Load PIE Database ---\n#     print(f\"Checking for PIE database cache at: {PIE_DATABASE_CACHE_PATH}\")\n#     if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n#         if PIE is None: raise ImportError(\"PIE class not imported, cannot generate database.\")\n#         print(\"PIE database cache not found. Generating...\");\n#         pie_dataset_interface = PIE(data_path=PIE_ROOT_PATH, regen_database=True)\n#         pie_database = pie_dataset_interface.generate_database()\n#         if not pie_database: raise RuntimeError(\"Failed to generate PIE database.\")\n#         print(\"PIE database generated successfully.\")\n#     else:\n#         print(\"Loading PIE database from cache...\")\n#         try:\n#             with open(PIE_DATABASE_CACHE_PATH, 'rb') as f: pie_database = pickle.load(f)\n#             print(\"PIE database loaded successfully.\")\n#         except Exception as e: raise RuntimeError(f\"Failed to load PIE database: {e}\")\n\n#     # --- Calculate Standardization Parameters ---\n#     print(\"\\nCalculating standardization parameters from training set...\")\n#     all_train_ego_speeds = []; all_train_accX = []; all_train_accY = []; all_train_gyroZ = []\n#     for set_id in TRAIN_SETS_STR:\n#          if set_id in pie_database:\n#              for video_id, video_data in pie_database[set_id].items():\n#                   if 'vehicle_annotations' in video_data:\n#                        for frame_num, ego_frame_data in video_data['vehicle_annotations'].items():\n#                            speed = ego_frame_data.get('OBD_speed', 0.0);\n#                            if speed == 0.0: speed = ego_frame_data.get('GPS_speed', 0.0)\n#                            all_train_ego_speeds.append(speed); all_train_accX.append(ego_frame_data.get('accX', 0.0));\n#                            all_train_accY.append(ego_frame_data.get('accY', 0.0)); all_train_gyroZ.append(ego_frame_data.get('gyroZ', 0.0))\n#     scalers = {}\n#     if all_train_ego_speeds: scalers['ego_speed_mean'] = np.mean(all_train_ego_speeds); scalers['ego_speed_std'] = np.std(all_train_ego_speeds) if np.std(all_train_ego_speeds) > 1e-6 else 1.0; print(f\"  Ego Speed: Mean={scalers['ego_speed_mean']:.2f}, Std={scalers['ego_speed_std']:.2f}\")\n#     if all_train_accX: scalers['accX_mean'] = np.mean(all_train_accX); scalers['accX_std'] = np.std(all_train_accX) if np.std(all_train_accX) > 1e-6 else 1.0; scalers['accY_mean'] = np.mean(all_train_accY); scalers['accY_std'] = np.std(all_train_accY) if np.std(all_train_accY) > 1e-6 else 1.0; print(f\"  Ego AccX: Mean={scalers['accX_mean']:.2f}, Std={scalers['accX_std']:.2f}\"); print(f\"  Ego AccY: Mean={scalers['accY_mean']:.2f}, Std={scalers['accY_std']:.2f}\")\n#     if all_train_gyroZ: scalers['gyroZ_mean'] = np.mean(all_train_gyroZ); scalers['gyroZ_std'] = np.std(all_train_gyroZ) if np.std(all_train_gyroZ) > 1e-6 else 1.0; print(f\"  Ego GyroZ: Mean={scalers['gyroZ_mean']:.2f}, Std={scalers['gyroZ_std']:.2f}\")\n#     print(\"Standardization parameters calculated.\")\n\n#     # --- Initialize FULL Datasets ---\n#     print(\"\\nInitializing full datasets...\")\n#     full_train_dataset = PIEDataset(pie_database, TRAIN_SETS_STR, POSE_DATA_DIR, SEQ_LEN, PRED_LEN, scalers, ACTIVE_STREAMS)\n#     val_dataset = PIEDataset(pie_database, VAL_SETS_STR, POSE_DATA_DIR, SEQ_LEN, PRED_LEN, scalers, ACTIVE_STREAMS)\n#     if len(full_train_dataset) == 0 or len(val_dataset) == 0: raise ValueError(\"Dataset loading failed.\")\n\n#     # --- Prepare and Balance Training Data ---\n#     print(\"\\nExtracting training data for balancing...\")\n#     training_data_dict = {stream: [] for stream in ACTIVE_STREAMS}; training_data_dict['label'] = []\n#     for i in tqdm(range(len(full_train_dataset)), desc=\"Extracting data\"):\n#          features, label = full_train_dataset[i]\n#          for stream_name in ACTIVE_STREAMS: training_data_dict[stream_name].append(features[stream_name].numpy())\n#          training_data_dict['label'].append([label.item()]) # Store label as list containing the item\n#     print(f\"Original training samples: {len(training_data_dict['label'])}\")\n#     del full_train_dataset # Free memory\n\n#     label_key_for_balancing = 'label' # Key used in training_data_dict\n#     balanced_train_data_dict = balance_samples_count(training_data_dict, label_type=label_key_for_balancing)\n#     del training_data_dict # Free up memory\n\n#     # --- Create Balanced Training Dataset and DataLoaders ---\n#     print(\"\\nCreating DataLoaders...\")\n#     try: balanced_train_dataset = BalancedDataset(balanced_train_data_dict, ACTIVE_STREAMS, label_key=label_key_for_balancing); del balanced_train_data_dict\n#     except Exception as e: print(f\"Error creating BalancedDataset: {e}\"); raise\n#     if len(balanced_train_dataset) == 0: raise ValueError(\"Balanced training dataset is empty!\")\n\n#     train_loader = DataLoader(balanced_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n#     val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n#     print(\"DataLoaders created.\")\n\n#     # --- Initialize Model ---\n#     input_sizes = {}\n#     for stream in ACTIVE_STREAMS:\n#         size_constant_name = f'INPUT_SIZE_{stream.upper()}'\n#         special_cases = {'TRAFFIC_LIGHT': 'TL_STATE', 'STATIC_CONTEXT': 'STATIC', 'EGO_SPEED': 'EGO_SPEED', 'EGO_ACC': 'EGO_ACC', 'EGO_GYRO': 'EGO_GYRO', 'PED_ACTION': 'PED_ACTION', 'PED_LOOK': 'PED_LOOK', 'PED_OCCLUSION': 'PED_OCC'}\n#         stream_upper_key = stream.upper()\n#         suffix = special_cases.get(stream_upper_key)\n#         if suffix: size_constant_name = f'INPUT_SIZE_{suffix}'\n#         elif stream == 'bbox': size_constant_name = 'INPUT_SIZE_BBOX'\n#         elif stream == 'pose': size_constant_name = 'INPUT_SIZE_POSE'\n#         if size_constant_name in globals(): input_sizes[stream] = globals()[size_constant_name]\n#         else: raise ValueError(f\"Input size constant {size_constant_name} not found for stream {stream}\")\n\n#     model = MultiStreamAdaptiveLSTM(\n#         input_sizes=input_sizes, lstm_hidden_size=LSTM_HIDDEN_SIZE,\n#         num_lstm_layers=NUM_LSTM_LAYERS, num_classes=NUM_CLASSES, attention_dim=ATTENTION_DIM,\n#         dropout_rate=DROPOUT_RATE, stream_names=ACTIVE_STREAMS ).to(DEVICE)\n\n#     print(\"\\n--- Model Architecture ---\"); print(model); num_params = sum(p.numel() for p in model.parameters() if p.requires_grad); print(f\"Total Trainable Parameters: {num_params:,}\"); print(\"-\" * 30)\n\n#     # --- Class Weighting & Optimizer ---\n#     print(\"\\nCalculating Class Weights for Loss Function...\")\n#     balanced_train_labels_list = balanced_train_dataset.labels.tolist() # Use balanced list\n#     count_0 = balanced_train_labels_list.count(0); count_1 = balanced_train_labels_list.count(1)\n#     total = len(balanced_train_labels_list)\n#     if total == 0: print(\"Warning: Balanced dataset empty. Use equal weights.\"); weight_0 = 1.0; weight_1 = 1.0\n#     elif count_0 == 0: print(\"Warning: Class 0 missing. Adjust weights.\"); weight_0 = 0.0; weight_1 = 1.0\n#     elif count_1 == 0: print(\"Warning: Class 1 missing. Adjust weights.\"); weight_0 = 1.0; weight_1 = 0.0\n#     else: weight_0 = total / (2.0 * count_0); weight_1 = total / (2.0 * count_1) # Inverse frequency\n#     class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float32).to(DEVICE)\n#     print(f\"Using Class Weights for Loss: 0={weight_0:.2f}, 1={weight_1:.2f}\")\n#     criterion = nn.CrossEntropyLoss(weight=class_weights)\n#     # criterion = nn.CrossEntropyLoss() # Uncomment to disable class weighting\n#     optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n#     best_val_f1 = -1.0; train_losses, val_losses = [], []; train_accs, val_accs = [], []; val_f1s = []\n\n#     # --- Training Loop ---\n#     print(\"\\n--- Starting Training on Balanced Data---\")\n#     for epoch in range(NUM_EPOCHS):\n#         epoch_start_time = time.time()\n#         train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n#         val_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n#         epoch_duration = time.time() - epoch_start_time\n#         train_losses.append(train_loss); val_losses.append(val_metrics['loss'])\n#         train_accs.append(train_acc); val_accs.append(val_metrics['accuracy'])\n#         val_f1s.append(val_metrics['f1'])\n#         print(f\"--- Epoch {epoch+1}/{NUM_EPOCHS} ({epoch_duration:.2f} sec) ---\")\n#         print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n#         print(f\"  Val Loss:   {val_metrics['loss']:.4f}, Val Acc:  {val_metrics['accuracy']:.4f}\")\n#         print(f\"  Val Prec:   {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n#         print(f\"  Val AUC:    {val_metrics['auc']:.4f}\")\n#         if val_metrics['f1'] > best_val_f1:\n#             best_val_f1 = val_metrics['f1']\n#             torch.save(model.state_dict(), 'best_model_balanced.pth')\n#             print(f\"  >> Saved new best model with F1: {best_val_f1:.4f}\")\n#         print(\"-\" * 30)\n#     print(\"--- Training Finished ---\")\n\n#     # --- Plotting ---\n#     print(\"\\n--- Plotting Training History ---\")\n#     fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n#     axes[0].plot(range(1, NUM_EPOCHS + 1), train_losses, label='Train Loss')\n#     axes[0].plot(range(1, NUM_EPOCHS + 1), val_losses, label='Val Loss')\n#     axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].set_title('Loss Curve'); axes[0].legend(); axes[0].grid(True)\n#     axes[1].plot(range(1, NUM_EPOCHS + 1), train_accs, label='Train Accuracy')\n#     axes[1].plot(range(1, NUM_EPOCHS + 1), val_accs, label='Val Accuracy')\n#     axes[1].plot(range(1, NUM_EPOCHS + 1), val_f1s, label='Val F1-Score', linestyle='--')\n#     axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Metric'); axes[1].set_title('Accuracy & F1-Score Curve'); axes[1].legend(); axes[1].grid(True)\n#     plt.tight_layout(); plt.show()\n\n#     # --- Final Evaluation ---\n#     print(\"\\n--- Final Evaluation on Validation Set using Best Model ---\")\n#     best_model_path = 'best_model_balanced.pth'\n#     if os.path.exists(best_model_path):\n#         print(f\"Loading best saved model '{best_model_path}'\")\n#         model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n#     else: print(\"Warning: No saved best model found. Evaluating final model.\")\n#     final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n#     true_labels, pred_labels = get_predictions_and_labels(model, val_loader, DEVICE)\n#     cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1])\n#     labels_display = ['Not Crossing', 'Crossing']\n#     print(\"\\n--- Final Performance Metrics ---\")\n#     print(f\"  Accuracy:  {final_metrics['accuracy']:.4f}\"); print(f\"  Precision: {final_metrics['precision']:.4f}\"); print(f\"  Recall:    {final_metrics['recall']:.4f}\"); print(f\"  F1 Score:  {final_metrics['f1']:.4f}\"); print(f\"  AUC:       {final_metrics['auc']:.4f}\"); print(f\"  Loss:      {final_metrics['loss']:.4f}\")\n#     print(\"\\n--- Confusion Matrix ---\")\n#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_display); disp.plot(cmap=plt.cm.Blues); plt.title('Confusion Matrix (Validation Set)'); plt.show()\n\n#     print(\"\\n--- Script Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.637207Z","iopub.execute_input":"2025-04-15T19:55:16.637572Z","iopub.status.idle":"2025-04-15T19:55:16.655468Z","shell.execute_reply.started":"2025-04-15T19:55:16.637506Z","shell.execute_reply":"2025-04-15T19:55:16.654641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CELL 1: DATA PREPARATION AND BALANCING (RUN ONCE) ---\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport xml.etree.ElementTree as ET\nimport os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.preprocessing import StandardScaler  # For standardizing ego features\nfrom tqdm.notebook import tqdm\nimport random\nimport math\nimport zipfile\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport time\nimport sys\n\n# --- Add PIE utilities path if necessary (adjust path) ---\npie_utilities_path = '/kaggle/working/PIE/utilities'\nif pie_utilities_path not in sys.path:\n    sys.path.insert(0, pie_utilities_path)\ntry:\n    from pie_data import PIE\nexcept ImportError as e:\n    print(f\"Warning: Could not import PIE class from {pie_utilities_path}. Database must already exist. Error: {e}\")\n    PIE = None\n\n# --- Configuration (Copy relevant parts here) ---\nPIE_ROOT_PATH = '/kaggle/working/PIE'\nVIDEO_INPUT_DIR = '/kaggle/input'\nPOSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2'\nPIE_DATABASE_CACHE_PATH = '/kaggle/input/pie-database/pie_database.pkl'\n\n# --- Define ALL possible streams you might want to experiment with ---\n# --- The data extraction will prepare ALL of these ---\nALL_POSSIBLE_STREAMS = [\n    'bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro',\n    'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context'\n]\nprint(f\"Data will be prepared for streams: {ALL_POSSIBLE_STREAMS}\")\n\n# --- Model Hyperparameters (Needed for Dataset) ---\nSEQ_LEN = 30\nPRED_LEN = 1\n\n# --- Input Sizes (Needed for Dataset Error Handling & Definitions) ---\nINPUT_SIZE_BBOX = 4\nINPUT_SIZE_POSE = 34\nINPUT_SIZE_EGO_SPEED = 1\nINPUT_SIZE_EGO_ACC = 2\nINPUT_SIZE_EGO_GYRO = 1\nINPUT_SIZE_PED_ACTION = 1\nINPUT_SIZE_PED_LOOK = 1\nINPUT_SIZE_PED_OCC = 1\nINPUT_SIZE_TL_STATE = 4\nNUM_SIGNALIZED_CATS = 4\nNUM_INTERSECTION_CATS = 5\nNUM_AGE_CATS = 4\nNUM_GENDER_CATS = 3\nINPUT_SIZE_STATIC = NUM_SIGNALIZED_CATS + NUM_INTERSECTION_CATS + NUM_AGE_CATS + NUM_GENDER_CATS  # = 16\n\n# --- Dataset Splits ---\nTRAIN_SETS_STR = ['set01', 'set02', 'set04']\nVAL_SETS_STR = ['set05', 'set06']\n\n# --- Mappings ---\nTL_STATE_MAP = {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}\nNUM_TL_STATES = len(TL_STATE_MAP)\nSIGNALIZED_MAP = {'n/a': 0, 'C': 1, 'S': 2, 'CS': 3}\nINTERSECTION_MAP = {'midblock': 0, 'T': 1, 'T-left': 2, 'T-right': 3, 'four-way': 4}\nAGE_MAP = {'child': 0, 'young': 1, 'adult': 2, 'senior': 3}\nGENDER_MAP = {'n/a': 0, 'female': 1, 'male': 2}\n\n# --- Output Files from this Cell ---\nBALANCED_DATA_PKL_PATH = \"/kaggle/working/balanced_train_data.pkl\"\nSCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\"\nVAL_SEQUENCES_PKL_PATH = \"/kaggle/working/val_sequences.pkl\"  # Save sequence info for val set\n\n# --- Helper: One-Hot Encoding ---\ndef to_one_hot(index, num_classes):\n    vec = np.zeros(num_classes, dtype=np.float32)\n    if 0 <= index < num_classes:\n        vec[index] = 1.0\n    else:\n        vec[0] = 1.0  # Default\n    return vec\n\n# --- Balancing Function ---\ndef balance_samples_count(seq_data, label_type, random_seed=42):\n    print('---------------------------------------------------------')\n    print(f\"Balancing samples based on '{label_type}' key\")\n    if label_type not in seq_data:\n        raise KeyError(f\"Label type '{label_type}' not found.\")\n    try:\n        gt_labels = [lbl[0] for lbl in seq_data[label_type]]\n    except (IndexError, TypeError):\n        raise ValueError(f\"Labels under '{label_type}' not in expected format [[label_val]].\")\n    if not all(l in [0, 1] for l in gt_labels):\n        print(f\"Warning: Labels for balancing contain values other than 0 or 1.\")\n    num_pos_samples = np.count_nonzero(np.array(gt_labels))\n    num_neg_samples = len(gt_labels) - num_pos_samples\n    new_seq_data = {}\n    if num_neg_samples == num_pos_samples:\n        print('Samples already balanced.')\n        return seq_data.copy()\n    else:\n        print(f'Unbalanced: Positive (1): {num_pos_samples} | Negative (0): {num_neg_samples}')\n        majority_label = 0 if num_neg_samples > num_pos_samples else 1\n        minority_count = min(num_neg_samples, num_pos_samples)\n        print(f\"Undersampling majority class ({majority_label}) to match count ({minority_count}).\")\n        majority_indices = np.where(np.array(gt_labels) == majority_label)[0]\n        minority_indices = np.where(np.array(gt_labels) != majority_label)[0]\n        np.random.seed(random_seed)\n        keep_majority_indices = np.random.choice(majority_indices, size=minority_count, replace=False)\n        final_indices = np.concatenate((minority_indices, keep_majority_indices))\n        np.random.shuffle(final_indices)\n        for k, v_list in seq_data.items():\n            if isinstance(v_list, list) and len(v_list) == len(gt_labels):\n                try:\n                    if v_list and isinstance(v_list[0], np.ndarray):\n                        v_array = np.array(v_list)\n                        new_seq_data[k] = list(v_array[final_indices])\n                    else:\n                        new_seq_data[k] = [v_list[i] for i in final_indices]\n                except Exception as e:\n                    print(f\"Error processing key '{k}' during balancing: {e}. Skip.\")\n                    new_seq_data[k] = []\n            else:\n                print(f\"Warn: Skipping key '{k}' in balancing.\")\n                new_seq_data[k] = v_list\n        if label_type in new_seq_data:\n            new_gt_labels = [lbl[0] for lbl in new_seq_data[label_type]]\n            final_pos = np.count_nonzero(np.array(new_gt_labels))\n            final_neg = len(new_gt_labels) - final_pos\n            print(f'Balanced:   Positive (1): {final_pos} | Negative (0): {final_neg}')\n        else:\n            print(\"Error: Label key lost during balancing.\")\n        print('---------------------------------------------------------')\n        return new_seq_data\n\n# --- Dataset Class (Needed for initial loading) ---\nclass PIEDataset(Dataset):\n    # --- (Dataset class definition - identical to the previous working version) ---\n    # --- (Includes __init__, _load_pose_data, _generate_sequence_list, __len__, __getitem__) ---\n    def __init__(self, pie_database, set_names, pose_data_dir, seq_len, pred_len, scalers=None, active_streams=None):\n        self.pie_db = pie_database\n        self.set_names = set_names\n        self.pose_data_dir = pose_data_dir\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.scalers = scalers or {}\n        # Store ALL possible streams; __getitem__ will generate data for all streams.\n        self.active_streams = ALL_POSSIBLE_STREAMS  # Generate all streams for potential use\n        self.sequences = []\n        self.all_pose_data = {}\n        self._input_sizes_for_error = self._get_input_sizes_dict()  # Get sizes for all streams\n        self._load_pose_data()\n        self._generate_sequence_list()\n        if not self.sequences:\n            raise ValueError(f\"Dataset init failed: No sequences for {self.set_names}\")\n\n    def _get_input_sizes_dict(self):\n        input_sizes = {}\n        special_cases = {\n            'TRAFFIC_LIGHT': 'TL_STATE',\n            'STATIC_CONTEXT': 'STATIC',\n            'EGO_SPEED': 'EGO_SPEED',\n            'EGO_ACC': 'EGO_ACC',\n            'EGO_GYRO': 'EGO_GYRO',\n            'PED_ACTION': 'PED_ACTION',\n            'PED_LOOK': 'PED_LOOK',\n            'PED_OCCLUSION': 'PED_OCC'\n        }\n        for stream in self.active_streams:\n            stream_upper_key = stream.upper()\n            if stream_upper_key in special_cases:\n                size_constant_name = f\"INPUT_SIZE_{special_cases[stream_upper_key]}\"\n            elif stream == 'bbox':\n                size_constant_name = 'INPUT_SIZE_BBOX'\n            elif stream == 'pose':\n                size_constant_name = 'INPUT_SIZE_POSE'\n            else:\n                size_constant_name = f\"INPUT_SIZE_{stream.upper()}\"\n            if size_constant_name in globals():\n                input_sizes[stream] = globals()[size_constant_name]\n            else:\n                input_sizes[stream] = 1\n        return input_sizes\n\n    def _load_pose_data(self):\n        print(f\"\\nLoading pose data for sets: {self.set_names} from {self.pose_data_dir}\")\n        sets_loaded_count = 0\n        for set_id in tqdm(self.set_names, desc=\"Loading Pose Sets\"):\n            self.all_pose_data[set_id] = {}\n            pose_set_path = os.path.join(self.pose_data_dir, set_id)\n            if not os.path.isdir(pose_set_path):\n                print(f\"Warn: Pose dir missing for {set_id} at {pose_set_path}\")\n                continue\n            pkl_files_in_set = [f for f in os.listdir(pose_set_path)\n                                if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")]\n            if not pkl_files_in_set:\n                continue\n            loaded_video_count = 0\n            for pkl_filename in tqdm(pkl_files_in_set, desc=f\"Loading PKLs for {set_id}\", leave=False):\n                pkl_file_path = os.path.join(pose_set_path, pkl_filename)\n                try:\n                    with open(pkl_file_path, 'rb') as f:\n                        loaded_pkl_content = pickle.load(f)\n                    if len(loaded_pkl_content) != 1:\n                        continue\n                    unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n                    video_id = \"_\".join(unique_video_key.split('_')[1:])\n                    self.all_pose_data[set_id][video_id] = video_data\n                    loaded_video_count += 1\n                except FileNotFoundError:\n                    pass\n                except Exception as e:\n                    print(f\"Error loading pose PKL {pkl_file_path}: {e}\")\n            if loaded_video_count > 0:\n                sets_loaded_count += 1\n        print(f\"Finished loading pose data for {sets_loaded_count} sets.\")\n\n    def _generate_sequence_list(self):\n        print(f\"Generating sequence list from PIE database for sets: {self.set_names}\")\n        sequence_count = 0\n        ped_count = 0\n        for set_id in tqdm(self.set_names, desc=\"Generating Sequences\"):\n            if set_id not in self.pie_db:\n                continue\n            for video_id, video_data in self.pie_db[set_id].items():\n                if 'ped_annotations' not in video_data:\n                    continue\n                for ped_id, ped_data in video_data['ped_annotations'].items():\n                    ped_count += 1\n                    if 'frames' not in ped_data or len(ped_data['frames']) < self.seq_len + self.pred_len:\n                        continue\n                    sorted_frames = sorted(ped_data['frames'])\n                    for i in range(len(sorted_frames) - self.seq_len - self.pred_len + 1):\n                        start_frame = sorted_frames[i]\n                        end_frame_observe = sorted_frames[i + self.seq_len - 1]\n                        if end_frame_observe - start_frame != self.seq_len - 1:\n                            continue\n                        target_frame_actual_idx = i + self.seq_len + self.pred_len - 1\n                        if target_frame_actual_idx >= len(sorted_frames):\n                            continue\n                        target_frame = sorted_frames[target_frame_actual_idx]\n                        if target_frame - end_frame_observe != self.pred_len:\n                            continue\n                        self.sequences.append((set_id, video_id, ped_id, start_frame))\n                        sequence_count += 1\n        print(f\"Found {sequence_count} valid sequences from {ped_count} pedestrian tracks.\")\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        # Get sequence identifiers\n        set_id, video_id, ped_id, start_frame = self.sequences[idx]\n        frame_nums = list(range(start_frame, start_frame + self.seq_len))\n        target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n\n        video_db = self.pie_db.get(set_id, {}).get(video_id, {})\n        ped_db = video_db.get('ped_annotations', {}).get(ped_id, {})\n        ego_db = video_db.get('vehicle_annotations', {})\n        traffic_db = video_db.get('traffic_annotations', {})\n        ped_attributes = ped_db.get('attributes', {})\n\n        # Initialize feature sequences and label\n        feature_sequences = {stream: [] for stream in self.active_streams}\n        label = 0\n\n        # Determine target label (default is 0 if not found)\n        if 'frames' in ped_db and 'behavior' in ped_db and 'cross' in ped_db['behavior']:\n            try:\n                target_frame_db_idx = ped_db['frames'].index(target_frame_num)\n                label = ped_db['behavior']['cross'][target_frame_db_idx]\n                if label == -1:\n                    label = 0\n            except (ValueError, IndexError):\n                pass\n\n        # Static features\n        static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n        if 'static_context' in self.active_streams:\n            sig_idx = ped_attributes.get('signalized', 0)\n            int_idx = ped_attributes.get('intersection', 0)\n            age_idx = ped_attributes.get('age', 2)\n            gen_idx = ped_attributes.get('gender', 0)\n            static_vec = np.concatenate([\n                to_one_hot(sig_idx, NUM_SIGNALIZED_CATS),\n                to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n                to_one_hot(age_idx, NUM_AGE_CATS),\n                to_one_hot(gen_idx, NUM_GENDER_CATS)\n            ])\n            if static_vec.shape[0] != INPUT_SIZE_STATIC:\n                static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n\n        # Loop over each frame in the sequence\n        for frame_num in frame_nums:\n            frame_db_idx = -1\n            if 'frames' in ped_db:\n                try:\n                    frame_db_idx = ped_db['frames'].index(frame_num)\n                except ValueError:\n                    pass\n            ego_frame_data = ego_db.get(frame_num, {})\n\n            # 'bbox' stream\n            if 'bbox' in self.active_streams:\n                bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32)\n                if frame_db_idx != -1 and 'bbox' in ped_db and len(ped_db['bbox']) > frame_db_idx:\n                    try:\n                        x1, y1, x2, y2 = ped_db['bbox'][frame_db_idx]\n                        img_w = video_db.get('width', 1920)\n                        img_h = video_db.get('height', 1080)\n                        if img_w > 0 and img_h > 0:\n                            cx = ((x1 + x2) / 2) / img_w\n                            cy = ((y1 + y2) / 2) / img_h\n                            w = (x2 - x1) / img_w\n                            h = (y2 - y1) / img_h\n                            if w > 0 and h > 0 and 0 <= cx <= 1 and 0 <= cy <= 1:\n                                bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n                    except Exception:\n                        pass\n                feature_sequences['bbox'].append(bbox_norm)\n\n            # 'pose' stream\n            if 'pose' in self.active_streams:\n                pose_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n                vid_pose_data = self.all_pose_data.get(set_id, {}).get(video_id, {})\n                frame_pose_data = vid_pose_data.get(frame_num, {})\n                loaded_pose = frame_pose_data.get(ped_id)\n                if loaded_pose is not None and isinstance(loaded_pose, np.ndarray) and loaded_pose.shape == (INPUT_SIZE_POSE,):\n                    pose_vector = loaded_pose\n                feature_sequences['pose'].append(pose_vector)\n\n            # 'ego_speed' stream\n            if 'ego_speed' in self.active_streams:\n                speed = ego_frame_data.get('OBD_speed', 0.0)\n                if speed == 0.0:\n                    speed = ego_frame_data.get('GPS_speed', 0.0)\n                speed_scaled = (speed - self.scalers.get('ego_speed_mean', 0.0)) / self.scalers.get('ego_speed_std', 1.0)\n                feature_sequences['ego_speed'].append([speed_scaled])\n\n            # 'ego_acc' stream\n            if 'ego_acc' in self.active_streams:\n                accX = ego_frame_data.get('accX', 0.0)\n                accY = ego_frame_data.get('accY', 0.0)\n                accX_scaled = (accX - self.scalers.get('accX_mean', 0.0)) / self.scalers.get('accX_std', 1.0)\n                accY_scaled = (accY - self.scalers.get('accY_mean', 0.0)) / self.scalers.get('accY_std', 1.0)\n                feature_sequences['ego_acc'].append([accX_scaled, accY_scaled])\n\n            # 'ego_gyro' stream\n            if 'ego_gyro' in self.active_streams:\n                gyroZ = ego_frame_data.get('gyroZ', 0.0)\n                gyroZ_scaled = (gyroZ - self.scalers.get('gyroZ_mean', 0.0)) / self.scalers.get('gyroZ_std', 1.0)\n                feature_sequences['ego_gyro'].append([gyroZ_scaled])\n\n            # 'ped_action' stream\n            if 'ped_action' in self.active_streams:\n                action = 0\n                if frame_db_idx != -1 and 'behavior' in ped_db and 'action' in ped_db['behavior'] and len(ped_db['behavior']['action']) > frame_db_idx:\n                    action = ped_db['behavior']['action'][frame_db_idx]\n                feature_sequences['ped_action'].append([float(action)])\n\n            # 'ped_look' stream\n            if 'ped_look' in self.active_streams:\n                look = 0\n                if frame_db_idx != -1 and 'behavior' in ped_db and 'look' in ped_db['behavior'] and len(ped_db['behavior']['look']) > frame_db_idx:\n                    look = ped_db['behavior']['look'][frame_db_idx]\n                feature_sequences['ped_look'].append([float(look)])\n\n            # 'ped_occlusion' stream\n            if 'ped_occlusion' in self.active_streams:\n                occ = 0.0\n                if frame_db_idx != -1 and 'occlusion' in ped_db and len(ped_db['occlusion']) > frame_db_idx:\n                    occ_val = ped_db['occlusion'][frame_db_idx]\n                    occ = float(occ_val) / 2.0\n                feature_sequences['ped_occlusion'].append([occ])\n\n            # 'traffic_light' stream\n            if 'traffic_light' in self.active_streams:\n                state_int = 0\n                for obj_id, obj_data in traffic_db.items():\n                    if obj_data.get('obj_class') == 'traffic_light' and 'frames' in obj_data and 'state' in obj_data:\n                        try:\n                            tl_frame_idx = obj_data['frames'].index(frame_num)\n                            state_val = obj_data['state'][tl_frame_idx]\n                            if state_val != 0:\n                                state_int = state_val\n                                break\n                        except (ValueError, IndexError):\n                            continue\n                feature_sequences['traffic_light'].append(to_one_hot(state_int, NUM_TL_STATES))\n\n            # 'static_context' stream\n            if 'static_context' in self.active_streams:\n                feature_sequences['static_context'].append(static_vec)\n\n        features = {}\n        try:\n            for stream_name in self.active_streams:\n                features[stream_name] = torch.tensor(\n                    np.array(feature_sequences[stream_name], dtype=np.float32),\n                    dtype=torch.float32\n                )\n        except Exception as e:\n            print(f\"Error converting features idx {idx}: {e}. Return dummy.\")\n            features = {\n                name: torch.zeros((self.seq_len, self._input_sizes_for_error.get(name, 1)), dtype=torch.float32)\n                for name in self.active_streams\n            }\n        return features, torch.tensor(label, dtype=torch.long)\n\n# --- Main Data Prep Execution Block ---\nif __name__ == '__main__':\n\n    # --- Generate/Load PIE Database ---\n    print(f\"Checking for PIE database cache at: {PIE_DATABASE_CACHE_PATH}\")\n    if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n        if PIE is None:\n            raise ImportError(\"PIE class not imported, cannot generate database.\")\n        print(\"PIE database cache not found. Generating...\")\n        pie_dataset_interface = PIE(data_path=PIE_ROOT_PATH, regen_database=True)\n        pie_database = pie_dataset_interface.generate_database()\n        if not pie_database:\n            raise RuntimeError(\"Failed to generate PIE database.\")\n        print(\"PIE database generated successfully.\")\n    else:\n        print(\"Loading PIE database from cache...\")\n        try:\n            with open(PIE_DATABASE_CACHE_PATH, 'rb') as f:\n                pie_database = pickle.load(f)\n            print(\"PIE database loaded successfully.\")\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load PIE database: {e}\")\n\n    # --- Calculate Standardization Parameters ---\n    print(\"\\nCalculating standardization parameters from training set...\")\n    all_train_ego_speeds = []\n    all_train_accX = []\n    all_train_accY = []\n    all_train_gyroZ = []\n    for set_id in TRAIN_SETS_STR:\n        if set_id in pie_database:\n            for video_id, video_data in pie_database[set_id].items():\n                if 'vehicle_annotations' in video_data:\n                    for frame_num, ego_frame_data in video_data['vehicle_annotations'].items():\n                        speed = ego_frame_data.get('OBD_speed', 0.0)\n                        if speed == 0.0:\n                            speed = ego_frame_data.get('GPS_speed', 0.0)\n                        all_train_ego_speeds.append(speed)\n                        all_train_accX.append(ego_frame_data.get('accX', 0.0))\n                        all_train_accY.append(ego_frame_data.get('accY', 0.0))\n                        all_train_gyroZ.append(ego_frame_data.get('gyroZ', 0.0))\n    scalers = {}\n    if all_train_ego_speeds:\n        scalers['ego_speed_mean'] = np.mean(all_train_ego_speeds)\n        scalers['ego_speed_std'] = np.std(all_train_ego_speeds) if np.std(all_train_ego_speeds) > 1e-6 else 1.0\n        print(f\"  Ego Speed: Mean={scalers['ego_speed_mean']:.2f}, Std={scalers['ego_speed_std']:.2f}\")\n    if all_train_accX:\n        scalers['accX_mean'] = np.mean(all_train_accX)\n        scalers['accX_std'] = np.std(all_train_accX) if np.std(all_train_accX) > 1e-6 else 1.0\n        scalers['accY_mean'] = np.mean(all_train_accY)\n        scalers['accY_std'] = np.std(all_train_accY) if np.std(all_train_accY) > 1e-6 else 1.0\n        print(f\"  Ego AccX: Mean={scalers['accX_mean']:.2f}, Std={scalers['accX_std']:.2f}\")\n        print(f\"  Ego AccY: Mean={scalers['accY_mean']:.2f}, Std={scalers['accY_std']:.2f}\")\n    if all_train_gyroZ:\n        scalers['gyroZ_mean'] = np.mean(all_train_gyroZ)\n        scalers['gyroZ_std'] = np.std(all_train_gyroZ) if np.std(all_train_gyroZ) > 1e-6 else 1.0\n        print(f\"  Ego GyroZ: Mean={scalers['gyroZ_mean']:.2f}, Std={scalers['gyroZ_std']:.2f}\")\n    print(\"Standardization parameters calculated.\")\n\n    # --- Initialize FULL Training Dataset ---\n    print(\"\\nInitializing full training dataset (for extraction)...\")\n    full_train_dataset = PIEDataset(\n        pie_database,\n        TRAIN_SETS_STR,\n        POSE_DATA_DIR,\n        SEQ_LEN,\n        PRED_LEN,\n        scalers,\n        ALL_POSSIBLE_STREAMS\n    )\n    if len(full_train_dataset) == 0:\n        raise ValueError(\"Full Train Dataset loading failed.\")\n\n    # --- Prepare and Balance Training Data ---\n    print(\"\\nExtracting ALL stream data from training set for balancing...\")\n    training_data_dict = {stream: [] for stream in ALL_POSSIBLE_STREAMS}\n    training_data_dict['label'] = []\n    for i in tqdm(range(len(full_train_dataset)), desc=\"Extracting data\"):\n        features, label = full_train_dataset[i]\n        for stream_name in ALL_POSSIBLE_STREAMS:\n            if stream_name in features:\n                training_data_dict[stream_name].append(features[stream_name].numpy())\n            else:\n                print(f\"Warning: Stream {stream_name} missing from dataset output index {i}.\")\n                # Append zeros of correct shape as placeholder\n                size_const = f\"INPUT_SIZE_{stream_name.upper()}\"\n                special_cases = {\n                    'TRAFFIC_LIGHT': 'TL_STATE',\n                    'STATIC_CONTEXT': 'STATIC',\n                    'EGO_SPEED': 'EGO_SPEED',\n                    'EGO_ACC': 'EGO_ACC',\n                    'EGO_GYRO': 'EGO_GYRO',\n                    'PED_ACTION': 'PED_ACTION',\n                    'PED_LOOK': 'PED_LOOK',\n                    'PED_OCCLUSION': 'PED_OCC'\n                }\n                if stream_name.upper() in special_cases:\n                    size_const = f\"INPUT_SIZE_{special_cases[stream_name.upper()]}\"\n                elif stream_name == 'bbox':\n                    size_const = 'INPUT_SIZE_BBOX'\n                elif stream_name == 'pose':\n                    size_const = 'INPUT_SIZE_POSE'\n                stream_size = globals().get(size_const, 1)\n                training_data_dict[stream_name].append(np.zeros((SEQ_LEN, stream_size), dtype=np.float32))\n        training_data_dict['label'].append([label.item()])\n    print(f\"Original training samples: {len(training_data_dict['label'])}\")\n    del full_train_dataset  # Free memory\n\n    label_key_for_balancing = 'label'\n    balanced_train_data_dict = balance_samples_count(training_data_dict, label_type=label_key_for_balancing)\n    del training_data_dict  # Free up memory\n\n    # --- Save Balanced Data and Scalers ---\n    print(f\"\\nSaving balanced training data to: {BALANCED_DATA_PKL_PATH}\")\n    try:\n        with open(BALANCED_DATA_PKL_PATH, 'wb') as f:\n            pickle.dump(balanced_train_data_dict, f, pickle.HIGHEST_PROTOCOL)\n        print(\" -> Balanced data saved.\")\n    except Exception as e:\n        print(f\"  Error saving balanced data: {e}\")\n\n    print(f\"\\nSaving scalers to: {SCALERS_PKL_PATH}\")\n    try:\n        with open(SCALERS_PKL_PATH, 'wb') as f:\n            pickle.dump(scalers, f, pickle.HIGHEST_PROTOCOL)\n        print(\" -> Scalers saved.\")\n    except Exception as e:\n        print(f\"  Error saving scalers: {e}\")\n\n    # --- Prepare and Save Validation Sequence Info ---\n    print(\"\\nInitializing validation dataset (for sequence info)...\")\n    # Pass an empty list for streams if only sequence identifiers are needed.\n    val_dataset = PIEDataset(pie_database, VAL_SETS_STR, POSE_DATA_DIR, SEQ_LEN, PRED_LEN, scalers, [])\n    val_sequences_info = val_dataset.sequences\n    del val_dataset\n\n    print(f\"Saving validation sequence info ({len(val_sequences_info)} sequences) to: {VAL_SEQUENCES_PKL_PATH}\")\n    try:\n        with open(VAL_SEQUENCES_PKL_PATH, 'wb') as f:\n            pickle.dump(val_sequences_info, f, pickle.HIGHEST_PROTOCOL)\n        print(\" -> Validation sequence info saved.\")\n    except Exception as e:\n        print(f\" Error saving validation sequence info: {e}\")\n\n    print(\"\\n--- Data Preparation and Balancing Cell Finished ---\")\n    print(\"You can now run the next cell for training and evaluation.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:40:28.498140Z","iopub.execute_input":"2025-04-19T13:40:28.498472Z","iopub.status.idle":"2025-04-19T13:53:04.991178Z","shell.execute_reply.started":"2025-04-19T13:40:28.498446Z","shell.execute_reply":"2025-04-19T13:53:04.990200Z"}},"outputs":[{"name":"stdout","text":"Data will be prepared for streams: ['bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro', 'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context']\nChecking for PIE database cache at: /kaggle/input/pie-database/pie_database.pkl\nLoading PIE database from cache...\nPIE database loaded successfully.\n\nCalculating standardization parameters from training set...\n  Ego Speed: Mean=13.43, Std=13.31\n  Ego AccX: Mean=-0.03, Std=0.08\n  Ego AccY: Mean=-0.52, Std=0.85\n  Ego GyroZ: Mean=-0.04, Std=4.48\nStandardization parameters calculated.\n\nInitializing full training dataset (for extraction)...\n\nLoading pose data for sets: ['set01', 'set02', 'set04'] from /kaggle/input/pose-data/extracted_poses2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading Pose Sets:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3986cb8eb9c46cca48219e7154412b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set01:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set02:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set04:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Finished loading pose data for 3 sets.\nGenerating sequence list from PIE database for sets: ['set01', 'set02', 'set04']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating Sequences:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adc7dc7a04534f9a95cdbf463256371b"}},"metadata":{}},{"name":"stdout","text":"Found 333454 valid sequences from 880 pedestrian tracks.\n\nExtracting ALL stream data from training set for balancing...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting data:   0%|          | 0/333454 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"558a0f5850c74ae9badc9ce34d068380"}},"metadata":{}},{"name":"stdout","text":"Original training samples: 333454\n---------------------------------------------------------\nBalancing samples based on 'label' key\nUnbalanced: Positive (1): 54967 | Negative (0): 278487\nUndersampling majority class (0) to match count (54967).\nBalanced:   Positive (1): 54967 | Negative (0): 54967\n---------------------------------------------------------\n\nSaving balanced training data to: /kaggle/working/balanced_train_data.pkl\n -> Balanced data saved.\n\nSaving scalers to: /kaggle/working/scalers.pkl\n -> Scalers saved.\n\nInitializing validation dataset (for sequence info)...\n\nLoading pose data for sets: ['set05', 'set06'] from /kaggle/input/pose-data/extracted_poses2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading Pose Sets:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e62ae5e658ab43baa074b94d916132b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set05:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading PKLs for set06:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Finished loading pose data for 2 sets.\nGenerating sequence list from PIE database for sets: ['set05', 'set06']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating Sequences:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44a0edbd965b4a3ca6f130855a90e1bd"}},"metadata":{}},{"name":"stdout","text":"Found 77288 valid sequences from 243 pedestrian tracks.\nSaving validation sequence info (77288 sequences) to: /kaggle/working/val_sequences.pkl\n -> Validation sequence info saved.\n\n--- Data Preparation and Balancing Cell Finished ---\nYou can now run the next cell for training and evaluation.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- CELL 2: MODEL TRAINING AND EVALUATION ---\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\nfrom tqdm.notebook import tqdm\nimport random\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport time\nimport sys\n\n# --- Add PIE utilities path if necessary (adjust path) ---\n# Needs to be available here too if PIE class is used for loading DB\npie_utilities_path = '/kaggle/working/PIE/utilities'\nif pie_utilities_path not in sys.path:\n    sys.path.insert(0, pie_utilities_path)\ntry:\n    from pie_data import PIE\nexcept ImportError as e:\n    print(f\"Warn: Could not import PIE class: {e}\")\n    PIE = None\n\n# --- Configuration (Define constants again or ensure they are accessible) ---\nPIE_ROOT_PATH = '/kaggle/working/PIE'\nPOSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2'\nPIE_DATABASE_CACHE_PATH = '/kaggle/input/pie-database/pie_database.pkl'\n\n# --- *** CHOOSE ACTIVE STREAMS FOR THIS EXPERIMENT *** ---\nACTIVE_STREAMS = [\n    # 'bbox',\n    # 'pose'\n    # 'ego_speed',\n    # 'ped_action',\n    # 'ped_look',\n    # 'ego_acc',\n    # 'ego_gyro',\n    # 'ped_occlusion',\n    # 'traffic_light',\n    'static_context'\n]\nprint(f\"--- Running Experiment With Active Streams: {ACTIVE_STREAMS} ---\")\n# --- *** END ACTIVE STREAM SELECTION *** ---\n\n# --- Model Hyperparameters ---\nSEQ_LEN = 30\nPRED_LEN = 1\n\nINPUT_SIZE_BBOX = 4\nINPUT_SIZE_POSE = 34\nINPUT_SIZE_EGO_SPEED = 1\nINPUT_SIZE_EGO_ACC = 2\nINPUT_SIZE_EGO_GYRO = 1\nINPUT_SIZE_PED_ACTION = 1\nINPUT_SIZE_PED_LOOK = 1\nINPUT_SIZE_PED_OCC = 1\nINPUT_SIZE_TL_STATE = 4\n\nNUM_SIGNALIZED_CATS = 4\nNUM_INTERSECTION_CATS = 5\nNUM_AGE_CATS = 4\nNUM_GENDER_CATS = 3\n\nINPUT_SIZE_STATIC = NUM_SIGNALIZED_CATS + NUM_INTERSECTION_CATS + NUM_AGE_CATS + NUM_GENDER_CATS\n\nLSTM_HIDDEN_SIZE = 256\nNUM_LSTM_LAYERS = 2\nDROPOUT_RATE = 0.3\nNUM_CLASSES = 2\nATTENTION_DIM = 128\n\n# --- Training Hyperparameters ---\nLEARNING_RATE = 1e-4\nBATCH_SIZE = 32\nNUM_EPOCHS = 15\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\n# --- Dataset Splits (Needed for Validation Set) ---\nVAL_SETS_STR = ['set05', 'set06']\n\n# --- Load Pre-Processed Data ---\nBALANCED_DATA_PKL_PATH = \"/kaggle/working/balanced_train_data.pkl\"\nSCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\"\nVAL_SEQUENCES_PKL_PATH = \"/kaggle/working/val_sequences.pkl\"\n\nprint(\"\\nLoading pre-processed data...\")\ntry:\n    with open(BALANCED_DATA_PKL_PATH, 'rb') as f:\n        balanced_train_data_dict = pickle.load(f)\n    print(f\" -> Loaded balanced training data ({len(balanced_train_data_dict['label'])} samples).\")\n    with open(SCALERS_PKL_PATH, 'rb') as f:\n        scalers = pickle.load(f)\n    print(\" -> Loaded scalers.\")\n    with open(VAL_SEQUENCES_PKL_PATH, 'rb') as f:\n        val_sequences_info = pickle.load(f)\n    print(f\" -> Loaded validation sequence info ({len(val_sequences_info)} sequences).\")\nexcept FileNotFoundError as e:\n    print(f\"ERROR: Required pre-processed file not found: {e}\")\n    print(\"Please run Cell 1 (Data Preparation) first.\")\n    exit()\nexcept Exception as e:\n    print(f\"Error loading pre-processed data: {e}\")\n    exit()\n\n# --- Load Full PIE Database (needed for Validation Dataset __getitem__) ---\nprint(\"\\nLoading PIE database cache...\")\nif not os.path.exists(PIE_DATABASE_CACHE_PATH):\n    raise FileNotFoundError(\"PIE database cache not found. Please run Cell 1 or the cache generation script.\")\ntry:\n    with open(PIE_DATABASE_CACHE_PATH, 'rb') as f:\n        pie_database = pickle.load(f)\n    print(\" -> PIE database loaded successfully.\")\nexcept Exception as e:\n    raise RuntimeError(f\"Failed to load PIE database: {e}\")\n\n# --- Define Helper/Dataset/Model Classes (Copy from Cell 1 or previous script) ---\n# Definitions for to_one_hot, PIEDataset, BalancedDataset, Attention, MultiStreamAdaptiveLSTM\n\ndef to_one_hot(index, num_classes):\n    vec = np.zeros(num_classes, dtype=np.float32)\n    # Clamp index between 0 and num_classes-1\n    vec[min(max(index, 0), num_classes - 1)] = 1.0\n    return vec\n\nclass PIEDataset(Dataset):\n    def __init__(self, pie_database, set_names, pose_data_dir, seq_len, pred_len, scalers=None, active_streams=None):\n        self.pie_db = pie_database\n        self.set_names = set_names\n        self.pose_data_dir = pose_data_dir\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.scalers = scalers or {}\n        self.active_streams = active_streams or []\n        self.sequences = []\n        self.all_pose_data = {}\n        self._input_sizes_for_error = self._get_input_sizes_dict()\n        self._load_pose_data()\n        self._generate_sequence_list()\n        if not self.sequences:\n            raise ValueError(f\"Dataset init failed: No sequences for {self.set_names}\")\n\n    def _get_input_sizes_dict(self):\n        input_sizes = {}\n        special_cases = {\n            'TRAFFIC_LIGHT': 'TL_STATE',\n            'STATIC_CONTEXT': 'STATIC',\n            'EGO_SPEED': 'EGO_SPEED',\n            'EGO_ACC': 'EGO_ACC',\n            'EGO_GYRO': 'EGO_GYRO',\n            'PED_ACTION': 'PED_ACTION',\n            'PED_LOOK': 'PED_LOOK',\n            'PED_OCCLUSION': 'PED_OCC'\n        }\n        for stream in self.active_streams:\n            stream_upper_key = stream.upper()\n            if stream_upper_key in special_cases:\n                size_constant_name = f\"INPUT_SIZE_{special_cases[stream_upper_key]}\"\n            elif stream == 'bbox':\n                size_constant_name = \"INPUT_SIZE_BBOX\"\n            elif stream == 'pose':\n                size_constant_name = \"INPUT_SIZE_POSE\"\n            else:\n                size_constant_name = f\"INPUT_SIZE_{stream.upper()}\"\n            if size_constant_name in globals():\n                input_sizes[stream] = globals()[size_constant_name]\n            else:\n                input_sizes[stream] = 1\n        return input_sizes\n\n    def _load_pose_data(self):\n        print(f\"\\nLoading pose data for sets: {self.set_names} from {self.pose_data_dir}\")\n        sets_loaded_count = 0\n        for set_id in self.set_names:\n            self.all_pose_data[set_id] = {}\n            pose_set_path = os.path.join(self.pose_data_dir, set_id)\n            if not os.path.isdir(pose_set_path):\n                print(f\"Warn: Pose dir missing for {set_id} at {pose_set_path}\")\n                continue\n            pkl_files_in_set = [f for f in os.listdir(pose_set_path)\n                                if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")]\n            if not pkl_files_in_set:\n                continue\n            loaded_video_count = 0\n            for pkl_filename in pkl_files_in_set:\n                pkl_file_path = os.path.join(pose_set_path, pkl_filename)\n                try:\n                    with open(pkl_file_path, 'rb') as f:\n                        loaded_pkl_content = pickle.load(f)\n                    if len(loaded_pkl_content) != 1:\n                        continue\n                    unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n                    video_id = \"_\".join(unique_video_key.split('_')[1:])\n                    self.all_pose_data[set_id][video_id] = video_data\n                    loaded_video_count += 1\n                except FileNotFoundError:\n                    pass\n                except Exception as e:\n                    print(f\"Error loading pose PKL {pkl_file_path}: {e}\")\n            if loaded_video_count > 0:\n                sets_loaded_count += 1\n        print(f\"Finished loading pose data for {sets_loaded_count} sets.\")\n\n    def _generate_sequence_list(self):\n        sequence_count = 0\n        ped_count = 0\n        for set_id in self.set_names:\n            if set_id not in self.pie_db:\n                continue\n            for video_id, video_data in self.pie_db[set_id].items():\n                if 'ped_annotations' not in video_data:\n                    continue\n                for ped_id, ped_data in video_data['ped_annotations'].items():\n                    ped_count += 1\n                    if 'frames' not in ped_data or len(ped_data['frames']) < self.seq_len + self.pred_len:\n                        continue\n                    sorted_frames = sorted(ped_data['frames'])\n                    for i in range(len(sorted_frames) - self.seq_len - self.pred_len + 1):\n                        start_frame = sorted_frames[i]\n                        end_frame_observe = sorted_frames[i + self.seq_len - 1]\n                        if end_frame_observe - start_frame != self.seq_len - 1:\n                            continue\n                        target_frame_actual_idx = i + self.seq_len + self.pred_len - 1\n                        if target_frame_actual_idx >= len(sorted_frames):\n                            continue\n                        target_frame = sorted_frames[target_frame_actual_idx]\n                        if target_frame - end_frame_observe != self.pred_len:\n                            continue\n                        self.sequences.append((set_id, video_id, ped_id, start_frame))\n                        sequence_count += 1\n        print(f\"Dataset initialized with {sequence_count} sequences for sets {self.set_names}.\")\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        set_id, video_id, ped_id, start_frame = self.sequences[idx]\n        frame_nums = list(range(start_frame, start_frame + self.seq_len))\n        target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n\n        video_db = self.pie_db.get(set_id, {}).get(video_id, {})\n        ped_db = video_db.get('ped_annotations', {}).get(ped_id, {})\n        ego_db = video_db.get('vehicle_annotations', {})\n        traffic_db = video_db.get('traffic_annotations', {})\n        ped_attributes = ped_db.get('attributes', {})\n\n        feature_sequences = {stream: [] for stream in self.active_streams}\n        label = 0\n        if 'frames' in ped_db and 'behavior' in ped_db and 'cross' in ped_db['behavior']:\n            try:\n                target_frame_db_idx = ped_db['frames'].index(target_frame_num)\n                label = ped_db['behavior']['cross'][target_frame_db_idx]\n                if label == -1:\n                    label = 0\n            except (ValueError, IndexError):\n                pass\n\n        static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n        if 'static_context' in self.active_streams:\n            sig_idx = ped_attributes.get('signalized', 0)\n            int_idx = ped_attributes.get('intersection', 0)\n            age_idx = ped_attributes.get('age', 2)\n            gen_idx = ped_attributes.get('gender', 0)\n            static_vec = np.concatenate([\n                to_one_hot(sig_idx, NUM_SIGNALIZED_CATS),\n                to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n                to_one_hot(age_idx, NUM_AGE_CATS),\n                to_one_hot(gen_idx, NUM_GENDER_CATS)\n            ])\n            if static_vec.shape[0] != INPUT_SIZE_STATIC:\n                static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n\n        for frame_num in frame_nums:\n            frame_db_idx = -1\n            if 'frames' in ped_db:\n                try:\n                    frame_db_idx = ped_db['frames'].index(frame_num)\n                except ValueError:\n                    pass\n            ego_frame_data = ego_db.get(frame_num, {})\n\n            if 'bbox' in self.active_streams:\n                bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32)\n                if frame_db_idx != -1 and 'bbox' in ped_db and len(ped_db['bbox']) > frame_db_idx:\n                    try:\n                        x1, y1, x2, y2 = ped_db['bbox'][frame_db_idx]\n                        img_w = video_db.get('width', 1920)\n                        img_h = video_db.get('height', 1080)\n                        if img_w > 0 and img_h > 0:\n                            cx = ((x1 + x2) / 2) / img_w\n                            cy = ((y1 + y2) / 2) / img_h\n                            w = (x2 - x1) / img_w\n                            h = (y2 - y1) / img_h\n                            if w > 0 and h > 0 and 0 <= cx <= 1 and 0 <= cy <= 1:\n                                bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n                    except Exception:\n                        pass\n                feature_sequences['bbox'].append(bbox_norm)\n\n            if 'pose' in self.active_streams:\n                pose_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n                vid_pose_data = self.all_pose_data.get(set_id, {}).get(video_id, {})\n                frame_pose_data = vid_pose_data.get(frame_num, {})\n                loaded_pose = frame_pose_data.get(ped_id)\n                if loaded_pose is not None and isinstance(loaded_pose, np.ndarray) and loaded_pose.shape == (INPUT_SIZE_POSE,):\n                    pose_vector = loaded_pose\n                feature_sequences['pose'].append(pose_vector)\n\n            if 'ego_speed' in self.active_streams:\n                speed = ego_frame_data.get('OBD_speed', 0.0)\n                if speed == 0.0:\n                    speed = ego_frame_data.get('GPS_speed', 0.0)\n                speed_scaled = (speed - self.scalers.get('ego_speed_mean', 0.0)) / self.scalers.get('ego_speed_std', 1.0)\n                feature_sequences['ego_speed'].append([speed_scaled])\n\n            if 'ego_acc' in self.active_streams:\n                accX = ego_frame_data.get('accX', 0.0)\n                accY = ego_frame_data.get('accY', 0.0)\n                accX_scaled = (accX - self.scalers.get('accX_mean', 0.0)) / self.scalers.get('accX_std', 1.0)\n                accY_scaled = (accY - self.scalers.get('accY_mean', 0.0)) / self.scalers.get('accY_std', 1.0)\n                feature_sequences['ego_acc'].append([accX_scaled, accY_scaled])\n\n            if 'ego_gyro' in self.active_streams:\n                gyroZ = ego_frame_data.get('gyroZ', 0.0)\n                gyroZ_scaled = (gyroZ - self.scalers.get('gyroZ_mean', 0.0)) / self.scalers.get('gyroZ_std', 1.0)\n                feature_sequences['ego_gyro'].append([gyroZ_scaled])\n\n            if 'ped_action' in self.active_streams:\n                action = 0\n                if frame_db_idx != -1 and 'behavior' in ped_db and 'action' in ped_db['behavior'] and len(ped_db['behavior']['action']) > frame_db_idx:\n                    action = ped_db['behavior']['action'][frame_db_idx]\n                feature_sequences['ped_action'].append([float(action)])\n\n            if 'ped_look' in self.active_streams:\n                look = 0\n                if frame_db_idx != -1 and 'behavior' in ped_db and 'look' in ped_db['behavior'] and len(ped_db['behavior']['look']) > frame_db_idx:\n                    look = ped_db['behavior']['look'][frame_db_idx]\n                feature_sequences['ped_look'].append([float(look)])\n\n            if 'ped_occlusion' in self.active_streams:\n                occ = 0.0\n                if frame_db_idx != -1 and 'occlusion' in ped_db and len(ped_db['occlusion']) > frame_db_idx:\n                    occ_val = ped_db['occlusion'][frame_db_idx]\n                    occ = float(occ_val) / 2.0\n                feature_sequences['ped_occlusion'].append([occ])\n\n            if 'traffic_light' in self.active_streams:\n                state_int = 0\n                for obj_id, obj_data in traffic_db.items():\n                    if obj_data.get('obj_class') == 'traffic_light' and 'frames' in obj_data and 'state' in obj_data:\n                        try:\n                            tl_frame_idx = obj_data['frames'].index(frame_num)\n                            state_val = obj_data['state'][tl_frame_idx]\n                            if state_val != 0:\n                                state_int = state_val\n                                break\n                        except (ValueError, IndexError):\n                            continue\n                feature_sequences['traffic_light'].append(to_one_hot(state_int, NUM_TL_STATES))\n\n            if 'static_context' in self.active_streams:\n                feature_sequences['static_context'].append(static_vec)\n\n        features = {}\n        try:\n            for stream_name in self.active_streams:\n                if stream_name in feature_sequences:\n                    features[stream_name] = torch.tensor(np.array(feature_sequences[stream_name], dtype=np.float32),\n                                                          dtype=torch.float32)\n        except Exception as e:\n            print(f\"Error converting features idx {idx}: {e}. Return dummy.\")\n            features = {name: torch.zeros((self.seq_len, self._input_sizes_for_error.get(name, 1)),\n                                          dtype=torch.float32)\n                        for name in self.active_streams}\n        return features, torch.tensor(label, dtype=torch.long)\n\nclass BalancedDataset(Dataset):\n    def __init__(self, data_dict, active_streams, label_key='label'):\n        self.active_streams = active_streams\n        self.label_key = label_key\n        if self.label_key not in data_dict or not data_dict[self.label_key]:\n            raise ValueError(f\"Label key '{self.label_key}' missing/empty.\")\n        self.num_samples = len(data_dict[self.label_key])\n        if self.num_samples == 0:\n            print(\"Warning: BalancedDataset initialized with zero samples.\")\n        self.features = {}\n        for stream in self.active_streams:\n            if stream in data_dict and data_dict[stream]:\n                try:\n                    self.features[stream] = torch.tensor(np.array(data_dict[stream]), dtype=torch.float32)\n                except ValueError as e:\n                    raise ValueError(f\"Error converting stream '{stream}' data: {e}\")\n            else:\n                raise KeyError(f\"Stream '{stream}' missing/empty in balanced data.\")\n        try:\n            self.labels = torch.tensor([lbl[0] for lbl in data_dict[self.label_key]], dtype=torch.long)\n        except (IndexError, TypeError) as e:\n            raise ValueError(f\"Error converting labels: {e}\")\n        for stream in self.active_streams:\n            if len(self.features[stream]) != self.num_samples:\n                raise ValueError(f\"Len mismatch: Stream '{stream}' ({len(self.features[stream])}) vs Labels ({self.num_samples})\")\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        feature_dict = {stream: self.features[stream][idx] for stream in self.active_streams}\n        label = self.labels[idx]\n        return feature_dict, label\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.attention_net = nn.Sequential(\n            nn.Linear(hidden_dim, attention_dim),\n            nn.Tanh(),\n            nn.Linear(attention_dim, 1)\n        )\n\n    def forward(self, lstm_output):\n        att_scores = self.attention_net(lstm_output).squeeze(2)\n        att_weights = torch.softmax(att_scores, dim=1)\n        context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1)\n        return context_vector, att_weights\n\nclass MultiStreamAdaptiveLSTM(nn.Module):\n    def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes, attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n        super(MultiStreamAdaptiveLSTM, self).__init__()\n        if not stream_names:\n            raise ValueError(\"stream_names cannot be empty.\")\n        self.stream_names = stream_names\n        self.lstms = nn.ModuleDict()\n        self.attentions = nn.ModuleDict()\n        print(f\"Initializing model with streams: {self.stream_names}\")\n        for name in self.stream_names:\n            if name not in input_sizes:\n                raise KeyError(f\"Input size for stream '{name}' not provided.\")\n            current_input_size = input_sizes[name]\n            print(f\"  - Adding stream '{name}' with input size {current_input_size}\")\n            self.lstms[name] = nn.LSTM(current_input_size, lstm_hidden_size, num_lstm_layers,\n                                        batch_first=True,\n                                        dropout=dropout_rate if num_lstm_layers > 1 else 0,\n                                        bidirectional=True)\n            self.attentions[name] = Attention(lstm_hidden_size * 2, attention_dim)\n        num_active_streams = len(self.stream_names)\n        combined_feature_dim = lstm_hidden_size * 2 * num_active_streams\n        print(f\"  Combined feature dimension: {combined_feature_dim}\")\n        self.dropout = nn.Dropout(dropout_rate)\n        intermediate_dim = max(num_classes * 4, combined_feature_dim // 2)\n        self.fc1 = nn.Linear(combined_feature_dim, intermediate_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(intermediate_dim, num_classes)\n\n    def forward(self, x):\n        stream_context_vectors = []\n        stream_att_weights = {}\n        for name in self.stream_names:\n            if name not in x:\n                print(f\"Warning: Stream '{name}' expected but not in input data.\")\n                continue\n            lstm_out, _ = self.lstms[name](x[name])\n            context_vector, attention_weights = self.attentions[name](lstm_out)\n            stream_context_vectors.append(context_vector)\n            stream_att_weights[name] = attention_weights\n        if not stream_context_vectors:\n            raise RuntimeError(\"No stream outputs generated.\")\n        fused_features = torch.cat(stream_context_vectors, dim=1)\n        out = self.dropout(fused_features)\n        out = self.relu(self.fc1(out))\n        out = self.dropout(out)\n        logits = self.fc2(out)\n        return logits\n\ndef train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n    active_streams = model.stream_names\n    for features, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n        input_features = {name: features[name].to(device) for name in active_streams if name in features}\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(input_features)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n    avg_loss = total_loss / len(dataloader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    return avg_loss, accuracy\n\ndef evaluate_epoch(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_labels = []\n    all_preds = []\n    all_probs = []\n    active_streams = model.stream_names\n    with torch.no_grad():\n        for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n            input_features = {name: features[name].to(device) for name in active_streams if name in features}\n            labels = labels.to(device)\n            outputs = model(input_features)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            probs = torch.softmax(outputs, dim=1)\n            preds = torch.argmax(probs, dim=1)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n    avg_loss = total_loss / len(dataloader)\n    all_probs = np.array(all_probs)\n    all_labels = np.array(all_labels)\n    all_preds = np.array(all_preds)\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', pos_label=1, zero_division=0)\n    auc = roc_auc_score(all_labels, all_probs[:, 1]) if len(np.unique(all_labels)) > 1 else float('nan')\n    return {'loss': avg_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}\n\ndef get_predictions_and_labels(model, dataloader, device):\n    model.eval()\n    all_labels = []\n    all_preds = []\n    active_streams = model.stream_names\n    with torch.no_grad():\n        for features, labels in tqdm(dataloader, desc=\"Generating CM Data\", leave=False):\n            input_features = {name: features[name].to(device) for name in active_streams if name in features}\n            labels = labels.to(device)\n            outputs = model(input_features)\n            preds = torch.argmax(outputs, dim=1)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n    return np.array(all_labels), np.array(all_preds)\n\n# --- Main Execution Block ---\nif __name__ == '__main__':\n\n    # --- Load Pre-Processed/Balanced Data ---\n    print(f\"Loading balanced training data from: {BALANCED_DATA_PKL_PATH}\")\n    print(f\"Loading scalers from: {SCALERS_PKL_PATH}\")\n    print(f\"Loading validation sequence info from: {VAL_SEQUENCES_PKL_PATH}\")\n    try:\n        with open(BALANCED_DATA_PKL_PATH, 'rb') as f:\n            balanced_train_data_dict = pickle.load(f)\n        with open(SCALERS_PKL_PATH, 'rb') as f:\n            scalers = pickle.load(f)\n        with open(VAL_SEQUENCES_PKL_PATH, 'rb') as f:\n            val_sequences_info = pickle.load(f)\n        print(\" -> Pre-processed data loaded successfully.\")\n    except FileNotFoundError as e:\n        print(f\"ERROR: Required pre-processed file not found: {e}\")\n        print(\"Please run Cell 1 (Data Preparation) first.\")\n        exit()\n    except Exception as e:\n        print(f\"Error loading pre-processed data: {e}\")\n        exit()\n\n    # --- Load Full PIE Database (needed for Validation Dataset __getitem__) ---\n    print(\"\\nLoading PIE database cache...\")\n    if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n        raise FileNotFoundError(\"PIE database cache not found. Please run Cell 1 or the cache generation script.\")\n    try:\n        with open(PIE_DATABASE_CACHE_PATH, 'rb') as f:\n            pie_database = pickle.load(f)\n        print(\" -> PIE database loaded successfully.\")\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load PIE database: {e}\")\n\n    # --- Create Datasets and DataLoaders ---\n    print(\"\\nCreating Datasets and DataLoaders...\")\n    try:\n        # Training dataset uses the loaded balanced dictionary\n        balanced_train_dataset = BalancedDataset(balanced_train_data_dict, ACTIVE_STREAMS, label_key='label')\n        del balanced_train_data_dict  # Free memory\n\n        # Validation dataset uses the full PIE DB and loaded sequence info\n        val_dataset = PIEDataset(pie_database, VAL_SETS_STR, POSE_DATA_DIR, SEQ_LEN, PRED_LEN, scalers, ACTIVE_STREAMS)\n        # Optionally, you can override val_dataset.sequences with val_sequences_info if desired:\n        # val_dataset.sequences = val_sequences_info\n    except Exception as e:\n        print(f\"Error creating Datasets: {e}\")\n        raise\n\n    if len(balanced_train_dataset) == 0 or len(val_dataset) == 0:\n        raise ValueError(\"One or both datasets are empty!\")\n\n    train_loader = DataLoader(balanced_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n    print(\"DataLoaders created.\")\n\n    # --- Initialize Model ---\n    input_sizes = {}\n    for stream in ACTIVE_STREAMS:\n        size_constant_name = f\"INPUT_SIZE_{stream.upper()}\"\n        special_cases = {\n            'TRAFFIC_LIGHT': 'TL_STATE',\n            'STATIC_CONTEXT': 'STATIC',\n            'EGO_SPEED': 'EGO_SPEED',\n            'EGO_ACC': 'EGO_ACC',\n            'EGO_GYRO': 'EGO_GYRO',\n            'PED_ACTION': 'PED_ACTION',\n            'PED_LOOK': 'PED_LOOK',\n            'PED_OCCLUSION': 'PED_OCC'\n        }\n        stream_upper_key = stream.upper()\n        suffix = special_cases.get(stream_upper_key)\n        if suffix:\n            size_constant_name = f\"INPUT_SIZE_{suffix}\"\n        elif stream == 'bbox':\n            size_constant_name = \"INPUT_SIZE_BBOX\"\n        elif stream == 'pose':\n            size_constant_name = \"INPUT_SIZE_POSE\"\n        if size_constant_name in globals():\n            input_sizes[stream] = globals()[size_constant_name]\n        else:\n            raise ValueError(f\"Input size constant {size_constant_name} not found for active stream {stream}\")\n\n    model = MultiStreamAdaptiveLSTM(\n        input_sizes=input_sizes,\n        lstm_hidden_size=LSTM_HIDDEN_SIZE,\n        num_lstm_layers=NUM_LSTM_LAYERS,\n        num_classes=NUM_CLASSES,\n        attention_dim=ATTENTION_DIM,\n        dropout_rate=DROPOUT_RATE,\n        stream_names=ACTIVE_STREAMS\n    ).to(DEVICE)\n\n    print(\"\\n--- Model Architecture ---\")\n    print(model)\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total Trainable Parameters: {num_params:,}\")\n    print(\"-\" * 30)\n\n    # --- Class Weighting & Optimizer ---\n    print(\"\\nCalculating Class Weights for Loss Function...\")\n    balanced_train_labels_list = balanced_train_dataset.labels.tolist()\n    count_0 = balanced_train_labels_list.count(0)\n    count_1 = balanced_train_labels_list.count(1)\n    total = len(balanced_train_labels_list)\n    if total == 0:\n        print(\"Warning: Balanced dataset empty. Use equal weights.\")\n        weight_0 = 1.0\n        weight_1 = 1.0\n    elif count_0 == 0:\n        print(\"Warning: Class 0 missing. Adjust weights.\")\n        weight_0 = 0.0\n        weight_1 = 1.0\n    elif count_1 == 0:\n        print(\"Warning: Class 1 missing. Adjust weights.\")\n        weight_0 = 1.0\n        weight_1 = 0.0\n    else:\n        weight_0 = total / (2.0 * count_0)\n        weight_1 = total / (2.0 * count_1)\n    class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float32).to(DEVICE)\n    print(f\"Using Class Weights for Loss: 0={weight_0:.2f}, 1={weight_1:.2f}\")\n    criterion = nn.CrossEntropyLoss(weight=class_weights)\n    # Uncomment the following line to disable class weighting:\n    # criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    best_val_f1 = -1.0\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n    val_f1s = []\n\n    # --- Training Loop ---\n    print(\"\\n--- Starting Training ---\")\n    for epoch in range(NUM_EPOCHS):\n        epoch_start_time = time.time()\n        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n        val_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n        epoch_duration = time.time() - epoch_start_time\n        train_losses.append(train_loss)\n        val_losses.append(val_metrics['loss'])\n        train_accs.append(train_acc)\n        val_accs.append(val_metrics['accuracy'])\n        val_f1s.append(val_metrics['f1'])\n        print(f\"--- Epoch {epoch+1}/{NUM_EPOCHS} ({epoch_duration:.2f} sec) ---\")\n        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        print(f\"  Val Loss:   {val_metrics['loss']:.4f}, Val Acc:  {val_metrics['accuracy']:.4f}\")\n        print(f\"  Val Prec:   {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n        print(f\"  Val AUC:    {val_metrics['auc']:.4f}\")\n        if val_metrics['f1'] > best_val_f1:\n            best_val_f1 = val_metrics['f1']\n            # Include active streams in the filename for clarity during experiments\n            active_streams_str = \"_\".join(sorted(ACTIVE_STREAMS))\n            best_model_path = f\"best_model_{active_streams_str}_ep{epoch+1}.pth\"\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"  >> Saved new best model ({active_streams_str}) with F1: {best_val_f1:.4f} to {best_model_path}\")\n        print(\"-\" * 30)\n    print(\"--- Training Finished ---\")\n\n    # --- Plotting ---\n    print(\"\\n--- Plotting Training History ---\")\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    axes[0].plot(range(1, NUM_EPOCHS + 1), train_losses, label=\"Train Loss\")\n    axes[0].plot(range(1, NUM_EPOCHS + 1), val_losses, label=\"Val Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[0].set_title(\"Loss Curve\")\n    axes[0].legend()\n    axes[0].grid(True)\n    axes[1].plot(range(1, NUM_EPOCHS + 1), train_accs, label=\"Train Accuracy\")\n    axes[1].plot(range(1, NUM_EPOCHS + 1), val_accs, label=\"Val Accuracy\")\n    axes[1].plot(range(1, NUM_EPOCHS + 1), val_f1s, label=\"Val F1-Score\", linestyle=\"--\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Metric\")\n    axes[1].set_title(\"Accuracy & F1-Score Curve\")\n    axes[1].legend()\n    axes[1].grid(True)\n    plt.tight_layout()\n    plt.show()\n\n    # --- Final Evaluation ---\n    print(\"\\n--- Final Evaluation on Validation Set using Best Model ---\")\n    # Reconstruct the best model file path based on the epoch with highest F1\n    best_epoch = val_f1s.index(max(val_f1s)) + 1\n    active_streams_str = \"_\".join(sorted(ACTIVE_STREAMS))\n    best_model_path_final = f\"best_model_{active_streams_str}_ep{best_epoch}.pth\"\n    if not os.path.exists(best_model_path_final):\n        best_model_path_final = \"best_model_balanced.pth\"  # Fallback default\n    if os.path.exists(best_model_path_final):\n        print(f\"Loading best saved model '{best_model_path_final}'\")\n        try:\n            model.load_state_dict(torch.load(best_model_path_final, map_location=DEVICE))\n        except FileNotFoundError:\n            print(f\"Warning: Could not find model file '{best_model_path_final}'. Evaluating final epoch model.\")\n        except Exception as e:\n            print(f\"Warning: Error loading model state dict '{best_model_path_final}': {e}. Evaluating final epoch model.\")\n    else:\n        print(f\"Warning: No saved model found at '{best_model_path_final}'. Evaluating final epoch model.\")\n\n    final_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n    true_labels, pred_labels = get_predictions_and_labels(model, val_loader, DEVICE)\n    cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1])\n    labels_display = ['Not Crossing', 'Crossing']\n    print(\"\\n--- Final Performance Metrics ---\")\n    print(f\"  Streams:   {', '.join(ACTIVE_STREAMS)}\")\n    print(f\"  Accuracy:  {final_metrics['accuracy']:.4f}\")\n    print(f\"  Precision: {final_metrics['precision']:.4f}\")\n    print(f\"  Recall:    {final_metrics['recall']:.4f}\")\n    print(f\"  F1 Score:  {final_metrics['f1']:.4f}\")\n    print(f\"  AUC:       {final_metrics['auc']:.4f}\")\n    print(f\"  Loss:      {final_metrics['loss']:.4f}\")\n    print(\"\\n--- Confusion Matrix ---\")\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_display)\n    disp.plot(cmap=plt.cm.Blues)\n    plt.title(f\"Confusion Matrix ({', '.join(ACTIVE_STREAMS)})\")\n    plt.show()\n\n    print(\"\\n--- Script Complete ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T14:05:25.671600Z","iopub.execute_input":"2025-04-19T14:05:25.671938Z"}},"outputs":[{"name":"stdout","text":"--- Running Experiment With Active Streams: ['static_context'] ---\nUsing device: cuda\n\nLoading pre-processed data...\n -> Loaded balanced training data (109934 samples).\n -> Loaded scalers.\n -> Loaded validation sequence info (77288 sequences).\n\nLoading PIE database cache...\n -> PIE database loaded successfully.\nLoading balanced training data from: /kaggle/working/balanced_train_data.pkl\nLoading scalers from: /kaggle/working/scalers.pkl\nLoading validation sequence info from: /kaggle/working/val_sequences.pkl\n -> Pre-processed data loaded successfully.\n\nLoading PIE database cache...\n -> PIE database loaded successfully.\n\nCreating Datasets and DataLoaders...\n\nLoading pose data for sets: ['set05', 'set06'] from /kaggle/input/pose-data/extracted_poses2\nFinished loading pose data for 2 sets.\nDataset initialized with 77288 sequences for sets ['set05', 'set06'].\nDataLoaders created.\nInitializing model with streams: ['static_context']\n  - Adding stream 'static_context' with input size 16\n  Combined feature dimension: 512\n\n--- Model Architecture ---\nMultiStreamAdaptiveLSTM(\n  (lstms): ModuleDict(\n    (static_context): LSTM(16, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n  )\n  (attentions): ModuleDict(\n    (static_context): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc1): Linear(in_features=512, out_features=256, bias=True)\n  (relu): ReLU()\n  (fc2): Linear(in_features=256, out_features=2, bias=True)\n)\nTotal Trainable Parameters: 2,335,747\n------------------------------\n\nCalculating Class Weights for Loss Function...\nUsing Class Weights for Loss: 0=1.00, 1=1.00\n\n--- Starting Training ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 1/15 (45.99 sec) ---\n  Train Loss: 0.6382, Train Acc: 0.6154\n  Val Loss:   0.6905, Val Acc:  0.4702\n  Val Prec:   0.2058, Recall: 0.8629, F1: 0.3324\n  Val AUC:    0.6539\n  >> Saved new best model (static_context) with F1: 0.3324 to best_model_static_context_ep1.pth\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 2/15 (45.55 sec) ---\n  Train Loss: 0.6280, Train Acc: 0.6223\n  Val Loss:   0.6722, Val Acc:  0.4861\n  Val Prec:   0.2111, Recall: 0.8629, F1: 0.3392\n  Val AUC:    0.6557\n  >> Saved new best model (static_context) with F1: 0.3392 to best_model_static_context_ep2.pth\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 3/15 (45.34 sec) ---\n  Train Loss: 0.6250, Train Acc: 0.6231\n  Val Loss:   0.6697, Val Acc:  0.4915\n  Val Prec:   0.2129, Recall: 0.8629, F1: 0.3415\n  Val AUC:    0.6680\n  >> Saved new best model (static_context) with F1: 0.3415 to best_model_static_context_ep3.pth\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 4/15 (45.89 sec) ---\n  Train Loss: 0.6242, Train Acc: 0.6242\n  Val Loss:   0.6384, Val Acc:  0.4879\n  Val Prec:   0.2117, Recall: 0.8629, F1: 0.3400\n  Val AUC:    0.6577\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 5/15 (45.34 sec) ---\n  Train Loss: 0.6242, Train Acc: 0.6241\n  Val Loss:   0.6863, Val Acc:  0.4879\n  Val Prec:   0.2117, Recall: 0.8629, F1: 0.3400\n  Val AUC:    0.6498\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 6/15 (45.72 sec) ---\n  Train Loss: 0.6247, Train Acc: 0.6226\n  Val Loss:   0.6613, Val Acc:  0.4497\n  Val Prec:   0.1995, Recall: 0.8629, F1: 0.3240\n  Val AUC:    0.6386\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 7/15 (45.18 sec) ---\n  Train Loss: 0.6246, Train Acc: 0.6221\n  Val Loss:   0.6635, Val Acc:  0.4915\n  Val Prec:   0.2129, Recall: 0.8629, F1: 0.3415\n  Val AUC:    0.6562\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 8/15 (45.20 sec) ---\n  Train Loss: 0.6234, Train Acc: 0.6242\n  Val Loss:   0.6658, Val Acc:  0.4915\n  Val Prec:   0.2129, Recall: 0.8629, F1: 0.3415\n  Val AUC:    0.6523\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 9/15 (45.39 sec) ---\n  Train Loss: 0.6233, Train Acc: 0.6244\n  Val Loss:   0.6598, Val Acc:  0.4879\n  Val Prec:   0.2117, Recall: 0.8629, F1: 0.3400\n  Val AUC:    0.6514\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 10/15 (45.08 sec) ---\n  Train Loss: 0.6237, Train Acc: 0.6244\n  Val Loss:   0.6627, Val Acc:  0.5020\n  Val Prec:   0.2145, Recall: 0.8486, F1: 0.3425\n  Val AUC:    0.6565\n  >> Saved new best model (static_context) with F1: 0.3425 to best_model_static_context_ep10.pth\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 11/15 (45.53 sec) ---\n  Train Loss: 0.6234, Train Acc: 0.6245\n  Val Loss:   0.6978, Val Acc:  0.4825\n  Val Prec:   0.2099, Recall: 0.8629, F1: 0.3376\n  Val AUC:    0.6515\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 12/15 (45.10 sec) ---\n  Train Loss: 0.6235, Train Acc: 0.6244\n  Val Loss:   0.6793, Val Acc:  0.4879\n  Val Prec:   0.2117, Recall: 0.8629, F1: 0.3400\n  Val AUC:    0.6510\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 13/15 (45.22 sec) ---\n  Train Loss: 0.6230, Train Acc: 0.6249\n  Val Loss:   0.6577, Val Acc:  0.4915\n  Val Prec:   0.2129, Recall: 0.8629, F1: 0.3415\n  Val AUC:    0.6587\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 14/15 (45.54 sec) ---\n  Train Loss: 0.6229, Train Acc: 0.6245\n  Val Loss:   0.6732, Val Acc:  0.4522\n  Val Prec:   0.2002, Recall: 0.8629, F1: 0.3250\n  Val AUC:    0.6373\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Epoch 15/15 (44.97 sec) ---\n  Train Loss: 0.6229, Train Acc: 0.6248\n  Val Loss:   0.6561, Val Acc:  0.4915\n  Val Prec:   0.2129, Recall: 0.8629, F1: 0.3415\n  Val AUC:    0.6622\n------------------------------\n--- Training Finished ---\n\n--- Plotting Training History ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x500 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAABKUAAAHqCAYAAADVi/1VAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3xT1fvA8U+SpntQ6GRD2aNskSGCjAKKIMhyIKioCAriRL8CioqK+kNx4GKogDgQUBCoIA5AQLDsvcoquxTa0qbJ/f1xmrShBUpJe5P2eb9eed2bm5ub556m7c2Tc55j0DRNQwghhBBCCCGEEEKIYmTUOwAhhBBCCCGEEEIIUfpIUkoIIYQQQgghhBBCFDtJSgkhhBBCCCGEEEKIYidJKSGEEEIIIYQQQghR7CQpJYQQQgghhBBCCCGKnSSlhBBCCCGEEEIIIUSxk6SUEEIIIYQQQgghhCh2kpQSQgghhBBCCCGEEMVOklJCCCGEEEIIIYQQothJUkoIIYQQQgghhBBCFDtJSgkhis2MGTMwGAz8+++/eodSIAkJCdx3331UqlQJHx8fypYtS6dOnZg+fTpWq1Xv8IQQQohS4eOPP8ZgMNCyZUu9Q/FIa9as4dZbbyU4OJiIiAi6devGqlWrCvz8gwcPYjAY8r3dfPPNjv127drFU089RevWrfH19cVgMHDw4MHrivXUqVOMHDmSOnXq4OfnR0REBDfddBPPP/88Fy9evK5juYt9+/bx6KOPUr16dXx9fQkODqZNmza8//77pKen6x2eELrz0jsAIYRwR1988QWPPfYYkZGR3H///dSsWZMLFy6wfPlyHnroIY4fP86LL76od5hCCCFEiTdr1iyqVq3KunXr2Lt3LzVq1NA7JI+RmJhIXFwc5cqV45VXXsFmsxEfH8/y5ctp06bNdR1r4MCBdO/e3WlbeHi4Y33NmjV88MEH1KtXj7p165KQkHBdxz979izNmzcnJSWFBx98kDp16nDmzBk2b97MJ598wrBhwwgMDLyuY+pt0aJF9O3bFx8fHwYNGkSDBg3IzMzk77//5tlnn2Xbtm189tlneocphK4kKSWEEJf5559/eOyxx2jVqhWLFy8mKCjI8dioUaP4999/2bp1q0teKzU1lYCAAJccSwghhChpDhw4wOrVq5k3bx6PPvoos2bNYty4cXqHlS93/J++aNEix5dqLVq0AODpp58mIyPjuo/VtGlT7rvvvis+fuedd5KcnExQUBDvvPPOdSelvvzySxITE1m1ahWtW7d2eiwlJQVvb+/rjrmwXPGzPHDgAAMGDKBKlSqsWLGC6Ohox2PDhw9n7969LFq06EZDBdzzvSdEQcnwPSGE2/nvv//o1q0bwcHBBAYG0rFjR/755x+nfSwWC6+88go1a9bE19eXcuXK0bZtW+Lj4x37JCUlMWTIECpWrIiPjw/R0dH07Nnzml3JX3nlFQwGA7NmzXJKSNk1b96cwYMHA7By5UoMBgMrV6502sfe1X3GjBmObYMHDyYwMJB9+/bRvXt3goKCuPfeexkxYgSBgYGkpaXlea2BAwcSFRXlNFzw119/5ZZbbiEgIICgoCBuv/12tm3bdtVzEkIIITzRrFmzCA0N5fbbb+fuu+9m1qxZ+e6XnJzMU089RdWqVfHx8aFixYoMGjSI06dPO/a5dOkS48ePp1atWvj6+hIdHU3v3r3Zt28f4Jr/6QB//fUXffv2pXLlyvj4+FCpUiWeeuqpfIdq7dy5k379+hEeHo6fnx+1a9fmpZdeAuD333/HYDDw008/5Xne7NmzMRgMrFmz5qrtZzSqj3uapjlt9/HxuerzCqNs2bL5XjcV1L59+zCZTE5DAu2Cg4Px9fV12rZ27Vq6d+9OaGgoAQEBxMbG8v777zvts2LFCsc1U5kyZejZsyc7duxw2mf8+PEYDAa2b9/OPffcQ2hoKG3btnU8/s0339CsWTP8/PwoW7YsAwYM4PDhw9c8n7fffpuLFy/y5ZdfOiWk7GrUqMHIkSOB/N9jdgaDgfHjx18z3nfeeQeDwcChQ4fyHGPMmDF4e3tz7tw5p/br2rUrISEh+Pv7c+utt17XsE4hXEWSUkIIt7Jt2zZuueUWNm3axHPPPcfLL7/MgQMHaN++PWvXrnXsN378eF555RU6dOjAhx9+yEsvvUTlypXZuHGjY58+ffrw008/MWTIED7++GOefPJJLly4QGJi4hVfPy0tjeXLl9OuXTsqV67s8vPLysoiLi6OiIgI3nnnHfr06UP//v1JTU3N821ZWloaP//8M3fffTcmkwmAr7/+mttvv53AwEDeeustXn75ZbZv307btm2vu26DEEII4e5mzZpF79698fb2ZuDAgezZs4f169c77XPx4kVuueUWpkyZQpcuXXj//fd57LHH2LlzJ0eOHAHAarVyxx138Morr9CsWTPeffddRo4cyfnz5wvd+zm//+kA33//PWlpaQwbNowpU6YQFxfHlClTGDRokNPzN2/eTMuWLVmxYgVDhw7l/fffp1evXvz8888AtG/fnkqVKuWbiJs1axYxMTG0atXqqjH27t2bkJAQnn32WTIzMwt1nnZpaWmcPn3a6WaxWG7omLlVqVIFq9XK119/fc194+PjadeuHdu3b2fkyJG8++67dOjQgV9++cWxz2+//UZcXBwnT55k/PjxjB49mtWrV9OmTZt8r5n69u1LWloab7zxBkOHDgXg9ddfZ9CgQdSsWZP33nuPUaNGOa4Tk5OTrxrjzz//TPXq1fP0+nKVy+Pt168fBoOB7777Ls++3333HV26dCE0NBRQybp27dqRkpLCuHHjeOONN0hOTua2225j3bp1RRKvEFekCSFEMZk+fboGaOvXr7/iPr169dK8vb21ffv2ObYdO3ZMCwoK0tq1a+fY1qhRI+3222+/4nHOnTunAdqkSZOuK8ZNmzZpgDZy5MgC7f/7779rgPb77787bT9w4IAGaNOnT3dse+CBBzRAe+GFF5z2tdlsWoUKFbQ+ffo4bf/uu+80QPvzzz81TdO0CxcuaGXKlNGGDh3qtF9SUpIWEhKSZ7sQQgjhyf79918N0OLj4zVNU/8vK1asmOd/9NixYzVAmzdvXp5j2Gw2TdM0bdq0aRqgvffee1fcxxX/0zVN09LS0vJsmzhxomYwGLRDhw45trVr104LCgpy2pY7Hk3TtDFjxmg+Pj5acnKyY9vJkyc1Ly8vbdy4cXle53KrV6/WQkNDNW9vb61v375aVlbWNZ9zOfv553e7vK3sJk2apAHagQMHCvw6SUlJWnh4uAZoderU0R577DFt9uzZTueuaZqWlZWlVatWTatSpYp27tw5p8dyt13jxo21iIgI7cyZM45tmzZt0oxGozZo0CDHtnHjxmmANnDgQKdjHTx4UDOZTNrrr7/utH3Lli2al5dXnu25nT9/XgO0nj17Fujc83uP2QFOP+srxatpmtaqVSutWbNmTtvWrVunAdpXX32laZpqo5o1a2pxcXFO7ZWWlqZVq1ZN69y5c4FiFsJVpKeUEMJtWK1Wli1bRq9evahevbpje3R0NPfccw9///03KSkpAJQpU4Zt27axZ8+efI/l5+eHt7c3K1eudOqqfC32499I9/NrGTZsmNN9g8FA3759Wbx4sdPMMnPnzqVChQqOLuTx8fEkJyczcOBAp28pTSYTLVu25Pfffy+ymIUQQojiNmvWLCIjI+nQoQOg/l/279+fb7/91mlY+48//kijRo2466678hzDYDA49gkLC+OJJ5644j6Fcfn/dFDXIHapqamcPn2a1q1bo2ka//33H6Bmmfvzzz958MEH8/TMzh3PoEGDyMjI4IcffnBsmzt3LllZWVet7wRw6NAhunfvzkMPPcT8+fP56aefGDp0qNNQvkcffZRKlSoV6FwfeeQR4uPjnW6NGjUq0HMLIjIykk2bNvHYY49x7tw5pk6dyj333ENERAQTJkxwxP3ff/9x4MABRo0aRZkyZZyOYW+748ePk5CQwODBgylbtqzj8djYWDp37szixYvzvP5jjz3mdH/evHnYbDb69evndN0VFRVFzZo1r3rdVRzXk5fHC9C/f382bNjgGJIK6v3i4+NDz549ATW79J49e7jnnns4c+aM47xSU1Pp2LEjf/75JzabrcjiFuJykpQSQriNU6dOkZaWRu3atfM8VrduXWw2m2MM/6uvvkpycjK1atWiYcOGPPvss2zevNmxv4+PD2+99Ra//vorkZGRtGvXjrfffpukpKSrxhAcHAzAhQsXXHhmOby8vKhYsWKe7f379yc9PZ2FCxcCaijC4sWL6du3r+MCy56Au+222wgPD3e6LVu2jJMnTxZJzEIIIURxs1qtfPvtt3To0IEDBw6wd+9e9u7dS8uWLTlx4gTLly937Ltv3z4aNGhw1ePt27eP2rVr4+XlunmervQ/PTEx0ZEMCQwMJDw8nFtvvRWA8+fPA7B//36Aa8Zdp04dWrRo4TSEb9asWdx8883XnIVw4sSJGI1GXnvtNbp168a0adOYMWMGo0aNcuyzdetWWrZsWaDzrVmzJp06dXK62YeDXY9Tp06RlJTkuOX+Qi46OppPPvmE48ePs2vXLj744APCw8MZO3YsX375JYAj4XK1trPXVbrSNaU9CZNbtWrVnO7v2bMHTdOoWbNmnuuuHTt2XPW6q6ivJ/OLF9SQPqPRyNy5cwFVS+z777931GqFnOvJBx54IM95ffHFF2RkZDjep0IUB5l9Twjhkdq1a8e+fftYsGABy5Yt44svvuD//u//mDp1Kg8//DCgZsrr0aMH8+fPZ+nSpbz88stMnDiRFStW0KRJk3yPW6NGDby8vNiyZUuB4rjSt6u5v8HNzcfHx1F0NLebb76ZqlWr8t1333HPPffw888/k56eTv/+/R372L+1+vrrr4mKispzDFdeaAshhBB6WrFiBcePH+fbb7/l22+/zfP4rFmz6NKli0tf0xX/061WK507d+bs2bM8//zz1KlTh4CAAI4ePcrgwYML1QNl0KBBjBw5kiNHjpCRkcE///zDhx9+eM3nrV69msaNGzuKmt9///2cOHGCZ599lqCgIAYMGMCaNWv48ccfrzumG9GiRQunYtzjxo1zKuQN6mdRq1YtatWqxe23307NmjWZNWuW4xqvKOTu4QbqustgMPDrr786anvmFhgYeMVjBQcHU758+QLXK7ve915+8QKUL1+eW265he+++44XX3yRf/75h8TERN566y3HPvb34KRJk2jcuHG+x77auQnhavIJRgjhNsLDw/H392fXrl15Htu5cydGo9Gpi3nZsmUZMmQIQ4YM4eLFi7Rr147x48c7XbDExMTw9NNP8/TTT7Nnzx4aN27Mu+++yzfffJNvDP7+/tx2222sWLGCw4cPX7NLu/0bwsuLXeY388m19OvXj/fff5+UlBTmzp1L1apVnWagiYmJASAiIoJOnTpd9/GFEEIITzFr1iwiIiL46KOP8jw2b948fvrpJ6ZOnYqfnx8xMTHX/PAfExPD2rVrsVgsmM3mfPdxxf/0LVu2sHv3bmbOnOlU2Dz37MCAo0xBQZIWAwYMYPTo0cyZM4f09HTMZrPTl1ZXYjAY8swS98wzz3DixAlef/11Zs2aRZMmTRzDuorLrFmznGYizF2yIT/Vq1cnNDSU48ePAznXQ1u3br3i9VCVKlUArnhNGRYWRkBAwFVfNyYmBk3TqFatGrVq1brqvvm54447+Oyzz1izZs01C9K78nqyf//+PP744+zatYu5c+fi7+9Pjx49HI/b2y84OFiuJ4VbkOF7Qgi3YTKZ6NKlCwsWLHCaFeXEiRPMnj2btm3bOroenzlzxum5gYGB1KhRg4yMDEDNEHPp0iWnfWJiYggKCnLscyXjxo1D0zTuv/9+py7ldhs2bGDmzJmAuugxmUz8+eefTvt8/PHHBTvpXPr3709GRgYzZ85kyZIl9OvXz+nxuLg4goODeeONN/Kd7ebUqVPX/ZpCCCGEu0lPT2fevHnccccd3H333XluI0aM4MKFC44h73369GHTpk389NNPeY5lr0PUp08fTp8+nW8PI/s+rvifbu9Rk7tuk6ZpvP/++077hYeH065dO6ZNm5ZnVuDczwUICwujW7dufPPNN8yaNYuuXbsSFhZ2zVg6derEnj178sxm9+abb1KvXj0OHjzInXfemW8P7qLUpk0bpyGA9qTU2rVr8wypA1i3bh1nzpxxDMVr2rQp1apVY/LkyXmSOPa2i46OpnHjxsycOdNpn61bt7Js2TK6d+9+zTh79+6NyWTilVdeyfMz0TQtz7Xo5Z577jkCAgJ4+OGHOXHiRJ7H9+3b53hfBAcHExYW5pLryT59+mAymZgzZw7ff/89d9xxh1MCrlmzZsTExPDOO+/ke50r15OiuElPKSFEsZs2bRpLlizJs33kyJG89tprxMfH07ZtWx5//HG8vLz49NNPycjI4O2333bsW69ePdq3b0+zZs0oW7Ys//77Lz/88AMjRowAYPfu3XTs2JF+/fpRr149vLy8+Omnnzhx4gQDBgy4anytW7fmo48+4vHHH6dOnTrcf//91KxZkwsXLrBy5UoWLlzIa6+9BkBISAh9+/ZlypQpGAwGYmJi+OWXXwpV36lp06bUqFGDl156iYyMjDzfggYHB/PJJ59w//3307RpUwYMGEB4eDiJiYksWrSINm3aFKg7vxBCCOHOFi5cyIULF7jzzjvzffzmm28mPDycWbNm0b9/f5599ll++OEH+vbty4MPPkizZs04e/YsCxcuZOrUqTRq1IhBgwbx1VdfMXr0aNatW8ctt9xCamoqv/32G48//jg9e/Z0yf/0OnXqEBMTwzPPPMPRo0cJDg7mxx9/zHfSlQ8++IC2bdvStGlTHnnkEapVq8bBgwdZtGgRCQkJTvsOGjSIu+++G4AJEyYUKJYxY8Ywf/58HnjgAeLj42ndujUXL15kzpw5HDhwgBYtWvDaa6/RqlUrlwyFPH/+PFOmTAFg1apVAHz44YeUKVOGMmXKOK7RruTrr79m1qxZ3HXXXTRr1gxvb2927NjBtGnT8PX15cUXXwTAaDTyySef0KNHDxo3bsyQIUOIjo5m586dbNu2jaVLlwJqeFq3bt1o1aoVDz30EOnp6UyZMoWQkJA8wwXzExMTw2uvvcaYMWM4ePAgvXr1IigoiAMHDvDTTz/xyCOP8Mwzz1z1+bNnz6Z///7UrVuXQYMG0aBBAzIzM1m9ejXff/89gwcPduz/8MMP8+abb/Lwww/TvHlz/vzzT3bv3n3NOC8XERFBhw4deO+997hw4UKe60mj0cgXX3xBt27dqF+/PkOGDKFChQocPXqU33//neDgYH7++efrfl0hCk2HGf+EEKXU9OnTrzilMKAdPnxY0zRN27hxoxYXF6cFBgZq/v7+WocOHbTVq1c7Heu1117TbrrpJq1MmTKan5+fVqdOHe3111/XMjMzNU3TtNOnT2vDhw/X6tSpowUEBGghISFay5Ytte+++67A8W7YsEG75557tPLly2tms1kLDQ3VOnbsqM2cOVOzWq2O/U6dOqX16dNH8/f310JDQ7VHH31U27p1a77TRwcEBFz1NV966SUN0GrUqHHFfX7//XctLi5OCwkJ0Xx9fbWYmBht8ODB2r///lvgcxNCCCHcVY8ePTRfX18tNTX1ivsMHjxYM5vN2unTpzVN07QzZ85oI0aM0CpUqKB5e3trFStW1B544AHH45qmprx/6aWXtGrVqmlms1mLiorS7r77bm3fvn2OfVzxP3379u1ap06dtMDAQC0sLEwbOnSotmnTpjzH0DRN27p1q3bXXXdpZcqU0Xx9fbXatWtrL7/8cp5jZmRkaKGhoVpISIiWnp5ekGbUNE1dD40YMUKrVKmS5uXlpUVFRWmDBg3Sdu7cqaWkpGh16tTRgoODtS1btlzxGAcOHNAAbdKkSVd9Lft++d2qVKlyzVg3b96sPfvss1rTpk21smXLal5eXlp0dLTWt29fbePGjXn2//vvv7XOnTtrQUFBWkBAgBYbG6tNmTLFaZ/ffvtNa9Omjebn56cFBwdrPXr00LZv3+60z7hx4zRAO3XqVL5x/fjjj1rbtm21gIAALSAgQKtTp442fPhwbdeuXdc8J03TtN27d2tDhw7Vqlatqnl7e2tBQUFamzZttClTpmiXLl1y7JeWlqY99NBDWkhIiBYUFKT169dPO3nypAZo48aNK3C8mqZpn3/+uQZoQUFBV3y//Pfff1rv3r21cuXKaT4+PlqVKlW0fv36acuXLy/QeQnhKgZNu6wvohBCCCGEEEIIt5GVlUX58uXp0aOHYxY6IYQoCaSmlBBCCCGEEEK4sfnz53Pq1Cmn4ulCCFESSE8pIYQQQgghhHBDa9euZfPmzUyYMIGwsDA2btyod0hCCOFS0lNKCCGEEEIIIdzQJ598wrBhw4iIiOCrr77SOxwhhHA56SklhBBCCCGEEEIIIYqd9JQSQgghhBBCCCGEEMVOklJCCCGEEEIIIYQQoth56R2AO7LZbBw7doygoCAMBoPe4QghhBBCR5qmceHCBcqXL4/RKN/nXY1cQwkhhBACCn79JEmpfBw7doxKlSrpHYYQQggh3Mjhw4epWLGi3mG4NbmGEkIIIURu17p+kqRUPoKCggDVeMHBwTpHUzQsFgvLli2jS5cumM1mvcPRlbSFM2mPHNIWOaQtckhb5CgtbZGSkkKlSpUc1wfiyuQaqnSRtsghbZFD2iKHtIUzaY8cpaEtCnr9JEmpfNi7mwcHB5foCyp/f3+Cg4NL7C9BQUlbOJP2yCFtkUPaIoe0RY7S1hYyHO3a5BqqdJG2yCFtkUPaIoe0hTNpjxylqS2udf3kFoURPvroI6pWrYqvry8tW7Zk3bp1V9y3ffv2GAyGPLfbb7/dsY+maYwdO5bo6Gj8/Pzo1KkTe/bsKY5TEUIIIYQQQgghhBAFoHtSau7cuYwePZpx48axceNGGjVqRFxcHCdPnsx3/3nz5nH8+HHHbevWrZhMJvr27evY5+233+aDDz5g6tSprF27loCAAOLi4rh06VJxnZYQQgghhBBCCCGEuArdk1LvvfceQ4cOZciQIdSrV4+pU6fi7+/PtGnT8t2/bNmyREVFOW7x8fH4+/s7klKapjF58mT+97//0bNnT2JjY/nqq684duwY8+fPL8YzE0IIIYQQQgghhBBXomtNqczMTDZs2MCYMWMc24xGI506dWLNmjUFOsaXX37JgAEDCAgIAODAgQMkJSXRqVMnxz4hISG0bNmSNWvWMGDAANeehBBCiFLJarVitVr1DkNXFosFLy8vLl265NFtYTabMZlMeochhBBCCFHq6JqUOn36NFarlcjISKftkZGR7Ny585rPX7duHVu3buXLL790bEtKSnIc4/Jj2h+7XEZGBhkZGY77KSkpgLrYtlgsBTsZD2M/r5J6ftdD2sKZtEcOaYsc0hY5MjMzCQoKYt++faW+8LWmaURFRZGYmOjxbREcHExERES+5yHveyGEEEKIouHRs+99+eWXNGzYkJtuuumGjjNx4kReeeWVPNuXLVuGv7//DR3b3cXHx+sdgtuQtnAm7ZFD2iKHtIWa8j40NJTg4GC8vb09PhlT2mmaRmZmJqdOnWL37t1cuHAhzz5paWk6RCaEEEIIUfLpmpQKCwvDZDJx4sQJp+0nTpwgKirqqs9NTU3l22+/5dVXX3Xabn/eiRMniI6Odjpm48aN8z3WmDFjGD16tON+SkoKlSpVokuXLiV6OuP4+Hg6d+5c4qegvBZpC2fSHjmkLXJIWyhWq5V9+/YRHBxM+fLlS31CStM0Lly4QFBQkMe3ha+vLz4+PrRu3TrPUD57D2ohhBBCCOFauialvL29adasGcuXL6dXr14A2Gw2li9fzogRI6763O+//56MjAzuu+8+p+3VqlUjKiqK5cuXO5JQKSkprF27lmHDhuV7LB8fH3x8fPJsN5vNJf7DV2k4x4KStnAm7ZFD2iJHaW8Lq9WKwWBw9JAyGnWfL0RXNpsNoES0RWBgIKdPnwbI8x4vze95IYQQQoiipPvwvdGjR/PAAw/QvHlzbrrpJiZPnkxqaipDhgwBYNCgQVSoUIGJEyc6Pe/LL7+kV69elCtXzmm7wWBg1KhRvPbaa9SsWZNq1arx8ssvU758eUfiSwghhLgRnt4rSOQlP1MhhBBCiOKne1Kqf//+nDp1irFjx5KUlETjxo1ZsmSJo1B5YmJinm9fd+3axd9//82yZcvyPeZzzz1HamoqjzzyCMnJybRt25YlS5bg6+tb5OcjhBBCCCGEEEIIIa5N96QUwIgRI644XG/lypV5ttWuXRtN0654PIPBwKuvvpqn3pQQQgghXKd69eo8+uijPP/883qHIoQQQgghPJBnF4AQQgghxDUZDIar3saPH1+o465du5YHHnjghmJr3749o0aNuqFjCCGEEEIIz+QWPaWEEEIIUXSOHz/uWJ87dy5jx45l165djm2BgYGOdU3TsFqteHld+xIhPDxcZqYTQgghhBCFJj2lhBBCiBIuKirKcQsJCcFgMDju79y5k6CgIH799VeaNWuGj48Pf//9N/v27aNnz55ERkYSGBhIixYt+O2335yOW716dT755BPHfYPBwBdffMFdd92Fv78/NWvWZOHChTcU+48//kj9+vXx8fGhatWqvPvuu06Pf/zxx9SsWRNfX18iIyO5++67HY/98MMPNGzYED8/P8qVK0enTp1ITU29oXiEEEIIIYTrSE8pIYQQnuPIBoioC97+ekfioGka6RarLq/tZza5bNa4F154gXfeeYfq1asTGhrK4cOH6d69O6+//jo+Pj589dVX9OjRg127dlG5cuUrHueVV17h7bffZtKkSUyZMoV7772XQ4cOUbZs2euOacOGDfTr14/x48fTv39/Vq9ezeOPP065cuUYPHgw//77L08++SRff/01rVu35uzZs/z111+A6h02cOBA3n77be666y4uXLjAX3/9ddWalEIIIYRwH5qmYbVpWLOXWTYNqzV7adPIstlytue6qfs2sqya0+P2pU3TMBoMeBkNmK50M+SsexkNGO1LgwEvk/Pj+T3P1bP6apqGTcMRv31ps+FoH5uW67Fc2zUtpw3t29Vz1bbyZfyoVFa/a2tJSgkhhPAMu36FOQOgYV/o84Xe0TikW6zUG7tUl9fe/moc/t6u+Vf+6quv0rlzZ8f9smXL0qhRI8f9CRMm8NNPP7Fw4cIrTk4CMHjwYAYOHAjAG2+8wQcffMC6devo2rXrdcf03nvv0bFjR15++WUAatWqxfbt25k0aRKDBw8mMTGRgIAA7rjjDoKCgqhSpQpNmjQBVFIqKyuL3r17U6VKFQAaNmx43TEIUVpYbSrBnp5p5ZLF6ljPb3kpe/3iJQt7DhnZunQ3Xl4mDIDBAEaDIXvdgMEABgwYDeqxfLdhyHkMsrdfvs3+PLVOrnWT0YC/t4lAHy8Cfb0I8PEiKHvdlcl74Swzy0ZyWibn0iycTc3kXFomZ1MzOX3hEpsPGtn06y4wGLFlfxlg/8Bs07I/YNtAQ923aRqahuNxm6aBls9zci2dn6PWtcueY2d/C9jfl/Z1+2MGcu1whcccx8hnG1c4rqZpnDppZN7pjWAwoIHjy5HcMWtkrwNcdt+xH4CmZR8j1z7Zj2mO/TTnbdnHzJfhqnfz/O7k95t0+a+X4bK9cj+uaRrnU0xM3v031uwEi1MS6bLkkdV2pcDdn9EAXkYjRiOXJbCMmLK3paWbeGPbH+r9nCv5ptnbxp44yv7ZFpXhHWJ4Nq5O0b3ANUhSSgghhGfYv1Itt82Hrm9BQDk9oylxmjdv7nT/4sWLjB8/nkWLFjkSPOnp6SQmJl71OLGxsY71gIAAgoODOXnyZKFi2rFjBz179nTa1qZNGyZPnozVaqVz585UqVKF6tWr07VrV7p27eoYOtioUSM6duxIw4YNiYuLo0uXLtx9992EhoYWKhYh9GC1aWRm2cjIsmYv1c2+LSdBZCMtMytXMsnmeCwtM4t0i80p2ZSWmZNYSs/elpllK2SURlYcO+jK03YpowFHkiogO1EV6JNzC/DxIsjXeT3A+7L9std9vIwlNsFlsdpITrNwLi2Tc44Ek8WRaHJsS7Oo9dRMLmRkXeWIRjh+qNjid29GSD6tdxBuxADpaTd8FK98ejGZjMac7aacx+3bc3o72ZM+YLXZsNrsy1w9s6z2JFE+jzl6XF05PpsGmVYbXLUzvQEyM264LezsPbUMhpx1Y3Z7GA05SfycJRiNBkL9vV0WQ2FIUkoIIYRnOL5JLW0W2PojtHxE33iy+ZlNbH81TrfXdpWAgACn+8888wzx8fG888471KhRAz8/P+6++24yMzOvehyz2ex032AwYLMV9sPu1QUFBbFx40ZWrlzJsmXLGDt2LOPHj2f9+vWUKVOG+Ph4Vq9ezbJly5gyZQovvfQSa9eupVq1akUSjyj5LFYbW4+msCvZgN+uU1g1A5lW52RR7iRSZu7t1isll2xkZlnV4xa1n327Xr0E/Mwm/LxN+JlN+JqNjnU/by/8zEbH42ajgcRDB6lWrRqGXD1iHD1asPdkAcjp1ZK7Bwj59AbJ2Z7Tmyb3cey9ZdS+qodBamYWqRlZXLyUxYUMtW7Lfs6FS1lcuHS1BErBeBkN+Sa1An29CDAbSTpqZPOSXXibvfAyGvAyGvEyGTCbctYv32Y2ZX9gNhkwX/aYyWjAbHJ+zOkYjmM6DxWy2rTsHkw5iaVzqZmctS/t2xz3M0kpZPsYDBDq700ZfzNl/b0JDfCmjJ8Xp44dpmZMdUwmk+PDsL0HnH3daHTuNWfvDef8eE5PuTz7GLN7zTkez/0aOb3s7L9Fjl5DkKvXSU4PlMsf0/J9LOd3Mt/9cu1vnzhk0+bNNIqNxcvLy7k3oaPnlcGx3dFr0H4/d69Dx7bs3kiO/bLPN9dzyXMsw2U9lpx/jpcPbb/8L0/uh7XLH73K3ctfx5KVxYb162jVqiW+3mbnJJJjacRkyjVUzmhw3Lc/bv/56s0+vDAr15A6p5umkWVVj2XZVK8ney+wjEwLq1ev4pa2bfE2m7N7UTknjoyOxJLqWeXYlmu70ZCTfPJUkpQSQgjh/mw2SNqSc3/TbLdJShkMBpcNoXMnq1atYvDgwdx1112A6jl18ODBYo2hbt26rFq1Kk9ctWrVwmRSCTkvLy86depEp06dGDduHGXKlGHFihX07t0bg8FAmzZtaNOmDWPHjqVKlSr89NNPjB49uljPQ3iuLKuNrcdSWLPvDGv2n+Hfg2dJy7QCJtjxX7HGYjSAt5cRHy8T3l5GvE1G/L1NuRJG2UuzCd9c65c/7pu97n/Zffv+Pl7GAn+4sVgsLF68n+7daudJSOvNXu/v4qUsLmbkumXfT81QyauLly5bz8xyfs6lLFIzVVeHLJtGcpqF5DTLFV7VyJ9J+vQOyv3BPs1iLfRQn9zJpVB/M6H+3pQNUPfL2pNPue4H+6kP07mp98UhusfVcrv3RXGzWCz4JW2ie9MKpb4tQLVHym6Nm6qWLRHtYciuL+VViO8ILRYLRwKhfvngEtEWN6LkXUULIYQoec7uh8yLYPIBzQrH/oOTOyFCv/HvJV3NmjWZN28ePXr0wGAw8PLLLxdZj6dTp06RkJDgtC06Opqnn36aFi1aMGHCBPr378+aNWv48MMP+fjjjwH45Zdf2L9/P+3atSM0NJTFixdjs9moXbs2a9euZfny5XTp0oWIiAjWrl3LqVOnqFu3bpGcgygZrDaNbcfO88/+M6zZd4b1B89x8bIhSiF+XvhjoVxoCL7m7ASRlxEfLyPeXqbspUoa+ZiN+JiMTskkx+OXJZh8zGrpazbibcq77+W9YcTV2b8w8Pf2IuIGj2XvieVIZl3KIjXDysUMS/Z6FufTMtm6czdVqlXHphmw2mxYbBpZVlVs2bGea2nJfiwru2BzljV7m03L3p53m8VmyzfhlJXdAyP3QKBgXy+nBJIj0ZTrftkAb0L91fYQPzNeJpmcXQhRvCQpJYQQwv0dT1DLqIYQGAG7FsOmOdD5FV3DKsnee+89HnzwQVq3bk1YWBjPP/88KSkpRfJas2fPZvbs2U7bJkyYwP/+9z++++47xo4dy4QJE4iOjubVV19l8ODBAJQpU4Z58+Yxfvx4Ll26RM2aNZkzZw7169dnx44d/Pnnn0yePJmUlBSqVKnCu+++S7du3YrkHIRnsto0dhxPcSSh1h04m6dOTrCvFy2rl+Pm6uVoVb0cMeV8WbLkV7p3v7nUf7tdWhiNBoJ8zQT5XvnnbbFYWJy2s1h6B9lsKjmVZc1JVFlzJbkCfb0oIwkmIYSHkKSUEEII92evJxXdCKq3V0mpzXOh41gwuq6uUmkwePBgR1IHoH379nlqSQBUrVqVFStWOG0bPny40/39+/c7JaryO05ycvJV41m5cuVVH+/Tpw99+vTJ97G2bdte8fl169ZlyZIlVz22KH1sNo2dSRdUEmq/SkKdT3ceihXk48VN1crSKkYloupGBzsNT7JYrjR0S4jiYTQa8DGa8JFPckKIEkD+lAkhhHB/uZNSteLAtwxcOA4H/oCY23QNTQjhvmw2jT0nL7Jm32nW7D/D2gNn89QDCvTxokXVUNUTKqYc9cuH5KmRI4QQQoiiIUkpIYQQ7k3TnJNSXj7Q8G5Y/wUkzJGklBDCQdM09p686OgJ9c/+s5xNdZ4x0t/bRPOqZWmVnYRqUD5YhjkJIYQQOpGklBBCCPeWnAiXksFohojsItWNBqqk1I6fIeMC+ATpGqIQQh+aprH/dCpr9p3hn+wk1OmLGU77+JqNtKhalpuz60LFVgzBLEkoIYQQwi1IUkoIIYR7s/eSiqirekkBVGgG5WrCmT2wfQE0uU+/+IQQxUbTNA6eSXMUJv9n/xlOXnBOQvl4GWlWJdTREyq2Yhm8vSQJJYQQQrgjSUoJIYRwb7mH7tkZDNB4ICx/VQ3hk6SUECXaxYwsXp6/lTX7zpCUcsnpMW8vI00rl6FV9TBurl6WxpXL4OMlEyAIIYQQnkCSUkIIIdxbfkkpgNj+sHwCHPobzh2C0CrFH5sQolgEeJv4a88pTl/MxNtkpHHlMqowefVyNKlcBl+zJKGEEEIITyRJKSGEEO7NkZRq7Lw9pCJUa6dm4Ns8F259rthDE0IUD4PBwNge9SkX4E3TyqH4eUsSSgghhCgJZIC9EEII93UhCVJPgsEIkfXzPt5ooFpumqNm6RNClFh3NipPmxphkpASQgghShBJSgkhhHBf9l5SYbXA2z/v43V7gDkAzu6Hw+uKNzYhhBBCCCHEDZGklBBCCPd1pXpSdj6BUK+nWt80u3hiKsXat2/PqFGj9A5DCCGEEEKUEJKUEkII4b6ulZQCaDRALbf+BJZLV96vFOvRowddu3bN97G//voLg8HA5s2bb/h1ZsyYQZkyZW74OEIIIYQQonSQpJQQQgj3VZCkVNVbIKQSZJyHXYuLJy4P89BDDxEfH8+RI0fyPDZ9+nSaN29ObGysDpEJIYQQQojSTJJSQggh3FPqGTh/WK1HNbzyfkYjxPZX65vmFH1cHuiOO+4gPDycGTNmOG2/ePEi33//PQ899BBnzpxh4MCBVKhQAX9/fxo2bMicOa5tz8TERHr27ElgYCDBwcH069ePEydOOB7ftGkTHTp0ICgoiODgYJo1a8a///4LwKFDh+jRowehoaEEBARQv359Fi+WJKQQQgghhCfz0jsAIYQQIl9J2b2kylYH35Cr79toAPz1DuxdDhdOQFBk0cdnp2lgSSu+18vN7A8GwzV38/LyYtCgQcyYMYOXXnoJQ/Zzvv/+e6xWKwMHDuTixYs0a9aM559/nuDgYBYtWsT9999PTEwMN9100w2HarPZHAmpP/74g6ysLIYPH07//v1ZuXIlAPfeey9NmjThk08+wWQykZCQgNlsBmD48OFkZmby559/EhAQwPbt2wkMDLzhuIQQQgghhH4kKSWEEMI9FWTonl1YTajYAo6shy3fQ+sRRRtbbpY0eKN88b1ebi8eA++AAu364IMPMmnSJP744w/at28PqKF7ffr0ISQkhJCQEJ555hnH/k888QRLly7lu+++c0lSavny5WzZsoUDBw5QqVIlAL766ivq16/P+vXradGiBYmJiTz77LPUqVMHgJo1azqen5iYSJ8+fWjYUPWaq169+g3HJIQQQggh9CXD94QQQrin60lKQU7B803fFk08Hq5OnTq0bt2aadOmAbB3717++usvHnroIQCsVisTJkygYcOGlC1blsDAQJYuXUpiYqJLXn/Hjh1UqlTJkZACqFevHmXKlGHHjh0AjB49mocffphOnTrx5ptvsm/fPse+Tz75JK+99hpt2rRh3LhxLinMLoQQQggh9CU9pYQQQrin601K1e8NS8bAiS2QtOXqdahcyeyveizpwex/Xbs/9NBDPPHEE3z00UdMnz6dmJgYbr31VgAmTZrE+++/z+TJk2nYsCEBAQGMGjWKzMzMoog8X+PHj+eee+5h0aJF/Prrr4wbN45vv/2Wu+66i4cffpi4uDgWLVrEsmXLmDhxIu+++y5PPPFEscUnhBBCCCFcS3pKCSGEcD+XUuDsfrUeVcCklH9ZqN1NrScUY8Fzg0ENodPjVoB6Urn169cPo9HI7Nmz+eqrr3jwwQcd9aVWrVpFz549ue+++2jUqBHVq1dn9+7dLmumunXrcvjwYQ4fPuzYtn37dpKTk6lXr55jW61atXjqqadYtmwZvXv3Zvr06Y7HKlWqxGOPPca8efN4+umn+fzzz10WnxBCCCGEKH6SlBJCCOF+kraoZXBFCChX8Oc1GqiWW74Da5br4/JwgYGB9O/fnzFjxnD8+HEGDx7seKxmzZrEx8ezevVqduzYwaOPPuo0M15BWa1WEhISnG47duygU6dONGzYkHvvvZeNGzeybt06Bg0axK233krz5s1JT09nxIgRrFy5kkOHDrFq1SrWr19P3bp1ARg1ahRLly7lwIEDbNy4kd9//93xmBBCCCGE8EwyfE8IIYT7ud6he3Y1OoF/GKSegn3LoVac62PzcA899BBffvkl3bt3p3z5nALt//vf/9i/fz9xcXH4+/vzyCOP0KtXL86fP39dx7948SJNmjRx2hYTE8PevXtZsGABTzzxBO3atcNoNNK1a1emTJkCgMlk4syZMwwaNIgTJ04QFhZG7969eeWVVwCV7Bo+fDhHjhwhODiYrl278n//93832BpCCCGEEEJPkpQSQgjhfgqblDKZIbYf/PMxJMyWpFQ+WrVqhaZpebaXLVuW+fPnX/W5K1euvOrjgwcPdup9dbnKlSuzYMGCfB/z9vZmzpwrD7u0J69Ejo8++ohJkyaRlJREo0aNmDJlylVnSkxOTuall15i3rx5nD17lipVqjB58mS6d+9e6GMKIYQQQtwIGb4nhBDC/RQ2KQU5s/Dt+hXSz7kuJiHcyNy5cxk9ejTjxo1j48aNNGrUiLi4OE6ePJnv/pmZmXTu3JmDBw/yww8/sGvXLj7//HMqVKhQ6GMKIYQQQtwoSUoJIYRwL5lpcHqXWi9MUioqFiLqgzUDtv3k2tiEcBPvvfceQ4cOZciQIdSrV4+pU6fi7+/PtGnT8t1/2rRpnD17lvnz59OmTRuqVq3KrbfeSqNGjQp9TCGEEEKIGyXD94QQQriXE9tAs0FABARFXf/zDQbVWyr+Zdj0LTR/0PUxCqGjzMxMNmzYwJgxYxzbjEYjnTp1Ys2aNfk+Z+HChbRq1Yrhw4ezYMECwsPDueeee3j++ecxmUyFOiZARkYGGRkZjvspKSkAWCwWLBbLjZ6qW7KfV0k9v+shbZFD2iKHtEUOaQtn0h45SkNbFPTcJCklhBDCvRxPUMvoRirBVBix/eC3cXB4LZzZB+ViXBaeEHo7ffo0VquVyMhIp+2RkZHs3Lkz3+fs37+fFStWcO+997J48WL27t3L448/jsViYdy4cYU6JsDEiRMdxehzW7ZsGf7+/oU4O88RHx+vdwhuQ9oih7RFDmmLHNIWzqQ9cpTktkhLSyvQfpKUEkII4V5upJ6UXVAUxHSEvfGwaQ7c9j/XxCaEh7LZbERERPDZZ59hMplo1qwZR48eZdKkSYwbN67Qxx0zZgyjR4923E9JSaFSpUp06dKF4OBgV4TudiwWC/Hx8XTu3Bmz2ax3OLqStsghbZFD2iKHtIUzaY8cpaEt7L2nr0WSUkIIIdyLK5JSoIbw7Y2HTXOh/YtgdF0ZxfxmrxOezWaz6R1CgYWFhWEymThx4oTT9hMnThAVlf+Q1+joaMxmMyaTybGtbt26JCUlkZmZWahjAvj4+ODj45Nnu9lsLrEX2Xal4RwLStoih7RFDmmLHNIWzqQ9cpTktijoeele6Pyjjz6iatWq+Pr60rJlS9atW3fV/ZOTkxk+fDjR0dH4+PhQq1YtFi9e7Hj8woULjBo1iipVquDn50fr1q1Zv359UZ+GEEIIV8jKhJM71PqNJqXq3A4+IXA+EQ6tuvHYAG9vb4xGI6dOnSIlJYX09HQuXbpUqm+ZmZm6x3Ajt/T0dM6fP8+xY8cwGo14e3u75L1SlLy9vWnWrBnLly93bLPZbCxfvpxWrVrl+5w2bdqwd+9ep+Tb7t27iY6Oxtvbu1DHFEIIIYS4Ubr2lLJPPTx16lRatmzJ5MmTiYuLY9euXUREROTZ3z6dcUREBD/88AMVKlTg0KFDlClTxrHPww8/zNatW/n6668pX74833zzDZ06dWL79u1O0x7rJisT9iyDwAiodJPe0QghhHs5tQNsFvAtA2Uq39ixzH5QvxdsnKmG8FW75YbDMxqNVK5cmQ0bNmA0GjEUtuZVCaFpGunp6fj5+Xl8W/j7+1O5cmWMLuxRV5RGjx7NAw88QPPmzbnpppuYPHkyqampDBkyBIBBgwZRoUIFJk6cCMCwYcP48MMPGTlyJE888QR79uzhjTfe4MknnyzwMYUQQgghXE3XpFTuqYcBpk6dyqJFi5g2bRovvPBCnv3t0xmvXr3a0RWsatWqjsfT09P58ccfWbBgAe3atQNg/Pjx/Pzzz3zyySe89tprRX9S1/LnJPjzbajdHQbO0TsaIYRwL46he7GFL3KeW6OBKim1fQF0nwTeATd8SLPZzNmzZ2nevLnHJ2JulMVi4c8//6Rdu3Ye3fXcZDLh5eXlUT/P/v37c+rUKcaOHUtSUhKNGzdmyZIljkLliYmJTgm2SpUqsXTpUp566iliY2OpUKECI0eO5Pnnny/wMYUQQgghXE23pFRRTGeclZWF1WrF19fX6Xl+fn78/fffRXo+BdbwbpWU2r0ULp5UPaaEEEIorqonZVf5ZgitCucOwo5foFF/1xwX8PLy8uhEjCvY//f6+vqW+rbQw4gRIxgxYkS+j61cuTLPtlatWvHPP/8U+phCCCGEEK6mW1KqKKYzDgoKolWrVkyYMIG6desSGRnJnDlzWLNmDTVq1LhiLBkZGWRkZDju26vEWywWLBaLC842lzLVMZVvhvHYBqwJ32JrOcy1xy8g+3m5/Pw8kLSFM2mPHNIWOYqrLUzHEjACWREN0Fz0WsYG/TD99Ta2hNlY6/W+4ePJ+yJHaWmLkn5+QgghhBB68ajZ9woynfHXX3/Ngw8+SIUKFTCZTDRt2pSBAweyYcOGKx534sSJvPLKK3m2L1u2DH9/f5efR1VTQxqxgYt/f8bK05VdM0SlkOLj43V7bXcjbeFM2iOHtEWOomwLg2al+7HNGIE/diVz8dDiaz6nIPwzIukMGA78wYr533DJu6xLjivvixwlvS3S0tL0DkEIIYQQokTSLSlVFNMZe3t7ExMTwx9//EFqaiopKSlER0fTv39/qlevfsVYxowZw+jRox33U1JSqFSpEl26dCE4OPgGzzQf6a3R3p9DyKXDdG9WCaJiXf8a12CxWIiPj6dz586lfsiFtIUzaY8c0hY5iqUtTu3EKyETzTuAdncNAYPrCk7bLv6AMXENnSLOYGt93w0dS94XOUpLW9h7UAshhBBCCNfSLSmVe+rhXr16ATlTD1+plkGbNm2YPXs2NpvNUbwz93TGuQUEBBAQEMC5c+dYunQpb7/99hVj8fHxwcfHJ892s9lcNBfZ5nA1Vfm2eZi3fgeVmrn+NQoaSlGdoweStnAm7ZFD2iJHkbbFqW0AGKJiMXvn/Zt8QxrfA4lrMG35DlO7p13SQ1XeFzlKeluU5HMTQgghhNCTrvMejx49ms8//5yZM2eyY8cOhg0blmc649yF0IcNG8bZs2cZOXIku3fvZtGiRbzxxhsMHz7csc/SpUtZsmQJBw4cID4+ng4dOlCnTh33m8648b1qufk7yMrUNxYhhHAHri5ynlu9XuDlC6d3wbGNrj++EEIIIYQQ4rrpWlOqKKYzPn/+PGPGjOHIkSOULVuWPn368Prrr7vft5wxHSAoGi4chz1LoW4PvSMSQgh9FWVSyjcY6twBW3+AhDlQQb8eqkIIIYQQQghF90Lnrp7OuF+/fvTr189V4RUdowli+8OqyZAwW5JSQojSzWaD45vVelEkpQAaD1RJqa0/QNwb4OV97ecIIYQQQgghioyuw/dKvcb3qOXupXDxpL6xCCGEns4dgMwLaohdWO2ieY3qHSAwCtLPqR6qQgghhBBCCF1JUkpP4bXVEBLNClu+1zsaIYTQj33oXkQ9MBVRJ16jCWKze9Ju+rZoXkMIIYQQQghRYJKU0pu9t9R/s0DT9I1FCCH0UpT1pHLL3UM19UzRvpYQQgghhBDiqiQppbcGfcDkDSe3QdJmvaMRQgh9FFdSKqIuRDcGm0XVlhJCCCGEEELoRpJSevMLhTq3q/WEOfrGIoQQetC04ktKATQaqJab5G+uEEIIIYQQepKklDtofK9abvkOsjL1jUUIIYrb+SOQfhaMXqqmVFFreLd6rWP/wcmdRf96QgghhBBCiHxJUsod2GeESjsDe5bpHY0QQhQvey+p8Lpg9i361wsIg5pd1Lr0lhJCCCGEEEI3kpRyByYvaNRfrSfM1jcWIYQobsU5dM/OPoRv81ywWYvvdYUQQgghhBAOkpRyF42yZ4TasxQuntI3FiGEKE56JKVqxamafheOw/6Vxfe6QgghhBBCCAdJSrmLiDpQoRnYsmDL93pHI4QQxUePpJSXj5r9FGDTt8X3ukIIIYQQQggHSUq5k8bZvaVkCJ8QorS4cAIuJgEGiGpQvK9t76G642e4lFK8ry2EEEIIIYSQpJRbadAHTN5wYgsc36x3NKK4bfoW1nykdxRCFK+k7L91YbXAO6B4X7tCU/W6WemwfUHxvrYQQgghhBBCklJuxS8U6tyu1qW3VOmSlQELRsDSFyFpq97RCFF8jieoZXRs8b+2wQCNBqh1GcInhBBCCCFEsZOklLtpfK9abvkOsjL1jUUUn3OHwGZR6/uW6xuLEMVJj3pSucX2Bwxw6G84d1CfGIQQQgghhCilJCnlbqp3gMAoSDsDe5bpHY0oLmf35azvW6FfHEIUN72TUiEVoVo7tb75O31iEEIIIYQQopSSpJS7MXlBbD+1LkP4So+z+3PWD62BzDT9YhGiuKSdheREtR6lw/A9O/skE5vmgKbpF4cQQgghhBCljCSl3JH9A9KepXDxlL6xiOJxJldPKWsGJK7WLxYhiou9yHloVfAro18cdXuAOUAlhw+v1S8OIYQQQgghShlJSrmjiLpQvinYsmDL93pHI4qDffieT7Ba7vtdv1iEKC56D92z8w6Aej3V+qY5+sYihBBCCCFEKSJJKXdl7y0lQ/hKB/vwvUYD1XKvFDsXpYC7JKUAGmf/7m39CSzp+sYihBBCCCFEKSFJKXfVoA+YvOHEFji+We9oRFHKyoDzR9R68wcBA5zaASnHdA1LiCLnTkmpKm0hpBJknIddi/WORgghhBBCiFJBklLuyr8s1O6u1qW3VMl27hBoNvAOhPDaUKGp2i5D+ERJlnEhp5ZalBskpYxGiO2v1jd9q28sQgghhBBClBKSlHJnje9Vyy3fQVamvrGIomOvJ1W2GhgMENNR3d+3Qr+YhChqSVsBDYIrQGC43tEouYfPXjihbyxCCCGEEEKUApKUcmcxt0FgJKSdgb3xekcjioq9t0jZ6moZc5ta7v8dbDZ9YhKiqLnT0D27sBpQsQVoVplkQgghhBBCiGIgSSl3ZvLKGU4iQ/hKLnuR87IxalmxOXgHqWRk0ib94hKiKNmTUlGx+sZxOXtvKZmFTwghhBBCiCInSSl3Z5+Fb/cSSD2tbyyiaNiH75XLTkqZzFCtnVqXIXyipHLHnlIADXpnTzKxVSaZEEIIIYQQoohJUsrdRdSF8k3BliXDSUoqR0+p6jnbYjqopRQ7FyWRJR1O7VTr7paU8guF2t3UuhQ8F0IIIYQQokhJUsoT2HtLJczSNw7helkZcP6IWrcP34OculKJ/0DGxeKPS4iidGK7qtvkHwbB5fWOJq9G2X9zt3wHVou+sQghhBBCCFGCSVLKEzToo4aTJG2R4SQlzbmDoNnAOxACI3K2l4uB0Kpgs8ChVXpFJ0TROJ6gltGN1IyT7qZGRwgIh9RTMoRWCCGEEEKIIiRJKU/gXxZqd1frUny3ZHEM3auW98O5vbfU3uXFG5MQRc1d60nZmczQsK9al0kmhBBCCCGEKDKSlPIUje9Vy81zIStT31iE65zJLnKee+ienT0pJT01REnj7kkpyJmFb9diSD+nbyxCCCGEEEKUUJKU8hQxt0FgJKSdgb3xekcjXCW/Iud2VW8BgwnO7IHkxOKNS4iiYrXAye1q3Z2TUlENIaI+WDNh2096RyOEEEIIIUSJJEkpT2Hygth+al2Gk5QcZ7N7SpXLp6eUXxmo2Fytyyx8oqQ4tVMlenxCVN00d2UwQOPs3lIJMmxaCCGEEEKIoiBJKU9inxFq9xJIPa1vLMI1zlylpxRATEe1lCF8oqRwDN2Ldc8i57k17AcGIxxZB6f36h2NEEIIIYQQJY4kpTxJZD0o3wRsWbDle72jETcqKwPOH1br+dWUgpy6UvtXgs1aLGEJUaQ8oZ6UXVBkTmJ487f6xiKEEEIIIUQJJEkpT2MveJ4wS984xI07dxDQwDsQAiPy36d8E/ANgUvJcOy/YgxOiCLiSUkpyBnCt+lbsNn0jUUIIYQQQogSRpJSnqZBHzB5Q9IWOL5Z72jEjXAUOa925WFMJi+odqtalyF8wtPZrOpvF3hOUqp2d1X/6vxhOLRK72iEEEIIIYQoUSQp5Wn8y0Ltbmp9kxTf9WhnsoucX2nonp19CJ8kpYSnO7MXLGlg9odyNfSOpmDMflC/l1qXv7lCCCGEEEK4lCSlPJF9CN/m79T06sIz2Wfeu1KRczt7UurwOriUUrQxCVGU7EP3ohqC0aRvLNejcfYkE9sXQGaqvrEIIYQQQghRgkhSyhPFdISACEg7DXvi9Y5GFJZ9+F65a/SUCq2iepVoVjj4V9HHJURR8bR6UnaVWkJoNci8CDt+0TsaIYQQQgghSgzdk1IfffQRVatWxdfXl5YtW7Ju3bqr7p+cnMzw4cOJjo7Gx8eHWrVqsXjxYsfjVquVl19+mWrVquHn50dMTAwTJkxA07SiPpXiY/KCRv3VuhQ891xn7DWlrpGUgpzeUnuXF108QhQ1T01KGQzQyF7wfLa+sQghhBBCCFGC6JqUmjt3LqNHj2bcuHFs3LiRRo0aERcXx8mTJ/PdPzMzk86dO3Pw4EF++OEHdu3axeeff06FChUc+7z11lt88sknfPjhh+zYsYO33nqLt99+mylTphTXaRWPRtnDSXYvgdTT+sYirl9WhiqcDNcevgdSV0p4Pk3LmZzB05JSkPNFwP4/IOWovrEIIYQQQghRQuialHrvvfcYOnQoQ4YMoV69ekydOhV/f3+mTZuW7/7Tpk3j7NmzzJ8/nzZt2lC1alVuvfVWGjXK+YCzevVqevbsye23307VqlW5++676dKlyzV7YHmcyHpQvgnYsmDLD3pHI67XuYOABt6BEBhx7f2rtgWjF5w7kDPsTwhPcu4gZJxXs4eG19E7musXWhWqtAE0jFvlb64QQgghhBCu4KXXC2dmZrJhwwbGjBnj2GY0GunUqRNr1qzJ9zkLFy6kVatWDB8+nAULFhAeHs4999zD888/j8mkiua2bt2azz77jN27d1OrVi02bdrE33//zXvvvXfFWDIyMsjIyHDcT0lRxaQtFgsWi/sWEjc2HIDp2H9o/31DVrOHruu59vNy5/MrLnq0heHkLrwALbQaWVlZ136C0RdTxRYYE9dg3f0btmZDiiw2eW/kkLbIcaNtYTiyES/AFlEPqw2weV6bGhr0w+vQKgyb5kDll+V9Qen5HSnp5yeEEEIIoRfdklKnT5/GarUSGRnptD0yMpKdO3fm+5z9+/ezYsUK7r33XhYvXszevXt5/PHHsVgsjBs3DoAXXniBlJQU6tSpg8lkwmq18vrrr3PvvfdeMZaJEyfyyiuv5Nm+bNky/P39b+Asi5Y5K5A4gxemE1v4+4eppPhXvu5jxMdLoXS74myLmJO/0gA4luHHv7lqol1NTUsF6gEn1nzL+hOR19z/Rsl7I4e0RY7CtkXdYz9RC0jMDGVTAd/z7sbL6kucwRuvs3spE3ZA3he5lPS2SEtL0zuEfH300UdMmjSJpKQkGjVqxJQpU7jpppvy3XfGjBkMGeL8hYaPjw+XLl1y3B88eDAzZ8502icuLo4lS5a4PnghhBBCCHRMShWGzWYjIiKCzz77DJPJRLNmzTh69CiTJk1yJKW+++47Zs2axezZs6lfvz4JCQmMGjWK8uXL88ADD+R73DFjxjB69GjH/ZSUFCpVqkSXLl0IDg4ulnMrLINlKexcSLuQI9g6P1bg51ksFuLj4+ncuTNms7kII3R/erSF8dcVcBSi6rWhe4fuBXqO4Vg0TP+B6Eu76R7XGUxFE6u8N3JIW+S40bYwzZkBQKWbbqdC04K9592R0boMtv1IpbN/06LXo/K+KCW/I/Ye1O7EXpdz6tSptGzZksmTJxMXF8euXbuIiMh/WHhwcDC7du1y3DcYDHn26dq1K9OnT3fc9/HxcX3wQgghhBDZdEtKhYWFYTKZOHHihNP2EydOEBUVle9zoqOjMZvNjqF6AHXr1iUpKYnMzEy8vb159tlneeGFFxgwYAAADRs25NChQ0ycOPGKSSkfH598L7rMZrP7X2Q3uRd2LsS09QdMca9dd6LCI86xmBRrWyQfBMAUXgNTQV+zUjPwK4sh/Szmk5uh8s1FFx/y3shN2iJHodpC0yBJFTk3VWha8Pe8O2pyL2z7kYrn1mAw2OR9ka2k/46447nlrssJMHXqVBYtWsS0adN44YUX8n2OwWC44jWWnY+PzzX3EUIIIYRwFd2SUt7e3jRr1ozly5fTq1cvQPWEWr58OSNGjMj3OW3atGH27NnYbDaMRlWjfffu3URHR+Pt7Q2oLvb2x+xMJhM2m63oTkZPNTpCQASknoQ98VDHc3sglCpnsouVl40p+HOMJqjeHrbNU7PwFXFSSgiXSTkGaafBYFKTNHiy6u3RAqPwvphE1v4VUP9OvSMSpVBh6nICXLx4kSpVqmCz2WjatClvvPEG9evXd9pn5cqVREREEBoaym233cZrr71GuXLlrnhMT63LeSNKSy21gpC2yCFtkUPaIoe0hTNpjxyloS0Kem66Dt8bPXo0DzzwAM2bN+emm25i8uTJpKamOr71GzRoEBUqVGDixIkADBs2jA8//JCRI0fyxBNPsGfPHt544w2efPJJxzF79OjB66+/TuXKlalfvz7//fcf7733Hg8++KAu51jkTGaI7QdrPoSEWZKU8gRZGXD+sFovW/36nhtzW05SqsOLro9NiKJwfJNahtcBs5++sdwoowlb3Z6Y1n+KcdciSUoJXRSmLmft2rWZNm0asbGxnD9/nnfeeYfWrVuzbds2KlasCKihe71796ZatWrs27ePF198kW7durFmzRqnXuq5eWpdTlco6bXUroe0RQ5pixzSFjmkLZxJe+QoyW1R0Jqcuial+vfvz6lTpxg7dixJSUk0btyYJUuWOC6yEhMTnXo9VapUiaVLl/LUU08RGxtLhQoVGDlyJM8//7xjnylTpvDyyy/z+OOPc/LkScqXL8+jjz7K2LFji/38ik3je1RSavcSSD0NAWF6RySu5txBQAPvQAjMv+7HFcXcppZHN0D6OfALdXV0QriePSkV3UjfOFxEq90d1n+KYc9SsGaByaPKM4pSqlWrVrRq1cpxv3Xr1tStW5dPP/2UCRMmADhKH4AqfxAbG0tMTAwrV66kY8eO+R7Xk+tyFlZpqaVWENIWOaQtckhb5JC2cCbtkaM0tEVBa3LqfiU9YsSIKw7XW7lyZZ5trVq14p9//rni8YKCgpg8eTKTJ092UYQeILI+RDeG4wmw5Qe4ueAFz4UOzuxTy7LVIZ8is1cVUkH1Njm1Ew78CfV6uj4+IVytpCWlKt1MhlcQPunn4NAqqH6r3iGJUqYwdTkvZzabadKkCXv37r3iPtWrVycsLIy9e/deMSnl0XU5b1BpOMeCkrbIIW2RQ9oih7SFM2mPHCW5LQp6XsZr7yI8QuN71TJhlr5xiGs7a68ndZ1D9+zsvaX2LndNPEIUtRKWlMJoIimkqVrf8bO+sYhSKXddTjt7Xc7cvaGuxmq1smXLFqKjo6+4z5EjRzhz5sxV9xFCCCGEuBGSlCopGt4NRrOa4Sppi97RiKs5m91Tqtx1FDnPzZ6U2ve7mtVMCHd28RRcOAYYIKqB3tG4zPGQZmpl5yIoqRNpCLc2evRoPv/8c2bOnMmOHTsYNmxYnrqcuQuhv/rqqyxbtoz9+/ezceNG7rvvPg4dOsTDDz8MqCLozz77LP/88w8HDx5k+fLl9OzZkxo1ahAXF6fLOQohhBCi5NN9+J5wEf+yULsb7FgICXOga0O9IxJXcqM9paq0BpM3nE9UQwHDarguNiFcLSm7l1S5GuATpG8sLnQqqB6adyCGC8fg2Eao2FzvkEQpc711Oc+dO8fQoUNJSkoiNDSUZs2asXr1aurVUzNimkwmNm/ezMyZM0lOTqZ8+fJ06dKFCRMm5Ds8TwghhBDCFSQpVZI0vlclpTbPhc6vqJn5hPs5Y09KFbKnlHcAVL5Z1ZTat0KSUsK9lbShe9lsRm+0Gp0wbJ+vhvBJUkro4Hrqcv7f//0f//d//3fFY/n5+bF06VJXhieEEEIIcU0yfK8kqdERAiIg7TTsKblTS3o0yyU4f1itF3b4HuQawrfixmMSoiiV0KQUgK327Wplx88ylFYIIYQQQohCkKRUSWIyQ2w/tb5ptr6xiPwlHwI08A6EgPDCHycmexakg39BVqZLQhOiSDiSUrH6xlEEtJhOaijt2X1qRkwhhBBCCCHEdZGkVEnT+B613LUEUs/oG4vI60x2kfOy1cFgKPxxIhuopFbmRTiyzjWxCeFq6efg3EG1HlXyklL4BEH1Dmp9xy/6xiKEEEIIIYQHkqRUSRNZH6Ibg80CW3/QOxpxuRstcm5nNOZ8GJYhfMJd2WcCLVNZTcZQEtXtoZY7FuobhxBCCCGEEB5IklIlUeN71TJhlr5xiLzOZveUupF6UnZSV0q4uxJcT8qhdjcwGCFpM5w7pHc0QgghhBBCeBRJSpVEDe8Go1l9IEzaqnc0IjfH8D1XJKWye0odS5ChmsI9lYakVEAYVG6t1ncu0jcWIYQQQgghPIwkpUoi/7JQu6ta3zRH31iEs7MH1PJGh+8BBEWp2lJosP/3Gz+eEK7mSEo11jWMIucYwvezvnEIIYQQQgjhYSQpVVLZh/BtngtWi76xCMVyCc4fVuuuGL4HOb2l9klSSriZjItweo9aL8k9pQDq3K6WiWvg4il9YxFCCCGEEMKDSFKqpKrRSc3OlnoK9v6mdzQCIPkQoIF3oPrZuELuulKa5ppjCuEKJ7YBGgRFQ2CE3tEUrTKVsnuDabBrsd7RCCGEEEII4TEkKVVSmcwQ21+tS8Fz9+CoJ1UdDAbXHLNyK/DyhQvH4NQu1xxTCFcoDfWkcpMhfEIIIYQQQlw3SUqVZI0GquWuJVII2x24cuY9O7MfVMkusiyz8Al3UlqTUgf+gEvn9Y1FCCGEEEIIDyFJqZIsqoH6QGizwNYf9I5GnN2vlq4ocp6bYwjfctceV4gbUdqSUuG1IawWWDNhT7ze0QghhBBCCOERJClV0tkLnssQPv05hu+5sKcUQExHtTy4ShVTF0Jvlktwaodaj4rVN5biVOcOtZQhfEIIIYQQQhSIJKVKugZ3g9Gsei0kbdU7mtLt7AG1dHVPqYi6EBgFWelw+B/XHluIwji5HWxZ4FcWQirqHU3xqZudlNoTLwliIYQQQgghCkCSUiVdQDmo3VWtb5qjbyylmeUSnD+s1l1ZUwpU0fTcs/AJobfcQ/dcVdTfE5RvCsEVwJIK+3/XOxohhBBCCCHcniSlSgP7EL7Nc8Fq0TeW0urcQUAD7yAICHf98SUpJdxJaasnZWcw5BrC94u+sQghhBBCCOEBJClVGtTopBIhqadg7296R1M6OYqcVyuaniPV26tl0ha4eNL1xxfiepTWpBTkzMK3azFYs/SNRQghhBBCCDcnSanSwGSG2P5qXQqe6+NsdpFzVw/dswsMzykovU+GDQkdWS1wYptaL41JqcqtVC2t9LOQuFrvaIQQQgghhHBrkpQqLRoNVMtdSyDtrL6xlEaOnlIuLnKeW43sWfhkCJ/Q0+ndYM0An2AIraZ3NMXP5AW1u6t1GcInhBBCCCHEVUlSqrSIaqB6LdgssOUHvaMpfc5k95QqW0Q9pcC5rpSmFd3rCHE19qF7UbFgLKX/YuxD+Hb+Ir+LQgghhBBCXEUp/cRQStkLnssQvuJn7ylVVMP3ACq1BLM/pJ7MGT4lRHErzfWk7Kq3B+9ASDkKxzbqHY0QQgghhBBuS5JSpUmDu8FohuMJcHK73tGUHpZLcP6IWi/K4XtePlC1rVqXIXxCL5KUArOvmmACYMfP+sZSxAyJa2i95004tVPvUIQQQgghhAeSpFRpElAOasUBYNz8rc7BlCLnDgIaeAepWRCLkmMI3/KifR0h8mOzwfHNar00J6UgZwhfSa4rpWmYlj5P+MXtmP6YqHc0QgghhBDCA0lSqrTJHsJn3PoDBk2mKy8WjiLn1cBgKNrXiskudn5oDWSmFe1rCXG5s/vAkgpefhBWU+9o9FWzC5i84cweOLVL72iKxv6VGLJ73Rp2L4ELSToHJIQQQgghPI0kpUqbmp3BPwxD6kkiUrboHU3pcDa7yHlR1pOyC6sJwRXV7GcyHb0obo4i5w3AaNI3Fr35BqvaUgA7FuoaSpH552PHqkGzwn9f6xiMEEIIIYTwRJKUKm1MZojtD0Dls3/rHEwpURwz79kZDBDTQa3v+73oX0+I3I4nqGVpH7pnV+cOtSyJQ/hO7YY9y9AwsCuqp9q24Ss1hFMIIYQQQogCkqRUadT4HgCizm/EsP93sFl1DqiEcwzfK8Ii57k56kpJsXNRzKTIubPa3cFgVMm65MN6R+Naaz8BQKsZx+7IHmi+IXA+EfbL3x0hhBBCCFFwkpQqjaIaYCvfFKNmxWtOX5jcEH4br775Fq5nT0oVx/A9yB4yZFAzLKYcK57XFELTJCl1ucBwqNxKre8sQb2l0s5CwhwAbC0fw2b0xtZQ9cBlwwz94hJCCCGEEB5HklKllLXPTA6EdUTzLQMpR+Hv/4OPWsDnt8G6z9WHDnHjLJfg/BG1Xlw9pfzLQvkmal2G8InikpwIl86D0QzhdfWOxn2UxCF8G6ZDVjpENUSr3AYAW+P71WO7fpWC50IIIYQQosAkKVVaBUezudIDZI3cBv2+glrdwGCCoxtg8TPwbm2Ye7/6gGG16B2t5zp3ENDAOwgCwovvdWtkz8InQ/hEcbH3koqsB17e+sbiTupmJ6USV0PqaX1jcYWsTPXFBcDNw3NmFI2oC5Vagi0L/vtGv/iEEEIIIYRHkaRUaeflA/V6wj3fwtO7IG4iRDUEa6aaMWrOAHivLiwZA8c36x2t53HMvFc958NbcbDXldr/uxQeFsVDhu7lr0xl1SaaDXYt1juaG7d9Plw4DoGR0KCP82PNBqvlxpnyd0cIIYQQQhSIJKVEjsBwaPU4PPY3PLYKWo2AgAhIPaWm/v70FvikDaz+EC6e1Dtaz1DcRc7tKrYA70BIOwNJkkwUxUCSUldWp4daevoQPk2DNR+p9RZD8/aIq38X+IaooZz7ZeiwEEIIIYS4NklKifxFNYC412H0DrjnO6jXC0zecGIrLHsJ3q0Ds/rC1nmqbpLI35nsnlJli6nIuZ3JDNXaqXUZwieKmqapGeYAohvrGYl7qpudlNr/O1xK0TeWG5G4Rv2cvXyh+YN5Hzf7QewAtS4Fz4UQQgghRAFIUkpcnckLasVBv5lqeN/t70KF5qBZYc8y+GEIvFsLfh4Fh9epD6cih149pSBnCJ8kpURRu5CkelQaTBBZX+9o3E94bShXQw2L3huvdzSFZ+8lFdsfAsrlv0+zB9Ry12K4cKJ44hJCCCGEEB7LLZJSH330EVWrVsXX15eWLVuybt26q+6fnJzM8OHDiY6OxsfHh1q1arF4cU6tjqpVq2IwGPLchg8fXtSnUrL5l4UWD8PQ5TDiX2g7GoIrqBm3NkyHLzvDh83hz0mQfFjvaN2DPSlVrph7SkFOUirxH8i4WPyvL0oP+9C9sFqqt4xwZjDkmoXvZ31jKayzB2DnIrV+8+NX3i+yPlS8SRU8T5CC50IIIYQQ4up0T0rNnTuX0aNHM27cODZu3EijRo2Ii4vj5Mn8axZlZmbSuXNnDh48yA8//MCuXbv4/PPPqVChgmOf9evXc/z4ccctPl59M923b99iOadSIawmdBoHo7bA/fPVkA2zP5zZCyteg8kNYWYPSJgDmal6R6sPyyU4f0StF/fwPVC9s8pUAZsFDq0q/tcXpYfUk7q2uneq5Z54zxzyvPZTQIOYjhBR5+r72gueb5CC50IIIYQQ4up0T0q99957DB06lCFDhlCvXj2mTp2Kv78/06ZNy3f/adOmcfbsWebPn0+bNm2oWrUqt956K40a5XwYCg8PJyoqynH75ZdfiImJ4dZbby2u0yo9jCaI6QC9P4VndkPPj6HqLYAGB/6E+Y/BpJrw0zB1vzR9QDl3ENDAOwgCwor/9Q0GGcIniockpa6tfBMIKg+ZF+HAH3pHc30unYf/vlbrra7SS8qu/l3gEwLJh+DAyiINTQghhBBCeDYvPV88MzOTDRs2MGbMGMc2o9FIp06dWLNmTb7PWbhwIa1atWL48OEsWLCA8PBw7rnnHp5//nlMJlO+r/HNN98wevRoDAZDvsfMyMggIyPDcT8lRRWitVgsWCyWGzlFt2U/L5een9EXGvRTt+REjFu+w7hlLoZzB2DTbNg0Gy2kErYG/bDF9tOn91A+iqQtAMPJXXgBWtlqZGVlufTYBY6h6q14bZiOtnc5WQU8v6JqD08kbZHjam3hdTwBA5AVUR+tFLRVYd8XxtrdMf37BbZtC7BWu60oQisSxvXTMWVeRAurTVbldpDrvPNtC4MZY8O+6lzXT8da+ZbiDtnl5G+AEEIIIUTR0DUpdfr0aaxWK5GRkU7bIyMj2blzZ77P2b9/PytWrODee+9l8eLF7N27l8cffxyLxcK4cePy7D9//nySk5MZPHjwFeOYOHEir7zySp7ty5Ytw9/f//pOysPYhzYWjXpQZTxlw/ZQ6ezfVDi3FvP5w5hWvYtp1bucCajJ4bJtORp6M1km/evQuLotYk78SgPg2CVf/s1V86w4eWWl0x0DhjN7+H3+V6R7F7zHVtG+NzyLtEWOy9vCO+sC3VKOArB003GyturzXtfD9b4vwi6E0QawbFvIUkMnNEPeL1LcjUGz0mnbB/gDm/xac+jXX/Pd7/K2CEqvxm0AOxexfMEcMswhRR5rUUpLS9M7BCGEEEKIEknXpFRh2Gw2IiIi+OyzzzCZTDRr1oyjR48yadKkfJNSX375Jd26daN8+fJXPOaYMWMYPXq0435KSgqVKlWiS5cuBAcHF8l56M1isRAfH0/nzp0xm83F8IqjwJJO1u5fMW6ei+HA75RL3UO51D3EZiVgfVC/D/1F1RbGxcvhGETVb0v39t1ddtzrpZ2dhuHoem6rYkBrcu04iv+94b6kLXJcqS0M+1fCFtDKVqdLjz76BViMCv2+sHVBm/wpPunn6N4gFK1K26IL0kUMOxbglXAGzb8c9Qe+Sv3LCtlfrS1sM37CePRfOoefxNZ6YHGG7XL2HtRCCCGEEMK1dE1KhYWFYTKZOHHCedroEydOEBUVle9zoqOjMZvNTkP16tatS1JSEpmZmXh7ezu2Hzp0iN9++4158+ZdNQ4fHx98fHzybDebzSX+g2ixnqPZDI37q1vKcdg8F34bh/H4fxgzU648xXgxcXlbJB8EwBReE5Oe76OaneDoerwO/gE3PVjgp5WG939BSVvkyNMWp7YCYIhuXOra6PrfF2ao3R0SZuG1ZwnU6FBksbnMuk8BMDR/ELP/lb+kybctmg+Bo/9iSvga0y2jwah7GctCK23vbSGEEEKI4qLrFaK3tzfNmjVj+fLljm02m43ly5fTqlWrfJ/Tpk0b9u7diy1Xwezdu3cTHR3tlJACmD59OhEREdx+++1FcwKi8IKjoe0oNYU8wNF/dQ2nSJzdr5Zlq+sbh73Y+f6VYLPqGooogaTI+fWpc4da7vgFNE3fWK7lyL9wZB2YvKHF0Ot/fv27wCdYTfrgacXdhRBCCCFEsdD9a8vRo0fz+eefM3PmTHbs2MGwYcNITU1lyJAhAAwaNMipEPqwYcM4e/YsI0eOZPfu3SxatIg33niD4cOHOx3XZrMxffp0HnjgAby8PG6UYulRsYVaHlmvbxyuZrkE54+odb0LupdvqmbCupQMxxL0jUWUPJKUuj4xHcAcAClH4Nh/ekdzdWs+UssGd0NQ5NX3zY93AMT2V+sbZrgsLCGEEEIIUXLonpTq378/77zzDmPHjqVx48YkJCSwZMkSR/HzxMREjh8/7ti/UqVKLF26lPXr1xMbG8uTTz7JyJEjeeGFF5yO+9tvv5GYmMiDDxZ8uJLQQcXmalnSklLnDgIaeAdBQMGLixcJkxdUb6fW9y2/+r5CXI9L53N6BEpSqmDMfmpILcDOX/SN5WqSD8P2BWq91eOFP06zB9Ry5y9w8eSNxyWEEEIIIUoUt+hCNGLECEaMGJHvYytXrsyzrVWrVvzzzz9XPWaXLl3Q3H1ohMjVU2qDGlpmdP/ZqArk7D61LFcdDAZ9YwE1hG/Hz7BvBdz6nN7RiJIiaYtahlQG/7L6xuJJ6vRQCZ8dv0DHsXpHk791n4Fmhaq3QFTDwh8nqiFUaK6GaCfMgrZPuS5GIYQQQgjh8XTvKSVKufC6aihL5gU4vVvvaFzHUU9K56F7dva6UofXwSWZRUq4iGPoXqy+cXiaWl3AaIbTu+CUG/7dy7gIG2aq9VbDr75vQTQbrJYbZkKuepBCCCGEEEJIUkroy+QFFZqq9ZI0hO9Mdk8pvYuc24VWVQkyzQoH/9I7GlFSSD2pwvENgeq3qvWdP+sbS34SZkPGefU3o2bcjR+vQW81lPncATj4540fTwghhBBClBiSlBL6K4l1pRzD99ykpxTk9Jbat0LfOETJIUmpwnPMwudmSSmbDdZ+otZvHgZGF1wmeAdAbD+1LgXPhRBCCCFELpKUEvpz1JX6V984XOnsAbV0l55SkJOU2ivFzoULZKblDLmVpNT1q3M7YFAz8Nln6nQHu5eo4ce+IdBooOuO21zNqMuOX+DiKdcdVwghhBBCeDRJSgn9VcjuKXVyR8mod2S5lPMh011qSgFUbQtGLzWExl7zSojCOrENNBsERkJQlN7ReJ7ACKh8s1rfuUjfWHL752O1bDYYfAJdd9yohlChGdgssGm2644rhBBCCCE8miSlhP6CIqFMZUCDoxv0jubGnTsIaOATDAFhekeTwzcYKt6k1vf9rm8swvMdT1BL6SVVeHV7qKW7DOE7vlnVnDOY4KZHXH98R8HzGVLw3EU++ugjqlatiq+vLy1btmTdunVX3HfGjBkYDAanm6+vr9M+mqYxduxYoqOj8fPzo1OnTuzZs6eoT0MIIUouWxbB6YlqlnEhRL689A5ACEAN4UtOVEP4YjroHc2NsdeTKlsNDAZ9Y7lcjdsgcbWqK9XiIb2jEZ5M6knduDp3wNIX4dAqSD0DAeX0jcfeS6p+Lwip6Prj1+8NS15UPTUP/pVT7F0Uyty5cxk9ejRTp06lZcuWTJ48mbi4OHbt2kVERES+zwkODmbXrl2O+4bL/ke9/fbbfPDBB8ycOZNq1arx8ssvExcXx/bt2/MksIQorWw2G5mZmXm2WywWvLy8uHTpElZr6U5ASFugklB7lqGt+4xW5w9j+X4t1i6vQnB5p93MZjMmk0mnIIVwD5KUEu6hYgvY+mPJKHbumHnPjYbu2cXcBitegwN/gjVLzX4oRGFIUurGhVZRw9qStsCuxdD0fv1iuZAEW35Q6zcPL5rX8AmE2L7w7zTVW0qSUjfkvffeY+jQoQwZoup1TZ06lUWLFjFt2jReeOGFfJ9jMBiIisp/uK2maUyePJn//e9/9OzZE4CvvvqKyMhI5s+fz4ABA4rmRITwIJmZmRw4cABbPr09NU0jKiqKw4cP50n4ljalui00DbLS4dJ5sPpDg1E5j+3fB37nwNvf6SllypQhKiqq9LWVENnkE6lwD45i5+vVH3NP/qNsr9fkTkXO7aIbg18opJ9TQyUrt9Q7IuGJsjJUDTiQpNSNqnunSkrt/EXfpNT6L1S9p0otoWKzonudZkNUUmrHz5B62r2GOHuQzMxMNmzYwJgxYxzbjEYjnTp1Ys2aNVd83sWLF6lSpQo2m42mTZvyxhtvUL9+fQAOHDhAUlISnTp1cuwfEhJCy5YtWbNmzRWTUhkZGWRkZDjup6So2pAWiwWLxXJD5+mu7OdVUs/vepSmttA0jaNHj2I0GqlQoQLGy2Yn1TSN1NRUAgICSn1yobS2hSHzIqSexmA1AGXQMKH5h5KWZSTAdgFDVjoAmrc3WlAkGgbS0tI4deoUVquVyMhIfU+gGJSmvxnXUhraoqDnJkkp4R6iGoLJG9LPqqROOTfsZVRQ9uF77ngORhNUbw/bfoJ9yyUpJQrn5A6VwPALhZBKekfj2ercAb+/ruq8ZVwAn6Dij8GSrhJFADc/XrSvFR0L5ZvCsY2QMBvaPFm0r1dCnT59Ot8PMJGRkezcuTPf59SuXZtp06YRGxvL+fPneeedd2jdujXbtm2jYsWKJCUlOY5x+THtj+Vn4sSJvPLKK3m2L1u2DH9//3yeUXLEx8frHYLbKA1tYTQaiY6Opnz58mRlZeW7j7e3d4n+gHk9Sk1baBpetnR8Lcl42TLAADYvIxleIWSYg8FgwssLMrQAfCzJ+GYlY7ClYE1JI807HLPZl6CgII4fP87GjRvRNE3vMyoWpeFvRkGV5LZIS0sr0H6SlBLuwctH9bg4sl7VlXLHhE5BnT2glu44fA/UEL5tP6m6Uh1e1Dsa4YlyD90rRd+AFomIuqpX5dn9sCceGvQu/hg2z4W0MxBSWSXJilqzwSoptWEGtH5C3kPFpFWrVrRq1cpxv3Xr1tStW5dPP/2UCRMmFPq4Y8aMYfTo0Y77KSkpVKpUiS5duhAcHHxDMbsri8VCfHw8nTt3xmw26x2OrkpTW2RkZJCYmEhISAh+fn55Htc0jQsXLhAUFFSqegflp7S0hSEzFS4cx2BJBUAzGME/DAIi8DGa8CFXWwQHYzCEgCUcLfkQJmsmgRnHITASc9lQLly4wG233YaPj4++J1XEStPfjGspDW1h7z19LZKUEu6jYovspNR6aNRf72gKx3IJzh9R6+44fA9UUgrU8L30c6q3ixDXw56UiorVN46SwGBQs/Ctel8N4SvupJSmwT+fqPWWjxZPnbkGfVSB97P7VMHzau2K/jVLmLCwMEwmEydOnHDafuLEiSvWjLqc2WymSZMm7N27F8DxvBMnThAdHe10zMaNG1/xOD4+Pvl+iDKbzSX2ItuuNJxjQZWGtrBarRgMBkwmU56he4CjzpTBYMj38dKkxLdFZiqkHIPMi9kbDBAQhiEwEkxmcqfh8rSFTyCE14HzRzCkn4WLJzBxHoMtCy8vrxL/e2RXGv5mFFRJbouCnlcJ/CshPFbF5mrpycXOzx0ANPAJdt9aKSEVIaw2aDZV8Fzk7+JJDAmzCE3dq3ck7keKnLtWnR5quXuZqtdVnPYth1M7wTuw+Gpa+QRCw75qfcOM4nnNEsbb25tmzZqxfPlyxzabzcby5cudekNdjdVqZcuWLY4EVLVq1YiKinI6ZkpKCmvXri3wMYUQokTLTFMTGp3enZ2QMqieUZH11PW1qYCJBaNJTXYSWhUMJlUY/eIJ2L5QfVkkRCkjSSnhPuzFzk9sVX/0PZGjyHk19x6SYu8ttW+FvnG4m9Qz8O90mHknvFsbr0Ujabv7NQwJs/SOzH3YstTvKKjC+eLGVWgGQdGQeQH2/1G8r73mY7Vscj/4hhTf6zZXM8Y5Cp6L6zZ69Gg+//xzZs6cyY4dOxg2bBipqamO2fgGDRrkVAj91VdfZdmyZezfv5+NGzdy3333cejQIR5++GFAfYs/atQoXnvtNRYuXMiWLVsYNGgQ5cuXp1evXnqcohDCTVWtWpXJkyfrHUbxsaSra/zTuyAjeziSfzk1BL9MJVUXtzD8QlWvKbO/+rJ4xavw3SBIO+u62IXwAJKUEu4jpBIERqkPvfaeGJ7mTHaRc3etJ2VnT0rtXSHfyKSfg41fw9e94Z2a8MsoOPAHaDa0kMoYseG1aCT8PlHaCuDMXsi6pHrWuOsQVU9jNEKd29X6zp+L73VP7lQ9pQxGNXSvOEU3gvJNwJoJm+YU72uXEP379+edd95h7NixNG7cmISEBJYsWeIoVJ6YmMjx48cd+587d46hQ4dSt25dunfvTkpKCqtXr6ZevXqOfZ577jmeeOIJHnnkEVq0aMHFixdZsmQJvr6+xX5+QogbZzAYrnobP358oY67fv16HnnkEZfEOGfOHEwmE8OHD3fJ8VzKcgnOHlQ9ii+dV9v8ymYnoyqrmrg3yssbylQB3zJg8IIdC+GTNsX/JZUQOpKklHAfBoPnD+Gz95Ry90LtVduA0QznE3MSaaXJpfOQMAdm9YNJNWHhCPXhXLOqD8udxsPITWQN38DuyOyhVX+8CQuGg7UUzCRzFYakzWolKlYlU4Rr2AuM71wMNmvxvOY/2b2k6tyuencWt2aD1XLDDEn4FtKIESM4dOgQGRkZrF27lpYtc2ZUXblyJTNmzHDc/7//+z/HvklJSSxatIgmTZo4Hc9gMPDqq6+SlJTEpUuX+O2336hVq1ZxnY4QwsWOHz/uuE2ePJng4GCnbc8884xjX03Trjir4OXCw8NdNrvml19+yXPPPcecOXO4dOmSS45ZWJmZmWolKwPOHYJTO+DSObXNtwyE11XD7rxcnKg3GMA3GPpOg3I14MIx+KonLHu5+If1C6ED+UQh3It9CJ/HJqXsPaXcvAeJdwBUvlmtl5YhfBkXYPP3MOcemFQD5j8Ge5aCzQKRDeC2/8ETG+HRP6HtU9nj/A3sKN+XrG7vqjH/CbNgVl+4VLCZJEoiQ5LUkyoSVduqC96005D4T9G/XuppNesewM06fTvdoI/qcXdmLxz8W58YhBCiBIuKinLcQkJCMBgMjvs7d+4kKCiIX3/9lWbNmuHj48Pff//Nvn376NmzJ5GRkQQGBtKiRQt+++03p+NePnzPYDDwxRdfcNddd+Hv70/t2rVZvHjxNeM7cOAAq1ev5oUXXqBWrVrMmzcvzz7Tpk2jfv36+Pj4EB0dzYgRIxyPJScn8+ijjxIZGYmvry8NGjTgl19+AWD8+PF5JmmYPHkyVatWddwfPHgwvXr14vXXX6d8+fLUrl0LkhP5+uNJNG/fjaBabYlq0oV7Rk/kZFYAmHOSUdu2beOOO+4gODiYoKAgbrnlFvbt28eff/6J2WwmKSnJ6bWfeuopbrnllqs3SEQ9dR3abAigweoP4IuOcGrXNdtSCE8mSSnhXhxJqX/1jaOwzthrSrl5TymAGh3VsiQnpTLTYNtPMPd+lYia9zDsWqSGDIXVhvZjYPg6GLYK2j17xR5uWtMHYOC3asz//t9henc160op5OgpJUkp1zKZoXY3tb7zl6J/vX+nqWGY5ZvkJKiLm08QNLxbrUvBcyGEh9E0jbTMLKdbeqY1z7aiuGku7F36wgsv8Oabb7Jjxw5iY2O5ePEi3bt3Z/ny5fz333907dqVHj16kJiYeNXjvPLKK/Tr14/NmzfTrVs3Hn30Uc6evXptpOnTp3P77bcTEhLCfffdx5dffun0+CeffMLw4cN55JFH2LJlCwsXLqRGjRqAmtyhW7durFq1im+++Ybt27fz5ptvYjKZruv8ly9fzq6dO4if9xW/THsX0s5gybIw4cWn2fTvWuYv+JmDiUcYPHiw4zlHjx6lXbt2+Pj4sGLFCjZs2MCDDz5IVlYW7dq1o3r16nz99deO/S0WC7Nnz+bBBx+8dkDeAdBjMgyYrYYKJm2BT9vBus+lV7EosYph7mchrkP5xqpHyoVjcP4ohFTQO6KCs6RDyhG17u49pUDVlfptvJqSPStTjWkvCSyXYG88bJ0Hu5eAJVfR/LIx0KA31L9LfRt1PcXoa3WBwYtgdn84sQW+6AT3/qBmXCktNBuGpC1qXZJSrle3h6qvtONniHuj6CZLyMpQF7egeknpOSlDs8EqIbVjoZpoIKCcfrEI4SmsWargcspxCC6vhhN5B+gdVamTbrFSb+xSXV57+6tx+Hvn8zFO0yAzVdVnvVxmGqBBerK6n3ERgFdfep7ObVs4ditbqwqNalVx3J/w4tP8NO8HFv7wLSOGZdeR0mzqutd+LGDwvQMY2Et9ufLG2OeZMmUK69b8Tffbe6jZ5i5js9mYMWMGU6ZMAWDAgAE8/fTTHDhwgGrV1JDy1157jaeffpqRI0c6nteihYr1t99+Y926dezYscMxzLh69eu8/tZsBPj78cWEkXh7ewHlwDuQB4c/q2aKBaoDH3zwgaPOXmBgIB999BEhISF8++23jinvcw91fuihh5g+fTrPPvssAEuWLOHSpUv069ev4LHVuV1NhDJ/mPoCefEzsCceen4EgeHXd55C0TQ1y2FydoK10k36xuMOLp2HdZ9B6yddUyOtkCQpJdyLdwBE1oekzWoInyclpc4dVEufYAgI0zWUAolsqKaxTTsNR9ap4UOeKitD/cPeOg92/apmMbMrU0UloRr0VnWQbuQDeIWm8HA8fHM3nNkD07pC/6+h+q03fg4eICDjJIbMi6qWQpjUmXG5mNtUb7zzh9VkD+UbF83rbP0RUk9CUHmo36toXqOgyjdRszgeT1AJudYjrvUMIUqXrAw4uUP9TTieoJYntqmejrn5h6nkVJkqqgCzfT20qppIpqR88SSuLu2M+h+Sn9RTKpl07oC6f0FNhNA8plzONuBiahrj3/2URcv/4vjJ02RlWUm/lEHi3m05+9my1Gvlel5stXDH/UAgOCiQ0/u3QFIVMHqpGepM3uqDr8mb+BV/kpqaSvduXQEICwujc+fOTJs2jQkTJnDy5EmOHTtGx44d8z2dhIQEKlasWLi6d7YsuHgSLiXTsHZ1lZAyB0BwNPgEsWHDBsaPH8+mTZs4d+4cNpsNUBNI1KtXj4SEBG655RZHQupygwcP5n//+x///PMPN910E7Nnz6Zv374EBFxn8jgoCu79EdZ9CvHjVNmJT1pBr0+gZufrP++STtPUBEbJh1Ti6dwhtW5fJic6/+3s8X5OfcvSavFzsPlbOJYAA/SbbVySUsL9VGyRk5TS+wPT9bAXOS9bXd+eBwVlNEJMB9jyvUroeFpSymqB/StVImrnIsg4n/NYcEX13mnQG8o3de3PI7QqPLQMvr0HEtfAN33Ut1aN+rvuNdxUSPpBtRLZAEzy78PlzH5qWO2On9WtKJJSmgZrsguc3zRUDRvUW7PBatbLDTOglc49t4TQkyVdJZzsyadjCSohZctngg3vIDUVfcpR9U132ml1O7ohnwMbVI+qMlVyJauyk1dlqqjH8unJIq7Oz2xi+6txjvs2m40LKRcICg7CWMQTgfiZ8/l5ZWWq9wOAl5+aWTU3Lx/AoJIv4CjWHRBSLmcb8MxrbxH/x2reGf88NapVxs/Xl7sfepJMKzn7GYwqwZTreWbfQMd9DVVnymofbWbLUrdcvde//GwqZ8+exc8/5xg2m43Nm/7jleefxM+YPenHFYas+fn5XbF9AIxGY55hjpbMDDWpzYntaqlBQECg6knvEwQGA6mpqcTFxREXF8esWbMIDw8nMTGRuLg4RyH0a712REQEPXr0YPr06VSpUoXffvuNFSsKWS7DaISbh0G1dvDjw3ByO8y6G256BDq/qq4dSpPM1PwTTvZlxrXqvhpU54HUU7BkDFS9xf0nqCoq235SCSmDUfWU0pF8qhDup2IL+PdLz6srdcZDipznFnNbTlKq41i9o7k2axYc/DM7EfWL+jbELiga6vVSiagKzYt2Zjj/snD/fFUsfdtP8NMj6pvJW54u0R+oy6QdUivRsfoGUpLVvVMlpHb+Ah1fdv3xD/6lhp+a/d3n28GGd8PSl1Tvw0OrPC9BLkRhZFxUtWKOb8q5ndqpPihfzreMGjJdvrFaRjeG0Go5/+fSk/P5cJaYs56VrpIVKUchcXXe4xvNEFLxsoRVdi+rMlXUB7gS/L+tsAwGg9MQOpvNRpa3CX9vryJPSuWhaWpGZc2mEkNhNfP+zIKi1IfP8OyeRWWya2OG1YAyZRy7rdq4jcEPDeWuwWoSjIsXL3LwyHHwC815rtELAiNy7gOElHfc12w2NAxowRUgqqFKmFnttwzOnDzBgmUr+fbjt6hfO2f2V6vVRtu7HmTZgrl07dCGqpXKs3zBHDrUj8ruZeXt6HEVW7cmR44cYfeuXdSqXTtPk4SHh5OUlISmaRg0G6SeJuGfP9UMt5pVJe58AiHTqGa+y7Zz507OnDnDm2++SaVKlQD491/nzySxsbHMnDkTi8Vyxd5SDz/8MAMHDqRChQpUq1aNNm3a5P+zK6jI+jD0d1V6Y+0nasjVgT+hzxeqjUuKrEx1TZ1fwuncIZV8v5aA8HyS8NnL4Irq/fvVneqaaN4j8ODS0vdla8ox+HmUWm87Giq3vOruRa2Utb7wCPZi58cTPKvWkX3mPU/KtlfvoJbHEty3novNqj6obvsJti90/mcUEAH1eqpEVKWbizYRdTmzL/SZpi7kV0+BFRPUP9Hu75bYf2yOnlJST6ro1OyiLpZO7YTTe9QHC1ey95JqNFAlV92BveD5xpmqt1QJT0qtX78em81Gy5bOF4Br167FZDLRvHlznSITRSY9OTsBlZCTgDq9B9Wf5DL+Yc7Jp+hGqkfT1ZJCfmXULb+/zZqmegQ4PtQddB7acv6w6ol17oDTUCwnZv+cXlWXf8gL9KAyCyVZ+lk1yzCGa79frqFmzZrMmzePHj16YDAYePnllx3D1wrF6AXeXoC/Y9PX03+kXLkw+j36DAbNqpJVWRlgzaR7l9v4cu4iunbqwPjRj/LYmDeICAulW4c2XEhNZdX6TTzx4ABurRdJu5ZN6dPrDt579QVqxMSwc/9hDCYzXbt2pX3rlpw6dYq3J4zl7i6tWLLiT379/W+CgwJVwtW3jEpwkeYUbuXKlfH29mbKlCk89thjbN26lQkTJjjtM2LECKZMmcKAAQMYM2YMISEhjqF6tbMTZHFxcQQHB/P6668zZsyYwrdfbmZf6PYm1OwE8x9X1wqf3wYdx8HNjxfvdXBh2az4Zp7BkLgaLhzNm0xPOUq+fxtz8wmB0MrOyfPcw5e9/a/+fIC7psLHreHov/DXO9D+BVecnWew2dT751Ky+j/jBudeMj85Cc9WLkb9o7iUDCe2qjo+niD38D1PERwNEfXh5DY4sFJN0e4ObDY4vBa2zYPtC1RRQjv/cqo3Sf271IdXPYccGI3Q5TUIqQy/Pqc+UKccg7unOwpklhiaRpm0g2pdklJFx6+M6qK/b4XqMXXLaNcd+8w+Vfwf1FAAd9JssEpKbV8A3d52n4RZERg+fDjPPfdcnqTU0aNHeeutt1i7dq1OkQmXSD0DSZtyht8d33TlZE9Q+ezkU6OcnlBB0a7tlWQwqB4tgRFQqUXex21W9X/rSj2tUo6pIVendqrbZcxAT4D/XBdyoZh84M4PoNEAnQPRgTVTTQ4E6v1j9r2hw7333ns8+OCDtG7dmrCwMJ5//nlSUq41JOr6TJs2jbvuuguD0QgY1VDy7GL9fQbcz/33389pUyQPjHyZS97l+L/33+eZCZMJK1eWu+/spvbNyuTHzyfxzIT/Y+Cjz5Cank6NqpV4c8wTcO4AdcMNfPzGGN6Y8gUT3nqXPrd35plRT/DZ9G9Ur68rCA8PZ8aMGbz44ot88MEHNG3alHfeeYc777zTsU+5cuVYsWIFzz77LLfeeismk4nGjRs79YYyGo0MHjyYN954gwEDXPy+rNEJhq2GhU/ArsWw7CU1yU+vqera3l1kZajhhrl6hHqd2EZc1iXYdpXnefldVhvvsmHHV/n5FVhIRbj9XTUz9x9vqzatWEq+FFr3mZpN3MsPen/uFqUcJCkl3I/BoHpL7Y1XQ/g8JSl1xp6U8qCeUqDqSp3cpj4E65mUslnh0Gr1oXTnL44CnIBKUta9A+r3hmq3ul9PpJaPqJocPz4Ee5bBjNvhnu8gKFLvyFwn5Qje1lQ0oxeGiFI046Ae6vZQv487f3FtUuqfTwANasa5vgfWjarQVH0oP75JFTxvNVzviIrM9u3bado07/+1Jk2asH37dh0iEoV28QSc2u5chPxKRabLVM6VgGqsloERxRlt/owmVZuqTKX8eylmZcD5Izk9rHIXDC7oUJriYM2AX55SdSRzDykr6TQNkg+r4Whm/6u+pwYPHszgwYMd99u3b5+n5hJA1apV89Q/Gj7c+W/ywYMHLwsj73EOHTpEcHBwnu0AmzdvvmKc/fr1c5ql7tHHh/Po4/n/TygbaWPaNzfnGh6Y4TRU8LFBd/PYkHsgMFJ92WEw8uK41xzPnzFjRr7HHThwIAMHDrzqOcbGxrJ06dVnXzx69CjdunUjKirqqvsVSkAYDJgNG6bDkhdVndVPWsGdU9R1RHGz18Q79l9OEiqfmngGwIYJQ5lKGELzGS5cprJ6HxfHkOHYvrD7VzUBzLxH4LG/Sv5Mpid3wm/j1HqXCW7z99LNPtkJkc2RlFqvPvC7O0s6pBxR6540fA9UXak1H8LeFVcsJllkrBY1Hn7HQtjxi/PFrU+wmg63fm+o3t79h3HWvQMe+AXm9FcfTr7spGZMcZM/9jfKkLRFrYTX1XXK2FKh9u3wy2hVsPj8UdfMQpp+DhKyZ1Vp9fiNH68oNBusPlT+O10NQyihNWx8fHw4ceJEnqnLjx8/jpeXXJa5vWMJmP58h7i9f2H+Lzn/fcpWz0k82W+e2vvPy0dd11zh2saSeo7ffv2FTp06YvbS69t2DeYNVR/Kf3wIHv6t9PyfSj+XXdj5xofteSSDURVs97pC7zCbVe1TzO1y/vx5tmzZwuzZs5k/f37RvZDBAM0fhCptVY+f45tg7n3QdBDETSy6XvtONfESsmvi7bpyTTzHkORGWMLr8+ua7XS7vccV63EVq9vfhcR/VBmWpS9Bj8l6R1R0sjLV+yTrkuoZ1uJhvSNykKsf4Z7s3SePrNc3joI6d1AtfYLV8DJPUqW16vZ+4Zj6hxJaxEm1rAx14bh9gZo171JyzmN+oeoDeb2eUP1Wz7uorNQCHopXs6Kc3Q9fdoaBc1QbezhD0iYAtKhYStklb/ELioRKLeHwP+p3xBWJ+Q0z1RCcyAaqt6E7anA3LP1fdsHz1VD1BovCuqkuXbowZswYFixYQEhICADJycm8+OKLdO4sU3y7rYsnYfmr8N83GNHwBTQMGMJqORchj2oIviF6R1t8vAPJNAerwsJ6fsC861P4pLWavXn5qxD3un6xFBerRfViA1XEvLTNwlYQOpV46NmzJ+vWreOxxx6jc+fOLh/+mEd4LXjoN/j9dVj1Pmz8Cg6ugj6fQ4VmN3bs9GT1e2Xv/XQsAc7sJd+6TwHheRPylydLLRY0w64bi8mV/EKh18fwVU/V66xWV6jdVe+oisbKN1Qy0a+smjncjZLYkpQS7sn+B/TcAUg9rbqourPcM++50S94gZj9VNJk/+9qyFDzIkhKWdJh72+qUPnuJc7TtfqHqW7G9e5U07K6wbjmG1IuRiWm5gxQSdWveqqL5Qa99Y7shhiSVFd7LUpm3isWde/ITkr9fONJKatF1Q8AVUvKXf9G+QZDwz7qYnrDjBKblHrnnXdo164dVapUoUmTJgAkJCQQGRnJ119/rXN0Io+sDFg7Ff6YBJkXALDV78OqzLrc3OthzAEuqG0iblxQlPqQNWeA6v0d00H1BCjJzh/JmUXOHYaCCoeVK1c61m+oSPz18PKGzq+o9/1Pj6qeP192gfZjoO1TBUvQpZ5xnpDheELOF++XCyrv1AOK6Eaur4lXXKq3h5uHwz8fwcIRMGwNBIbrHZVrHVoNf09W6z3eV38z3YgkpYR78isDYbXh9C5VV8rdM9aeWOQ8t5jbciWlhrrmmBkXYc9SlYjaEw+W1JzHgqKzE1E9oXIrfYuVF4WAMBi0UA0n2PkL/DBEXTy2fsLz/llnXIDNczEc/geQpFSxqXMHLPuf+qYz7eyNDf3ZvkDNZhMQrnojubNmg1VSavsC6PaW5w55uooKFSqwefNmZs2axaZNm/Dz82PIkCEMHDjQPYYyCEXTYNevqoCw/X98+SbQ9S2s0U05u3gxeJewCS08Xe1u0GIorP8cfhqmCkGXtA+Wdunnsnua24ftecCsa6J4VLsFhq1Sw+G3/aRmh967HHp/qt4rdhdOOA+/u2ZNvMa5auLFlrxEaMex6rPQye3w85OqXpenXbNfyaXzMO9RQIPG96mOAG6mUEmpw4cPYzAYqFixIgDr1q1j9uzZ1KtXj0ce8YD6P8IzVGyRnZRa7wFJqeyeUp5WT8quRkeIfxkO/q2+FS6sS+dh1xL1gXLfcjVm2S6kkkpC1b1T/Ww9YdraG+HtD/2+giVjYN2nqn3PH4aub3pGEu7kTlj/BWz6FjIvYADSzaF4RTbQO7LSoWw1iGwIJ7aoD8ZN7i3ccTQN/vlYrbd4+IZnZSpy5Zuq4U9JW9R7z13rX92ggIAAuV5yZyd3qL/d+39X9wMjodN4iB2g/ndZLFd9utBRlwnqWubUDljwuJp0pKR8sLSzZuUM2wuMVNcbQuTmF6pmgq4ZB4ufgcTV8ElbaHKfGnp3fBNcTMr/uWVjLhuSHFsivyDKw+wLvT+Dz29TMxpu/AqaPaB3VK7x6/NwPlEVku/2pt7R5KtQSal77rmHRx55hPvvv5+kpCQ6d+5M/fr1mTVrFklJSYwdO9bVcYrSqGJzSPjGM+pKOXpKeWhSKqKeurC5eALD4eucjjztrKp7s2Mh7PvdeZaNstVVEqpeT/UNc0m7MLwWo0n19ihTSfV6WfeZml679+fueRFptaif5fov4OBfOdvL1cTabAgrjofSRWpWFJ+6d6ik1M5fCp+UOrxOFUw3+UDzh1wbX1EwGKDZEFg0WtV2cOfhhtdh4cKFdOvWDbPZzMKFC6+6b+5px0UxSzsLKyfC+i/VsCiTt5oJ8panwSdI7+hEQZj94O4v4bMOajbcdZ9By0f1jsq1Uo6ALUsV9y5Js/wK1zIYoPFAqHyzmlnuyDo1PM3xuBHsNfHsPaCiGqqh9KVVVEO47X8QP1Z9MVG1red2OLDbNl/NamwwqqSbm/4vK1RSauvWrdx0000AfPfddzRo0IBVq1axbNkyHnvsMUlKCdeo2EItj25Us2e4c++SMx4+fM9gUEP4Ns3BcOB3oMXV9794Un1Q3r4ADvzlPNtGeJ3sRNSdqqhyCfhAeUMMBjVsL7iCGuO/8xeY2QPumes+tdIuJKlC2Bumw4XjapvBCLW7q9411dtjy8oia/FifeMsber2UB+Q9y5Xw2ELM4uO/QI0tq/nDGNp2FclcU/vhsQ1JWKigF69epGUlERERAS9evW64n4GgwGrNZ/Zi0TRsmbBv9NUkWD75Bt17oAur6lei8KzRNZXhc4XPwPLXoYqbSCqhPTyTT+vhu6BDNsTBVO2Ggz5VQ1rPblDXZtHN1K/E94BekfnflqNgN3L4NDf6rp9yBIweWjFo5Tj8Msotd72KZWgdFOFamGLxYKPj5oV67fffnN8q1enTh2OHz/uuuhE6RZRF8wBqrDoqV0QWU/viPJnSVffWoFnZ9Ozk1LG/SuhfD5JqZRjsONnVSMqcTVouQo3RjZUvaHq3QnhtYstZI/SoLcqKjhnIBz9F77oBPf9qN97RtPg0CrVK2rHz+pbV1B1h5o+AM2HQEhFfWITSkQ9CK2mJnzY+xvU73V9zz93SP1sQRXw9BS+wdCgD/z3tSp4XgKSUrkL3RZb0VtRMPtWwJIX1XAvgIj60HWimgFWeK4WD6u/m7uXwI8PwdDf3bOH8vWwZeXU/AmMkISCKDiTl+p5LK7NaIK7PoFP2qjROn+/B7c+p3dU189mU0OY08+pJOStL+gd0VUVKr1ev359pk6dyl9//UV8fDxdu6p6P8eOHaNcuXIuDVCUYkYTVGiq1t15CJ99VgqfYPD34Pd/9Q4AGE5swcdyXm07dwhWT4EvOsN7deHX59Q3B5pN1X7pNB6e2AjD/oZbn5WE1LVUaa1m5itTWSUavuwMh4v5vZ1xQSWiPmkNM25XRTBtWVDpZujzJTy1HTq+LAkpd2AwqCF8kJNcuh7rPlO/q9U7uG9S/0qaDVHLbfPVkKoSwmKx0LFjR/bs2aN3KOLMPvUlwdd3qYSUX1m4/V149E9JSJUEBoOajS8wEk7tVL0vPd35o6pEgskHAqP1jkaIkqtMZej+jlpf+SYc2aBvPIWx/nP1pYuXryob4uWtd0RXVaik1FtvvcWnn35K+/btGThwII0aNQJUzQT7sD4hXMI+hM+dk1Jnsoucl63u2UPVAsNVMUOg4ZGvMU3rBO/Hqgu5I+vUPpVaQtwbMGoLPPK76grqyb3D9BBeCx76TY3dTzsDM+8oXMLhep3cCYuegXfrwqKn1ewiZn8129ljf8NDS6Hh3W7/T6vUqZtdX2jPMsjKLPjzMi6oIp2gauJ4mgpNVQ9MawZsnqt3NC5jNpvZvHmz3mGUbpdS1JCuj1qqYrZGL7j5cXhyo+pd46nDNEReAWFw11S1/u+Xqmaip7qUAunZCfoylYt9spj27dszatSoYn1NIXQV2w/q91YlSuYNhczUaz/HXZzcqepiAXSe4BGdBgr1F619+/acPn2a06dPM23aNMf2Rx55hKlTp7osOCFyklL/6hvH1diLnJeE5EzMbQBUSF6H8XiCqlVQ9Rb+v737jm+yXP84/knSdNIJdFD2ngWhgIiIskFRFAUVZYjjsARBRVw4QdGDuA4oP5bniHuhLBEFmYIgCsreq2WU0t2mSX5/hCaEtlCgbTq+79crryTPyvXchPTJlfu+bnq+AWO3w9AfHV9wz59SVi5fYAQMXgj1ujlmKPzsfvjtg8J/HavF0dNk7i3wn7aOX02ykqFiPejxuuPftPfbjsKOUjJFx0KFSMhMgv2/Fny/P/7n2KdSfajTuejiKyoGA8QOdjzeNNcx3LSMuO+++5g1a5anwyh/bFZHovbdlrD2HUePk7pdYNg6x3A9v1BPRyhFoU4nR11HgO9GOEoRlDY2KyQecjwOqHxZ9QV79+7tHNFyoVWrVmEwGAo1UZ6enk5YWBjh4eFkZl7FbM4inmYwwC1TIbCKY5b10tLbMjvLkUTLznBc/7V5yNMRFcgV/RyUnp6O3W4nNNTxB/zgwYN88803NGrUiO7duxdqgFLOVY113J/cARlnwTfYs/HkJeG8nlKlXcuB2Hcu4mSmN2HXD8GryW2lpzhyaeNTAe7+BBaNc3zpXvyk46Kz68tX/wtocpzjmJvm5lu4vFT36itPjEZo2MtRhHn7AqjX5dL72Kywfrrj8bXDiv0X9ULT7C5Hj5aTO+DQeqjRztMRFYrs7Gxmz57NTz/9RKtWrQgIcK8LM3XqVA9FVoYdXAtLnnJMgw5QsS50nwz1u3k2LikenZ53JPWP/+koXHz/d6XrczEpZ9ieNwRe3rC9oUOH0rdvX44cOULVqu7D8ufMmUNsbCwxMTGFFupXX31FkyZNsNvtLFy4kMGDBxfasS+X3W7HarXi5aXej3KF/EId9aU+us1xHVa/B9Qv4bmOFZMh7i9H7Le9X2qu96/oE/m2227jo48cwwISExNp27Yt//73v+nTpw/Tp08v1AClnKsQDiE1ALtjFr6SKKenVFgZ6ClVsQ7Zj6xlXd0nsbccpIRUUTN5wS3ToPO5Lrbr3oMvh4Al4/KPZbfDgdXwxWB4q4njj1Lyccevqh0edwy5vPtjqHNTqfkDJec06u2437nIkXC6lJ2LIPGg44Ik5u6ija0o+QY7JggAR4K1jNi2bRstW7YkMDCQXbt28ccff7jdpBAlHoYvhsCcno6EhE+wYwj6sHVKSJUnXt6Omolmf0dyau07no6o4DKTHUP94dywvcubifqWW26hcuXKzJ071215SkoKX3zxBUOHDuX06dPcc889REdH4+/vT7Nmzfjkk0+uKNxZs2Zx3333ce+99/K///0v1/q///6bW265haCgIAIDA+nQoQN79+51rp89ezZNmjTBx8eHqKgoRo4cCcCBAwcwGAxs2bLFuW1iYiIGg4EVK1YAsGLFCgwGA4sXL6ZVq1b4+PiwevVq9u7dy2233UZERAQVKlSgdevW/PTTT25xZWZmMn78eKpVq4aPjw9169Zl1qxZ2O126taty5tvvum2/ZYtWzAYDOzZs+eK2klKkdo3OoZ4A3w3ElJPeTScizq4DtZMczzu/TYElZ7ac1eUlNq8eTMdOnQA4MsvvyQiIoKDBw/y0Ucf8c47l/dB//7771OzZk18fX1p27YtGzZsuOj2iYmJjBgxgqioKHx8fKhfvz6LLpim/OjRo9x3331UrFgRPz8/mjVrxu+/l+DhX3JxJX0I3+mcpFQZ6Cklxc9ggA7jHEUIjWb451v4b5+CF3fOTIYNM+E/7VS4vKyq2cGRoEk9CYd/u/T26/7juI99oPTPNuUseP5NmSl4/ssvv1z0JoUgKxV+mQTvxcLfXwMGx3vp0c2OIeiqnVf+VKoHPac4Hv/8MhwthMLFdrvjvXb+zZKWe9mV3jKSHLNPW9IdCTWD0bWugEOavby8GDhwIHPnzsV+3j5ffPEFVquVe+65h4yMDFq1asXChQvZtm0bDz/8MPfff/8lv5NdaO/evaxbt45+/frRr18/1q1bx8GDB53rjx49yg033ICPjw8///wzmzZt4oEHHiA72zH77/Tp0xkxYgQPP/wwW7duZcGCBdStW/eyYgB46qmneO2119i+fTsxMTGkpKTQq1cvli9fzh9//EGPHj3o3bs3hw4dcu4zcOBAPvnkE9555x22b9/OBx98QIUKFTAYDDzwwAPMmTPH7TXmzJnDDTfccEXxSSnUeSJUbgSpJ2DBoyWzpEBGEnzzsGOCm+b3OmZFL0WuqD9jWloagYGBAPz444/ccccdGI1Grr32WrcPn0v57LPPGDt2LDNmzKBt27ZMmzaN7t27s3PnTsLDw3Ntn5WVRdeuXQkPD+fLL78kOjqagwcPEhIS4tzmzJkztG/fnptuuonFixdTuXJldu/e7RxqKKVQ1daw7cuSWezckg5JRxyPy0JNKfGcmH4QGAmf3geH1sGsbnDflxBaM+/tT+xwzKL356eOOlHguGiN6ecYoqc6UWWHyQz1e8Jfn8L2HxyzOObn2B9waK0jwdm6dNQRuKjoVhDRFOK3wV+fw7X/8nREV+2BBx7g7bffdl5H5UhNTWXUqFFutTrlMtntsPVLR4HX5HO1g2pc76gZFVV4Q5SklLrmPtjzk+PHn68edMy06BN4yd3yZUmDSVWcT41AyNXGWFBPHwPvgEtvh+Mz54033mDlypXceOONgCOp0rdvX4KDgwkODubxxx93bj9q1CiWLl3K559/flkTWM2ePZuePXsSGhqKzWajU6dOzJ07lxdffBFwdEQIDg7m008/xWw2A1C/fn3n/q+88grjxo1j9OjRzmWtW7cu8OvneOmll+jatavzeVhYmHNSLoCXX36Zb775hgULFjBy5Eh27drF559/zrJly+jSxTFEvnZt1w/NgwcP5vnnn2fDhg20adMGi8XC/Pnzc/WekjLM7At9Z8KHN8HOhfDHf6HlQE9H5W7JU44yICHVoefrno7msl1RT6m6devy7bffcvjwYZYuXUq3bo4u0CdOnCAoKKjAx5k6dSoPPfQQQ4YMoXHjxsyYMQN/f/98L8hmz55NQkIC3377Le3bt6dmzZp07NjR7YPm9ddfp1q1asyZM4c2bdpQq1YtunXrRp06ShiUWufPwFfSMtNnDjjufYLBv6JHQ5EyoNYN8MASCKoKp3fD/3V1H7aab+HyutDjNRUuL8sa3eK43/H9xT8Hc3pJNb2jVHXbzpfB4JghEspMwfN58+aRnp6ea3l6erqzNIJcgaObYHZ3+PpBR0IqpDr0+wgG/6CElDgYDNB7muNvbMI+WDze0xEVi4YNG3Ldddc5v1/t2bOHVatWMXToUACsVisvv/wyzZo1IywsjAoVKrB06VK3nkSXYrVamTdvHvfdd59zWb9+/Zg3bx42mw1wDHnr0KGDMyF1vhMnTnDs2DE6d776iTliY2PdnqekpPD444/TqFEjQkJCqFChAtu3b3ee35YtWzCZTHTs2DHP41WpUoWbb77Z2X7ff/89mZmZ3HXXXVcdq5Qikc2g07li54ufcpVvKQn+WQBbPgYMcPsH4FvwfExJcUU9pZ5//nnuvfdeHnvsMTp16kS7do7ioz/++CPXXHNNgY6RlZXFpk2bmDBhgnOZ0WikS5curFu3Ls99FixYQLt27RgxYgTfffcdlStX5t5772X8+PGYTCbnNt27d+euu+5i5cqVREdHM3z4cB56KP9fjDMzM91miEhKSgLAYrFgsVgKdD6lTc55lYrzq9QQL5MPhvQELCd2FfowuatpC8OJXXgBttCaWM91Py7tStV7o4h5pC3C6sGgxXh9dg+GE9uwz70Za69/Y0jYj/GPjzCkxAFgNxix1+uBLXYo9po3uOpEFVGsel+4eKQtatyAl5cfhsRDWI5shsg8vmQnHcfr768xAJbYh4vsvXC+YmmLRnfg9eNzGE5uJ3v/GuzV2hbda+WjMM4vKSkJu92O3W4nOTkZX19f5zqr1cqiRYvy7CUul5AcB8tfOndBDpgDoMNYaDfS8eu2yPn8Qh09Hube7HjP1OkEze68smOZ/R09ls6x2WwkJScTFBiI8WoKqduscGo3WDPBPwyCq+X92pdh6NChjBo1ivfff585c+ZQp04dZxLmjTfe4O2332batGk0a9aMgIAAxowZQ1ZWVoGPv3TpUo4ePUr//v3dllutVpYvX07Xrl3x8/PLd/+LrQOc7Xn+EMT8PpcvnDzi8ccfZ9myZbz55pvUrVsXPz8/7rzzTuf5Xeq1AR588EHuv/9+3nrrLebMmUP//v3x9y/lw+Pl8l03Cnb/CAfXwNePwJDFjvqwnpR0HL5/1PH4+jEX701fgl1RK955551cf/31HD9+3K2XUufOnbn99tsLdIxTp05htVqJiIhwWx4REcGOHTvy3Gffvn38/PPPDBgwgEWLFrFnzx6GDx+OxWJh4sSJzm2mT5/O2LFjefrpp9m4cSOPPvoo3t7eDBo0KM/jTp482dm19Hw//vhjmf/AWbZsmadDKJAOvtUIS93DX4tmcSSsfZG8xpW0RZ34RTQFjmX4semC2malXWl5bxQHT7SFV+QoWqe/S3jyNry+G+ZcnuEVxMGKN3Kg0k1keFeE7amwfXGxxaX3hUtxt0XrgMZUObuJfQvfZkdU31zrGx37nPq2bE5VaMCaP47CH0eLLbaibosWQbHUSFjFsR8m8UeNR4r0tfKSlpZ21ccICQnBYDBgMBjchqzkMBgMeV6LSD4sGbD+P7Dq35CV4ljW/B5H7Y+y0EtQik6N6xwTgPw6BX4Y6+iRH1rj8o9jMLgPobPZwGx1LLuapNTZo479vYIcPaGNV/+lt1+/fowePZr58+fz0UcfMWzYMAznfsxas2YNt912m7OXk81mY9euXTRu3LjAx581axZ33303zzzzjPMYKSkpvPPOO8yaNYuuXbsSExPDvHnzsFgsuXpLBQYGUrNmTZYvX85NN92U6/iVKzsm3jl+/LizA8T5Rc8vZs2aNQwePNj5HTUlJYUDBw441zdr1gybzcbKlSudw/cu1KtXLwICApg+fTpLlizh119/LdBrSxljNMHtM2B6eziyAVa/BR2f8Fw8djt8NwLSzzh+rLzxac/FcpWu+FMuMjKSyMhIjhxx1NOpWrXqZY07vhI2m43w8HA+/PBDTCYTrVq14ujRo7zxxhvOpJTNZiM2NpZJkyYBcM0117Bt2zZmzJiRb1JqwoQJjB071vk8KSmJatWq0a1bt8sajliaWCwWli1bRteuXfPsRlvSGM1rYcMeWlTKJqZHr0I99tW0hXHRT3AMopq0p9eNhRuXp5S290ZR8nhbWG/FtvhxjH9+jK1qW2yxD2Bq2JvaJm+Ku6y+x9uiBPFUWxi2psCCTdS37qR2rws+b7JS8XrX8UtZSI9n6NWgeD6PiqstDEfDYW4PqiVtIuqm68AvpMheKy85Paivxi+//ILdbqdTp0589dVXhIWFOdd5e3tTo0YNqlSpcpEjCOC4CN/xA/z4rGsIfXSso4ZG1diL7iri1HE87Fvh+GL59cMweKHnezyAo4h56gnH45BqhZKQAqhQoQL9+/dnwoQJJCUlMXjwYOe6evXq8eWXX7J27VpCQ0OZOnUq8fHxBU5KnTx5ku+//54FCxbQtGlT4FyvsaQk7r//fvr27UtCQgIjR47k3Xff5e6772bChAkEBwezfv162rRpQ4MGDXjhhRf417/+RXh4OD179iQ5OZk1a9YwatQo/Pz8uPbaa3nttdeoVasWJ06c4Nlnny1QfPXq1ePrr7+md+/eGAwGnnvuOeeQQoCaNWsyaNAgHnjgAd555x2aN2/OwYMHOXHiBP369QPAZDIxePBgJkyYQL169ZyjhKQcCqkOvd6Abx6Bla9B3c4Q3dIzsWyYCXuXg5evY8KkUjyJxxV90tlsNl555RX+/e9/k5Li+HUqMDCQcePG8cwzzxSoy2qlSpUwmUzEx8e7LY+PjycyMjLPfaKiojCbzc6hegCNGjUiLi6OrKwsvL29iYqKyvUh2qhRI7766qt8Y/Hx8cHHxyfXcrPZXOa/fJWac6zeFjbMwHRsE6YiiveK2uLMfgBMlesVWVyeUmreG8XAY21hNsPt/4Ger2H0DbqyIoCFTO8Ll2Jvi0a94AcvDCd3YE465D65wpYvISMRQmvi1fiWy542/GoVeVvUuBbCm2A48Tfm7V9D2+LtLVUY55YzVGb//v1Ur17d2UtBLkP8345irvvP9VIIjIIuL0Kzu66uZ4qUPyYvxzC+GR3g8HpY9Sbc+JRnY7LZHIWKAfzCHLOuFqKhQ4cya9YsevXq5ZYAf/bZZ9m3bx/du3fH39+fhx9+mD59+nD27NkCHfejjz4iICAgz3pQnTt3xs/Pj//97388+uij/PzzzzzxxBN07NgRk8lEixYtaN/eMQJi0KBBZGRk8NZbb/H4449TqVIl7rzTNbRy9uzZDB06lFatWtGgQQOmTJnirGt8MVOnTuWBBx7guuuuo1KlSowfPz7XDw3Tp0/n6aefZvjw4Zw+fZrq1avz9NPuvU6GDh3KpEmTGDJkSIHaRcqwmP6wc7Fj0oSvH3ZMmlDcsx2f3AnLnnM87voShDcs3tcvZFeUlHrmmWeYNWsWr732mvODZPXq1bzwwgtkZGTw6quvXvIY3t7etGrViuXLl9OnTx/Akexavnw5I0eOzHOf9u3bM3/+fGw2mzPxtWvXLqKiovD29nZus3PnTrf9du3aRY0aV9AtV0qOnGLn8dsgK63kTHOe4EhKaeY9KVKlsGChFAG/UKjZAfb9Atu/d9QOAMcXmfXTHY/bDiv2hFSxyCl4vvgJR8HzNg+76qiVMjVq1GDVqlV88MEH7Nu3jy+++ILo6Gj++9//UqtWLa6//npPh1gyJR6GD24AWzaYfKD9o9B+DPhU8HRkUlqF1oRb3oKvhsLK16FWR6jhwR4wKXGQneHoHRUUXeiHb9eunVtNphxhYWF8++23F913xYoV+a4bN24c48aNy3Odt7c3Z86ccT6PiYlh6dKl+R7rkUce4ZFH8v7RoVGjRqxdu9Zt2fnnc+ONN+Z5fjVr1uTnn392WzZixAi3576+vkydOpWpU6fmG9vRo0cxm80MHFjCZl2T4mcwOD47Dv/mmJho2XNw87+L7/Wzs+DrhxyfF3U6lYnZlq/oZ6V58+bxf//3fwwbNoyYmBhiYmIYPnw4M2fOZO7cuQU+ztixY5k5cybz5s1j+/btDBs2jNTUVGcGeuDAgW6F0IcNG0ZCQgKjR49m165dLFy4kEmTJrl9sDz22GOsX7+eSZMmsWfPHubPn8+HH36Y68NHSpngqlAh0nExevxPT0fjYEmHJMfw1cIuvi4ikqecWfi2f+9atmcZnN4DPkFwzQDPxFUcYvqBlx+c+AcOb/B0NFfsq6++onv37vj5+bF582bnRCtnz551lh6QPIRUc/SIatwHRm50zIKkhJRcrWZ3OmqR2W2OL3npiZ6JIysNUs6NHgmuVjKGEgrgmBDryJEjvPDCC9x111256iFLOeUfBn3OzXi88f9gdzHWGV35muP7sF8o3PafMtFT+IrOICEhgYYNc3cRa9iwIQkJCQU+Tv/+/XnzzTd5/vnnadGiBVu2bGHJkiXO/+yHDh3i+PHjzu2rVavG0qVL2bhxIzExMTz66KOMHj2ap55ydbdt3bo133zzDZ988glNmzbl5ZdfZtq0aQwYUIYv1MsDg8FVK+LIRs/GkiOnloVPMPhX9GgoIlJONLwFMMDR3yHp3KxP69533LccCD6BHgutyPmFQNM7HI83zfVkJFfllVdeYcaMGcycOdNtWGD79u3ZvHmzByMrBW59D/rNu7Ki1CL56fWGo9fU2cPwwxhH3bLiZLdB4kHHY9+QYq+ZJxf3ySefUKNGDRITE5kyZYqnw5GSpE4nRw91cBQcTz1d9K95aL2jwDrALdPKzMQeV5SUat68Oe+9916u5e+99x4xMXlMU30RI0eO5ODBg2RmZvLbb7/Rtq1rqucVK1bk6nnVrl071q9fT0ZGBnv37uXpp592qzEFcMstt7B161YyMjLYvn07Dz1U+ru0Ca4hfCUlKXV6r+M+rFapHUYiIqVMYKTrs3DHQojbBvtXgsFY7HWWPKLVYMf93187ZpsphXbu3MkNN9yQa3lwcDCJiYnFH1Bpot4jUhR8AqHvbMewub+/gS3zi/f1U+Jdw/aCqxbva8slDR48GKvVyqZNm4iOLvxhlVLKdZkIlRs6/h9//2jRJrUzkhw1rOw2Rw/PJn2K7rWK2RUlpaZMmcLs2bNp3LgxQ4cOZejQoTRu3Ji5c+fy5ptvFnaMIg7OpNTvno0jR8K5pJTqSYlIcTp/CF9OLanGtzlmhCnrqraG8MaOL3B/feHpaK5IZGQke/bsybV89erV1K6toeAiHlG1Fdx0rrD1oidcPzwWNUs6JOcM26sKJk0kIlKqmP0cM98ZzY6ZYbd8XHSvtWSCo1dlcHXHjLNlyBUlpTp27MiuXbu4/fbbSUxMJDExkTvuuIO///6b//73v4Udo4hDlRZgMEHyMTh71NPRQMI+x32YklIiUowanktKHVgNWz93PL62nNRNzCl4Do4hfMU9zKYQPPTQQ4wePZrffvsNg8HAsWPH+Pjjj3n88ccZNmyYp8MTKb/aj3FMJmFJdRQ/z84q2tdzDtuzO2ba8w0p2tcTkaIRFQOdnnE8XjzeNRFWYfpnAWz5H2CAOz4o9Nk5Pe2Kq2JVqVKFV199la+++oqvvvqKV155hTNnzjBr1qzCjE/ExTsAIpo4HpeEIXzO4Xv6ZVtEilHFOhDeBOxWsGY5eg9Va+3pqIpPTD/w8oUTf5eMvwWX6amnnuLee++lc+fOpKSkcMMNN/Dggw/yyCOPMGrUKE+HJ1J+GU1w+weO4sHH/oBfXina10s54egpZTA5ipurFIRI6XXdo1D9OshKgW/+Bdbswjt2chx8P9rxuP1oqHFd4R27hCj9pdqlfClJdaVysuAavicixa1Rb9fja4d7Lg5P8AuFJqW34LnBYOCZZ54hISGBbdu2sX79ek6ePMnLL7/s6dBEJDgabn3X8XjN27D3l6J5HUu644smaNieSFlgNMHtM8A7EA6vhzVvFc5x7Xb4biSkJ0BkM7jpmcI5bgmjipFSulRtDb/P8nxdKUs6JB1xPFZPKREpbk1uh1VvOupINbrV09EUv1aD4c/5sO1r6D6pVMxW9cADDxRou9mzZxdxJCJyUY16Q6shsGmOo8fDsLUQUIizLNvtkHgIsINPkCPRLiKlX2gNx2ye3/4LVrwGdTpDdMurO+bG/4M9y8Dk46hd5eVdOLGWMOopJaVLTk+p41uKfqz/xeT0kvIJBv9CvFARESmI8IbwyK8wZHH5nJGsWhuo3Aiy02Fr6Sh4PnfuXH755RcSExM5c+ZMvjcRKQG6T4JK9SElDhaMLNz6daknwJJWqobt3XjjjYwZM8bTYYiUfM3vdkw+Y8t2zJSXlXblxzq5C358zvG464sQ3qhwYiyBLispdccdd1z09thjjxVVnCIOFes4flHKzoD4bZ6LI6fIecXapeJiQkTKoIgmEBjp6Sg84/yC57/PKRUFz4cNG8bZs2fZv38/N910E7NmzeKbb77JdROREsDbH/rOApM37Fzk6KVfGLIzIOm443FQlSLv9dC7d2969OiR57pVq1ZhMBj466+/rvp15s6di8FgcN5MJhOhoaH83//9HwDHjx/n3nvvpX79+hiNxgInuE6ePMmwYcOoXr06Pj4+REZG0r17d9asWXPVMYsUCYMBbpkGFSLh9G5Y9vyVHSc7C75+yPHjW+0boc0jhRlliXNZSang4OCL3mrUqMHAgQOLKlYRx3/0klBXKkFFzkVEPKp5//MKnnt4SHcBvP/++xw/fpwnn3yS77//nmrVqtGvXz+WLl2KvRQk1UTKnagY6PKi4/HSZ1wT3Fyp84fteQcWS0/7oUOHsmzZMo4cOZJr3Zw5c4iNjSUmJqZQXisoKIjjx49z/Phxjh49yo4dOxgwYAAAmZmZVK5cmWeffZbmzZsX+Jh9+/bljz/+YN68eezatYsFCxZw4403cvr06UKJOS9ZWR4ciSFlg38Y9PmP4/HGmbD7p8s/xsrXHSODfEOgz3Qwlu0Bbpd1dnPmzCnQTaRIlYik1LmeUmEqci4i4hF+oY7aWlBqCp77+Phwzz33sGzZMv755x+aNGnC8OHDqVmzJikpKZ4OT0Qu1PZfULeLo4fT0mevrldm6inISgWDEUKKZ9jeLbfcQuXKlZk7d67b8pSUFL744guGDh3K6dOnueeee4iOjsbf359mzZrxySefXPZrGQwGIiMjnbeIiAj8/PwAqFmzJm+//TYDBw4kOLhgU9knJiayatUqXn/9dW666SZq1KhBmzZtmDBhArfeeqvbdo888ggRERH4+vrStGlTfvjhB+f6r776iiZNmuDj40PNmjX597//7fY6NWvW5OWXX2bgwIEEBQXx8MMPA7B69Wo6dOiAn58f1apV49FHHyU1NfWy20XKqbqdXb2bvhsOqZeRSD30G6ye6nh8y1uOXpVlXNlOuUnZVDXWce/JpNRp9ZQSEfG4nCF8276CjLMeDeVyGY1GDAYDdrsdq9Xq6XBEJC9Go6OXQkBlSNgNGYlXdpzsTEg+5ngcVAW8fAotxIvx8vJi4MCBzJ07161H5hdffIHVauWee+4hIyODVq1asXDhQrZt28bDDz/M/fffz4YNG4olxvxUqFCBChUq8O2335KZmZnnNjabjZ49e7JmzRr+97//8c8///Daa69hMpkA2LRpE/369ePuu+9m69atvPDCCzz33HO5knRvvvkmzZs3548//uC5555j79699OjRg759+/LXX3/x2WefsXr1akaOHFnUpy1lSdcXoVIDSImHH0YXLKmdmQzfPAx2G8TcDU3vKPo4SwAlpaT0iW4FGODMAUg56ZkYnDWl1FNKRMRjqrWFyg0dNRf++tzT0VxSZmYmn3zyCV27dqV+/fps3bqV9957j0OHDlGhQgVPhycieakQDn1mOB5nJjtu50mzpDlv6dnpbs8zrZmuYXt2G2lGL9LM/m7b5NwysjPyPe75t8v1wAMPsHfvXlauXOlcNmfOHPr27UtwcDDR0dE8/vjjtGjRgtq1azNq1Ch69OjB559f3mfq2bNnnYmkoKAgGjRocNmxns/Ly4u5c+cyb948QkJCaN++PU8//bRbDayffvqJDRs28PXXX9O1a1dq167NLbfcQs+ePQGYOnUqnTt35rnnnqN+/foMHjyYkSNH8sYbb7i9VqdOnRg3bhx16tShTp06TJ48mQEDBjBmzBjq1avHddddxzvvvMNHH31ERob7v5NIvsx+cMeHYDTD9u9hy/xL77PkKcd33ODq0GtKkYdYUpTDKXuk1PMNhsoN4OQOOPo7NOhZvK9vSYeko47HGr4nIuI5OQXPlzzlGMLX+sESO/nE8OHD+fTTT6lWrRoPPPAAn3zyCZUqVfJ0WCJSEPW6QPN7HY+TjkOFEDCZAWg7v22+u3WI7sB/2r0EWSmAkRt/eoB0a95JjdiIWOb0cJVB6fFVD85k5p6Rc+ugrZcVesOGDbnuuuuYPXs2N954I3v27GHVqlW89NJLAFitViZNmsTnn3/O0aNHycrKIjMzE39//8t6ncDAQDZv3gw4ejClpRU8gbZq1SpnIgnggw8+YMCAAfTt25ebb76ZVatWsX79ehYvXsyUKVP4v//7PwYPHsyWLVuoWrUq9evXz/O427dv57bbbnNb1r59e6ZNm4bVanX2qIqNjXXb5s8//+Svv/7i448/di6z2+3YbDb2799Po0ZldxY0KWRVWsBNT8PyF2HxeKjZHkJr5r3t9h/gj/8BBrh9huM7bzmhpJSUTlVjHUmpIxuLPymVsN9x7xPsKGQnIiKeE9Mflk10zMh6dDNUbeXpiPI0Y8YMqlevTu3atVm5cqVbr4Xzff3118UcmYgUSLuR8M8msGdD4kHHD5OXSoLbba4fMoOiPJY0Hzp0KKNGjeL9999nzpw51KlTh44dOwLwxhtv8PbbbzNt2jSaNWtGQEAAY8aMueyC30ajkbp16wKOpFRSUlKB942NjWXLli3O5xEREc7Hvr6+dO3ala5du/Lcc8/x4IMPMnHiRAYPHuysWXW1AgIC3J6npKTwyCOP8Oijj+batnr16oXymlKOtB8Nu5fBobXw9SMwZBEYTe7bJMfD9+feb+0fdSSvyhElpaR0qtrakUn2RF0p59C92iX2F3kRkXLDPww6P+f45TGqcGaRKgoDBw7EoL8ZIqWXl/e5GfNsjiF8qSehQji/3fsb4EjEJCcnExgYiNFoBLsdU+JBRz0pcwAEVGZFvxX5Ht5ocK+qsqTvkkILvV+/fowePZr58+fz0UcfMWzYMOfn0Zo1a7jtttu47777nOexa9cuGjduXGivfyl+fn7OhNalNG7cmG+//RaAmJgYjhw5wq5du/LsLdWoUSPWrFnjtmzNmjXUr1/f2UsqLy1btuSff/4pcEwiF2U0OXo+TW8Ph9fDmmnQYZxrvd0O342AtNMQ0QxuesZjoXqKklJSOuXMwHd0M9isubPNRSlBRc5FREqU60Z5OoJLurCwroiUQiazY+heRhwkHQPvCvh7O4a52Ww2sr2y8Tf7O5JSaacdCSkMEFIdDAb8zQUfEnc5215KhQoV6N+/PxMmTCApKYnBgwc719WrV48vv/yStWvXEhoaytSpU4mPjy/0pFROT6iUlBROnjzJli1b8Pb2zvd1Tp8+zV133cUDDzxATEwMgYGB/P7770yZMsU5JK9jx47ccMMN9O3bl6lTp1K3bl127NiBwWCgR48ejBs3jtatW/Pyyy/Tv39/1q1bx3vvvcd//vOfi8Y6fvx4rr32WkaOHMmDDz5IQEAA//zzD8uWLeO9994r1HaRciK0hqNG1LfD4JdJUKczVG4CgHHzHNizDEw+0HdmsU2EUJKo0LmUTpUbgncFxxj9kzuK97WdM++pnpSIiIhIueIXcq7Wi91RkNiWx+yZ1iw4e27YXmAUmH2LMcC8DR06lDNnztC9e3eqVHFNMf/ss8/SsmVLunfvzo033khkZCR9+vQp9Ne/5ppruOaaa9i0aRPz58/nmmuuoVevXvluX6FCBdq2bctbb73FDTfcQNOmTXnuued46KGH3BJDX331Fa1bt+aee+6hcePGPPnkk84ZTVu2bMnnn3/Op59+StOmTXn++ed56aWX3JJyeYmJiWHlypXs2rWLDh06cM011/D888+7tZvIZWt+DzS6FWzZ8PXDYEmnQsZxjD9NdKzv8gKEl896ZeopJaWT0QTRLWH/r44hfBFNiu+1NfOeiIiISPlkMDhmxsraAdZMR82okAvqDCUeBrsVzP6O2ftKgHbt2mHPY0r6sLAw53C4/KxYseKi6wcPHnzJRE9er30xPj4+TJ48mcmTJ190u7CwMGbPnp3v+r59+9K3b9981x84cCDP5a1bt+bHH38sUKwiBWIwQO+34fAGOLUT40/P0/LgSgzZ6VCrI7T9l6cj9Bj1lJLSK2cIX3HXlcpJSmn4noiIeND7779PzZo18fX1pW3btmzYsKFA+3366acYDIZcvSEGDx6MwWBwu/Xo0aMIIhcp5UxejuE44Biml+6aJc+QcQYykzh/2J6ICOCog9nnfQBMm+cQmrYPu28w9JkOxvKbmim/Zy6lnzMp9XvxvaYl3TWLiobviYiIh3z22WeMHTuWiRMnsnnzZpo3b0737t05ceLERfc7cOAAjz/+OB06dMhzfY8ePTh+/Ljz9sknnxRF+CKln08gVDg3S1ziYbBZMNiyzxu2FwnmwpkdTkTKkLpdoM3DzqfWnm9CcLQHA/I8JaWk9IqOddyf3AHpicXzmgn7Hfc+wY5Mt4iIiAdMnTqVhx56iCFDhtC4cWNmzJiBv7//RYexWK1WBgwYwIsvvkjt2nn39vXx8SEyMtJ5Cw0NLapTECn9AiMdQ/TsVgyJB/GznMZgt4KXX4kZticiJVDXl7A1vYsdkbdjb3y7p6PxONWUktKrQmXHFOBnDsCxzVCnU9G/prOeVG11xxYREY/Iyspi06ZNTJgwwbnMaDTSpUsX1q1bl+9+L730EuHh4QwdOpRVq1bluc2KFSsIDw8nNDSUTp068corr1CxYsV8j5mZmUlmZqbzeVJSEgAWiwWLxXK5p1Yq5JxXWT2/y1Ge2sJisWC327HZbNhsNteKkBoYTu3EkJWKN2DHgD2kGtgBuy2/w5VpOfWjctqrPCtIW9hsNux2OxaLBZOpGGcU94Dy9JlxcV5Yer3DzmXLqF6G26Kg/85KSknpVrW1Iyl15PdiSkrlzLynelIiIuIZp06dwmq1EhER4bY8IiKCHTvynpF29erVzJo1yzkte1569OjBHXfcQa1atdi7dy9PP/00PXv2ZN26dfl+UZo8eTIvvvhiruU//vgj/v6FN6V9SbRs2TJPh1BilIe28PLyIjIykpSUFLKystzWmc1hBGSdAiDTK4iMNAtQdr9oFlRycrKnQygxLtYWWVlZpKen8+uvv5KdnV2MUXlOefjMKKiy3BZpaWkF2k5JKSndqraGrV8UX7Hz0zlJKdWTEhGR0iE5OZn777+fmTNnUqlSpXy3u/vuu52PmzVrRkxMDHXq1GHFihV07tw5z30mTJjA2LFjnc+TkpKoVq0a3bp1IygoqPBOogSxWCwsW7aMrl27YjabPR2OR5WntsjMzOTQoUMEBATg53dhraggbMkmLBmpmCtWx9tQviuk2O12kpOTCQwMxFDORxYUpC3S09Px8/OjY8eO+Pj4FHOExas8fWZcSnloi5ze05eipJSUblXP1ZU6shHs9qIfUuccvqeklIiIeEalSpUwmUzEx8e7LY+PjycyMjLX9nv37uXAgQP07t3buSxnGImXlxc7d+6kTp3cf9dq165NpUqV2LNnT75JKR8fnzy/RJnN5jJ7kZ2jPJxjQZWXtjAYDGRnZ2PMY5YsW2Ak6fYkggzGPNeXJzmfLwaDQW1RgLbIyMjAYDDg5+dX5ofv5SgvnxkFUZbboqDnpaSUlG4RzcDk45iKN2Ff0SeLcpJSGr4nIiIe4u3tTatWrVi+fDl9+vQBHF98li9fzsiRI3Nt37BhQ7Zu3eq27NlnnyU5OZm3336batWq5fk6R44c4fTp00RFRRX6OYiUNl5eXvj7+3Py5EnMZnOuBIPNZiMrK4uMjAwlYtQWThdrC7vdTlpaGidOnCAkJKTcJKRELqSklJRuXt5QpQUc/s3RW6ook1KWdEg6N82vhu+JiIgHjR07lkGDBhEbG0ubNm2YNm0aqampDBkyBICBAwcSHR3N5MmT8fX1pWnTpm77h4SEADiXp6Sk8OKLL9K3b18iIyPZu3cvTz75JHXr1qV79+7Fem4iJZHBYCAqKor9+/dz8ODBXOvtdrtzGJaGrKktchSkLUJCQvLs5SpSXigpJaVf1daupFTzuy+9/ZVK2O+49wkG/7Ciex0REZFL6N+/PydPnuT5558nLi6OFi1asGTJEmfx80OHDl1WDwWTycRff/3FvHnzSExMpEqVKnTr1o2XX365zNc4ESkob29v6tWrl6vQOTjqw/z666/ccMMNZXYoTkGpLVwu1RZms1k9pKTcU1JKSr/z60oVpZyZ9yrWLvraVSIiIpcwcuTIPIfrAaxYseKi+86dO9ftuZ+fH0uXLi2kyETKLqPRiK+vb67lJpOJ7OxsfH19y30iRm3horYQubTyPchXyoaqrR33cdsgq2DTTl4RZz0pDd0TERERERERuVpKSknpFxQNgVFgt8LxLUX3OqfP9ZRSkXMRERERERGRq6aklJR+BkPxDOHL6SlV1DP8iYiIiIiIiJQDSkpJ2ZAzhK84klLqKSUiIiIiIiJy1ZSUkrIhJyl1eCPY7YV//Kw0SDrqeKyaUiIiIiIiIiJXTUkpKRuiWoDBBClxruRRYTpzwHHvGwz+YYV/fBEREREREZFyRkkpKRu8/SGyqeNxUQzhSzivyLnBUPjHFxERERERESlnlJSSssNZV+r3wj+2s56Uhu6JiIiIiIiIFAYlpaTsKMpi56fP6yklIiIiIiIiIldNSSkpO3KSUse2QHZW4R47p6dURfWUEhERERERESkMJSIp9f7771OzZk18fX1p27YtGzZsuOj2iYmJjBgxgqioKHx8fKhfvz6LFi1yrn/hhRcwGAxut4YNGxb1aYinhdUGv1CwZkL81sI9tobviYiIiIiIiBQqL08H8NlnnzF27FhmzJhB27ZtmTZtGt27d2fnzp2Eh4fn2j4rK4uuXbsSHh7Ol19+SXR0NAcPHiQkJMRtuyZNmvDTTz85n3t5efxUpagZDI7eUrt/dNSVim5VOMfNSnPN6KfheyIiIiIiIiKFwuOZmqlTp/LQQw8xZMgQAGbMmMHChQuZPXs2Tz31VK7tZ8+eTUJCAmvXrsVsNgNQs2bNXNt5eXkRGRlZpLFLCeRMSm2Eto8UzjHPHHDc+waDf1jhHFNERERERESknPNoUiorK4tNmzYxYcIE5zKj0UiXLl1Yt25dnvssWLCAdu3aMWLECL777jsqV67Mvffey/jx4zGZTM7tdu/eTZUqVfD19aVdu3ZMnjyZ6tWr53nMzMxMMjMznc+TkpIAsFgsWCyWwjjVEifnvMra+Rkir8ELsB/eSHYBz+1SbWE4sQsvwBZaC2t2diFFWnKV1ffGlVBbuKgtXNQWLuWlLcr6+YmIiIh4ikeTUqdOncJqtRIREeG2PCIigh07duS5z759+/j5558ZMGAAixYtYs+ePQwfPhyLxcLEiRMBaNu2LXPnzqVBgwYcP36cF198kQ4dOrBt2zYCAwNzHXPy5Mm8+OKLuZb/+OOP+Pv7F8KZllzLli3zdAiFysuaRi8MGBIP8NN3n5JlDirwvvm1Rd34RTQBjmX4sum82mVlXVl7b1wNtYWL2sJFbeFS1tsiLS3N0yGIiIiIlEkeH753uWw2G+Hh4Xz44YeYTCZatWrF0aNHeeONN5xJqZ49ezq3j4mJoW3bttSoUYPPP/+coUOH5jrmhAkTGDt2rPN5UlIS1apVo1u3bgQFFTypUZpYLBaWLVtG165dncMgy4xjb8GpnXRtFIK9fo9Lbn6ptjAtXAbHIKrJ9fTq2KsoIi5RyvR74zKpLVzUFi5qC5fy0hY5PahFREREpHB5NClVqVIlTCYT8fHxbsvj4+PzrQcVFRWF2Wx2G6rXqFEj4uLiyMrKwtvbO9c+ISEh1K9fnz179uR5TB8fH3x8fHItN5vNZfoiG8roOVZrDad24hX3BzTpXeDd8m2LxAMAmCrXw1TW2uoiyuR74wqpLVzUFi5qC5ey3hZl+dxEREREPMnoyRf39vamVatWLF++3LnMZrOxfPly2rVrl+c+7du3Z8+ePdhsNueyXbt2ERUVlWdCCiAlJYW9e/cSFRVVuCcgJVPV1o77IxsL53gJ+xz3YXUK53giIiIiIiIi4tmkFMDYsWOZOXMm8+bNY/v27QwbNozU1FTnbHwDBw50K4Q+bNgwEhISGD16NLt27WLhwoVMmjSJESNGOLd5/PHHWblyJQcOHGDt2rXcfvvtmEwm7rnnnmI/P/GAnKTU0c1gs17dsbLSIOmo43FY7as7loiIiIiIiIg4ebymVP/+/Tl58iTPP/88cXFxtGjRgiVLljiLnx86dAij0ZU7q1atGkuXLuWxxx4jJiaG6OhoRo8ezfjx453bHDlyhHvuuYfTp09TuXJlrr/+etavX0/lypWL/fzEAyo3BO9AyEqGkzsgosmVH+vMfse9bzD4hxVOfCIiIiIiIiLi+aQUwMiRIxk5cmSe61asWJFrWbt27Vi/fn2+x/v0008LKzQpjYwmiG4J+1c6hvBdTVLq/KF7BkPhxCciIiIiIiIinh++J1IkCquu1Om9jnsN3RMREREREREpVEpKSdnkTEr9fnXHyekpVVFFzkVEREREREQKk5JSUjZVjXXcn9wB6YlXfhzn8D31lBIREREREREpTEpKSdkUUAlCazkeH9t85cdxDt9TTykRERERERGRwqSklJRdVzuELysNko85Hmv4noiIiIiIiEihUlJKyq6rLXZ+Zr/j3jcY/EILJyYRERERERERAZSUkrIsp67UkY1gt1/+/s56UnXAYCi8uERERERERERESSkpwyKagpcvpJ9xJZguh7OelIqci4iIiIiIiBQ2JaWk7PLyhqgWjsdXMoQv4VxSSvWkRERERERERAqdklJStuUM4Tu84fL3TThXU0oz74mIiIiIiIgUOiWlpGy7mmLnGr4nIiIiIiIiUmSUlJKyLScpFf83ZKUWfL+sNEg+5nis4XsiIiIiIiIihU5JKSnbgqMhsArYrXBsS8H3O3Nu6J5vMPiFFkloIiIiIiIiIuWZklJS9uXUlbqcIXw5s/WF1QGDofBjEhERERERESnnlJSSsu9K6kqd1sx7IiIiIiIiIkVJSSkp+85PStntBdsnQUXORURERERERIqSklJS9kU1B6MXpMTD2SMF2yfhXE2pMPWUEhERERERESkKSkpJ2eftDxFNHY8LOoTvtHpKiYiIiIiIiBQlJaWkfHAO4fv90ttmpUHyMcdj1ZQSERERERERKRJKSkn5cDnFzs+cG7rnGwL+YUUWkoiIiIiIiEh5pqSUlA9VYx33x/+E7MyLb6uheyIiIiIiIiJFTkkpKR/CaoNfGFgzIW7bxbdN2Oe419A9ERERERERkSKjpJSUDwZDwYfwJainlIiIiIiIiEhRU1JKyo8CJ6XO1ZQKU08pERERERERkaKipJSUHzl1pS6VlMqpKaXheyIiIiIiIiJFRkkpKT+iWwIGSDwIKSfy3iYrDZKPOR5r+J6IiIiIiIhIkVFSSsoP32Co3NDx+MjveW9z5tzQPd8Q8A8rlrBEREREREREyiMlpaR8udQQvtMqci4iIiIiIiJSHJSUkvLlUsXOE/Y57lVPSkRERERERKRIKSkl5UtOUuroZrBZc69PyOkppaSUiIiUbO+//z41a9bE19eXtm3bsmHDhgLt9+mnn2IwGOjTp4/bcrvdzvPPP09UVBR+fn506dKF3bt3F0HkIiIiIg5KSkn5UrkBeAeCJRVObM+9/vS5nlIaviciIiXYZ599xtixY5k4cSKbN2+mefPmdO/enRMn8pnI45wDBw7w+OOP06FDh1zrpkyZwjvvvMOMGTP47bffCAgIoHv37mRkZBTVaYiIiEg5p6SUlC9G07lZ+Mh7CJ+G74mISCkwdepUHnroIYYMGULjxo2ZMWMG/v7+zJ49O999rFYrAwYM4MUXX6R2bfcfX+x2O9OmTePZZ5/ltttuIyYmho8++ohjx47x7bffFvHZiIiISHnl5ekARIpd1dawf6VjBr7YIa7lljRIPuZ4rJ5SIiJSQmVlZbFp0yYmTJjgXGY0GunSpQvr1q3Ld7+XXnqJ8PBwhg4dyqpVq9zW7d+/n7i4OLp06eJcFhwcTNu2bVm3bh133313nsfMzMwkMzPT+TwpKQkAi8WCxWK5ovMr6XLOq6ye3+VQW7ioLVzUFi5qC3dqD5fy0BYFPTclpaT8ya/Y+ZkDjnvfEPAPK86IRERECuzUqVNYrVYiIiLclkdERLBjx44891m9ejWzZs1iy5Ytea6Pi4tzHuPCY+asy8vkyZN58cUXcy3/8ccf8ff3v9hplHrLli3zdAglhtrCRW3horZwUVu4U3u4lOW2SEtLK9B2SkpJ+VM11nF/aiekJ4JXAAAGDd0TEZEyKDk5mfvvv5+ZM2dSqVKlQj32hAkTGDt2rPN5UlIS1apVo1u3bgQFBRXqa5UUFouFZcuW0bVrV8xms6fD8Si1hYvawkVt4aK2cKf2cCkPbZHTe/pSlJSS8iegEoTWgjP74egmqHEDcF5SSkP3RESkBKtUqRImk4n4+Hi35fHx8URGRubafu/evRw4cIDevXs7l9lsNgC8vLzYuXOnc7/4+HiioqLcjtmiRYt8Y/Hx8cHHxyfXcrPZXGYvsnOUh3MsKLWFi9rCRW3horZwp/ZwKcttUdDzUqFzKZ+cQ/h+dy4ynMlJSqmnlIiIlFze3t60atWK5cuXO5fZbDaWL19Ou3btcm3fsGFDtm7dypYtW5y3W2+9lZtuuoktW7ZQrVo1atWqRWRkpNsxk5KS+O233/I8poiIiEhhUE8pKZ+qtoatn7vXlVJPKRERKSXGjh3LoEGDiI2NpU2bNkybNo3U1FSGDHFM4DFw4ECio6OZPHkyvr6+NG3a1G3/kJAQALflY8aM4ZVXXqFevXrUqlWL5557jipVqtCnT5/iOi0REREpZ0pET6n333+fmjVr4uvrS9u2bdmwYcNFt09MTGTEiBFERUXh4+ND/fr1WbRoUZ7bvvbaaxgMBsaMGVMEkUupVe28Yud2OwCGM/sdy1RTSkRESrj+/fvz5ptv8vzzz9OiRQu2bNnCkiVLnIXKDx06xPHjxy/rmE8++SSjRo3i4YcfpnXr1qSkpLBkyRJ8fX2L4hREREREPN9T6rPPPmPs2LHMmDGDtm3bMm3aNLp3787OnTsJDw/PtX1WVhZdu3YlPDycL7/8kujoaA4ePOj8xe98Gzdu5IMPPiAmJqYYzkRKlYim4OULGYmQsBeTLRND8rmLd/WUEhGRUmDkyJGMHDkyz3UrVqy46L5z587NtcxgMPDSSy/x0ksvFUJ0IiIiIpfm8Z5SU6dO5aGHHmLIkCE0btyYGTNm4O/vz+zZs/Pcfvbs2SQkJPDtt9/Svn17atasSceOHWnevLnbdikpKQwYMICZM2cSGhpaHKcipYnJDFWuAcBwdBMBmeeKxfqGgH+Y5+ISERERERERKSc82lMqKyuLTZs2MWHCBOcyo9FIly5dWLduXZ77LFiwgHbt2jFixAi+++47KleuzL333sv48eMxmUzO7UaMGMHNN99Mly5deOWVVy4aR2ZmJpmZmc7nOVMXWiwWLBbL1ZxiiZVzXmX1/ArCWKUlpkPrsB/eQECmY9pqW1htrOW4TUDvjfOpLVzUFi5qC5fy0hZl/fxEREREPMWjSalTp05htVqd9Q9yREREsGPHjjz32bdvHz///DMDBgxg0aJF7Nmzh+HDh2OxWJg4cSIAn376KZs3b2bjxo15HuNCkydP5sUXX8y1/Mcff8Tf3/8yz6p0WbZsmadD8JioRCNtgNSdKwgIbQvA0XQfNudTn6y8Kc/vjQupLVzUFi5qC5ey3hZpaWmeDkFERESkTPJ4TanLZbPZCA8P58MPP8RkMtGqVSuOHj3KG2+8wcSJEzl8+DCjR49m2bJlBS7MOWHCBMaOHet8npSURLVq1ejWrRtBQUFFdSoeZbFYWLZsGV27dsVsNns6HM9IugbefZfgjCOkpEcBUKXp9UTe0MvDgXmW3hsuagsXtYWL2sKlvLRFTg9qERERESlcHk1KVapUCZPJRHx8vNvy+Ph4IiMj89wnKioKs9nsNlSvUaNGxMXFOYcDnjhxgpYtWzrXW61Wfv31V9577z0yMzPd9gXw8fHBx8cn12uZzeYyfZEN5eMc81WxOgRFY0g6SuTZPwAwVa6Pqby2xwXK9XvjAmoLF7WFi9rCpay3RVk+NxERERFP8mihc29vb1q1asXy5cudy2w2G8uXL6ddu3Z57tO+fXv27NmDzWZzLtu1axdRUVF4e3vTuXNntm7dypYtW5y32NhYBgwYwJYtW3IlpKScqxoLgJftXE0xzbwnIiIiIiIiUiw8Pvve2LFjmTlzJvPmzWP79u0MGzaM1NRUhgwZAsDAgQPdCqEPGzaMhIQERo8eza5du1i4cCGTJk1ixIgRAAQGBtK0aVO3W0BAABUrVqRp06YeOUcpwaq2dn+upJSIiIiIiIhIsfB4Tan+/ftz8uRJnn/+eeLi4mjRogVLlixxFj8/dOgQRqMrd1atWjWWLl3KY489RkxMDNHR0YwePZrx48d76hSkNDsvKWX3DcHgH+bBYERERERERETKD48npQBGjhzJyJEj81y3YsWKXMvatWvH+vXrC3z8vI4hAkBUc+xGLwy2bOxhtTF4Oh4RERERERGRcsLjw/fKo8S0LCxW26U3lKJn9sMecW5Yp4buiYiIiIiIiBQbJaWK2S87TtBl6q98sHKvp0ORc+y1OjruI5t7OBIRERERERGR8kNJqWKWmJ7FqZRM3l6+m+3HkzwdjgC26x/nt1qjscUO9XQoIiIiIiIiIuWGklLFrE+LaLo0isBitfP4F39qGF9JYPYjLqQVmLw9HYmIiIiIiIhIuaGkVDEzGAxMuqMpIf5m/j6WxH9+0TA+ERERERERESl/lJTygPBAX168tQkA7/68m7+PnfVwRCIiIiIiIiIixUtJKQ+5tXkVejSJJNtmZ9znf5KVrWF8IiIiIiIiIlJ+KCnlIQaDgVdub0pYgDc74pJ57+fdng5JRERERERERKTYKCnlQZUq+PDybU0BeH/FXrYe0TA+ERERERERESkflJTysJtjorg5Jgqrzc64L7aQmW31dEgiIiIiIiIiIkVOSakS4OXbmlKpgje74lN4+ycN4xMRERERERGRsk9JqRIgLMCbV/o0A2DGyr1sOZzo2YBERERERERERIqYklIlRI+mkdzWogo2O4z7fAsZFg3jExEREREREZGyS0mpEuSF3k2oVMGHvSdTeeunXZ4OR0RERERERESkyCgpVYKEBngz6XbHbHwzf93HpoNnPByRiIiIiIiIiEjRUFKqhOnWJJI7ronGZocnvvhTw/hEREREREREpExSUqoEmti7CeGBPuw7lcqbS3d6OhwRERERERERkUKnpFQJFOxv5rW+jtn4Zq3Zz8YDCR6OSERERERERESkcCkpVUJ1ahjBXa2qYj83jC8tK9vTIYmIiIiIiIiIFBolpUqwZ29pTFSwLwdOpzFliYbxiYiIiIiIiEjZoaRUCRbsZ+a1vjEAzF17gPX7Tns4IhERERERERGRwqGkVAnXsX5l7mlTDYAnvvyT1EwN4xMRERERERGR0k9JqVLg6V6NiA7x43BCOq8v2eHpcERERERERERErpqSUqVAoK+Z188N4/to3UHW7jnl4YhERERERERERK6OklKlxPX1KjGgbXUAnvjyL1I0jE9ERERERERESjElpUqRCb0aUTXUj6OJ6UxatN3T4YiIiIiIiIiIXDElpUqRCj5eTLnTMYxv/m+H+HXXSQ9HJCIiIiIiIiJyZZSUKmWuq1OJQe1qAPDUV3+RlGHxcEQiIiIiIiIiIpdPSalSaHzPhlQP8+fY2Qxe/UHD+ERERERERESk9FFSqhTy9/bizbuaYzDAZ78f5pedJzwdkoiIiIiIiIjIZVFSqpRqUyuMIdfVAmDCV1s5m65hfCIiIiIiIiJSeigpVYo90b0BtSoFEJeUwcs//OPpcERERERERERECkxJqVLMz9vEG3fGYDDAl5uOsHx7vKdDEhEREREREREpECWlSrnYmmE8eP25YXxfbyUxLcvDEYmIiIiIiIiIXJqSUmXAuG4NqF05gBPJmbz4vYbxiYiIiIiIiEjJp6RUGeBrNvHmXc0xGuCbP46y9O84T4ckIiIiIiIiInJRSkqVES2rh/LwDXUAeOabrSSkahifiIiIiIiIiJRcSkqVIWO61KNeeAVOpWQxccHfng5HRERERERERCRfJSIp9f7771OzZk18fX1p27YtGzZsuOj2iYmJjBgxgqioKHx8fKhfvz6LFi1yrp8+fToxMTEEBQURFBREu3btWLx4cVGfhsflDOMzGQ18/+cxFm897umQRERERERERETy5PGk1GeffcbYsWOZOHEimzdvpnnz5nTv3p0TJ07kuX1WVhZdu3blwIEDfPnll+zcuZOZM2cSHR3t3KZq1aq89tprbNq0id9//51OnTpx22238fffZb/3UPNqIQzr6BjG9+y32zidkunhiEREREREREREcvN4Umrq1Kk89NBDDBkyhMaNGzNjxgz8/f2ZPXt2ntvPnj2bhIQEvv32W9q3b0/NmjXp2LEjzZs3d27Tu3dvevXqRb169ahfvz6vvvoqFSpUYP369cV1Wh41qnNdGkYGcjo1i+e/K/uJOBEREREREREpfbw8+eJZWVls2rSJCRMmOJcZjUa6dOnCunXr8txnwYIFtGvXjhEjRvDdd99RuXJl7r33XsaPH4/JZMq1vdVq5YsvviA1NZV27drleczMzEwyM109ipKSkgCwWCxYLJarOUWPMAKv3d6EOz/4jYVbj9Nt82F6NYt02ybnvErj+RU2tYU7tYeL2sJFbeGitnApL21RUs/v/fff54033iAuLo7mzZvz7rvv0qZNmzy3/frrr5k0aRJ79uzBYrFQr149xo0bx/333+/cZvDgwcybN89tv+7du7NkyZIiPQ8REREpvzyalDp16hRWq5WIiAi35REREezYsSPPffbt28fPP//MgAEDWLRoEXv27GH48OFYLBYmTpzo3G7r1q20a9eOjIwMKlSowDfffEPjxo3zPObkyZN58cUXcy3/8ccf8ff3v4oz9KzOVYwsPWLk6a//JGnvZoK8c2+zbNmy4g+shFJbuFN7uKgtXNQWLmoLl7LeFmlpaZ4OIZec8gczZsygbdu2TJs2je7du7Nz507Cw8NzbR8WFsYzzzxDw4YN8fb25ocffmDIkCGEh4fTvXt353Y9evRgzpw5zuc+Pj7Fcj4iIiJSPnk0KXUlbDYb4eHhfPjhh5hMJlq1asXRo0d544033JJSDRo0YMuWLZw9e5Yvv/ySQYMGsXLlyjwTUxMmTGDs2LHO50lJSVSrVo1u3boRFBRULOdVFLpk2+j7wW/siEvm17QqvH9bcwwGA+D41XfZsmV07doVs9ns4Ug9S23hTu3horZwUVu4qC1cyktb5PSgLknOL38AMGPGDBYuXMjs2bN56qmncm1/4403uj0fPXo08+bNY/Xq1W5JKR8fHyIjIxEREREpDh5NSlWqVAmTyUR8fLzb8vj4+HwviKKiojCbzW5D9Ro1akRcXBxZWVl4ezu6A3l7e1O3bl0AWrVqxcaNG3n77bf54IMPch3Tx8cnz18CzWZzqb7INpthar8W3PreapZtP8Hif05yW4voC7Yp3edYmNQW7tQeLmoLF7WFi9rCpay3RUk7tyspf3A+u93Ozz//zM6dO3n99dfd1q1YsYLw8HBCQ0Pp1KkTr7zyChUrViz0cxAREREBDyelvL29adWqFcuXL6dPnz6AoyfU8uXLGTlyZJ77tG/fnvnz52Oz2TAaHXXad+3aRVRUlDMhlRebzeZWN6q8aFwliEc712Pqsl08/93ftKtdkfAgX0+HJSIiIlfoSsofAJw9e5bo6GgyMzMxmUz85z//oWvXrs71PXr04I477qBWrVrs3buXp59+mp49e7Ju3bo863ZC2avLWRDlpZZaQagtXNQWLmoLF7WFO7WHS3loi4Kem8eH740dO5ZBgwYRGxtLmzZtmDZtGqmpqc7u6AMHDiQ6OprJkycDMGzYMN577z1Gjx7NqFGj2L17N5MmTeLRRx91HnPChAn07NmT6tWrk5yczPz581mxYgVLly71yDl62rAb6/DjP3FsO5rE099sZebAWE+HJCIiIsUsMDCQLVu2kJKSwvLlyxk7diy1a9d2Du27++67nds2a9aMmJgY6tSpw4oVK+jcuXOexyyrdTkLoqzXUrscagsXtYWL2sJFbeFO7eFSltuioDU5PZ6U6t+/PydPnuT5558nLi6OFi1asGTJEuevf4cOHXL2iAKoVq0aS5cu5bHHHiMmJobo6GhGjx7N+PHjnducOHGCgQMHcvz4cYKDg4mJiWHp0qVuvwaWJ2aTkX/f1YJb3l3FT9tP8PXmo9waE3HpHUVERKTEuZLyB+AY4pdT2qBFixZs376dyZMn56o3laN27dpUqlSJPXv25JuUKqt1OS+mvNRSKwi1hYvawkVt4aK2cKf2cCkPbVHQmpweT0oBjBw5Mt/heitWrMi1rF27dqxfvz7f482aNauwQiszGkQGMqZLfd5YupMXv/+bNjWDPR2SiIiIXIErKX+Ql0uVNjhy5AinT58mKioq323Kal3OgigP51hQagsXtYWL2sJFbeFO7eFSltuioOdlvPQmUlY8ckNtmlcNJikjm2e/+we73dMRiYiIyJUYO3YsM2fOZN68eWzfvp1hw4blKn9wfiH0yZMns2zZMvbt28f27dv597//zX//+1/uu+8+AFJSUnjiiSdYv349Bw4cYPny5dx2223UrVvXbXY+ERERkcJUInpKSfHwMhl5867m3PzualbuOkWVOgZu9nRQIiIictkut/xBamoqw4cP58iRI/j5+dGwYUP+97//0b9/fwBMJhN//fUX8+bNIzExkSpVqtCtWzdefvnlPHtCiYiIiBQGJaXKmXoRgYzrWp/Ji3fwzQEjTX8/Qt/Y6via855VR0REREqmyyl/8Morr/DKK6/keyw/P79yOyGMiIiIeI6G75VDD3aoTavqIWRYDTzz3T+0f+1npv20i1Mp+deVEBEREREREREpTEpKlUMmo4FZA1vSp4aVKsG+nE7NYtpPu7nutZ+Z8PVf7I5P9nSIIiIiIiIipVKWNYvjKcfZdmobeyx7OJJyBJvd5umwREokDd8rpwJ8vLipip3JQ65n+a7TzFy1nz8PJ/LJhsN8suEwNzaozIPX16Z93YoYDAZPhysiIiIi56RZ0kjLTsPfyx8/Lz9dq4lTSlYKqZZUAAwGA+H+4c51ZzLOkGXNynffiIAI5+PEjEQyrfmPogj3D3e+785mniUjO8O5Ljs7m2RbMunZ6Xh5eZWZ96fFZiEhPYFTGac4nX6a0+mnuaXOLZiNjhnGPvjzAxbuX8ip9FMkZ7n/yD93wVy+6P0FDcMaArAzYSdJWUnUD61PsI9mRS8u2bZssqxZ+Jv9PR2KnEdJqXLOy2Tklpgq3Nwsik0Hz/B/q/az9J84Vuw8yYqdJ2kYGciDHWrTu3kUPl6qOyUiIiJS2LJt2SRmJnI6/TQJGQluNwMGHm35qHPbAQsH8Nepv5zPTQYTAeYAAr0DCfUJ5ZNbPnGu+2zHZxxNPUqgOdC5TYA5gArmClTwrkCjsEZlJmFQ3qVnpzN9y3T++89/ybZnAxDoHcjae9Y6t3ny1ydZf3x9nvt7Gb344/4/nM+fW/scKw6vyPf1Nt+/GbPBkYx59bdXWbx/ca5tXv/8dUwGEz/3+5kw3zAA/vfP/1h/fL3b+/D8+87VOzsTBmczz2K1Wwk0B2I2FWxq+cuVbcvmTMYZTmec5lS6I9nUq3YvZ6Lp/7b+Hz/s/YHTGadJzEzMtX/76PbOxN/ZrLPsP7vfuc7L6EVF34rYMmykGFOoHVzbuW7+jvl8vftrwJHgqxdaj/oh9R33ofWpG1IXk1Hfvc6XZkkjxZJCSlYKyZZkUrNSHfeWVIJ9gulcvbNz2wmrJnA6/bRj+3P7pFhSSM9O55rwa/io50cePJOSwWKzON/nnqaklACOX1Jia4YRWzOMg6dTmbPmAJ//fpgdcck8/sWfvL5kB4Pa1WBA2xqEBnh7OlwRERGREi05KzlXkul0xmkS0hPwMnoxvs1457b3LLyHHQk78jxOiE+IW1Lqwl/4rXYrSVlJJGUlkZ6d7rZuyYEl/B7/e57HNRvNbL5/s/P54ysfZ2PcRvdEwXmPn2z9pPNL8p8n/+Rs5ll8Db4cyT7C36f/xsvL9bWiScUmzmTX4aTDnM06m287NQpr5DzukeQjeX7xz9EgrIHzS9SxlGMkZCQA4G3ypm5IXYyG8lmZZN2xdby07iWOpBwBHMkQINcXTpPR5Fx3oQu39TJ45bvthUyG3Me12qzYsWO1WwkwBziX/3P6H1YeWZnvsX7p94vzPf6fLf9h/o75AHgbvd0SWIHmQCZ1mORMCK0/vp7tp7e7JV8DvQPJyM7gVPopetTq4TzHWVtnsXD/Qk6nn+ZMxhns2N1iuDbqWmevsTMZZ9h7dq/buVb0rUhFP8fNarM6191Z705urHojlfwqUdGvIkHeQWRnZ7No0SK69+iOt8n1HSrYO5joCtEcTTnKibQTnEg7wZqja5zr19+7ngCjo93WHl2LxWahfmh9IgMiS2UiOcuaRUJGApkWV+87u93O/B3znb37zk8epVhSaBjWkKfbPu3cvuNnHcmwZuR1eFqGt3RLSq0/vp5T6afy3DbFkuIWw7pj62hXpV2pbNcrkWnNZPa22Szat4jPbvmsRPQaU1JKcqlRMYAXbm3CY13qM3/DIeau3U98UiZv/riL937Zw52tqvJA+1rUrlzB06GKiIiIFBurzcruxN3ssuzCus/KWctZZ8LJy+jFi9e96Nx2yJIh7DyzM8/jBPsEuyWlQnxCMBqMhPiEEOYbRkXfioT5hhHm53hst9udX5he6/Aavl6++Hv5k56d7vZFzmKzuL1Or9q9aFSxEamWVJKzHD0Kcra9sBfG+cmzC3kZvXiqzVPO57O2zuKXw784n89YOsNt+z8H/okBR7zv/PEOSw4sybdNz//y/cFfH/Dtnm/z3faXfr9Qya8SAHP/nssnO1y9wppVasZTbZ4ipnJMvvuXRd/s/obn1z4PQGRAJM9d+xw3VL0hz21ndJmR5/K8vHXTWwXednKHyUzuMNn53GKxsHDhQm7qdhMZ9gx8TD7Odf0a9CM2MtYt+XD++zPQO9C57fnDB7NsWbnen+cnIVccXsHH2z/ON8bWka2JDIgEHImm3Wd2ux0n1CfUkWjyrejsaQZwR707uD76emeiKef/al5qh9SmdkjtPNdd+P9tbOxYxsaOJSUrhT2Je9h1Zhe7zuxi95ndpGenuyXyZm6d6UwuB5oDqRtal/qh9akXUo/6YfVpUblFsSdUrDYriZmJJGQkYDKanL3Asm3ZvPrbqySkJ3Am84zj3yw9gWSLYzhjhyod6E53wNEp4q1Nb+U7TDTnMyRHgDkAi82SZ6/PeiH13LYdFzsOu92eb5I9x5IDS3jy1ydpE9mGp9s+TZ2QOoXWRiXRqiOrmLxhMoeTDwPww74f6Negn4ejUlJKLiLY38ywG+sw9PpaLNp6nJmr9vH3sST+t/4QH/92iM4Nw3mwQ23a1gorN5llERERKT/OTwYB3LHgDvad3ed4csEoqCDvILekVJhvGAHmAEdy6YJbRT/3RNO0m6bha/It0HCdin4VnY/9zf74m/3d6gad7676dxX0VHmtw2skZiY6kwXnJwosNotbO1QNrEqjsEYkZyWTnJqMn1/+da1CfEKICojK93XP/+J5OdsGeQc5t03ISGDrqa0MWDSAW+vcypiWY6jsX7nA516adareiXf/eJduNbsx6ppRbskMTzIYDPib/Qk2u9dLahHeghbhLQp0jBeue4Fnr32WVEuqe2L1XCI22Nt17CYVm3BrnVvdkl0pWSl4m7yp6FeRbJsr0dSnbh+uq3Kds7dTqE9ovv/36oTUKdJERQXvCpdskzohdRxDAxP3k2xJ5o8Tf/DHCcdQy0p+lfilnytB/O2eb/Ex+VAvpB41gmsUeHiW3W4nxZLiTPz5mHxoXLEx4BjmNWHVBM5knHGuP7+H2Y3VbuTdTu8Cjp5kP+z9Ic8eTXn1vrul9i2AI+F0YfLows+1JX2X4GPyKdD3zpzjXkpiZiI+Jh82xG3gzgV3cl/j+/hX83+VmP9HheVoylGmbJjCz4d/BiDcL5wnWj9B95rdPRyZg5JSckneXkb6XBPNbS2qsH5fArNW7+On7Sect6bRQTx4fW1ujonCbCqf3aZFRESk9LPYLPx96m9+j/+d3+N+51DyIRbevtD5Jah+aH3iU+MJsgdRo3INKvlXyjfR9J8u/ynw8KeS8AUo3D883+TWhZ5s/STg6BGzaNEievXqhdmc95ffZ659hmd4pkDHHRc7jnGx4wq07chrRjLympEAnEw7ydub3+a7vd+xYO8Cdibs5IveX5TJH02Pphzluz3fMaz5MAwGA8E+wSzos4AK3mVzBIOX0Ytgn+BLFgPvXac3vev0LtAx64bWpW5o3cIIr1g8e+2zAFisFvYn7Wf3md3OXlUhPiFu2777x7ucSDsBOIZk1g6uTb3QetQLrUeLyi1oGdHScSybhVHLR7n1kDy/p2XHqh15r/N7zuOsPLwyz0RTiE8IfiY/53ODwcCoa0bhY/IhzM89GX/+cMYcL1z3QoHbwdfLt8DbFtQ9De+hQ3QHpmycwi+Hf2Hu33NZtH8RT8Q6Ejal/TPEbrczc+tMZv41kwxrBl4GrxKZeFNSSgrMYDDQrk5F2tWpyN6TKcxevZ+vNh9h29Ekxny2hdcW72Bw+5rc07o6wf4lo2iaiIiIyMXsSNjBysMr+T3+d/48+Weuukz7zu5z9pR49tpn8cabH5f8SK9O+SdigAInpOTqVfavzCvXv0L/Bv15bcNrPNDsAeeXSZvdhgFDqf9ymW3L5uPtH/P+lvdJz06nRlANbq59M0CZTUiJO7PJTP3Q+tQPrc/N3JxrfbYtmxuq3sDuM7vZfWY3adlp7Dyz0zmMuGPVjs6klNloZvOJzbk+73J6d+YMk83xdNuncyWaQnxC8vycG9hkYGGdcrGoGliVdzq9w69HfmXyb5M5knKEJ359gt/jf3cmBEsrg8HAzoSdZFgzaB3ZmqfbPF0iE7L6aylXpE7lCrx6ezPGdWvA/N8OMm/dQeKSMnht8Q7eWb6bfrHVeKB9LapX9HzhNBERERGAjOwMtp7aSpOKTZzFXRfvX8zsbbOd24T6hNIqohWxkbHERsRSK7iWc12wTzAWiyXXcaVkaFa5Gf/t9V+3YX6f7fyMXw79wvg240ttvZgdCTuYuHYi/5z+B4BWEa2cQ6tEcngZvZjYbiLgSMYeSznmSFAlOnpWVfZzH9L6cvuX8ffydyaZQn1D8+2NdHu924s8fk+7oeoNtI1qy+xts5m1dVaJGdp2uY6nHMdsMjsTi0+0foJO1TvRq1avEpucV1JKrkpYgDcjO9XjoRtqs2DLMWat3s+OuGTmrj3AR+sO0K1xJA92qEWrGqEl9j+BiIiIlE1pljT+PPmnczje1lNbsdgs/Kfzf+hQtQMA10dfz5HkI84kVJ2QOuV2Frey4Px/O4vVwsy/ZnIy/SR9F/Tl7oZ3M6z5sEsOBSsp0rPTmf7ndD76+yOsdiuB5kDGxo7ljnp36D0qF2U0GKkaWJWqgVW5qfpNeW5TWpMuRcnH5MOw5sPo36A/Yb5hzuVf7PqCYO9gutboWmK/02ZZs/jon4/44M8PuKnaTUzpOAVwTICQ06uypFJSSgqFj5eJu2KrcWerqqzZc5r/W72PFTtPsuTvOJb8HUeLaiE82KEWPZpE4qW6UyIiIlKE/jr5F1M2TuHvU3+7zaQFUNmvMslZyc7nrSNb0zqydXGHKMXAbDIzr8c83vj9DX45/Asfb/+YhfsWMuqaUfSt17dAheU9aeyKsaw+uhqArjW6MqHNhHJTwF3Ek85PSB1POc4bG98gPTuddlHtmNB2glsP2pJg7dG1TNowiYNJBwE4kX6CjOyMIqnDVRSUlJJCZTAYuL5eJa6vV4ld8cnMXr2fr/84ypbDiYyc/wfRIX4MaV+T/q2rEeirulMiIiJy5ZKykvgj/g9+j/+dFuEt6Fy9M+Coi/LnyT8Bx6/EsRGOXlCxkbFUD6xeYn/plsJXLaga73R6h7XH1jJlwxT2nt3Ly+tf5rOdn/HSdS/RpFITT4eYrweaPsCexD083ebpfHu7iEjRCvUNZXCTwczaOot1x9dxx4I7GNR4EA/HPOwcBu4pcalxTNk4hWUHlwGO2RjHxY7j5lo3l6q/c0pKSZGpHxHIa31jGNetAf9bf5D/rj/I0cR0Xlm4nWk/7ebu1tUY3L4mVUNVd0pEREQu7WzmWedQvE3xm9iRsMM5LXnP1J7OpFTt4NpMun4S14RfQ3SF6FJ1cS5F47oq1/HlrV/y2c7PeH/L++w+s7tE9ZSy2+0s3r+YtOw07qx/J+Doxbfw9oV4m7w9HJ1I+eXr5cvwFsPpXbs3kzdMZtXRVczaNouF+xfyZOsn6VK9i0f+xmw4voGRP48kPTsdk8HEPQ3vYXiL4QR6BxZ7LFdLSSkpcpUDfXisa32G3ViHb/84yv+t3s+eEyn83+r9zFl7gKbRwUQF+RIZ7EtEkC+RwT6O+3PL/L31NhURESnv0ixp3PjZjbmG41UPrE5sZCw3VL3BucxgMBR4engpP7yMXgxoNIBetXqx7tg6GoY1dK5bc3QNLSNa4ufld5EjFI1jKcd4ef3LrD66Gj8vP9pVaUd0hWgAJaRESohqQdV4v/P7rDi8gtc2vMax1GM8+euTLLp9EVEVooo9niaVmhDoHUijsEY83fZpGoQ1KPYYCou+7Uux8TWbuLtNdfrFVmPl7pPMWrWf1XtO8efhRP68yH6Bvl7OBFXk+cmr8x5XDPDGaNSvoJcrM9vK2TQLZ9IsnEnLIjEti4SUDP6KN5C66SjeZi9MRjAaDJiMBowGg/Oxyei46Dedt85xD0aj+3KjEUwGg9tygwHHcc5b7tzWaMDbZFT9MRERcfI3+9MgrAFp2Wluw/HC/cM9HZqUMqG+ofSq3cv5fN/ZfYxcPpJK/pUY12oc3Wt2L5aeD1ablfk75vPuH++Snp2O2WjmgaYPEO6n97RISWQwGLip+k1cW+VaZm2dhclocktIZduy8TIWTYolLjWOL3d9yfAWwzEajASYA/hvz/8SFRBV6nsDKyklxc5oNHBTg3BuahDO3pMp7I5PIT4pg7ikDOLPOu7jkjKIO5tBWpaV5IxskjNS2H0iJd9jmk0GwgN9iQjycSaqovJIXvmaS0437cJktdlJSj+XWEq3kJiWxZlUx/Oz55afSXMsT0yzkHguCZWWZc3niCY+3fd3sZ5DrgiMBmpW9KdBZCD1IwJpeO6+RsUATEpAioiUS3N7zC01hVul9EhITyDcP5xjqcd44tcn+GTHJzzV5ikaVWxUZK+5M2EnL6x9gW2ntwHQMrwlE6+bSO3g2kX2miJSOPy8/Bh5zUi3ZX+f/ptxK8bxROwTdKreqdASRRarhf9t/x/T/5xOenY6VQOr0qduHwCqVKhSKK/haUpKiUfVqVyBOpUr5LnObreTnJntSlSdzXAmr+LOZjofn0rJxGK1czQxnaOJ6Rd9vRB/M5FBrmRVRLAvlQPMHEowELj7FGazl7PnjqPnD269gy7s9WM05PT0ubA3kSOpkuc2Bkcvobw+qOx2O2lZ1nO9lizO+8RzSaUzaVnnejadl2RKt3A23YLdfmX/BkYDBPuZCfX3JsTfTKCvF6dPnqBi5XDsGLDb7VhtjpvNnnOP87HVZsduB6vdjs1mx2o/b9m557YL9s1Zbj9v2YWsNjt7T6ay92Qqi7bGOZd7exmpF16BBhGB1I8MdN5XCfYt9b8SiIjIxSkhJUUhNjKW7/p8x9y/5zJr6yw2n9hM/x/607d+X0ZdM8ptJq7CkJiRyP2L7yc9O50K5go81uox7qx/J0aDeoiLlFazts7iaMpRxqwYQ/vo9kxoM4EaQTWu6pi/Hf+NSb9NYt/ZfQC0qNyCRmFFlyz3FCWlpMQyGAwE+ZoJ8jVTLyL/gm0Wq40TyZmupJVb8sr1OMNic/YS2hGXfMFRTMzcubloT+gCOYmr84fAZWXbyLLarviYFXy8HAmmgJwkkzeh/mZC/MyOxwGO+5BzSahQf28Cfb3chj5aLBYWLVpEr14tMZuLZ4ZEu9090WWz2zmbbmFXfAq74pLZGZ/Mzrhkdp9IJsNi4+9jSfx9LCnXudePqODsWZWTrKpUwadYzkFERERKL18vX/7V/F/0qduHqZumsnj/Yr7c9SWrj65m0R2LMBsL75ooxDeE+xvfz77EfUxoO0FDUEXKgFevf5WaQTWZ+/dc1hxdw+3f3c6QpkN4sNmDl12rLj41nn///m8WH1gMQJhvGGNbjaV3nd5lMnmtpJSUemaTkegQP6JD8v/PbrfbSUrPPm9oYDpxZzOJS8rgeGIae4+epEJgEHZciZGc3j8X9vqx59FD6MLHBWGzg81qB3Jv720yEuLvSBwF+5sJ9XclmULOPQ85l1QK8Tc7bn7eeHuVzg8pR2IOTBjIGWHp7+1FVLAfHetXdm5ntdk5nJDGzvhkZ7JqV3wy+06mkpKZzeZDiWw+lOh27EoVvKkfcS5RdS5hVT+iAoG+xZNwExERkdIjMiCSKTdM4e4Gd/PahtfoWavnVSekzmae5a1Nb9G/QX/nkMARLUaUyS+XIuWVn5cfj7Z8lNvq3sbk3yaz5tgaPvzrQ37Y+wPPXPuM22QclzJ+1Xg2xW/CaDDSr34/Rl4zkmCf4CKM3rOUlJJywWAwEOxvJtjfTINI915Xrp5B7QqtZ5AzcWXPaxgbzt5Azp5BNkcyzMtkINTfG39vk4ai5cFkNFCzUgA1KwXQvUmkc3lWto39p1JzJasOJaRxKiWLUymnWbv3tNuxokP8qB9RwTUEMCKQuuEVymzdsUux2eykWaykZWaTmmUlNTObtCwrqVnZpGVaSUrP5M94A9a/jhPg642f2YSftwk/swlfswlfs9G5zNfLpIkHRESkVGsZ0ZJPbv4EG64e7GuPreWT7Z/weOvHCzQsx263s/TAUiZvmExCRgI7Enbwyc2fYDAYlJASKaNqBNVgepfpLD+0nNc3vs6x1GPEp8Vfcj+73e78/jem5Rje/P1Nnm77NI0rNi7qkD1OSSmRImAwGPAyGfQfrJh4exlpEOnoCUVz1/K0rGz2nEhhZ5wjSbXz3HDAuKQMZw2yX3aedG5vNEDNigGO3lSRgdSt5Mees/D7wTN4m814Gd1nGnTeztUM8zIanTXHzl9vNBic+xZGstFitZGWZSUtK5vUzAvuzyWULkwqud2fl3TK2Tfdkl/R+/OZ+HTf1gLF6O11LkmVk6g6P3FlNuHrTGi5L/P1ck92OZJcRrdlrl6JNqw2nD0Xz1+WbbNhy7m328m25iSBcy/LtjkSxtnn1U47f5lznd2O1WrHYrVy8ICRfb/spWKgL8F+rmGxOb0WLxwWKyIipY/JaMKE48cqu93O1N+nsvPMTlYfW839je/n4WYPU8E779qox1OO88pvr/DrkV8BqB1cm/FtxutHR5FywGAw0KVGF66rch3f7PmGO+re4Vy358weqgZWdX62nEw7ydt/vk2NoBoMbzEcgBbhLfhvz/+Wm88LfWcWkTLL39uLmKohxFQNcVt+Ns3CrhPJrmTVud5ViWkW9p1KZd+pVJb8nVNc3Yt3/9lYaDEZDBckrc4Vzfc6Vxg/d0LL8YctPcuVdMrKvvK6Y5diNECAtxf+Pibnvb+3F35mIydPnCA4rBKZ2TbSLTYyLFbSsxwJrQyLlczz4srKtpGVbeNsuqXIYvUsIyuO7813rcEAQb45SSozwW5JK8fzYD+za5m/mWA/x7LSOgxXRKQsMxgMTLlhClM2TmHNsTXM2TaH7/d+z+iWo7m1zq3O7aw2K59v/5x3Nr9DWnYaXkYvHm72MEObDcXb5O3BMxCR4uZv9mdAowHO5+nZ6QxfPhyjwci4luNYk7GGyT9MJjU7FT8vPwY0GuAcpldeElKgpJSIlEPB/mZa1wyjdU3XbDp2u52TKZnsikthR1ySM1kVdzoRP/8A16yB5w3FzOlFYz2vzljOkM382O2Qfa4nztXyNhldySNvE/4+XgR4O5JIAeeSSQHnL89r/Xn7B/h44eNlzPOPoGuYa2y+w1xtNjsZ2Y5EVUa2zXFvcSWtzk9g5buNxbUsZ3m6xUrGedsbzk0SYMpJ5l3Yc+2C5F5Owi+vbXOWeZlcPdpytj2/l9v5iUOj3c6uPXupVKU6yZnZzgkUzqY7ZsRMzbJit8PZczNjHrzMf9cAbxMh/t4E5ZG0CvF3LQvyM+NnNuFlNDrOxeQ6J8e90fXc5L7cmM8MoCIikr/aIbWZ3mU6vx75lSkbp3Ao+RDPrXmOz3Z8xvjY8QD8dOgnXtvwGgDXhF/DC+1eoHZIbU+GLSIlxOHkw9jsNo6nHmfsr2Ody2MqxfD0tU+X6bpRF6OklIgIji/o4YG+hAf6cn29SsD5iZjrL6veWE5NsWybe92wbJvtXEIL51CwnISW+xAz98SX3c65pFFOoskLP29TietRYzQa8Pf2wt+7bP9psVgsLLLsplevxnm+L3J6iJ1NzzovWWUhMd3C2bQsEvN5npRhwW7HMQQzyzG8tCh5GS9IYpmMFyS1Lkhu5ZH0MhrsnDpp5JvTm51Jrpx0a05y1vXcPRHrWm93f57fcueO5x3jgm1mDWpNsL8mMRCRomMwGOhYrSPtqrTj4+0f88FfH7Dt9DbiUh09rLvW6MoPB3+gU7VO3Fn/TtWOEhGn+qH1WdBnAR/+9SHz/pmHt92bx9s8Tt8Gfcv1Z0XZ/uYgIuIBzppi5bNmernn7WWkcqAPlQN9Lms/m81OckY2ieeSWYnnel45k1rnElxnz1ufmW3FanXVw3Ld27Da7Fis+ffIyz63febVnjBG/kk8ddVHKQzZtqIb2ioicj5vkzdDmg6hd53efLfnOzpV68TibYsxGoxM7zxdvVFFJE/+Zn/GtBrDPfXv4dflv9KnTp9ynZACJaVERERKBKPRNUtojYqFd1xbHskqtySWNZ/lNhvZ1guTXeeWn3uemZXNH3/+SUxMDCaTiZyvYDlfxlzPL7g/t+bC72z57nfB9ga3fZyPqOCryxoRKV6V/CoxtNlQLBZXDUUlpETkUsJ8w/A2qM4cKCklIiJSphmNBrydMwEWbvc9i8WCz/Et9GoZfVlDXEVEREREAMp3PzEREREREREREfEIJaVERERERERERKTYKSklIiIiIiIiIiLFTkkpEREREREREREpdkpKiYiIiIiIiIhIsVNSSkREREREREREip2SUiIiIiIiIiIiUuxKRFLq/fffp2bNmvj6+tK2bVs2bNhw0e0TExMZMWIEUVFR+Pj4UL9+fRYtWuRcP3nyZFq3bk1gYCDh4eH06dOHnTt3FvVpiIiIiIiIiIhIAXk8KfXZZ58xduxYJk6cyObNm2nevDndu3fnxIkTeW6flZVF165dOXDgAF9++SU7d+5k5syZREdHO7dZuXIlI0aMYP369SxbtgyLxUK3bt1ITU0trtMSEREREREREZGL8PJ0AFOnTuWhhx5iyJAhAMyYMYOFCxcye/ZsnnrqqVzbz549m4SEBNauXYvZbAagZs2abtssWbLE7fncuXMJDw9n06ZN3HDDDUVzIiIiIiIiIiIiUmAe7SmVlZXFpk2b6NKli3OZ0WikS5curFu3Ls99FixYQLt27RgxYgQRERE0bdqUSZMmYbVa832ds2fPAhAWFla4JyAiIiIiIiIiIlfEoz2lTp06hdVqJSIiwm15REQEO3bsyHOfffv28fPPPzNgwAAWLVrEnj17GD58OBaLhYkTJ+ba3mazMWbMGNq3b0/Tpk3zPGZmZiaZmZnO50lJSQBYLBYsFsuVnl6JlnNeZfX8Lofawp3aw0Vt4aK2cFFbuJSXtijr5yciIiLiKR4fvne5bDYb4eHhfPjhh5hMJlq1asXRo0d544038kxKjRgxgm3btrF69ep8jzl58mRefPHFXMt//PFH/P39CzX+kmbZsmWeDqHEUFu4U3u4qC1c1BYuaguXst4WaWlpng5BREREpEzyaFKqUqVKmEwm4uPj3ZbHx8cTGRmZ5z5RUVGYzWZMJpNzWaNGjYiLiyMrKwtvb2/n8pEjR/LDDz/w66+/UrVq1XzjmDBhAmPHjnU+T0pKolq1anTr1o2goKArPb0SzWKxsGzZMrp27eqszVVeqS3cqT1c1BYuagsXtYVLeWmLnB7UIiIiIlK4PJqU8vb2plWrVixfvpw+ffoAjp5Qy5cvZ+TIkXnu0759e+bPn4/NZsNodJTE2rVrF1FRUc6ElN1uZ9SoUXzzzTesWLGCWrVqXTQOHx8ffHx8ci03m81l+iIbysc5FpTawp3aw0Vt4aK2cFFbuJT1tijL5yYiIiLiSR4fvjd27FgGDRpEbGwsbdq0Ydq0aaSmpjpn4xs4cCDR0dFMnjwZgGHDhvHee+8xevRoRo0axe7du5k0aRKPPvqo85gjRoxg/vz5fPfddwQGBhIXFwdAcHAwfn5+l4zJbrcDZfuXUYvFQlpaGklJSeX+Yltt4U7t4aK2cFFbuKgtXMpLW+RcD+RcH0j+dA1VvqgtXNQWLmoLF7WFO7WHS3loiwJfP9lLgHfffddevXp1u7e3t71Nmzb29evXO9d17NjRPmjQILft165da2/btq3dx8fHXrt2bfurr75qz87Odq4H8rzNmTOnQPEcPnw432Popptuuummm27l83b48OHCuOwp03QNpZtuuummm266nX+71PWTwW7Xz34XstlsHDt2jMDAQAwGg6fDKRI5dbMOHz5cZutmFZTawp3aw0Vt4aK2cFFbuJSXtrDb7SQnJ1OlShVn6QDJm66hyhe1hYvawkVt4aK2cKf2cCkPbVHQ6yePD98riYxG40ULo5clQUFBZfY/weVSW7hTe7ioLVzUFi5qC5fy0BbBwcGeDqFU0DVU+aS2cFFbuKgtXNQW7tQeLmW9LQpy/aSf+0REREREREREpNgpKSUiIiIiIiIiIsVOSalyysfHh4kTJ+Lj4+PpUDxObeFO7eGitnBRW7ioLVzUFlIe6X3vorZwUVu4qC1c1Bbu1B4uagsXFToXEREREREREZFip55SIiIiIiIiIiJS7JSUEhERERERERGRYqeklIiIiIiIiIiIFDslpcqZyZMn07p1awIDAwkPD6dPnz7s3LnT02GVCK+99hoGg4ExY8Z4OhSPOHr0KPfddx8VK1bEz8+PZs2a8fvvv3s6rGJntVp57rnnqFWrFn5+ftSpU4eXX36Z8lJ+79dff6V3795UqVIFg8HAt99+67bebrfz/PPPExUVhZ+fH126dGH37t2eCbaIXawtLBYL48ePp1mzZgQEBFClShUGDhzIsWPHPBdwEbrU++J8//rXvzAYDEybNq3Y4hMparp+yl95v34CXUPlKM/XULp+ctH1kztdQ12aklLlzMqVKxkxYgTr169n2bJlWCwWunXrRmpqqqdD86iNGzfywQcfEBMT4+lQPOLMmTO0b98es9nM4sWL+eeff/j3v/9NaGiop0Mrdq+//jrTp0/nvffeY/v27bz++utMmTKFd99919OhFYvU1FSaN2/O+++/n+f6KVOm8M477zBjxgx+++03AgIC6N69OxkZGcUcadG7WFukpaWxefNmnnvuOTZv3szXX3/Nzp07ufXWWz0QadG71PsixzfffMP69eupUqVKMUUmUjx0/ZS38n79BLqGOl95vobS9ZOLrp/c6RqqAOxSrp04ccIO2FeuXOnpUDwmOTnZXq9ePfuyZcvsHTt2tI8ePdrTIRW78ePH26+//npPh1Ei3HzzzfYHHnjAbdkdd9xhHzBggIci8hzA/s033zif22w2e2RkpP2NN95wLktMTLT7+PjYP/nkEw9EWHwubIu8bNiwwQ7YDx48WDxBeUh+bXHkyBF7dHS0fdu2bfYaNWrY33rrrWKPTaS46PpJ1085dA3lomsoB10/uej6yZ2uofKmnlLl3NmzZwEIRBnLhgAACYpJREFUCwvzcCSeM2LECG6++Wa6dOni6VA8ZsGCBcTGxnLXXXcRHh7ONddcw8yZMz0dlkdcd911LF++nF27dgHw559/snr1anr27OnhyDxv//79xMXFuf1fCQ4Opm3btqxbt86DkZUMZ8+exWAwEBIS4ulQip3NZuP+++/niSeeoEmTJp4OR6TI6fpJ1085dA3lomuovOn66eLK8/UT6BoKwMvTAYjn2Gw2xowZQ/v27WnatKmnw/GITz/9lM2bN7Nx40ZPh+JR+/btY/r06YwdO5ann36ajRs38uijj+Lt7c2gQYM8HV6xeuqpp0hKSqJhw4aYTCasViuvvvoqAwYM8HRoHhcXFwdARESE2/KIiAjnuvIqIyOD8ePHc8899xAUFOTpcIrd66+/jpeXF48++qinQxEpcrp+0vXT+XQN5aJrqLzp+il/5f36CXQNBUpKlWsjRoxg27ZtrF692tOheMThw4cZPXo0y5Ytw9fX19PheJTNZiM2NpZJkyYBcM0117Bt2zZmzJhR7i6oPv/8cz7++GPmz59PkyZN2LJlC2PGjKFKlSrlri2kYCwWC/369cNutzN9+nRPh1PsNm3axNtvv83mzZsxGAyeDkekyOn6SddP59M1lIuuoeRylPfrJ9A1VA4N3yunRo4cyQ8//MAvv/xC1apVPR2OR2zatIkTJ07QsmVLvLy88PLyYuXKlbzzzjt4eXlhtVo9HWKxiYqKonHjxm7LGjVqxKFDhzwUkec88cQTPPXUU9x99900a9aM+++/n8cee4zJkyd7OjSPi4yMBCA+Pt5teXx8vHNdeZNzQXXw4EGWLVtWLn/lW7VqFSdOnKB69erOz9KDBw8ybtw4atas6enwRAqVrp90/XQhXUO56Boqb7p+yk3XTw66hnJQT6lyxm63M2rUKL755htWrFhBrVq1PB2Sx3Tu3JmtW7e6LRsyZAgNGzZk/PjxmEwmD0VW/Nq3b59rautdu3ZRo0YND0XkOWlpaRiN7vl6k8mEzWbzUEQlR61atYiMjGT58uW0aNECgKSkJH777TeGDRvm2eA8IOeCavfu3fzyyy9UrFjR0yF5xP3335+rpkz37t25//77GTJkiIeiEilcun5y0fWTO11DuegaKm+6fnKn6ycXXUM5KClVzowYMYL58+fz3XffERgY6BzHHBwcjJ+fn4ejK16BgYG5akEEBARQsWLFclcj4rHHHuO6665j0qRJ9OvXjw0bNvDhhx/y4Ycfejq0Yte7d29effVVqlevTpMmTfjjjz+YOnUqDzzwgKdDKxYpKSns2bPH+Xz//v1s2bKFsLAwqlevzpgxY3jllVeoV68etWrV4rnnnqNKlSr06dPHc0EXkYu1RVRUFHfeeSebN2/mhx9+wGq1Oj9Pw8LC8Pb29lTYReJS74sLLyjNZjORkZE0aNCguEMVKRK6fnLR9ZM7XUO5lOdrKF0/uej6yZ2uoQrAs5P/SXED8rzNmTPH06GVCOV5SuPvv//e3rRpU7uPj4+9YcOG9g8//NDTIXlEUlKSffTo0fbq1avbfX197bVr17Y/88wz9szMTE+HVix++eWXPD8jBg0aZLfbHdMaP/fcc/aIiAi7j4+PvXPnzvadO3d6NugicrG22L9/f76fp7/88ounQy90l3pfXKg8TmcsZZuuny6uPF8/2e26hspRnq+hdP3kousnd7qGujSD3W63F2aSS0RERERERERE5FJU6FxERERERERERIqdklIiIiIiIiIiIlLslJQSEREREREREZFip6SUiIiIiIiIiIgUOyWlRERERERERESk2CkpJSIiIiIiIiIixU5JKRERERERERERKXZKSomIiIiIiIiISLFTUkpEpJAZDAa+/fZbT4chIiIiUmro+kmkfFJSSkTKlMGDB2MwGHLdevTo4enQREREREokXT+JiKd4eToAEZHC1qNHD+bMmeO2zMfHx0PRiIiIiJR8un4SEU9QTykRKXN8fHyIjIx0u4WGhgKOruHTp0+nZ8+e+Pn5Ubt2bb788ku3/bdu3UqnTp3w8/OjYsWKPPzww6SkpLhtM3v2bJo0aYKPjw9RUVGMHDnSbf2pU6e4/fbb8ff3p169eixYsKBoT1pERETkKuj6SUQ8QUkpESl3nnvuOfr27cuff/7JgAEDuPvuu9m+fTsAqampdO/endDQUDZu3MgXX3zBTz/95HbRNH36dEaMGMHDDz/M1q1bWbBgAXXr1nV7jRdffJF+/frx119/0atXLwYMGEBCQkKxnqeIiIhIYdH1k4gUCbuISBkyaNAgu8lksgcEBLjdXn31VbvdbrcD9n/9619u+7Rt29Y+bNgwu91ut3/44Yf20NBQe0pKinP9woUL7Uaj0R4XF2e32+32KlWq2J955pl8YwDszz77rPN5SkqKHbAvXry40M5TREREpLDo+klEPEU1pUSkzLnpppuYPn2627KwsDDn43bt2rmta9euHVu2bAFg+/btNG/enICAAOf69u3bY7PZ2LlzJwaDgWPHjtG5c+eLxhATE+N8HBAQQFBQECdOnLjSUxIREREpUrp+EhFPUFJKRMqcgICAXN3BC4ufn1+BtjObzW7PDQYDNputKEISERERuWq6fhIRT1BNKREpd9avX5/reaNGjQBo1KgRf/75J6mpqc71a9aswWg00qBBAwIDA6lZsybLly8v1phFREREPEnXTyJSFNRTSkTKnMzMTOLi4tyWeXl5UalSJQC++OILYmNjuf766/n444/ZsGEDs2bNAmDAgAFMnDiRQYMG8cILL3Dy5ElGjRrF/fffT0REBAAvvPAC//rXvwgPD6dnz54kJyezZs0aRo0aVbwnKiIiIlJIdP0kIp6gpJSIlDlLliwhKirKbVmDBg3YsWMH4JjZ5dNPP2X48OFERUXxySef0LhxYwD8/f1ZunQpo0ePpnXr1vj7+9O3b1+mTp3qPNagQYPIyMjgrbfe4vHHH6dSpUrceeedxXeCIiIiIoVM108i4gkGu91u93QQIiLFxWAw8M0339CnTx9PhyIiIiJSKuj6SUSKimpKiYiIiIiIiIhIsVNSSkREREREREREip2G74mIiIiIiIiISLFTTykRERERERERESl2SkqJiIiIiIiIiEixU1JKRERERERERESKnZJSIiIiIiIiIiJS7JSUEhERERERERGRYqeklIiIiIiIiIiIFDslpUREREREREREpNgpKSUiIiIiIiIiIsVOSSkRERERERERESl2/w/NTavUgP39rQAAAABJRU5ErkJggg==\n"},"metadata":{}},{"name":"stdout","text":"\n--- Final Evaluation on Validation Set using Best Model ---\nLoading best saved model 'best_model_static_context_ep10.pth'\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-4-98b691d38732>:735: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(best_model_path_final, map_location=DEVICE))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2416 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"541d22f7d85a4d66ac029c30d431b5c4"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}