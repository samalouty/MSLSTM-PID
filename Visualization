{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11201333,"sourceType":"datasetVersion","datasetId":6993690},{"sourceId":11201362,"sourceType":"datasetVersion","datasetId":6993708},{"sourceId":11201388,"sourceType":"datasetVersion","datasetId":6993722},{"sourceId":11201422,"sourceType":"datasetVersion","datasetId":6993740},{"sourceId":11201506,"sourceType":"datasetVersion","datasetId":6993794},{"sourceId":11201543,"sourceType":"datasetVersion","datasetId":6993809},{"sourceId":11255589,"sourceType":"datasetVersion","datasetId":7034191},{"sourceId":11382982,"sourceType":"datasetVersion","datasetId":7127490},{"sourceId":11402679,"sourceType":"datasetVersion","datasetId":7142036},{"sourceId":11684148,"sourceType":"datasetVersion","datasetId":7333398},{"sourceId":11720877,"sourceType":"datasetVersion","datasetId":7357780},{"sourceId":11801892,"sourceType":"datasetVersion","datasetId":7411543},{"sourceId":302300,"sourceType":"modelInstanceVersion","modelInstanceId":258142,"modelId":279383},{"sourceId":307831,"sourceType":"modelInstanceVersion","modelInstanceId":262207,"modelId":283333},{"sourceId":316944,"sourceType":"modelInstanceVersion","modelInstanceId":267476,"modelId":288527},{"sourceId":329886,"sourceType":"modelInstanceVersion","modelInstanceId":276781,"modelId":297682},{"sourceId":329908,"sourceType":"modelInstanceVersion","modelInstanceId":276800,"modelId":297702},{"sourceId":352620,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":294156,"modelId":314775},{"sourceId":391172,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":322103,"modelId":342770},{"sourceId":391205,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":322120,"modelId":342793}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/aras62/PIE.git\n!unzip /content/PIE/annotations/annotations.zip -d /content/PIE\n!unzip /content/PIE/annotations/annotations_vehicle.zip -d /content/PIE\n!git clone https://github.com/hustvl/YOLOP.git\n!mkdir /kaggle/working/PIE/content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:27:07.771729Z","iopub.execute_input":"2025-04-19T13:27:07.772029Z","iopub.status.idle":"2025-04-19T13:27:08.263278Z","shell.execute_reply.started":"2025-04-19T13:27:07.771999Z","shell.execute_reply":"2025-04-19T13:27:08.262097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q ultralytics opencv-python-headless ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:18:37.101133Z","iopub.execute_input":"2025-05-26T21:18:37.101396Z","iopub.status.idle":"2025-05-26T21:18:42.608579Z","shell.execute_reply.started":"2025-05-26T21:18:37.101364Z","shell.execute_reply":"2025-05-26T21:18:42.607720Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport xml.etree.ElementTree as ET\nimport os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\nfrom tqdm.notebook import tqdm\nimport random\nimport math\nimport zipfile\nimport cv2\nfrom ultralytics import YOLO\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:10.154360Z","iopub.execute_input":"2025-04-15T19:55:10.154719Z","iopub.status.idle":"2025-04-15T19:55:16.590012Z","shell.execute_reply.started":"2025-04-15T19:55:10.154687Z","shell.execute_reply":"2025-04-15T19:55:16.589278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + 'annotations'):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.590843Z","iopub.execute_input":"2025-04-15T19:55:16.591210Z","iopub.status.idle":"2025-04-15T19:55:16.596284Z","shell.execute_reply.started":"2025-04-15T19:55:16.591187Z","shell.execute_reply":"2025-04-15T19:55:16.595261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations_vehicle.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + 'annotations_vehicle'):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.597390Z","iopub.execute_input":"2025-04-15T19:55:16.597839Z","iopub.status.idle":"2025-04-15T19:55:16.618074Z","shell.execute_reply.started":"2025-04-15T19:55:16.597803Z","shell.execute_reply":"2025-04-15T19:55:16.617129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"/kaggle/working/PIE/annotations/annotations_attributes.zip\"\nextract_to = \"/kaggle/working/PIE/annotations/\"\n\nif os.path.exists(extract_to + \"annotations_attributes\"):\n    print(\"Exists already. Not unzipping.\")\nelse:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(\"Unzipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:55:16.619029Z","iopub.execute_input":"2025-04-15T19:55:16.619360Z","iopub.status.idle":"2025-04-15T19:55:16.635884Z","shell.execute_reply.started":"2025-04-15T19:55:16.619328Z","shell.execute_reply":"2025-04-15T19:55:16.634975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # -----------------------------------------------------------------------------\n# # CELL 1: DATA PREPARATION & BALANCING  (run once before training)\n# # -----------------------------------------------------------------------------\n# #  This cell:\n# #    1. Loads (or regenerates) the PIE database\n# #    2. Computes per-signal standardisation scalers\n# #    3. Extracts ALL training sequences for every stream\n# #    4. Balances the dataset 50 / 50 on the crossing label\n# #    5. Writes two pickles:\n# #         - /kaggle/working/balanced_train_data.pkl\n# #         - /kaggle/working/scalers.pkl\n# # -----------------------------------------------------------------------------\n\n# import os\n# import sys\n# import time\n# import pickle\n# import gc\n# from pathlib import Path\n\n# import cv2                               # used internally by PIE utilities\n# import numpy as np\n# import torch\n# from torch.utils.data import Dataset\n# from tqdm.notebook import tqdm\n\n# # -----------------------------------------------------------------------------#\n# #                                PIE utilities                                 #\n# # -----------------------------------------------------------------------------#\n# pie_utilities_path = \"/kaggle/working/PIE/utilities\"\n# if pie_utilities_path not in sys.path:\n#     sys.path.insert(0, pie_utilities_path)\n\n# try:\n#     from pie_data import PIE\n# except ImportError as e:\n#     print(\n#         f\"[WARN] Could not import PIE from {pie_utilities_path}. \"\n#         f\"If the DB cache already exists this is fine.\\n→ {e}\"\n#     )\n#     PIE = None\n\n# # -----------------------------------------------------------------------------#\n# #                              configuration                                   #\n# # -----------------------------------------------------------------------------#\n# PIE_ROOT_PATH           = \"/kaggle/working/PIE\"\n# POSE_DATA_DIR           = \"/kaggle/input/pose-data/extracted_poses2\"\n# PIE_DATABASE_CACHE_PATH = \"/kaggle/input/pie-database/pie_database.pkl\"\n\n# TRAIN_SETS_STR = [\"set01\", \"set02\", \"set04\"]\n\n# BALANCED_DATA_PKL_PATH  = \"/kaggle/working/balanced_train_data.pkl\"\n# SCALERS_PKL_PATH        = \"/kaggle/working/scalers.pkl\"\n\n# # Streams used throughout the project ----------------------------------------\n# ALL_POSSIBLE_STREAMS = [\n#     \"bbox\",\n#     \"pose\",\n#     \"ego_speed\",\n#     \"ego_acc\",\n#     \"ego_gyro\",\n#     \"ped_action\",\n#     \"ped_look\",\n#     \"ped_occlusion\",\n#     \"traffic_light\",\n#     \"static_context\",\n# ]\n\n# # Feature sizes & categorical constants --------------------------------------\n# SEQ_LEN, PRED_LEN = 30, 1\n\n# INPUT_SIZE_BBOX       = 4\n# INPUT_SIZE_POSE       = 34\n# INPUT_SIZE_EGO_SPEED  = 1\n# INPUT_SIZE_EGO_ACC    = 2\n# INPUT_SIZE_EGO_GYRO   = 1\n# INPUT_SIZE_PED_ACTION = 1\n# INPUT_SIZE_PED_LOOK   = 1\n# INPUT_SIZE_PED_OCC    = 1\n# INPUT_SIZE_TL_STATE   = 4\n\n# NUM_SIGNALIZED_CATS   = 4\n# NUM_INTERSECTION_CATS = 5\n# NUM_AGE_CATS          = 4\n# NUM_GENDER_CATS       = 3\n# NUM_TRAFFIC_DIR_CATS  = 2\n\n# LANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7: 4, 8: 4}\n# NUM_LANE_CATS   = len(set(LANE_CATEGORIES.values()))\n\n# INPUT_SIZE_STATIC = (\n#     NUM_SIGNALIZED_CATS\n#     + NUM_INTERSECTION_CATS\n#     + NUM_AGE_CATS\n#     + NUM_GENDER_CATS\n#     + NUM_TRAFFIC_DIR_CATS\n#     + NUM_LANE_CATS\n# )  # → 23\n\n# TL_STATE_MAP = {\"__undefined__\": 0, \"red\": 1, \"yellow\": 2, \"green\": 3}\n# NUM_TL_STATES = len(TL_STATE_MAP)\n\n# # -----------------------------------------------------------------------------#\n# #                               helper utils                                   #\n# # -----------------------------------------------------------------------------#\n\n\n# def to_one_hot(index: int, num_classes: int) -> np.ndarray:\n#     vec = np.zeros(num_classes, dtype=np.float32)\n#     vec[int(np.clip(index, 0, num_classes - 1))] = 1.0\n#     return vec\n\n\n# def balance_samples_count(seq_data: dict, label_key: str, seed: int = 42) -> dict:\n#     \"\"\"Undersample majority class so positive and negative labels are equal.\"\"\"\n#     labels = [lbl[0] for lbl in seq_data[label_key]]\n#     n_pos  = int(np.sum(labels))\n#     n_neg  = len(labels) - n_pos\n\n#     if n_pos == n_neg:\n#         print(\"Dataset already balanced.\")\n#         return seq_data.copy()\n\n#     majority_label    = 0 if n_neg > n_pos else 1\n#     minority_count    = min(n_pos, n_neg)\n#     majority_indices  = np.where(np.array(labels) == majority_label)[0]\n#     minority_indices  = np.where(np.array(labels) != majority_label)[0]\n\n#     rng = np.random.default_rng(seed)\n#     keep_majority = rng.choice(majority_indices, size=minority_count, replace=False)\n#     final_indices = np.concatenate([minority_indices, keep_majority])\n#     rng.shuffle(final_indices)\n\n#     balanced = {}\n#     for k, v in seq_data.items():\n#         balanced[k] = [v[i] for i in final_indices]\n\n#     print(f\"Balanced: 1s={minority_count} | 0s={minority_count}\")\n#     return balanced\n\n\n# # -----------------------------------------------------------------------------#\n# #                                PIEDataset                                    #\n# # -----------------------------------------------------------------------------#\n# class PIEDataset(Dataset):\n#     \"\"\"\n#     Lightweight dataset that can generate any subset of the PIE feature streams.\n#     \"\"\"\n\n#     def __init__(\n#         self,\n#         pie_db: dict,\n#         set_names: list[str],\n#         pose_dir: str,\n#         seq_len: int,\n#         pred_len: int,\n#         scalers: dict,\n#         streams_to_generate: list[str],\n#     ):\n#         self.pie_db            = pie_db\n#         self.set_names         = set_names\n#         self.pose_dir          = pose_dir\n#         self.seq_len           = seq_len\n#         self.pred_len          = pred_len\n#         self.scalers           = scalers\n#         self.streams           = streams_to_generate\n#         self._input_sizes      = self._build_input_size_map()\n#         self.all_pose_data     = {}\n#         self.sequences         = []\n\n#         if \"pose\" in self.streams:\n#             self._load_pose_pkls()\n#         self._enumerate_sequences()\n\n#     # ------------------------ internal helpers -------------------------------\n#     def _build_input_size_map(self) -> dict:\n#         special = {\n#             \"TRAFFIC_LIGHT\": \"TL_STATE\",\n#             \"STATIC_CONTEXT\": \"STATIC\",\n#             \"EGO_SPEED\": \"EGO_SPEED\",\n#             \"EGO_ACC\": \"EGO_ACC\",\n#             \"EGO_GYRO\": \"EGO_GYRO\",\n#             \"PED_ACTION\": \"PED_ACTION\",\n#             \"PED_LOOK\": \"PED_LOOK\",\n#             \"PED_OCCLUSION\": \"PED_OCC\",\n#         }\n#         sizes = {}\n#         for s in ALL_POSSIBLE_STREAMS:\n#             const = f\"INPUT_SIZE_{special.get(s.upper(), s.upper())}\"\n#             if s == \"bbox\":\n#                 const = \"INPUT_SIZE_BBOX\"\n#             elif s == \"pose\":\n#                 const = \"INPUT_SIZE_POSE\"\n#             sizes[s] = globals().get(const, 1)\n#         return sizes\n\n#     def _load_pose_pkls(self):\n#         print(\"Loading pose PKLs …\")\n#         for set_id in self.set_names:\n#             set_dir = Path(self.pose_dir) / set_id\n#             if not set_dir.is_dir():\n#                 continue\n#             self.all_pose_data[set_id] = {}\n#             for pkl_path in tqdm(set_dir.glob(f\"{set_id}_*_poses.pkl\"), leave=False):\n#                 try:\n#                     with open(pkl_path, \"rb\") as fp:\n#                         loaded = pickle.load(fp)\n#                 except Exception as e:\n#                     print(f\"[pose load] {pkl_path}: {e}\")\n#                     continue\n\n#                 if len(loaded) != 1:\n#                     continue\n#                 (key, data), *_ = loaded.items()\n#                 vid = \"_\".join(key.split(\"_\")[1:])\n#                 if vid in self.pie_db.get(set_id, {}):\n#                     self.all_pose_data[set_id][vid] = data\n\n#     def _enumerate_sequences(self):\n#         print(\"Enumerating sequences …\")\n#         for set_id in self.set_names:\n#             for vid, vdb in self.pie_db.get(set_id, {}).items():\n#                 for pid, pdb in vdb.get(\"ped_annotations\", {}).items():\n#                     frames = pdb.get(\"frames\", [])\n#                     if len(frames) < self.seq_len + self.pred_len:\n#                         continue\n#                     frames = sorted(frames)\n#                     for i in range(len(frames) - self.seq_len - self.pred_len + 1):\n#                         start = frames[i]\n#                         obs_end = frames[i + self.seq_len - 1]\n#                         if obs_end - start != self.seq_len - 1:\n#                             continue\n#                         target = frames[i + self.seq_len + self.pred_len - 1]\n#                         if target - obs_end != self.pred_len:\n#                             continue\n#                         self.sequences.append((set_id, vid, pid, start))\n#         print(f\"Total sequences: {len(self.sequences)}\")\n\n#     # ------------------ Dataset API ------------------------------------------\n#     def __len__(self):\n#         return len(self.sequences)\n\n#     def __getitem__(self, idx: int):\n#         set_id, vid, pid, start = self.sequences[idx]\n#         vdb  = self.pie_db[set_id][vid]\n#         pdb  = vdb[\"ped_annotations\"][pid]\n#         ego  = vdb.get(\"vehicle_annotations\", {})\n#         tldb = vdb.get(\"traffic_annotations\", {})\n\n#         frame_nums = list(range(start, start + self.seq_len))\n#         target_f   = start + self.seq_len + self.pred_len - 1\n\n#         # label ---------------------------------------------------------------\n#         label = 0\n#         if (\n#             \"frames\" in pdb\n#             and \"behavior\" in pdb\n#             and \"cross\" in pdb[\"behavior\"]\n#             and target_f in pdb[\"frames\"]\n#         ):\n#             try:\n#                 j = pdb[\"frames\"].index(target_f)\n#                 label = pdb[\"behavior\"][\"cross\"][j]\n#                 if label == -1:\n#                     label = 0\n#             except (ValueError, IndexError):\n#                 pass\n\n#         # static context ------------------------------------------------------\n#         static_vec = np.zeros(INPUT_SIZE_STATIC, np.float32)\n#         if \"static_context\" in self.streams:\n#             attr  = pdb.get(\"attributes\", {})\n#             sig   = attr.get(\"signalized\", 0)\n#             intr  = attr.get(\"intersection\", 0)\n#             age   = attr.get(\"age\", 2)\n#             gen   = attr.get(\"gender\", 0)\n#             tdir  = int(attr.get(\"traffic_direction\", 0))\n#             ln    = attr.get(\"num_lanes\", 2)\n#             lncat = LANE_CATEGORIES.get(ln, LANE_CATEGORIES[max(LANE_CATEGORIES)])\n#             static_vec = np.concatenate(\n#                 [\n#                     to_one_hot(sig,  NUM_SIGNALIZED_CATS),\n#                     to_one_hot(intr, NUM_INTERSECTION_CATS),\n#                     to_one_hot(age,  NUM_AGE_CATS),\n#                     to_one_hot(gen,  NUM_GENDER_CATS),\n#                     to_one_hot(tdir, NUM_TRAFFIC_DIR_CATS),\n#                     to_one_hot(lncat, NUM_LANE_CATS),\n#                 ]\n#             ).astype(np.float32)\n\n#         # per-frame feature assembly -----------------------------------------\n#         feats = {s: [] for s in self.streams}\n\n#         for fn in frame_nums:\n#             fidx = -1\n#             if \"frames\" in pdb:\n#                 try:\n#                     fidx = pdb[\"frames\"].index(fn)\n#                 except ValueError:\n#                     pass\n\n#             ego_f = ego.get(fn, {})\n\n#             # bbox ----------------------------------------------------------\n#             if \"bbox\" in self.streams:\n#                 bb = np.zeros(INPUT_SIZE_BBOX, np.float32)\n#                 if (\n#                     fidx != -1\n#                     and \"bbox\" in pdb\n#                     and len(pdb[\"bbox\"]) > fidx\n#                 ):\n#                     try:\n#                         x1, y1, x2, y2 = pdb[\"bbox\"][fidx]\n#                         w_img = vdb.get(\"width\", 1920)\n#                         h_img = vdb.get(\"height\", 1080)\n#                         if w_img > 0 and h_img > 0:\n#                             cx = ((x1 + x2) / 2) / w_img\n#                             cy = ((y1 + y2) / 2) / h_img\n#                             w  = (x2 - x1) / w_img\n#                             h  = (y2 - y1) / h_img\n#                             if 0 < w and 0 < h and 0 <= cx <= 1 and 0 <= cy <= 1:\n#                                 bb = np.array([cx, cy, w, h], np.float32)\n#                     except Exception:\n#                         pass\n#                 feats[\"bbox\"].append(bb)\n\n#             # pose ----------------------------------------------------------\n#             if \"pose\" in self.streams:\n#                 pvec = np.zeros(INPUT_SIZE_POSE, np.float32)\n#                 pose_set = self.all_pose_data.get(set_id, {}).get(vid, {})\n#                 p_loaded = pose_set.get(fn, {}).get(pid)\n#                 if (\n#                     isinstance(p_loaded, np.ndarray)\n#                     and p_loaded.shape == (INPUT_SIZE_POSE,)\n#                 ):\n#                     pvec = p_loaded\n#                 feats[\"pose\"].append(pvec)\n\n#             # ego signals ---------------------------------------------------\n#             if \"ego_speed\" in self.streams:\n#                 s = ego_f.get(\"OBD_speed\", 0.0) or ego_f.get(\"GPS_speed\", 0.0)\n#                 s = (s - self.scalers.get(\"ego_speed_mean\", 0.0)) / self.scalers.get(\n#                     \"ego_speed_std\", 1.0\n#                 )\n#                 feats[\"ego_speed\"].append([s])\n\n#             if \"ego_acc\" in self.streams:\n#                 ax = ego_f.get(\"accX\", 0.0)\n#                 ay = ego_f.get(\"accY\", 0.0)\n#                 ax = (ax - self.scalers.get(\"accX_mean\", 0.0)) / self.scalers.get(\n#                     \"accX_std\", 1.0\n#                 )\n#                 ay = (ay - self.scalers.get(\"accY_mean\", 0.0)) / self.scalers.get(\n#                     \"accY_std\", 1.0\n#                 )\n#                 feats[\"ego_acc\"].append([ax, ay])\n\n#             if \"ego_gyro\" in self.streams:\n#                 gz = ego_f.get(\"gyroZ\", 0.0)\n#                 gz = (gz - self.scalers.get(\"gyroZ_mean\", 0.0)) / self.scalers.get(\n#                     \"gyroZ_std\", 1.0\n#                 )\n#                 feats[\"ego_gyro\"].append([gz])\n\n#             # pedestrian behaviour -----------------------------------------\n#             if \"ped_action\" in self.streams:\n#                 action = (\n#                     pdb[\"behavior\"][\"action\"][fidx]\n#                     if fidx != -1\n#                     and \"behavior\" in pdb\n#                     and \"action\" in pdb[\"behavior\"]\n#                     and len(pdb[\"behavior\"][\"action\"]) > fidx\n#                     else 0\n#                 )\n#                 feats[\"ped_action\"].append([float(action)])\n\n#             if \"ped_look\" in self.streams:\n#                 look = (\n#                     pdb[\"behavior\"][\"look\"][fidx]\n#                     if fidx != -1\n#                     and \"behavior\" in pdb\n#                     and \"look\" in pdb[\"behavior\"]\n#                     and len(pdb[\"behavior\"][\"look\"]) > fidx\n#                     else 0\n#                 )\n#                 feats[\"ped_look\"].append([float(look)])\n\n#             if \"ped_occlusion\" in self.streams:\n#                 occ = (\n#                     float(pdb[\"occlusion\"][fidx]) / 2.0\n#                     if fidx != -1\n#                     and \"occlusion\" in pdb\n#                     and len(pdb[\"occlusion\"]) > fidx\n#                     else 0.0\n#                 )\n#                 feats[\"ped_occlusion\"].append([occ])\n\n#             # traffic light -------------------------------------------------\n#             if \"traffic_light\" in self.streams:\n#                 tl_state = 0\n#                 for obj in tldb.values():\n#                     if obj.get(\"obj_class\") != \"traffic_light\":\n#                         continue\n#                     if \"frames\" not in obj or \"state\" not in obj:\n#                         continue\n#                     try:\n#                         j = obj[\"frames\"].index(fn)\n#                         if obj[\"state\"][j] != 0:\n#                             tl_state = obj[\"state\"][j]\n#                             break\n#                     except (ValueError, IndexError):\n#                         continue\n#                 feats[\"traffic_light\"].append(to_one_hot(tl_state, NUM_TL_STATES))\n\n#             # static context -----------------------------------------------\n#             if \"static_context\" in self.streams:\n#                 feats[\"static_context\"].append(static_vec)\n\n#         # numpy → torch ------------------------------------------------------\n#         out = {\n#             s: torch.tensor(np.asarray(feats[s], np.float32), dtype=torch.float32)\n#             for s in self.streams\n#         }\n#         return out, torch.tensor(label, dtype=torch.long)\n\n\n# # =============================================================================\n# #                       MAIN: build balanced training set\n# # =============================================================================\n# if __name__ == \"__main__\" and '__file__' not in globals(): # Avoid running this if imported\n#     print(\"\\n--- CELL 1: DATA PREPARATION ---\")\n\n#     # 1) load / regenerate PIE DB -------------------------------------------\n#     cache = Path(PIE_DATABASE_CACHE_PATH)\n#     if cache.is_file():\n#         print(\"Loading PIE database cache …\")\n#         with cache.open(\"rb\") as fp:\n#             pie_db = pickle.load(fp)\n#         print(\"✓ PIE DB loaded.\")\n#     else:\n#         if PIE is None:\n#             raise RuntimeError(\"PIE class unavailable: cannot rebuild database.\")\n#         print(\"Cache not found – regenerating PIE DB …\")\n#         pie_db = PIE(data_path=PIE_ROOT_PATH, regen_database=True).generate_database()\n#         if not pie_db:\n#             raise RuntimeError(\"PIE DB generation failed.\")\n#         print(\"✓ PIE DB generated.\")\n\n#     # 2) compute scalers -----------------------------------------------------\n#     print(\"\\nComputing scalers …\")\n#     spd, accx, accy, gyz = [], [], [], []\n#     for sid in TRAIN_SETS_STR:\n#         for vid, vdb in pie_db.get(sid, {}).items():\n#             for frame, e in vdb.get(\"vehicle_annotations\", {}).items():\n#                 s  = e.get(\"OBD_speed\", 0.0) or e.get(\"GPS_speed\", 0.0)\n#                 spd.append(s)\n#                 accx.append(e.get(\"accX\", 0.0))\n#                 accy.append(e.get(\"accY\", 0.0))\n#                 gyz.append(e.get(\"gyroZ\", 0.0))\n\n#     scalers = {}\n#     if spd:\n#         scalers[\"ego_speed_mean\"] = float(np.mean(spd))\n#         scalers[\"ego_speed_std\"]  = float(max(np.std(spd), 1e-6))\n#     if accx:\n#         scalers[\"accX_mean\"] = float(np.mean(accx))\n#         scalers[\"accX_std\"]  = float(max(np.std(accx), 1e-6))\n#         scalers[\"accY_mean\"] = float(np.mean(accy))\n#         scalers[\"accY_std\"]  = float(max(np.std(accy), 1e-6))\n#     if gyz:\n#         scalers[\"gyroZ_mean\"] = float(np.mean(gyz))\n#         scalers[\"gyroZ_std\"]  = float(max(np.std(gyz), 1e-6))\n\n#     print(\"Scalers:\", scalers)\n\n#     # 3) extract full training dataset --------------------------------------\n#     print(\"\\nExtracting training sequences (all streams) …\")\n#     full_ds = PIEDataset(\n#         pie_db,\n#         TRAIN_SETS_STR,\n#         POSE_DATA_DIR,\n#         SEQ_LEN,\n#         PRED_LEN,\n#         scalers,\n#         ALL_POSSIBLE_STREAMS,\n#     )\n\n#     train_dict = {s: [] for s in ALL_POSSIBLE_STREAMS}\n#     train_dict[\"label\"] = []\n\n#     for i in tqdm(range(len(full_ds)), desc=\"seq\"):\n#         feat, lbl = full_ds[i]\n#         for s in ALL_POSSIBLE_STREAMS:\n#             train_dict[s].append(feat[s].numpy())\n#         train_dict[\"label\"].append([lbl.item()])\n\n#     print(f\"Raw training samples: {len(train_dict['label'])}\")\n\n#     # 4) balance -------------------------------------------------------------\n#     balanced = balance_samples_count(train_dict, \"label\")\n#     del train_dict, full_ds\n#     gc.collect()\n\n#     # 5) write pickles -------------------------------------------------------\n#     print(\"\\nSaving balanced data …\")\n#     with open(BALANCED_DATA_PKL_PATH, \"wb\") as fp:\n#         pickle.dump(balanced, fp, pickle.HIGHEST_PROTOCOL)\n#     print(f\"✓ {BALANCED_DATA_PKL_PATH}\")\n\n#     print(\"Saving scalers …\")\n#     with open(SCALERS_PKL_PATH, \"wb\") as fp:\n#         pickle.dump(scalers, fp, pickle.HIGHEST_PROTOCOL)\n#     print(f\"✓ {SCALERS_PKL_PATH}\")\n\n#     del pie_db\n#     gc.collect()\n\n#     print(\"\\n--- CELL 1: DATA PREPARATION COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:48:03.811065Z","iopub.execute_input":"2025-05-11T15:48:03.811411Z","execution_failed":"2025-05-11T16:04:33.218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- CELL 2: ABLATION STUDY – MODEL TRAINING AND EVALUATION (with Weighted Average Fusion) ---\n\n# import os\n# import sys\n# import gc\n# import time\n# import math\n# import random\n# import pickle\n# import torch\n# import numpy as np\n# import pandas as pd                      # results-summary table\n# import torch.nn as nn\n# import torch.optim as optim\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# from tqdm.notebook import tqdm\n# from torch.utils.data import Dataset, DataLoader\n# from sklearn.metrics import (\n#     accuracy_score,\n#     precision_recall_fscore_support,\n#     roc_auc_score,\n#     confusion_matrix,\n#     ConfusionMatrixDisplay,\n#     # f1_score # Explicitly import if used directly, or use from precision_recall_fscore_support\n# )\n\n# # --- Add PIE utilities path if necessary (adjust path) ------------------------\n# pie_utilities_path = \"/kaggle/working/PIE/utilities\"\n# if pie_utilities_path not in sys.path:\n#     sys.path.insert(0, pie_utilities_path)\n\n# try:\n#     from pie_data import PIE\n# except ImportError as e:\n#     print(f\"Warn: Could not import PIE class: {e}\")\n#     PIE = None\n\n# # --- Configuration ------------------------------------------------------------\n# PIE_ROOT_PATH = \"/kaggle/working/PIE\"\n# POSE_DATA_DIR = \"/kaggle/input/pose-data/extracted_poses2\"\n# PIE_DATABASE_CACHE_PATH = \"/kaggle/input/pie-database/pie_database.pkl\"\n\n# # --- Define ALL possible streams (used by Dataset class) ----------------------\n# ALL_POSSIBLE_STREAMS_CELL2 = [ # Renamed to avoid conflict if cell1 not run\n#     \"bbox\",\n#     \"pose\",\n#     \"ego_speed\",\n#     \"ego_acc\",\n#     \"ego_gyro\",\n#     \"ped_action\",\n#     \"ped_look\",\n#     \"ped_occlusion\",\n#     \"traffic_light\",\n#     \"static_context\",\n# ]\n\n# # --- *** CHOOSE ACTIVE STREAMS FOR THIS EXPERIMENT *** ------------------------\n# ACTIVE_STREAMS = [\n#     \"bbox\",\n#     \"ped_action\",\n#     \"ped_look\",\n#     \"ego_speed\",\n#     \"ego_acc\",\n#     \"static_context\",\n# ]\n# # ------------------------------------------------------------------------------\n\n# print(f\"--- Running Weighted Average Fusion With Active Streams: {ACTIVE_STREAMS} ---\")\n\n# # --- Model Hyper-parameters ---------------------------------------------------\n# SEQ_LEN, PRED_LEN = 30, 1\n# INPUT_SIZE_BBOX = 4\n# INPUT_SIZE_POSE = 34\n# INPUT_SIZE_EGO_SPEED = 1\n# INPUT_SIZE_EGO_ACC = 2\n# INPUT_SIZE_EGO_GYRO = 1\n# INPUT_SIZE_PED_ACTION = 1\n# INPUT_SIZE_PED_LOOK = 1\n# INPUT_SIZE_PED_OCC = 1\n# INPUT_SIZE_TL_STATE = 4\n# NUM_SIGNALIZED_CATS = 4\n# NUM_INTERSECTION_CATS = 5\n# NUM_AGE_CATS = 4\n# NUM_GENDER_CATS = 3\n# NUM_TRAFFIC_DIR_CATS = 2\n# LANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7: 4, 8: 4}\n# NUM_LANE_CATS = len(set(LANE_CATEGORIES.values()))\n# INPUT_SIZE_STATIC = (\n#     NUM_SIGNALIZED_CATS\n#     + NUM_INTERSECTION_CATS\n#     + NUM_AGE_CATS\n#     + NUM_GENDER_CATS\n#     + NUM_TRAFFIC_DIR_CATS\n#     + NUM_LANE_CATS\n# )\n\n# LSTM_HIDDEN_SIZE = 256\n# NUM_LSTM_LAYERS = 2\n# DROPOUT_RATE = 0.3\n# NUM_CLASSES = 2\n# ATTENTION_DIM = 128\n\n# # --- Training Hyper-parameters ------------------------------------------------\n# LEARNING_RATE = 1e-4\n# BATCH_SIZE = 32\n# NUM_EPOCHS = 5\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {DEVICE}\")\n\n# # --- Dataset splits -----------------------------------------------------------\n# VAL_SETS_STR = [\"set05\", \"set06\"]\n\n# # --- Paths for pre-processed data --------------------------------------------\n# BALANCED_DATA_PKL_PATH = \"/kaggle/working/balanced_train_data.pkl\"\n# SCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\"\n\n# # -----------------------------------------------------------------------------#\n# #                               Helper classes (Dataset)                       #\n# # -----------------------------------------------------------------------------\n\n# # Using to_one_hot from Cell 1. If Cell 1 is not run, define it here:\n# if 'to_one_hot' not in globals():\n#     def to_one_hot(index, num_classes):\n#         vec = np.zeros(num_classes, dtype=np.float32)\n#         safe_index = int(np.clip(index, 0, num_classes - 1))\n#         vec[safe_index] = 1.0\n#         return vec\n\n# class PIEDataset_Cell2(Dataset): # Renamed to avoid conflict if Cell 1 is run in same notebook\n#     \"\"\"\n#     Dataset that can dynamically enable/disable streams. (Copied from original cell 2)\n#     \"\"\"\n#     def __init__(\n#         self,\n#         pie_database,\n#         set_names,\n#         pose_data_dir,\n#         seq_len,\n#         pred_len,\n#         scalers=None,\n#         active_streams=None, # This should be ALL_POSSIBLE_STREAMS_CELL2 for val\n#     ):\n#         self.pie_db = pie_database\n#         self.set_names = set_names\n#         self.pose_data_dir = pose_data_dir\n#         self.seq_len = seq_len\n#         self.pred_len = pred_len\n#         self.scalers = scalers or {}\n#         # This dataset needs to be able to generate ALL streams,\n#         # then the loader or model will select the ACTIVE_STREAMS.\n#         # However, for efficiency, it's better if it only processes active_streams.\n#         # The original code passed ALL_POSSIBLE_STREAMS to PIEDataset for validation.\n#         # Let's assume it should prepare data for any of ALL_POSSIBLE_STREAMS_CELL2\n#         # but only return those specified by a different active_streams parameter later.\n#         # For now, let's assume active_streams passed here are the ones to process.\n#         self.streams_to_extract = active_streams or ALL_POSSIBLE_STREAMS_CELL2\n#         self.sequences = []\n#         self.all_pose_data = {}\n\n#         self._input_sizes_for_error = self._get_input_sizes_dict()\n\n#         if \"pose\" in self.streams_to_extract:\n#             self._load_pose_data()\n\n#         self._generate_sequence_list()\n#         if not self.sequences:\n#             # Make this a warning or handle it, as val set might be empty based on set_names\n#             print(f\"Warning: PIEDataset_Cell2 init: No sequences for {self.set_names}\")\n\n\n#     def _get_input_sizes_dict(self):\n#         input_sizes = {}\n#         special_cases = {\n#             \"TRAFFIC_LIGHT\": \"TL_STATE\", \"STATIC_CONTEXT\": \"STATIC\",\n#             \"EGO_SPEED\": \"EGO_SPEED\", \"EGO_ACC\": \"EGO_ACC\", \"EGO_GYRO\": \"EGO_GYRO\",\n#             \"PED_ACTION\": \"PED_ACTION\", \"PED_LOOK\": \"PED_LOOK\", \"PED_OCCLUSION\": \"PED_OCC\",\n#         }\n#         for stream in ALL_POSSIBLE_STREAMS_CELL2: # Check against all possible for safety\n#             size_constant_name = f\"INPUT_SIZE_{stream.upper()}\"\n#             suffix = special_cases.get(stream.upper())\n#             if suffix: size_constant_name = f\"INPUT_SIZE_{suffix}\"\n#             elif stream == \"bbox\": size_constant_name = \"INPUT_SIZE_BBOX\"\n#             elif stream == \"pose\": size_constant_name = \"INPUT_SIZE_POSE\"\n#             input_sizes[stream] = globals().get(size_constant_name, 1)\n#         return input_sizes\n\n#     def _load_pose_data(self):\n#         sets_loaded_count = 0\n#         for set_id in self.set_names:\n#             self.all_pose_data[set_id] = {}\n#             pose_set_path = os.path.join(self.pose_data_dir, set_id)\n#             if not os.path.isdir(pose_set_path): continue\n#             pkl_files_in_set = [f for f in os.listdir(pose_set_path) if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")]\n#             if not pkl_files_in_set: continue\n#             loaded_video_count = 0\n#             for pkl_filename in pkl_files_in_set:\n#                 pkl_file_path = os.path.join(pose_set_path, pkl_filename)\n#                 try:\n#                     with open(pkl_file_path, \"rb\") as f: loaded_pkl_content = pickle.load(f)\n#                 except FileNotFoundError: continue\n#                 except Exception as e: print(f\"Error loading pose PKL {pkl_file_path}: {e}\"); continue\n#                 if len(loaded_pkl_content) != 1: continue\n#                 unique_video_key, video_data = list(loaded_pkl_content.items())[0]\n#                 video_id = \"_\".join(unique_video_key.split(\"_\")[1:])\n#                 if video_id in self.pie_db.get(set_id, {}):\n#                     self.all_pose_data[set_id][video_id] = video_data\n#                     loaded_video_count += 1\n#             if loaded_video_count > 0: sets_loaded_count += 1\n\n#     def _generate_sequence_list(self):\n#         sequence_count = 0\n#         for set_id in self.set_names:\n#             if set_id not in self.pie_db: continue\n#             for video_id, video_data in self.pie_db[set_id].items():\n#                 if \"ped_annotations\" not in video_data: continue\n#                 for ped_id, ped_data in video_data[\"ped_annotations\"].items():\n#                     frames = ped_data.get(\"frames\", [])\n#                     if len(frames) < self.seq_len + self.pred_len: continue\n#                     frames_sorted = sorted(frames)\n#                     for i in range(len(frames_sorted) - self.seq_len - self.pred_len + 1):\n#                         start_f, obs_end_f = frames_sorted[i], frames_sorted[i + self.seq_len - 1]\n#                         if obs_end_f - start_f != self.seq_len - 1: continue\n#                         target_idx = i + self.seq_len + self.pred_len - 1\n#                         if target_idx >= len(frames_sorted): continue\n#                         target_f = frames_sorted[target_idx]\n#                         if target_f - obs_end_f != self.pred_len: continue\n#                         self.sequences.append((set_id, video_id, ped_id, start_f))\n#                         sequence_count += 1\n#         print(f\"PIEDataset_Cell2 initialized with {sequence_count} sequences for sets {self.set_names}.\")\n\n#     def __len__(self): return len(self.sequences)\n\n#     def __getitem__(self, idx):\n#         set_id, video_id, ped_id, start_frame = self.sequences[idx]\n#         frame_nums = list(range(start_frame, start_frame + self.seq_len))\n#         target_frame_num = start_frame + self.seq_len + self.pred_len - 1\n#         video_db, ped_db = self.pie_db.get(set_id, {}).get(video_id, {}), {}\n#         if video_db: ped_db = video_db.get(\"ped_annotations\", {}).get(ped_id, {})\n#         ego_db = video_db.get(\"vehicle_annotations\", {}) if video_db else {}\n#         traffic_db = video_db.get(\"traffic_annotations\", {}) if video_db else {}\n#         ped_attributes = ped_db.get(\"attributes\", {}) if ped_db else {}\n\n#         feature_sequences = {s: [] for s in self.streams_to_extract}\n#         label = 0\n#         if ped_db and \"frames\" in ped_db and \"behavior\" in ped_db and \"cross\" in ped_db[\"behavior\"]:\n#             try:\n#                 target_idx = ped_db[\"frames\"].index(target_frame_num)\n#                 label = ped_db[\"behavior\"][\"cross\"][target_idx]\n#                 if label == -1: label = 0\n#             except (ValueError, IndexError): pass\n\n#         static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n#         if \"static_context\" in self.streams_to_extract:\n#             sig_idx, int_idx = ped_attributes.get(\"signalized\", 0), ped_attributes.get(\"intersection\", 0)\n#             age_idx, gen_idx = ped_attributes.get(\"age\", 2), ped_attributes.get(\"gender\", 0)\n#             td_idx = int(ped_attributes.get(\"traffic_direction\", 0))\n#             nl_val = ped_attributes.get(\"num_lanes\", 2)\n#             nl_cat_idx = LANE_CATEGORIES.get(nl_val, LANE_CATEGORIES[max(LANE_CATEGORIES.keys())])\n#             static_vec = np.concatenate([\n#                 to_one_hot(sig_idx, NUM_SIGNALIZED_CATS), to_one_hot(int_idx, NUM_INTERSECTION_CATS),\n#                 to_one_hot(age_idx, NUM_AGE_CATS), to_one_hot(gen_idx, NUM_GENDER_CATS),\n#                 to_one_hot(td_idx, NUM_TRAFFIC_DIR_CATS), to_one_hot(nl_cat_idx, NUM_LANE_CATS),\n#             ])\n#             if static_vec.shape[0] != INPUT_SIZE_STATIC: static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n\n#         for frame_num in frame_nums:\n#             frame_db_idx = -1\n#             if ped_db and \"frames\" in ped_db:\n#                 try: frame_db_idx = ped_db[\"frames\"].index(frame_num)\n#                 except ValueError: pass\n#             ego_frame_data = ego_db.get(frame_num, {})\n\n#             if \"bbox\" in self.streams_to_extract:\n#                 bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32)\n#                 if ped_db and frame_db_idx!=-1 and \"bbox\" in ped_db and len(ped_db[\"bbox\"]) > frame_db_idx:\n#                     try:\n#                         x1,y1,x2,y2 = ped_db[\"bbox\"][frame_db_idx]\n#                         img_w, img_h = (video_db.get(\"width\",1920) if video_db else 1920), (video_db.get(\"height\",1080) if video_db else 1080)\n#                         if img_w > 0 and img_h > 0:\n#                             cx,cy,w,h = ((x1+x2)/2)/img_w, ((y1+y2)/2)/img_h, (x2-x1)/img_w, (y2-y1)/img_h\n#                             if 0<w and 0<h and 0<=cx<=1 and 0<=cy<=1:\n#                                 bbox_norm=np.array([cx,cy,w,h],dtype=np.float32)\n#                     except Exception:\n#                         pass\n#                 feature_sequences[\"bbox\"].append(bbox_norm)\n#             if \"pose\" in self.streams_to_extract:\n#                 pose_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32)\n#                 vid_pose = self.all_pose_data.get(set_id,{}).get(video_id,{})\n#                 loaded_pose = vid_pose.get(frame_num,{}).get(ped_id)\n#                 if isinstance(loaded_pose,np.ndarray) and loaded_pose.shape==(INPUT_SIZE_POSE,): pose_vector=loaded_pose\n#                 feature_sequences[\"pose\"].append(pose_vector)\n#             if \"ego_speed\" in self.streams_to_extract:\n#                 speed = ego_frame_data.get(\"OBD_speed\",0.0) or ego_frame_data.get(\"GPS_speed\",0.0)\n#                 speed_scaled = (speed - self.scalers.get(\"ego_speed_mean\",0.0)) / self.scalers.get(\"ego_speed_std\",1.0)\n#                 feature_sequences[\"ego_speed\"].append([speed_scaled])\n#             if \"ego_acc\" in self.streams_to_extract:\n#                 ax,ay = ego_frame_data.get(\"accX\",0.0), ego_frame_data.get(\"accY\",0.0)\n#                 ax_s = (ax - self.scalers.get(\"accX_mean\",0.0))/self.scalers.get(\"accX_std\",1.0)\n#                 ay_s = (ay - self.scalers.get(\"accY_mean\",0.0))/self.scalers.get(\"accY_std\",1.0)\n#                 feature_sequences[\"ego_acc\"].append([ax_s, ay_s])\n#             if \"ego_gyro\" in self.streams_to_extract:\n#                 gz = ego_frame_data.get(\"gyroZ\",0.0)\n#                 gz_s = (gz - self.scalers.get(\"gyroZ_mean\",0.0))/self.scalers.get(\"gyroZ_std\",1.0)\n#                 feature_sequences[\"ego_gyro\"].append([gz_s])\n#             if \"ped_action\" in self.streams_to_extract:\n#                 action=0\n#                 if ped_db and frame_db_idx!=-1 and \"behavior\" in ped_db and \"action\" in ped_db[\"behavior\"] and len(ped_db[\"behavior\"][\"action\"])>frame_db_idx:\n#                     action = ped_db[\"behavior\"][\"action\"][frame_db_idx]\n#                 feature_sequences[\"ped_action\"].append([float(action)])\n#             if \"ped_look\" in self.streams_to_extract:\n#                 look=0\n#                 if ped_db and frame_db_idx!=-1 and \"behavior\" in ped_db and \"look\" in ped_db[\"behavior\"] and len(ped_db[\"behavior\"][\"look\"])>frame_db_idx:\n#                     look = ped_db[\"behavior\"][\"look\"][frame_db_idx]\n#                 feature_sequences[\"ped_look\"].append([float(look)])\n#             if \"ped_occlusion\" in self.streams_to_extract:\n#                 occ=0.0\n#                 if ped_db and frame_db_idx!=-1 and \"occlusion\" in ped_db and len(ped_db[\"occlusion\"])>frame_db_idx:\n#                     occ = float(ped_db[\"occlusion\"][frame_db_idx])/2.0\n#                 feature_sequences[\"ped_occlusion\"].append([occ])\n#             if \"traffic_light\" in self.streams_to_extract:\n#                 state_int=0\n#                 for _,obj_data in traffic_db.items():\n#                     if obj_data.get(\"obj_class\")==\"traffic_light\" and \"frames\" in obj_data and \"state\" in obj_data:\n#                         try:\n#                             tl_idx = obj_data[\"frames\"].index(frame_num)\n#                             if obj_data[\"state\"][tl_idx]!=0: state_int=obj_data[\"state\"][tl_idx]; break\n#                         except (ValueError,IndexError): continue\n#                 feature_sequences[\"traffic_light\"].append(to_one_hot(state_int,INPUT_SIZE_TL_STATE))\n#             if \"static_context\" in self.streams_to_extract:\n#                 feature_sequences[\"static_context\"].append(static_vec)\n#         features = {}\n#         try:\n#             for name in self.streams_to_extract:\n#                 features[name] = torch.tensor(np.asarray(feature_sequences[name],dtype=np.float32),dtype=torch.float32)\n#         except Exception as e:\n#             print(f\"Error converting features idx {idx} ({set_id},{video_id},{ped_id},{start_frame}): {e}. Ret zeros.\")\n#             features = {name:torch.zeros((self.seq_len,self._input_sizes_for_error.get(name,1)),dtype=torch.float32) for name in self.streams_to_extract}\n#         return features, torch.tensor(label,dtype=torch.long)\n\n\n# class BalancedDataset(Dataset):\n#     def __init__(self, data_dict, active_streams, label_key=\"label\"):\n#         self.active_streams = active_streams\n#         self.label_key = label_key\n#         if self.label_key not in data_dict or not data_dict[self.label_key]:\n#             raise ValueError(f\"Label key '{self.label_key}' missing/empty.\")\n#         self.num_samples = len(data_dict[self.label_key])\n#         if self.num_samples == 0: print(\"Warning: BalancedDataset initialized with zero samples.\")\n#         self.features = {}\n#         for stream in self.active_streams:\n#             if stream in data_dict and data_dict[stream]:\n#                 try: self.features[stream] = torch.tensor(np.asarray(data_dict[stream]),dtype=torch.float32)\n#                 except ValueError as e: raise ValueError(f\"Error converting stream '{stream}': {e}\")\n#             else: raise KeyError(f\"Stream '{stream}' missing or empty in data.\")\n#         try: self.labels = torch.tensor([lbl[0] for lbl in data_dict[self.label_key]],dtype=torch.long)\n#         except (IndexError, TypeError) as e: raise ValueError(f\"Error converting labels: {e}\")\n#         for stream in self.active_streams:\n#             if len(self.features[stream]) != self.num_samples:\n#                 raise ValueError(f\"Len mismatch: '{stream}' ({len(self.features[stream])}) vs labels ({self.num_samples})\")\n#     def __len__(self): return self.num_samples\n#     def __getitem__(self, idx):\n#         feature_dict = {s: self.features[s][idx] for s in self.active_streams}\n#         label = self.labels[idx]\n#         return feature_dict, label\n\n\n# class Attention(nn.Module):\n#     def __init__(self, hidden_dim, attention_dim):\n#         super().__init__()\n#         self.attention_net = nn.Sequential(\n#             nn.Linear(hidden_dim, attention_dim), nn.Tanh(), nn.Linear(attention_dim, 1),\n#         )\n#     def forward(self, lstm_output):\n#         att_scores = self.attention_net(lstm_output).squeeze(2)\n#         att_weights = torch.softmax(att_scores, dim=1)\n#         context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1)\n#         return context_vector, att_weights\n\n# # -----------------------------------------------------------------------------#\n# #                  ***  MODEL WITH WEIGHTED AVERAGE FUSION  ***                #\n# # -----------------------------------------------------------------------------\n# class MultiStreamWeightedAvgLSTM(nn.Module):\n#     def __init__(\n#         self, input_sizes, lstm_hidden_size, num_lstm_layers,\n#         num_classes, attention_dim, dropout_rate, stream_names=None,\n#     ):\n#         super().__init__()\n#         if not stream_names: raise ValueError(\"stream_names cannot be empty.\")\n#         self.stream_names = stream_names\n#         self.num_active_streams = len(stream_names)\n#         self.lstm_output_dim = lstm_hidden_size * 2\n#         self.lstms, self.attentions = nn.ModuleDict(), nn.ModuleDict()\n#         print(f\"Initializing Weighted-Avg model with streams: {self.stream_names}\")\n#         for name in self.stream_names:\n#             if name not in input_sizes: raise KeyError(f\"Input size for stream '{name}' not provided.\")\n#             in_size = input_sizes[name]\n#             print(f\"  – Adding stream '{name}' (input {in_size})\")\n#             self.lstms[name] = nn.LSTM(\n#                 in_size, lstm_hidden_size, num_lstm_layers, batch_first=True,\n#                 dropout=dropout_rate if num_lstm_layers > 1 else 0, bidirectional=True,\n#             )\n#             self.attentions[name] = Attention(self.lstm_output_dim, attention_dim)\n#         self.fusion_weights = nn.Parameter(torch.ones(self.num_active_streams))\n#         fused_dim = self.lstm_output_dim\n#         self.dropout = nn.Dropout(dropout_rate)\n#         inter_dim = max(num_classes * 4, fused_dim // 2)\n#         self.fc1 = nn.Linear(fused_dim, inter_dim)\n#         self.relu = nn.ReLU()\n#         self.fc2 = nn.Linear(inter_dim, num_classes)\n\n#     def forward(self, x):\n#         ctx_vecs = []\n#         for name in self.stream_names:\n#             if name not in x:\n#                 zero_ctx = torch.zeros(x[next(iter(x))].shape[0], self.lstm_output_dim, device=x[next(iter(x))].device)\n#                 ctx_vecs.append(zero_ctx)\n#                 continue\n#             lstm_out, _ = self.lstms[name](x[name])\n#             context_vector, _ = self.attentions[name](lstm_out)\n#             ctx_vecs.append(context_vector)\n#         if len(ctx_vecs) != self.num_active_streams:\n#             raise RuntimeError(f\"context_vectors({len(ctx_vecs)}) != num_streams({self.num_active_streams})\")\n#         stacked = torch.stack(ctx_vecs, dim=1)\n#         weights = torch.softmax(self.fusion_weights, dim=0).view(1, -1, 1)\n#         fused = torch.sum(stacked * weights, dim=1)\n#         out = self.dropout(fused)\n#         out = self.relu(self.fc1(out))\n#         out = self.dropout(out)\n#         logits = self.fc2(out)\n#         return logits\n\n# # -----------------------------------------------------------------------------#\n# #         Training / evaluation / Threshold Tuning helpers                     #\n# # -----------------------------------------------------------------------------\n\n# def get_all_probabilities_and_labels(model, dataloader, device):\n#     \"\"\"Gets true labels and predicted probabilities for the positive class.\"\"\"\n#     model.eval()\n#     labels_all, probs_all_positive_class = [], []\n#     active_streams = model.stream_names\n\n#     with torch.no_grad():\n#         for feats, labels in tqdm(dataloader, desc=\"Getting Probs & Labels for Threshold Tuning\", leave=False):\n#             inputs = {name: feats[name].to(device) for name in active_streams if name in feats}\n#             outputs = model(inputs)  # Logits\n#             probs = torch.softmax(outputs, dim=1)  # Probabilities\n\n#             labels_all.extend(labels.cpu().numpy())\n#             probs_all_positive_class.extend(probs[:, 1].cpu().numpy()) # Prob for class 1\n\n#     return np.asarray(labels_all), np.asarray(probs_all_positive_class)\n\n\n# def find_optimal_threshold(y_true, y_probs_positive_class, metric='f1', steps=100):\n#     \"\"\"Finds the optimal probability threshold for a binary classifier to maximize a metric.\"\"\"\n#     best_threshold = 0.5\n#     best_metric_val = -1.0\n\n#     if metric == 'f1':\n#         # precision_recall_fscore_support returns (precision, recall, f1, support)\n#         metric_func = lambda yt, yp: precision_recall_fscore_support(yt, yp, average='binary', pos_label=1, zero_division=0)[2]\n#     else:\n#         raise ValueError(f\"Unsupported metric for threshold tuning: {metric}\")\n\n#     thresholds = np.linspace(0.0, 1.0, steps + 1) # e.g., 0.0, 0.01, ..., 1.0\n\n#     for threshold in tqdm(thresholds, desc=f\"Tuning Threshold ({metric.upper()})\", leave=False):\n#         y_pred_tuned = (y_probs_positive_class >= threshold).astype(int)\n#         current_metric_val = metric_func(y_true, y_pred_tuned)\n\n#         if current_metric_val > best_metric_val:\n#             best_metric_val = current_metric_val\n#             best_threshold = threshold\n#         elif current_metric_val == best_metric_val: # Tie-breaking\n#             if abs(threshold - 0.5) < abs(best_threshold - 0.5):\n#                 best_threshold = threshold\n    \n#     print(f\"Optimal threshold found: {best_threshold:.4f} (max {metric.upper()}: {best_metric_val:.4f})\")\n#     return best_threshold, best_metric_val\n\n\n# def train_epoch(model, dataloader, optimizer, criterion, device):\n#     model.train()\n#     total_loss = 0.0\n#     all_preds, all_labels = [], []\n#     active = model.stream_names\n#     for feats, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n#         inputs = {n: feats[n].to(device) for n in active if n in feats}\n#         labels = labels.to(device)\n#         optimizer.zero_grad()\n#         outputs = model(inputs)\n#         loss = criterion(outputs, labels)\n#         loss.backward()\n#         optimizer.step()\n#         total_loss += loss.item()\n#         all_preds.extend(torch.argmax(outputs, 1).cpu().numpy())\n#         all_labels.extend(labels.cpu().numpy())\n#     return total_loss / max(1, len(dataloader)), accuracy_score(all_labels, all_preds)\n\n\n# def evaluate_epoch(model, dataloader, criterion, device):\n#     \"\"\"Evaluates model for one epoch, returns metrics at 0.5 threshold and raw probabilities.\"\"\"\n#     model.eval()\n#     total_loss = 0.0\n#     all_labels_list, all_preds_list, all_probs_list = [], [], [] # Changed to list for easier appending\n#     active = model.stream_names\n\n#     with torch.no_grad():\n#         for feats, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n#             inputs = {n: feats[n].to(device) for n in active if n in feats}\n#             labels_gpu = labels.to(device) # Ensure labels are on device for loss\n#             outputs = model(inputs)\n#             loss = criterion(outputs, labels_gpu) # Use labels_gpu for loss\n#             total_loss += loss.item()\n#             probs = torch.softmax(outputs, 1)\n#             preds = torch.argmax(probs, 1)\n#             all_labels_list.extend(labels.cpu().numpy()) # Use original labels for metrics\n#             all_preds_list.extend(preds.cpu().numpy())\n#             all_probs_list.extend(probs.cpu().numpy()) # Store all class probabilities\n\n#     avg_loss = total_loss / max(1, len(dataloader))\n#     all_labels = np.asarray(all_labels_list)\n#     all_preds = np.asarray(all_preds_list)\n#     all_probs_np = np.asarray(all_probs_list) # Probabilities for all classes\n\n#     acc = accuracy_score(all_labels, all_preds)\n#     prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", pos_label=1, zero_division=0)\n#     auc = roc_auc_score(all_labels, all_probs_np[:, 1]) if len(np.unique(all_labels)) > 1 else float(\"nan\")\n    \n#     return {\n#         \"loss\": avg_loss, \"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"auc\": auc,\n#         \"labels\": all_labels, \"probs_positive_class\": all_probs_np[:, 1] # Return these for potential later use\n#     }\n\n# # get_predictions_and_labels is used for CM after training. We can update it to use tuned threshold\n# # or create a new one. For now, the main block will handle predictions with tuned threshold.\n# # The original get_predictions_and_labels can be kept if needed for 0.5 threshold CM elsewhere.\n\n# def get_predictions_at_threshold(y_probs_positive_class, threshold):\n#     return (y_probs_positive_class >= threshold).astype(int)\n\n# # -----------------------------------------------------------------------------#\n# #                            Main execution block                              #\n# # -----------------------------------------------------------------------------\n\n# if __name__ == \"__main__\":\n#     print(\"--- CELL 2: Running Model Training/Evaluation with Weighted Fusion ---\")\n#     print(f\"Active Streams: {ACTIVE_STREAMS}\")\n\n#     print(\"\\nLoading balanced training data …\")\n#     try:\n#         with open(BALANCED_DATA_PKL_PATH, \"rb\") as f: balanced_train_data_dict = pickle.load(f)\n#         with open(SCALERS_PKL_PATH, \"rb\") as f: scalers = pickle.load(f)\n#         print(\"   ✓ pre-processed data loaded.\")\n#     except FileNotFoundError as e: print(f\"ERROR: {e}. Run cell 1 first.\"); sys.exit(1)\n#     except Exception as e: print(f\"Error loading pre-processed data: {e}\"); sys.exit(1)\n\n#     print(\"\\nLoading PIE database cache for validation …\")\n#     if not os.path.exists(PIE_DATABASE_CACHE_PATH): raise FileNotFoundError(\"PIE db cache not found.\")\n#     try:\n#         with open(PIE_DATABASE_CACHE_PATH, \"rb\") as f: pie_database = pickle.load(f)\n#     except Exception as e: raise RuntimeError(f\"Failed to load PIE database: {e}\")\n#     print(\"   ✓ PIE database loaded.\")\n\n#     print(\"\\nCreating Datasets and DataLoaders …\")\n#     try:\n#         train_dataset = BalancedDataset(balanced_train_data_dict, ACTIVE_STREAMS, label_key=\"label\")\n#         del balanced_train_data_dict\n#         # For validation, PIEDataset_Cell2 should be configured to extract ALL_POSSIBLE_STREAMS_CELL2\n#         # if the model might use any of them, or just ACTIVE_STREAMS if validation uses same streams.\n#         # Original code used ALL_POSSIBLE_STREAMS for val_dataset's PIEDataset.\n#         # Assuming val_dataset also processes only ACTIVE_STREAMS for consistency with model input.\n#         val_dataset = PIEDataset_Cell2(\n#             pie_database, VAL_SETS_STR, POSE_DATA_DIR, SEQ_LEN, PRED_LEN,\n#             scalers, active_streams=ALL_POSSIBLE_STREAMS_CELL2, # Val dataset prepares all, model selects\n#         )\n#     except Exception as e: print(f\"Error creating datasets: {e}\"); raise\n#     if len(train_dataset) == 0 : raise ValueError(\"Train dataset is empty!\")\n#     if len(val_dataset) == 0 : print(\"Warning: Validation dataset is empty! Check VAL_SETS_STR.\")\n\n\n#     train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n#     val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n#     print(\"   ✓ DataLoaders ready.\")\n#     del pie_database; gc.collect()\n\n#     print(\"\\nInitialising model …\")\n#     current_input_sizes = {}\n#     SPECIAL_CELL2 = { # Renamed for safety\n#         \"TRAFFIC_LIGHT\": \"TL_STATE\", \"STATIC_CONTEXT\": \"STATIC\", \"EGO_SPEED\": \"EGO_SPEED\",\n#         \"EGO_ACC\": \"EGO_ACC\", \"EGO_GYRO\": \"EGO_GYRO\", \"PED_ACTION\": \"PED_ACTION\",\n#         \"PED_LOOK\": \"PED_LOOK\", \"PED_OCCLUSION\": \"PED_OCC\",\n#     }\n#     for s in ACTIVE_STREAMS: # Model is built only for ACTIVE_STREAMS\n#         name = f\"INPUT_SIZE_{SPECIAL_CELL2.get(s.upper(), s.upper())}\"\n#         if s == \"bbox\": name = \"INPUT_SIZE_BBOX\"\n#         elif s == \"pose\": name = \"INPUT_SIZE_POSE\"\n#         if name not in globals(): raise ValueError(f\"Input-size const {name} not found.\")\n#         current_input_sizes[s] = globals()[name]\n\n#     model = MultiStreamWeightedAvgLSTM(\n#         current_input_sizes, LSTM_HIDDEN_SIZE, NUM_LSTM_LAYERS, NUM_CLASSES,\n#         ATTENTION_DIM, DROPOUT_RATE, stream_names=ACTIVE_STREAMS,\n#     ).to(DEVICE)\n#     print(\"\\n--- Model architecture ---\"); print(model)\n#     total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n#     print(f\"Trainable parameters: {total_params:,}\\n{'-'*30}\")\n\n#     print(\"\\nCalculating class weights …\")\n#     train_labels = train_dataset.labels.tolist()\n#     n0, n1 = train_labels.count(0), train_labels.count(1)\n#     total = len(train_labels)\n#     w0, w1 = (1.0,1.0) if total==0 else ( (total/(2.*n0), total/(2.*n1)) if n0>0 and n1>0 else ((0.,1.) if n0==0 else (1.,0.)) )\n#     class_weights = torch.tensor([w0, w1], dtype=torch.float32).to(DEVICE)\n#     print(f\"Loss weights → 0: {w0:.2f}, 1: {w1:.2f}\")\n#     criterion = nn.CrossEntropyLoss(weight=class_weights)\n#     optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n#     best_val_f1_at_0_5_thresh = -1.0 # F1 at 0.5 threshold for saving model\n#     history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": [], \"val_f1_0.5\": []}\n#     best_model_path = \"\"\n\n#     print(\"\\n--- Starting training ---\")\n#     for epoch in range(NUM_EPOCHS):\n#         t0 = time.time()\n#         train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n#         # evaluate_epoch returns metrics at 0.5 threshold\n#         epoch_eval_metrics = evaluate_epoch(model, val_loader, criterion, DEVICE)\n\n#         history[\"train_loss\"].append(train_loss)\n#         history[\"val_loss\"].append(epoch_eval_metrics[\"loss\"])\n#         history[\"train_acc\"].append(train_acc)\n#         history[\"val_acc\"].append(epoch_eval_metrics[\"accuracy\"])\n#         history[\"val_f1_0.5\"].append(epoch_eval_metrics[\"f1\"]) # F1 at 0.5\n\n#         print(f\"\\nEpoch {epoch + 1:02d}/{NUM_EPOCHS} – {time.time() - t0:.1f}s\")\n#         print(f\"  train loss {train_loss:.4f} | acc {train_acc:.4f}\")\n#         print(f\"  val   loss {epoch_eval_metrics['loss']:.4f} | acc {epoch_eval_metrics['accuracy']:.4f} (at 0.5 thresh)\")\n#         print(f\"           prec {epoch_eval_metrics['precision']:.4f} | rec {epoch_eval_metrics['recall']:.4f} | f1 {epoch_eval_metrics['f1']:.4f} | auc {epoch_eval_metrics['auc']:.4f} (at 0.5 thresh / auc indep.)\")\n\n#         if epoch_eval_metrics[\"f1\"] > best_val_f1_at_0_5_thresh:\n#             best_val_f1_at_0_5_thresh = epoch_eval_metrics[\"f1\"]\n#             best_model_path = f\"best_model_weighted_{'_'.join(sorted(ACTIVE_STREAMS))}_ep{epoch + 1}.pth\"\n#             torch.save(model.state_dict(), best_model_path)\n#             print(f\"  ✓ new best model saved → {best_model_path} (F1@0.5 {best_val_f1_at_0_5_thresh:.4f})\")\n#     print(\"\\n--- Training finished ---\")\n\n#     print(\"\\nPlotting training curves …\")\n#     fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n#     ax[0].plot(range(1, NUM_EPOCHS + 1), history[\"train_loss\"], label=\"Train Loss\")\n#     ax[0].plot(range(1, NUM_EPOCHS + 1), history[\"val_loss\"], label=\"Val Loss\")\n#     ax[0].set_xlabel(\"Epoch\"); ax[0].set_ylabel(\"Loss\"); ax[0].set_title(\"Loss curve\"); ax[0].legend(); ax[0].grid(True)\n#     ax[1].plot(range(1, NUM_EPOCHS + 1), history[\"train_acc\"], label=\"Train Acc\")\n#     ax[1].plot(range(1, NUM_EPOCHS + 1), history[\"val_acc\"], label=\"Val Acc (at 0.5)\")\n#     ax[1].plot(range(1, NUM_EPOCHS + 1), history[\"val_f1_0.5\"], \"--\", label=\"Val F1 (at 0.5)\")\n#     ax[1].set_xlabel(\"Epoch\"); ax[1].set_ylabel(\"Metric\"); ax[1].set_title(\"Accuracy & F1 (at 0.5 Thresh)\"); ax[1].legend(); ax[1].grid(True)\n#     plt.tight_layout(); plt.show()\n\n#     # ------------------- final evaluation (best model) & Threshold Tuning -----------------------\n#     print(\"\\n--- Final Evaluation on Validation set with Threshold Tuning ---\")\n#     if best_model_path and os.path.exists(best_model_path):\n#         try:\n#             model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n#             print(f\"Loaded best model: {best_model_path}\")\n#         except Exception as e: print(f\"Warning: could not load best model ({e}). Using last epoch params.\")\n#     else: print(\"Warning: best model not found or path empty, using last epoch parameters.\")\n\n#     # 1. Get probabilities and true labels from the validation set using the loaded best model\n#     y_true_val, y_probs_val_positive = get_all_probabilities_and_labels(model, val_loader, DEVICE)\n\n#     if len(y_true_val) == 0:\n#         print(\"Validation set is empty. Cannot perform threshold tuning or final evaluation.\")\n#     else:\n#         # 2. Find the optimal threshold using F1-score\n#         optimal_threshold, best_f1_at_optimal_thresh = find_optimal_threshold(y_true_val, y_probs_val_positive, metric='f1', steps=100)\n        \n#         # 3. Apply the optimal threshold to get new predictions\n#         y_pred_val_tuned = get_predictions_at_threshold(y_probs_val_positive, optimal_threshold)\n\n#         # 4. Calculate metrics using the tuned predictions\n#         final_accuracy_tuned = accuracy_score(y_true_val, y_pred_val_tuned)\n#         final_precision_tuned, final_recall_tuned, final_f1_tuned, _ = \\\n#             precision_recall_fscore_support(y_true_val, y_pred_val_tuned, average='binary', pos_label=1, zero_division=0)\n        \n#         final_auc_val = roc_auc_score(y_true_val, y_probs_val_positive) if len(np.unique(y_true_val)) > 1 else float('nan')\n\n#         cm_tuned = confusion_matrix(y_true_val, y_pred_val_tuned, labels=[0, 1])\n\n#         print(\"\\n--- Final metrics (Weighted Avg Fusion) with Tuned Threshold ---\")\n#         print(f\"{'Optimal Threshold':<20}: {optimal_threshold:.4f}\")\n#         print(f\"{'Accuracy':<20}: {final_accuracy_tuned:.4f}\")\n#         print(f\"{'Precision':<20}: {final_precision_tuned:.4f}\")\n#         print(f\"{'Recall':<20}: {final_recall_tuned:.4f}\")\n#         print(f\"{'F1-score':<20}: {final_f1_tuned:.4f} (Max F1 at optimal threshold)\")\n#         print(f\"{'AUC':<20}: {final_auc_val:.4f}\")\n#         print(f\"(Best F1@0.5 during training: {best_val_f1_at_0_5_thresh:.4f})\")\n\n#         ConfusionMatrixDisplay(cm_tuned, display_labels=[\"Not Crossing\", \"Crossing\"]).plot(cmap=plt.cm.Blues)\n#         plt.title(f\"Confusion Matrix (Optimal Threshold: {optimal_threshold:.2f})\")\n#         plt.show()\n\n#     if hasattr(model, \"fusion_weights\"):\n#         w = torch.softmax(model.fusion_weights, 0).detach().cpu().numpy()\n#         print(\"\\n--- Learned fusion weights (from best model) ---\")\n#         for stream, weight in zip(model.stream_names, w): print(f\"{stream:<15}: {weight:.4f}\")\n#         print(\"-\" * 30)\n\n#     print(\"\\n--- CELL 2: Script complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:39:45.590273Z","iopub.execute_input":"2025-05-26T21:39:45.590563Z","iopub.status.idle":"2025-05-26T21:40:23.016412Z","shell.execute_reply.started":"2025-05-26T21:39:45.590544Z","shell.execute_reply":"2025-05-26T21:40:23.014955Z"}},"outputs":[{"name":"stdout","text":"--- Running Weighted Average Fusion With Active Streams: ['bbox', 'ped_action', 'ped_look', 'ego_speed', 'ego_acc', 'static_context'] ---\nUsing device: cuda\n--- CELL 2: Running Model Training/Evaluation with Weighted Fusion ---\nActive Streams: ['bbox', 'ped_action', 'ped_look', 'ego_speed', 'ego_acc', 'static_context']\n\nLoading balanced training data …\n   ✓ pre-processed data loaded.\n\nLoading PIE database cache for validation …\n   ✓ PIE database loaded.\n\nCreating Datasets and DataLoaders …\nPIEDataset_Cell2 initialized with 77288 sequences for sets ['set05', 'set06'].\n   ✓ DataLoaders ready.\n\nInitialising model …\nInitializing Weighted-Avg model with streams: ['bbox', 'ped_action', 'ped_look', 'ego_speed', 'ego_acc', 'static_context']\n  – Adding stream 'bbox' (input 4)\n  – Adding stream 'ped_action' (input 1)\n  – Adding stream 'ped_look' (input 1)\n  – Adding stream 'ego_speed' (input 1)\n  – Adding stream 'ego_acc' (input 2)\n  – Adding stream 'static_context' (input 23)\n\n--- Model architecture ---\nMultiStreamWeightedAvgLSTM(\n  (lstms): ModuleDict(\n    (bbox): LSTM(4, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ped_action): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ped_look): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ego_speed): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (ego_acc): LSTM(2, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (static_context): LSTM(23, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n  )\n  (attentions): ModuleDict(\n    (bbox): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ped_action): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ped_look): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ego_speed): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (ego_acc): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n    (static_context): Attention(\n      (attention_net): Sequential(\n        (0): Linear(in_features=512, out_features=128, bias=True)\n        (1): Tanh()\n        (2): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc1): Linear(in_features=512, out_features=256, bias=True)\n  (relu): ReLU()\n  (fc2): Linear(in_features=256, out_features=2, bias=True)\n)\nTrainable parameters: 13,224,206\n------------------------------\n\nCalculating class weights …\nLoss weights → 0: 1.00, 1: 1.00\n\n--- Starting training ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3436 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827baafca8ff4e8fa0f8a81ff31e1355"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-7480de372e52>\u001b[0m in \u001b[0;36m<cell line: 546>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;31m# evaluate_epoch returns metrics at 0.5 threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0mepoch_eval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-7480de372e52>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0mactive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactive\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m                     \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_t\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_print_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmininterval\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcur_t\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmin_start_t\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_print_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m                         \u001b[0mlast_print_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_print_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m                         \u001b[0mlast_print_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_print_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;31m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1240\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ema_dn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ema_dt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_miniters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m                     \u001b[0;31m# If no `miniters` was specified, adjust automatically to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos, close, bar_style, check_delay)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;31m# never clear the bar (signal: msg='')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 \u001b[0mrtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Change bar style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/traitlets/traitlets.py\u001b[0m in \u001b[0;36m__set__\u001b[0;34m(self, obj, value)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTraitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The \"%s\" trait is read-only.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/traitlets/traitlets.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, obj, value)\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;31m# we explicitly compare silent to True just in case the equality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;31m# comparison above returns something other than True/False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_notify_trait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__set__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/traitlets/traitlets.py\u001b[0m in \u001b[0;36m_notify_trait\u001b[0;34m(self, name, old_value, new_value)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_notify_trait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m         self.notify_change(\n\u001b[0m\u001b[1;32m   1502\u001b[0m             Bunch(\n\u001b[1;32m   1503\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipywidgets/widgets/widget.py\u001b[0m in \u001b[0;36mnotify_change\u001b[0;34m(self, change)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_send_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# Send new state to front-end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipywidgets/widgets/widget.py\u001b[0m in \u001b[0;36msend_state\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0mA\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mproperty\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterable\u001b[0m \u001b[0mof\u001b[0m \u001b[0mproperty\u001b[0m \u001b[0mnames\u001b[0m \u001b[0mto\u001b[0m \u001b[0msync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfront\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \"\"\"\n\u001b[0;32m--> 578\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_property_lock\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# we need to keep this dict up to date with the front-end values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"# --- SCRIPT TO GENERATE PREDICTION VIDEO ---\n\nimport torch\nimport torch.nn as nn\n# import torch.optim as optim # Not needed for inference\nfrom torch.utils.data import Dataset # For PIEDataset class structure\n# import xml.etree.ElementTree as ET # Not needed if using PIE database\nimport os\nimport numpy as np\n# from sklearn.metrics import ... # Not needed for inference\n# from sklearn.preprocessing import StandardScaler # Scalers are loaded\nfrom tqdm.notebook import tqdm\nimport random\n# import math\n# import zipfile\nimport cv2 # Crucial for video reading/writing\n# import pandas as pd\nimport matplotlib.pyplot as plt # For potential debugging\n# import seaborn as sns\nimport pickle\nimport time\nimport sys\nimport gc\n\n# --- Add PIE utilities path if necessary (can be removed if PIE class isn't needed for DB loading) ---\n# pie_utilities_path = '/kaggle/working/PIE/utilities'\n# if pie_utilities_path not in sys.path:\n#     sys.path.insert(0, pie_utilities_path)\n# try:\n#     from pie_data import PIE\n# except ImportError as e:\n#     print(f\"Warn: Could not import PIE class: {e}. DB must exist if PIE object is used.\")\n#     PIE = None\n\n# --- Configuration ---\n# <<< --- SPECIFY THE VIDEO AND MODEL TO PROCESS --- >>>\nTARGET_SET_ID = 'set04'\nTARGET_VIDEO_ID = 'video_0012'\nMODEL_PATH = '/kaggle/input/mslstm-pid/pytorch/default/1/best_model_weighted_bbox_ego_acc_ego_speed_ped_action_ped_look_static_context_ep2.pth'\n# --- *** IMPORTANT: Define the streams this SPECIFIC model was trained with *** ---\nMODEL_ACTIVE_STREAMS = [\n    'bbox',\n    'ego_acc',\n    'ego_speed',\n    'ped_action',\n    'ped_look',\n    'static_context'\n]\n# --- *** END IMPORTANT --- >>>\n\n# --- Paths ---\nVIDEO_INPUT_DIR = '/kaggle/input' # Base for pie-setXX video dataset folders\nPOSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2' # Where setXX subdirs with POSE PKLs are\nYOLOP_FEATURE_DIR = '/kaggle/input/yolop-data/yolop features' # Where setXX_videoYY_yolop_features.pkl are\nPIE_DATABASE_CACHE_PATH = '/kaggle/input/pie-database/pie_database.pkl'\nSCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\" # Load the saved scalers\n\nOUTPUT_VIDEO_FILENAME = f\"{TARGET_SET_ID}_{TARGET_VIDEO_ID}_model_predictions.mp4\"\nOUTPUT_VIDEO_PATH = f'/kaggle/working/{OUTPUT_VIDEO_FILENAME}'\n# ---\n\nprint(f\"--- Generating Prediction Video for {TARGET_SET_ID}/{TARGET_VIDEO_ID} ---\")\nprint(f\"Using Model: {MODEL_PATH}\")\nprint(f\"Model Trained With Streams: {MODEL_ACTIVE_STREAMS}\")\nprint(f\"Output Video: {OUTPUT_VIDEO_PATH}\")\n\n# --- Model Hyperparameters (Must match training) ---\nSEQ_LEN = 30\nPRED_LEN = 1\nINPUT_SIZE_BBOX = 4; INPUT_SIZE_POSE = 34; INPUT_SIZE_EGO_SPEED = 1\nINPUT_SIZE_EGO_ACC = 2; INPUT_SIZE_EGO_GYRO = 1; INPUT_SIZE_PED_ACTION = 1\nINPUT_SIZE_PED_LOOK = 1; INPUT_SIZE_PED_OCC = 1; INPUT_SIZE_TL_STATE = 4\nNUM_SIGNALIZED_CATS = 4; NUM_INTERSECTION_CATS = 5; NUM_AGE_CATS = 4; NUM_GENDER_CATS = 3\nNUM_TRAFFIC_DIR_CATS = 2; LANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7:4, 8:4}; NUM_LANE_CATS = len(set(LANE_CATEGORIES.values()))\nINPUT_SIZE_STATIC = NUM_SIGNALIZED_CATS + NUM_INTERSECTION_CATS + NUM_AGE_CATS + NUM_GENDER_CATS + NUM_TRAFFIC_DIR_CATS + NUM_LANE_CATS\nGRID_SIZE = 3; INPUT_SIZE_YOLOP = GRID_SIZE**2 * 2 + 2\n\nLSTM_HIDDEN_SIZE = 256; NUM_LSTM_LAYERS = 2; DROPOUT_RATE = 0.3\nNUM_CLASSES = 2; ATTENTION_DIM = 128\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\n# --- Mappings ---\nTL_STATE_MAP = {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}; NUM_TL_STATES = len(TL_STATE_MAP)\nSIGNALIZED_MAP = {'n/a': 0, 'C': 1, 'S': 2, 'CS': 3}\nINTERSECTION_MAP = {'midblock': 0, 'T': 1, 'T-left': 2, 'T-right': 3, 'four-way': 4}\nAGE_MAP = {'child': 0, 'young': 1, 'adult': 2, 'senior': 3}\nGENDER_MAP = {'n/a': 0, 'female': 1, 'male': 2}\nTRAFFIC_DIR_MAP = {'OW': 0, 'TW': 1}\n\n# --- Define ALL Possible Streams (for Dataset compatibility during data prep phase) ---\nALL_POSSIBLE_STREAMS = [\n    'bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro',\n    'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context',\n    'yolop'\n]\n\n# --- Helper: One-Hot Encoding ---\ndef to_one_hot(index, num_classes):\n    vec = np.zeros(num_classes, dtype=np.float32)\n    safe_index = int(np.clip(index, 0, num_classes - 1))\n    vec[safe_index] = 1.0\n    return vec\n\n# --- Dataset Class (Minimal for feature extraction) ---\nclass PIEDatasetInference:\n    def __init__(self, pie_database, set_names, pose_data_dir, yolop_data_dir, scalers=None):\n        self.pie_db = pie_database\n        self.set_names = set_names\n        self.pose_data_dir = pose_data_dir\n        self.yolop_data_dir = yolop_data_dir # Store it\n        self.scalers = scalers or {}\n        self.all_pose_data = {}\n        self.all_yolop_data = {} # Initialize for YOLOP data\n        self._input_sizes_for_error = self._get_input_sizes_dict()\n\n        # Load pose/yolop data for ALL sets found in PIE DB to be safe,\n        # or refine later to only load for self.set_names if PIE_DB is very large\n        # For simplicity here, we assume TARGET_SET_ID is among the sets in pie_database\n        # and that _load_pose_data/_load_yolop_data will handle relevant sets.\n        all_sets_in_db = list(self.pie_db.keys()) # Get all sets present in the loaded DB\n\n        # Only load if model actually needs these streams\n        if 'pose' in MODEL_ACTIVE_STREAMS:\n             self._load_pose_data(all_sets_in_db) # Pass sets to load for\n        if 'yolop' in MODEL_ACTIVE_STREAMS:\n             self._load_yolop_data(all_sets_in_db) # Pass sets to load for\n\n    # --- (The rest of the PIEDatasetInference class: _get_input_sizes_dict,\n    #      _load_pose_data, _load_yolop_data, get_feature_sequence methods\n    #      remain the same as in the previous CORRECTED full script) ---\n    def _get_input_sizes_dict(self):\n        input_sizes = {}; special_cases = {'TRAFFIC_LIGHT': 'TL_STATE', 'STATIC_CONTEXT': 'STATIC', 'EGO_SPEED': 'EGO_SPEED', 'EGO_ACC': 'EGO_ACC', 'EGO_GYRO': 'EGO_GYRO', 'PED_ACTION': 'PED_ACTION', 'PED_LOOK': 'PED_LOOK', 'PED_OCCLUSION': 'PED_OCC', 'YOLOP':'YOLOP'}\n        for stream in ALL_POSSIBLE_STREAMS: # Use ALL_POSSIBLE_STREAMS for completeness\n            size_constant_name = f'INPUT_SIZE_{stream.upper()}'; stream_upper_key = stream.upper(); suffix = special_cases.get(stream_upper_key)\n            if suffix: size_constant_name = f'INPUT_SIZE_{suffix}'\n            elif stream == 'bbox': size_constant_name = 'INPUT_SIZE_BBOX'\n            elif stream == 'pose': size_constant_name = 'INPUT_SIZE_POSE'\n            if size_constant_name in globals(): input_sizes[stream] = globals()[size_constant_name]\n            else: input_sizes[stream] = 1\n        return input_sizes\n\n    def _load_pose_data(self, set_names_to_load):\n        print(f\"Loading POSE data relevant to sets: {set_names_to_load}...\")\n        for set_id in set_names_to_load:\n            if set_id not in self.all_pose_data: self.all_pose_data[set_id] = {}\n            pose_set_path = os.path.join(self.pose_data_dir, set_id)\n            if not os.path.isdir(pose_set_path): continue\n            pkl_files = [f for f in os.listdir(pose_set_path) if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")]\n            for pkl_filename in pkl_files:\n                pkl_file_path = os.path.join(pose_set_path, pkl_filename)\n                try:\n                    with open(pkl_file_path, 'rb') as f: loaded_pkl = pickle.load(f)\n                    if len(loaded_pkl) != 1: continue\n                    key, data = list(loaded_pkl.items())[0]; vid = \"_\".join(key.split('_')[1:])\n                    if vid in self.pie_db.get(set_id, {}): self.all_pose_data[set_id][vid] = data\n                except Exception as e: print(f\"Err pose PKL {pkl_file_path}: {e}\")\n        print(\"POSE data loading attempt complete.\")\n\n    def _load_yolop_data(self, set_names_to_load):\n        print(f\"Loading YOLOP data relevant to sets: {set_names_to_load}...\")\n        for set_id in set_names_to_load:\n            if set_id not in self.all_yolop_data: self.all_yolop_data[set_id] = {}\n        if not os.path.isdir(self.yolop_data_dir): print(f\"Err: YOLOP dir not found: {self.yolop_data_dir}\"); return\n        try: all_pkls = [f for f in os.listdir(self.yolop_data_dir) if f.endswith(\"_yolop_features.pkl\")]\n        except Exception as e: print(f\"Err listing YOLOP dir {self.yolop_data_dir}: {e}\"); return\n        for pkl_filename in all_pkls:\n            try: parts = pkl_filename.replace(\"_yolop_features.pkl\", \"\").split('_'); sid_file = parts[0]; vid = \"_\".join(parts[1:])\n            except IndexError: continue\n            if sid_file in set_names_to_load: # Only load if set is relevant\n                pkl_path = os.path.join(self.yolop_data_dir, pkl_filename)\n                try:\n                    with open(pkl_path, 'rb') as f: loaded_pkl = pickle.load(f)\n                    if len(loaded_pkl) != 1: continue\n                    key, data = list(loaded_pkl.items())[0]\n                    self.all_yolop_data[sid_file][vid] = data\n                except Exception as e: print(f\"Err YOLOP PKL {pkl_path}: {e}\")\n        print(\"YOLOP data loading attempt complete.\")\n\n\n    def get_feature_sequence(self, set_id, video_id, ped_id, current_pred_frame_num, seq_len, streams_to_extract):\n        # Sequence will END at (current_pred_frame_num - PRED_LEN)\n        # So, the last observed frame is current_pred_frame_num - PRED_LEN\n        last_obs_frame = current_pred_frame_num - PRED_LEN\n        start_obs_frame = last_obs_frame - seq_len + 1\n        frame_nums = list(range(start_obs_frame, last_obs_frame + 1))\n\n        if frame_nums[0] < 0: return None # Sequence starts before video begins\n\n        video_db = self.pie_db.get(set_id, {}).get(video_id, {})\n        ped_db = video_db.get('ped_annotations', {}).get(ped_id, {})\n        ego_db = video_db.get('vehicle_annotations', {})\n        traffic_db = video_db.get('traffic_annotations', {})\n        ped_attributes = ped_db.get('attributes', {})\n        feature_sequences = {stream: [] for stream in streams_to_extract}\n        valid_sequence = True\n\n        static_vec = None\n        if 'static_context' in streams_to_extract:\n            sig_idx = ped_attributes.get('signalized', 0); int_idx = ped_attributes.get('intersection', 0); age_idx = ped_attributes.get('age', 2); gen_idx = ped_attributes.get('gender', 0)\n            td_val = ped_attributes.get('traffic_direction', 0); td_idx = int(td_val); nl_val = ped_attributes.get('num_lanes', 2); nl_cat_idx = LANE_CATEGORIES.get(nl_val, LANE_CATEGORIES[max(LANE_CATEGORIES.keys())])\n            static_features_list = [ to_one_hot(sig_idx, NUM_SIGNALIZED_CATS), to_one_hot(int_idx, NUM_INTERSECTION_CATS), to_one_hot(age_idx, NUM_AGE_CATS), to_one_hot(gen_idx, NUM_GENDER_CATS), to_one_hot(td_idx, NUM_TRAFFIC_DIR_CATS), to_one_hot(nl_cat_idx, NUM_LANE_CATS) ]\n            static_vec = np.concatenate(static_features_list)\n            if static_vec.shape[0] != INPUT_SIZE_STATIC: static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n\n        for frame_num in frame_nums:\n            frame_db_idx = -1\n            if 'frames' in ped_db:\n                 try: frame_db_idx = ped_db['frames'].index(frame_num)\n                 except ValueError: valid_sequence = False; break\n            else: valid_sequence = False; break\n            if not valid_sequence: break # Break outer loop if inner check fails\n\n            ego_frame_data = ego_db.get(frame_num)\n            if ego_frame_data is None and any(s in streams_to_extract for s in ['ego_speed', 'ego_acc', 'ego_gyro']):\n                valid_sequence = False; break\n\n            # --- Extract for streams_to_extract ---\n            # (Paste the full feature extraction logic from PIEDataset.__getitem__ here)\n            # (Ensure it uses streams_to_extract instead of self.streams_to_generate)\n            if 'bbox' in streams_to_extract:\n                bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32);\n                if frame_db_idx != -1 and 'bbox' in ped_db and len(ped_db['bbox']) > frame_db_idx:\n                    \n                     try:\n                         x1, y1, x2, y2 = ped_db['bbox'][frame_db_idx];\n                         img_w = video_db.get('width', 1920);\n                         img_h = video_db.get('height', 1080)\n                         if img_w > 0 and img_h > 0:\n                             cx = ((x1 + x2) / 2) / img_w;\n                             cy = ((y1 + y2) / 2) / img_h;\n                             w = (x2 - x1) / img_w;\n                             h = (y2 - y1) / img_h;\n                         if w>0 and h>0 and 0<=cx<=1 and 0<=cy<=1:\n                             bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n                     except Exception:pass\n                feature_sequences['bbox'].append(bbox_norm)\n            if 'pose' in streams_to_extract:\n                pose_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32); vid_pose_data = self.all_pose_data.get(set_id, {}).get(video_id, {}); frame_pose_data = vid_pose_data.get(frame_num, {}); loaded_pose = frame_pose_data.get(ped_id)\n                if loaded_pose is not None and isinstance(loaded_pose, np.ndarray) and loaded_pose.shape == (INPUT_SIZE_POSE,): pose_vector = loaded_pose\n                feature_sequences['pose'].append(pose_vector)\n            if 'ego_speed' in streams_to_extract:\n                speed = ego_frame_data.get('OBD_speed', 0.0);\n                if speed == 0.0: speed = ego_frame_data.get('GPS_speed', 0.0)\n                speed_scaled = (speed - self.scalers.get('ego_speed_mean', 0.0)) / self.scalers.get('ego_speed_std', 1.0)\n                feature_sequences['ego_speed'].append([speed_scaled])\n            if 'ego_acc' in streams_to_extract:\n                accX = ego_frame_data.get('accX', 0.0); accY = ego_frame_data.get('accY', 0.0)\n                accX_scaled = (accX - self.scalers.get('accX_mean', 0.0)) / self.scalers.get('accX_std', 1.0)\n                accY_scaled = (accY - self.scalers.get('accY_mean', 0.0)) / self.scalers.get('accY_std', 1.0)\n                feature_sequences['ego_acc'].append([accX_scaled, accY_scaled])\n            if 'ego_gyro' in streams_to_extract:\n                gyroZ = ego_frame_data.get('gyroZ', 0.0)\n                gyroZ_scaled = (gyroZ - self.scalers.get('gyroZ_mean', 0.0)) / self.scalers.get('gyroZ_std', 1.0)\n                feature_sequences['ego_gyro'].append([gyroZ_scaled])\n            if 'ped_action' in streams_to_extract:\n                action = 0;\n                if frame_db_idx != -1 and 'behavior' in ped_db and 'action' in ped_db['behavior'] and len(ped_db['behavior']['action']) > frame_db_idx: action = ped_db['behavior']['action'][frame_db_idx]\n                feature_sequences['ped_action'].append([float(action)])\n            if 'ped_look' in streams_to_extract:\n                look = 0;\n                if frame_db_idx != -1 and 'behavior' in ped_db and 'look' in ped_db['behavior'] and len(ped_db['behavior']['look']) > frame_db_idx: look = ped_db['behavior']['look'][frame_db_idx]\n                feature_sequences['ped_look'].append([float(look)])\n            if 'ped_occlusion' in streams_to_extract:\n                occ = 0.0;\n                if frame_db_idx != -1 and 'occlusion' in ped_db and len(ped_db['occlusion']) > frame_db_idx: occ_val = ped_db['occlusion'][frame_db_idx]; occ = float(occ_val) / 2.0;\n                feature_sequences['ped_occlusion'].append([occ])\n            if 'traffic_light' in streams_to_extract:\n                state_int = 0;\n                for obj_id_tl, obj_data in traffic_db.items():\n                     if obj_data.get('obj_class') == 'traffic_light' and 'frames' in obj_data and 'state' in obj_data:\n                          try:\n                              tl_frame_idx = obj_data['frames'].index(frame_num);\n                              state_val = obj_data['state'][tl_frame_idx];\n                              if state_val != 0:\n                                  state_int = state_val;\n                                  break\n                          except (ValueError, IndexError): continue\n                feature_sequences['traffic_light'].append(to_one_hot(state_int, NUM_TL_STATES))\n            if 'static_context' in streams_to_extract:\n                feature_sequences['static_context'].append(static_vec if static_vec is not None else np.zeros(INPUT_SIZE_STATIC, dtype=np.float32))\n            if 'yolop' in streams_to_extract:\n                yolop_vector = np.zeros(INPUT_SIZE_YOLOP, dtype=np.float32)\n                if set_id in self.all_yolop_data and video_id in self.all_yolop_data[set_id]:\n                    frame_yolop_data = self.all_yolop_data[set_id][video_id].get(frame_num, {})\n                    loaded_yolop = frame_yolop_data.get(ped_id)\n                    if loaded_yolop is not None and isinstance(loaded_yolop, np.ndarray) and loaded_yolop.shape == (INPUT_SIZE_YOLOP,):\n                        yolop_vector = loaded_yolop\n                feature_sequences['yolop'].append(yolop_vector)\n\n        if not valid_sequence: return None\n        features = {}\n        try:\n            for stream_name in streams_to_extract:\n                 features[stream_name] = torch.tensor(np.array(feature_sequences[stream_name], dtype=np.float32), dtype=torch.float32).unsqueeze(0) # Add batch dim\n        except Exception as e: print(f\"Error converting feature sequence to tensor: {e}\"); return None\n        return features\n\n# --- Model Architecture (Paste MultiStreamWeightedAvgLSTM and Attention here) ---\nclass Attention(nn.Module):\n    # ... (Same as before) ...\n    def __init__(self, hidden_dim, attention_dim): super(Attention, self).__init__(); self.attention_net = nn.Sequential(nn.Linear(hidden_dim, attention_dim), nn.Tanh(), nn.Linear(attention_dim, 1))\n    def forward(self, lstm_output): att_scores = self.attention_net(lstm_output).squeeze(2); att_weights = torch.softmax(att_scores, dim=1); context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1); return context_vector, att_weights\nclass MultiStreamWeightedAvgLSTM(nn.Module): # Or MultiStreamAdaptiveLSTM if using concat\n    # ... (Paste the full definition of the model used for the loaded weights) ...\n    def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes, attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n        super(MultiStreamWeightedAvgLSTM, self).__init__();\n        if not stream_names: raise ValueError(\"stream_names cannot be empty.\")\n        self.stream_names = stream_names; self.lstms = nn.ModuleDict(); self.attentions = nn.ModuleDict(); self.num_active_streams = len(self.stream_names); self.lstm_output_dim = lstm_hidden_size * 2\n        # print(f\"Initializing Weighted Avg model with streams: {self.stream_names}\") # Less verbose\n        for name in self.stream_names:\n            if name not in input_sizes: raise KeyError(f\"Input size for stream '{name}' not provided.\"); current_input_size = input_sizes[name]\n            # print(f\"  - Adding stream '{name}' with input size {current_input_size}\")\n            self.lstms[name] = nn.LSTM(current_input_size, lstm_hidden_size, num_lstm_layers, batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0, bidirectional=True); self.attentions[name] = Attention(self.lstm_output_dim , attention_dim)\n        self.fusion_weights = nn.Parameter(torch.ones(self.num_active_streams)); fused_feature_dim = self.lstm_output_dim\n        # print(f\"  Fused feature dimension (Weighted Avg): {fused_feature_dim}\")\n        self.dropout = nn.Dropout(dropout_rate); intermediate_dim = max(num_classes * 4, fused_feature_dim // 2); self.fc1 = nn.Linear(fused_feature_dim, intermediate_dim); self.relu = nn.ReLU(); self.fc2 = nn.Linear(intermediate_dim, num_classes)\n    def forward(self, x):\n        stream_context_vectors = []\n        for name in self.stream_names:\n            if name not in x: zero_ctx = torch.zeros(x[list(x.keys())[0]].shape[0], self.lstm_output_dim, device=x[list(x.keys())[0]].device); stream_context_vectors.append(zero_ctx); continue\n            lstm_out, _ = self.lstms[name](x[name]); context_vector, _ = self.attentions[name](lstm_out); stream_context_vectors.append(context_vector)\n        if len(stream_context_vectors) != self.num_active_streams: raise RuntimeError(f\"Context vectors ({len(stream_context_vectors)}) != active streams ({self.num_active_streams}).\")\n        stacked_context = torch.stack(stream_context_vectors, dim=1); normalized_weights = torch.softmax(self.fusion_weights, dim=0); weights_reshaped = normalized_weights.view(1, self.num_active_streams, 1); fused_features = torch.sum(stacked_context * weights_reshaped, dim=1)\n        out = self.dropout(fused_features); out = self.relu(self.fc1(out)); out = self.dropout(out); logits = self.fc2(out); return logits\n\n# --- *** MODIFIED MODEL FOR WEIGHTED AVERAGE FUSION *** ---\nclass MultiStreamWeightedAvgLSTM(nn.Module):\n    def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes,\n                 attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n        super(MultiStreamWeightedAvgLSTM, self).__init__()\n\n        if not stream_names:\n             raise ValueError(\"stream_names cannot be empty.\")\n\n        self.stream_names = stream_names # Store the active streams\n        self.lstms = nn.ModuleDict()\n        self.attentions = nn.ModuleDict()\n        self.num_active_streams = len(self.stream_names)\n        self.lstm_output_dim = lstm_hidden_size * 2 # BiLSTM doubles hidden size\n\n        print(f\"Initializing Weighted Avg model with streams: {self.stream_names}\")\n\n        # Create layers ONLY for active streams\n        for name in self.stream_names:\n            if name not in input_sizes:\n                 raise KeyError(f\"Input size for stream '{name}' not provided.\")\n            # --- CORRECTED ASSIGNMENT ---\n            current_input_size = input_sizes[name]\n            # --- END CORRECTION ---\n            print(f\"  - Adding stream '{name}' with input size {current_input_size}\")\n            self.lstms[name] = nn.LSTM(current_input_size, lstm_hidden_size, num_lstm_layers,\n                                       batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0,\n                                       bidirectional=True)\n            self.attentions[name] = Attention(self.lstm_output_dim , attention_dim)\n\n        # --- Learnable weights for fusion ---\n        self.fusion_weights = nn.Parameter(torch.ones(self.num_active_streams))\n        # --- End learnable weights ---\n\n        # --- Classification Head ---\n        fused_feature_dim = self.lstm_output_dim\n        print(f\"  Fused feature dimension (Weighted Avg): {fused_feature_dim}\")\n\n        self.dropout = nn.Dropout(dropout_rate)\n        intermediate_dim = max(num_classes * 4, fused_feature_dim // 2)\n        self.fc1 = nn.Linear(fused_feature_dim, intermediate_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(intermediate_dim, num_classes)\n        # --- End Classification Head ---\n\n    def forward(self, x):\n        # ... (forward pass remains the same as the corrected one from before) ...\n        stream_context_vectors = []\n        for name in self.stream_names:\n            if name not in x:\n                 # print(f\"Warning: Stream '{name}' expected but not found in input data `x`.\") # Can be noisy\n                 zero_ctx = torch.zeros(x[list(x.keys())[0]].shape[0], self.lstm_output_dim, device=x[list(x.keys())[0]].device)\n                 stream_context_vectors.append(zero_ctx)\n                 continue\n            lstm_out, _ = self.lstms[name](x[name])\n            context_vector, _ = self.attentions[name](lstm_out)\n            stream_context_vectors.append(context_vector)\n        if len(stream_context_vectors) != self.num_active_streams:\n             raise RuntimeError(f\"Context vectors ({len(stream_context_vectors)}) != active streams ({self.num_active_streams}).\")\n        stacked_context = torch.stack(stream_context_vectors, dim=1)\n        normalized_weights = torch.softmax(self.fusion_weights, dim=0)\n        weights_reshaped = normalized_weights.view(1, self.num_active_streams, 1)\n        fused_features = torch.sum(stacked_context * weights_reshaped, dim=1)\n        out = self.dropout(fused_features)\n        out = self.relu(self.fc1(out))\n        out = self.dropout(out)\n        logits = self.fc2(out)\n        return logits\n# --- End Modified Model ---\n\n# --- If you were using the Concatenation version (MultiStreamAdaptiveLSTM) ---\n# --- The fix would be identical within its __init__ method ---\nclass MultiStreamAdaptiveLSTM(nn.Module): # Original Concat Version\n    def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes, attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n        super(MultiStreamAdaptiveLSTM, self).__init__()\n        if not stream_names:\n            raise ValueError(\"stream_names cannot be empty.\")\n        self.stream_names = stream_names\n        self.lstms = nn.ModuleDict()\n        self.attentions = nn.ModuleDict()\n        print(f\"Initializing model with streams: {self.stream_names}\")\n        for name in self.stream_names:\n            if name not in input_sizes:\n                raise KeyError(f\"Input size for stream '{name}' not provided.\")\n            # --- CORRECTED ASSIGNMENT ---\n            current_input_size = input_sizes[name]\n            # --- END CORRECTION ---\n            print(f\"  - Adding stream '{name}' with input size {current_input_size}\")\n            self.lstms[name] = nn.LSTM(current_input_size, lstm_hidden_size, num_lstm_layers,\n                                       batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0,\n                                       bidirectional=True)\n            self.attentions[name] = Attention(lstm_hidden_size * 2 , attention_dim) # lstm_output_dim is lstm_hidden_size * 2\n\n        num_active_streams = len(self.stream_names)\n        combined_feature_dim = lstm_hidden_size * 2 * num_active_streams # For concatenation\n        print(f\"  Combined feature dimension (Concatenation): {combined_feature_dim}\")\n        self.dropout = nn.Dropout(dropout_rate)\n        intermediate_dim = max(num_classes * 4, combined_feature_dim // 2)\n        self.fc1 = nn.Linear(combined_feature_dim, intermediate_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(intermediate_dim, num_classes)\n\n    def forward(self, x):\n        # ... (forward pass for concatenation model remains the same) ...\n        stream_context_vectors = []\n        # stream_att_weights = {} # Only if you need to inspect weights\n        for name in self.stream_names:\n            if name not in x:\n                print(f\"Warning: Stream '{name}' expected but not in input data.\")\n                # For concatenation, if a stream is missing, you might need to add zeros\n                # of shape (batch_size, lstm_hidden_size * 2) or ensure all active streams are present.\n                # This example assumes x will contain all self.stream_names.\n                continue\n            lstm_out, _ = self.lstms[name](x[name])\n            context_vector, _ = self.attentions[name](lstm_out) # attention_weights ignored\n            stream_context_vectors.append(context_vector)\n            # stream_att_weights[name] = attention_weights\n        if not stream_context_vectors:\n            raise RuntimeError(\"No stream outputs generated.\")\n        fused_features = torch.cat(stream_context_vectors, dim=1)\n        out = self.dropout(fused_features)\n        out = self.relu(self.fc1(out))\n        out = self.dropout(out)\n        logits = self.fc2(out)\n        return logits\n\n# --- Main Visualization Generation Block ---\nif __name__ == '__main__':\n\n    # --- Load PIE Database ---\n    print(\"\\nLoading PIE database cache...\")\n    if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n        raise FileNotFoundError(\"PIE database cache not found. Generate it first.\")\n    try:\n        with open(PIE_DATABASE_CACHE_PATH, 'rb') as f: pie_database = pickle.load(f)\n        print(\" -> PIE database loaded successfully.\")\n    except Exception as e: raise RuntimeError(f\"Failed to load PIE database: {e}\")\n\n    # --- Load Scalers ---\n    print(f\"\\nLoading scalers from: {SCALERS_PKL_PATH}\")\n    try:\n        with open(SCALERS_PKL_PATH, 'rb') as f: scalers = pickle.load(f)\n        print(\" -> Scalers loaded successfully.\")\n    except FileNotFoundError: print(f\"ERROR: Scalers file not found: {SCALERS_PKL_PATH}. Cannot run inference.\"); exit()\n    except Exception as e: print(f\"Error loading scalers: {e}\"); exit()\n\n    # --- Initialize Minimal Dataset Object (for feature extraction method) ---\n    print(\"\\nInitializing helper dataset object...\")\n    # Pass the set ID of the video we want to process\n    dataset_helper = PIEDatasetInference(pie_database, [TARGET_SET_ID], POSE_DATA_DIR, YOLOP_FEATURE_DIR, scalers)\n    print(\"Helper dataset initialized.\")\n\n    # --- Initialize Model ---\n    print(\"\\nInitializing model architecture...\")\n    current_input_sizes = {}\n    for stream in MODEL_ACTIVE_STREAMS:\n        size_constant_name = f'INPUT_SIZE_{stream.upper()}'\n        special_cases = {'TRAFFIC_LIGHT': 'TL_STATE','STATIC_CONTEXT': 'STATIC','EGO_SPEED': 'EGO_SPEED','EGO_ACC': 'EGO_ACC','EGO_GYRO': 'EGO_GYRO','PED_ACTION': 'PED_ACTION','PED_LOOK': 'PED_LOOK','PED_OCCLUSION': 'PED_OCC','YOLOP':'YOLOP'}\n        stream_upper_key = stream.upper(); suffix = special_cases.get(stream_upper_key)\n        if suffix: size_constant_name = f'INPUT_SIZE_{suffix}'\n        elif stream == 'bbox': size_constant_name = 'INPUT_SIZE_BBOX'\n        elif stream == 'pose': size_constant_name = 'INPUT_SIZE_POSE'\n        if size_constant_name in globals(): current_input_sizes[stream] = globals()[size_constant_name]\n        else: raise ValueError(f\"Input size constant {size_constant_name} not found for model stream {stream}\")\n\n    model = MultiStreamWeightedAvgLSTM( # Or MultiStreamAdaptiveLSTM if that's what you saved\n        input_sizes=current_input_sizes, lstm_hidden_size=LSTM_HIDDEN_SIZE,\n        num_lstm_layers=NUM_LSTM_LAYERS, num_classes=NUM_CLASSES, attention_dim=ATTENTION_DIM,\n        dropout_rate=DROPOUT_RATE, stream_names=MODEL_ACTIVE_STREAMS\n    ).to(DEVICE)\n\n    # --- Load Trained Weights ---\n    print(f\"\\nLoading trained model weights from: {MODEL_PATH}\")\n    if not os.path.exists(MODEL_PATH): raise FileNotFoundError(f\"Model file not found: {MODEL_PATH}\")\n    try:\n        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE, weights_only=False)) # Set weights_only based on PyTorch version/warning\n        model.eval()\n        print(\" -> Model weights loaded successfully.\")\n    except Exception as e: raise RuntimeError(f\"Error loading model weights: {e}\")\n\n    # --- Get Frames with Annotations for the Target Video ---\n    print(f\"\\nFinding annotated frames for {TARGET_SET_ID}/{TARGET_VIDEO_ID}...\")\n    annotated_frames_in_video = set()\n    ped_annotations_vid = pie_database.get(TARGET_SET_ID, {}).get(TARGET_VIDEO_ID, {}).get('ped_annotations', {})\n    if not ped_annotations_vid: raise ValueError(f\"No pedestrian annotations for {TARGET_SET_ID}/{TARGET_VIDEO_ID}.\")\n\n    # We need to find the *target frames* for which predictions were made.\n    # A prediction is made for frame F_target, based on observations from F_target-PRED_LEN down to F_target-PRED_LEN-SEQ_LEN+1.\n    # So, if a pedestrian track ends at frame X, the last prediction we can make is for frame X.\n    # The actual observation window for that prediction would be [X-PRED_LEN-SEQ_LEN+1, ..., X-PRED_LEN].\n    # We need to collect all frames for which a pedestrian is present long enough to make a prediction for it.\n    target_prediction_frames = set()\n    for ped_id, ped_data in ped_annotations_vid.items():\n        if 'frames' in ped_data:\n            ped_frames_sorted = sorted(ped_data['frames'])\n            for i in range(len(ped_frames_sorted)):\n                # Can we form a sequence of SEQ_LEN ending at ped_frames_sorted[i]?\n                # The prediction would be for ped_frames_sorted[i] + PRED_LEN\n                potential_target_frame = ped_frames_sorted[i] + PRED_LEN\n                # Check if we have SEQ_LEN frames *before* this potential_target_frame\n                # (i.e., ending at ped_frames_sorted[i])\n                if i >= SEQ_LEN -1 :\n                    start_obs_idx = i - (SEQ_LEN -1)\n                    if ped_frames_sorted[i] - ped_frames_sorted[start_obs_idx] == SEQ_LEN -1: # Continuity check\n                         target_prediction_frames.add(potential_target_frame)\n\n\n    sorted_target_prediction_frames = sorted(list(target_prediction_frames))\n    if not sorted_target_prediction_frames: raise ValueError(f\"No frames suitable for prediction found for {TARGET_SET_ID}/{TARGET_VIDEO_ID}.\")\n    print(f\" -> Will generate predictions for {len(sorted_target_prediction_frames)} target frame instances.\")\n\n    # --- Generate Predictions for these Target Frames ---\n    print(\"\\nGenerating predictions for target frames...\")\n    predictions_map = {} # {target_frame_num: {ped_id: prediction (0 or 1)}}\n    frames_with_errors = 0\n\n    with torch.no_grad():\n        # Iterate through frames where a prediction *could* be made\n        for target_frame_num in tqdm(sorted_target_prediction_frames, desc=\"Predicting Target Frames\"):\n            predictions_map.setdefault(target_frame_num, {})\n            # Find pedestrians whose tracks are long enough to make a prediction for this target_frame_num\n            for ped_id, ped_data in ped_annotations_vid.items():\n                if 'frames' not in ped_data: continue\n                # The last observed frame for this prediction is target_frame_num - PRED_LEN\n                last_obs_needed = target_frame_num - PRED_LEN\n                if last_obs_needed in ped_data['frames']:\n                    input_features = dataset_helper.get_feature_sequence(\n                        TARGET_SET_ID, TARGET_VIDEO_ID, ped_id,\n                        target_frame_num, # Pass the frame for which prediction is made\n                        SEQ_LEN, MODEL_ACTIVE_STREAMS\n                    )\n                    if input_features is not None:\n                        try:\n                            input_features_dev = {name: feat.to(DEVICE) for name, feat in input_features.items()}\n                            logits = model(input_features_dev)\n                            prediction = torch.argmax(logits, dim=1).item()\n                            predictions_map[target_frame_num][ped_id] = prediction\n                        except Exception as e:\n                             print(f\"Error model inference F_target:{target_frame_num} P:{ped_id}: {e}\")\n                             predictions_map[target_frame_num][ped_id] = -1\n                             frames_with_errors += 1\n                    else:\n                        predictions_map[target_frame_num][ped_id] = -1 # No valid sequence\n\n    print(f\"Predictions generated. Errors/invalid sequences for {frames_with_errors} instances.\")\n\n    # --- Create Output Video ---\n    print(f\"\\nCreating output video: {OUTPUT_VIDEO_PATH}\")\n    # input_video_path = os.path.join(video_set_input_path, f\"{TARGET_VIDEO_ID}.mp4\")\n    # if not os.path.exists(input_video_path): raise FileNotFoundError(f\"Input video not found: {input_video_path}\")\n\n    target_video_set_folder_name = f\"pie-{TARGET_SET_ID}\" # e.g., pie-set04\n    input_video_path = os.path.join(VIDEO_INPUT_DIR, target_video_set_folder_name, f\"{TARGET_VIDEO_ID}.mp4\")\n\n    if not os.path.exists(input_video_path): raise FileNotFoundError(f\"Input video not found: {input_video_path}\")\n            \n    cap_read = cv2.VideoCapture(input_video_path)\n    if not cap_read.isOpened(): raise IOError(f\"Could not open input video: {input_video_path}\")\n    vid_width_in = int(cap_read.get(cv2.CAP_PROP_FRAME_WIDTH))\n    vid_height_in = int(cap_read.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_video_frames = int(cap_read.get(cv2.CAP_PROP_FRAME_COUNT))\n    fps_in = cap_read.get(cv2.CAP_PROP_FPS) if cap_read.get(cv2.CAP_PROP_FPS) > 0 else 30.0\n    print(f\" -> Input video properties: {vid_width_in}x{vid_height_in}, {total_video_frames} frames, {fps_in:.2f} FPS\")\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    video_writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps_in, (vid_width_in, vid_height_in))\n    if not video_writer.isOpened(): raise IOError(f\"Error opening video writer for {OUTPUT_VIDEO_PATH}\")\n\n    color_not_crossing = (0, 200, 0); color_crossing = (0, 0, 200); color_unknown = (200, 200, 0)\n\n    for frame_idx_video in tqdm(range(total_video_frames), desc=\"Writing Video\"):\n        ret_read, frame_to_write = cap_read.read()\n        if not ret_read: print(f\"Warning: Could not read frame {frame_idx_video} from input video.\"); break\n\n        current_frame_display_num = frame_idx_video + 1 # 1-based for display and lookup in PIE\n\n        # Check if this frame_idx_video corresponds to a *target prediction frame*\n        # and if there are pedestrians for whom predictions were made for this frame\n        if current_frame_display_num in predictions_map:\n            frame_predictions_for_peds = predictions_map[current_frame_display_num]\n            for ped_id, prediction in frame_predictions_for_peds.items():\n                # Get the ground truth bbox for this pedestrian AT THIS current_frame_display_num\n                ped_data_for_frame = ped_annotations_vid.get(ped_id, {})\n                if 'frames' in ped_data_for_frame:\n                    try:\n                        f_db_idx = ped_data_for_frame['frames'].index(current_frame_display_num)\n                        if 'bbox' in ped_data_for_frame and len(ped_data_for_frame['bbox']) > f_db_idx:\n                            bbox = ped_data_for_frame['bbox'][f_db_idx]\n                            try:\n                                x1, y1, x2, y2 = map(int, bbox)\n                                color = color_unknown; label_text = \"NoSeq\"\n                                if prediction == 0: color = color_not_crossing; label_text = \"Not Crossing\"\n                                elif prediction == 1: color = color_crossing; label_text = \"Crossing\"\n                                elif prediction == -1: label_text = \"No Seq / Err\" # Handled cases where sequence wasn't valid\n\n                                cv2.rectangle(frame_to_write, (x1, y1), (x2, y2), color, 2)\n                                cv2.putText(frame_to_write, f\"{label_text} ({ped_id[:5]})\", (x1, y1 - 7),\n                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n                            except ValueError: pass # Invalid bbox coords\n                    except (ValueError, IndexError): pass # Ped not in this exact frame in DB list\n\n        # Add frame number to display\n        cv2.putText(frame_to_write, f\"Frame: {current_frame_display_num}\", (10, 30),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2, cv2.LINE_AA)\n        video_writer.write(frame_to_write)\n\n    cap_read.release(); video_writer.release()\n    print(f\"\\nOutput video saved successfully to: {OUTPUT_VIDEO_PATH}\")\n    print(\"\\n--- Visualization Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:48:42.043777Z","iopub.execute_input":"2025-05-26T22:48:42.044226Z","iopub.status.idle":"2025-05-26T23:03:17.144355Z","shell.execute_reply.started":"2025-05-26T22:48:42.044192Z","shell.execute_reply":"2025-05-26T23:03:17.143527Z"}},"outputs":[{"name":"stdout","text":"--- Generating Prediction Video for set04/video_0012 ---\nUsing Model: /kaggle/input/mslstm-pid/pytorch/default/1/best_model_weighted_bbox_ego_acc_ego_speed_ped_action_ped_look_static_context_ep2.pth\nModel Trained With Streams: ['bbox', 'ego_acc', 'ego_speed', 'ped_action', 'ped_look', 'static_context']\nOutput Video: /kaggle/working/set04_video_0012_model_predictions.mp4\nUsing device: cuda\n\nLoading PIE database cache...\n -> PIE database loaded successfully.\n\nLoading scalers from: /kaggle/working/scalers.pkl\n -> Scalers loaded successfully.\n\nInitializing helper dataset object...\nHelper dataset initialized.\n\nInitializing model architecture...\nInitializing Weighted Avg model with streams: ['bbox', 'ego_acc', 'ego_speed', 'ped_action', 'ped_look', 'static_context']\n  - Adding stream 'bbox' with input size 4\n  - Adding stream 'ego_acc' with input size 2\n  - Adding stream 'ego_speed' with input size 1\n  - Adding stream 'ped_action' with input size 1\n  - Adding stream 'ped_look' with input size 1\n  - Adding stream 'static_context' with input size 23\n  Fused feature dimension (Weighted Avg): 512\n\nLoading trained model weights from: /kaggle/input/mslstm-pid/pytorch/default/1/best_model_weighted_bbox_ego_acc_ego_speed_ped_action_ped_look_static_context_ep2.pth\n -> Model weights loaded successfully.\n\nFinding annotated frames for set04/video_0012...\n -> Will generate predictions for 11364 target frame instances.\n\nGenerating predictions for target frames...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting Target Frames:   0%|          | 0/11364 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18a6bf0f4fec4b0d935e6656c12b218a"}},"metadata":{}},{"name":"stdout","text":"Predictions generated. Errors/invalid sequences for 0 instances.\n\nCreating output video: /kaggle/working/set04_video_0012_model_predictions.mp4\n -> Input video properties: 1920x1080, 21600 frames, 30.00 FPS\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Writing Video:   0%|          | 0/21600 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b55bb5492eac45e7adcb6e2a2bccfda8"}},"metadata":{}},{"name":"stdout","text":"\nOutput video saved successfully to: /kaggle/working/set04_video_0012_model_predictions.mp4\n\n--- Visualization Script Finished ---\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"os.remove('/kaggle/working/aug_balanced_train_data_with_yolop.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:48:36.386020Z","iopub.execute_input":"2025-05-26T22:48:36.386414Z","iopub.status.idle":"2025-05-26T22:48:37.217036Z","shell.execute_reply.started":"2025-05-26T22:48:36.386386Z","shell.execute_reply":"2025-05-26T22:48:37.216344Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# --- SCRIPT TO GENERATE PREDICTION VIDEO (Annotated Frames Only) ---\n\nimport torch\nimport torch.nn as nn\n# import torch.optim as optim\nfrom torch.utils.data import Dataset # For PIEDataset class structure\n# import xml.etree.ElementTree as ET # Not needed if using PIE database\nimport os\nimport numpy as np\n# from sklearn.metrics import ... # Not needed for inference\n# from sklearn.preprocessing import StandardScaler # Scalers are loaded\nfrom tqdm.notebook import tqdm\nimport random\n# import math\n# import zipfile\nimport cv2 # Crucial for video reading/writing\n# import pandas as pd\nimport matplotlib.pyplot as plt # For potential debugging\n# import seaborn as sns\nimport pickle\nimport time\nimport sys\nimport gc\n\n# --- Add PIE utilities path (if needed for PIE class during DB load) ---\n# pie_utilities_path = '/kaggle/working/PIE/utilities'\n# if pie_utilities_path not in sys.path:\n#     sys.path.insert(0, pie_utilities_path)\n# try:\n#     from pie_data import PIE\n# except ImportError as e:\n#     print(f\"Warn: Could not import PIE class: {e}. DB must exist if PIE object is used.\")\n#     PIE = None\n\n# --- Configuration ---\nTARGET_SET_ID = 'set04'\nTARGET_VIDEO_ID = 'video_0012'\nMODEL_PATH = '/kaggle/input/mslstm-pid/pytorch/default/1/best_model_weighted_bbox_ego_acc_ego_speed_ped_action_ped_look_static_context_ep2.pth'\nMODEL_ACTIVE_STREAMS = [\n    'bbox', 'ego_acc', 'ego_speed', 'ped_action', 'ped_look', 'static_context'\n]\nprint(f\"Model uses Active Streams: {MODEL_ACTIVE_STREAMS}\")\n\n# --- Paths ---\nVIDEO_INPUT_DIR = '/kaggle/input'\nPOSE_DATA_DIR = '/kaggle/input/pose-data/extracted_poses2'\nYOLOP_FEATURE_DIR = '/kaggle/input/yolop-data/yolop features'\nPIE_DATABASE_CACHE_PATH = '/kaggle/input/pie-database/pie_database.pkl'\nSCALERS_PKL_PATH = \"/kaggle/working/scalers.pkl\"\n\nOUTPUT_VIDEO_FILENAME = f\"{TARGET_SET_ID}_{TARGET_VIDEO_ID}_annotated_preds.mp4\"\nOUTPUT_VIDEO_PATH = f'/kaggle/working/{OUTPUT_VIDEO_FILENAME}'\n# ---\n\nprint(f\"--- Generating Prediction Video for {TARGET_SET_ID}/{TARGET_VIDEO_ID} (Annotated Frames Only) ---\")\nprint(f\"Using Model: {MODEL_PATH}\")\nprint(f\"Output Video: {OUTPUT_VIDEO_PATH}\")\n\n# --- Model Hyperparameters (Must match training) ---\nSEQ_LEN = 30; PRED_LEN = 1\nINPUT_SIZE_BBOX = 4; INPUT_SIZE_POSE = 34; INPUT_SIZE_EGO_SPEED = 1\nINPUT_SIZE_EGO_ACC = 2; INPUT_SIZE_EGO_GYRO = 1; INPUT_SIZE_PED_ACTION = 1\nINPUT_SIZE_PED_LOOK = 1; INPUT_SIZE_PED_OCC = 1; INPUT_SIZE_TL_STATE = 4\nNUM_SIGNALIZED_CATS = 4; NUM_INTERSECTION_CATS = 5; NUM_AGE_CATS = 4; NUM_GENDER_CATS = 3\nNUM_TRAFFIC_DIR_CATS = 2; LANE_CATEGORIES = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 4, 7:4, 8:4}; NUM_LANE_CATS = len(set(LANE_CATEGORIES.values()))\nINPUT_SIZE_STATIC = NUM_SIGNALIZED_CATS + NUM_INTERSECTION_CATS + NUM_AGE_CATS + NUM_GENDER_CATS + NUM_TRAFFIC_DIR_CATS + NUM_LANE_CATS\nGRID_SIZE = 3; INPUT_SIZE_YOLOP = GRID_SIZE**2 * 2 + 2\n\nLSTM_HIDDEN_SIZE = 256; NUM_LSTM_LAYERS = 2; DROPOUT_RATE = 0.3\nNUM_CLASSES = 2; ATTENTION_DIM = 128\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\n# --- Mappings ---\nTL_STATE_MAP = {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}; NUM_TL_STATES = len(TL_STATE_MAP)\nSIGNALIZED_MAP = {'n/a': 0, 'C': 1, 'S': 2, 'CS': 3}\nINTERSECTION_MAP = {'midblock': 0, 'T': 1, 'T-left': 2, 'T-right': 3, 'four-way': 4}\nAGE_MAP = {'child': 0, 'young': 1, 'adult': 2, 'senior': 3}\nGENDER_MAP = {'n/a': 0, 'female': 1, 'male': 2}\nTRAFFIC_DIR_MAP = {'OW': 0, 'TW': 1}\n\n# --- Define ALL Possible Streams (for Dataset compatibility during data prep phase) ---\nALL_POSSIBLE_STREAMS = [\n    'bbox', 'pose', 'ego_speed', 'ego_acc', 'ego_gyro',\n    'ped_action', 'ped_look', 'ped_occlusion', 'traffic_light', 'static_context',\n    'yolop'\n]\n\n# --- Helper: One-Hot Encoding ---\ndef to_one_hot(index, num_classes):\n    vec = np.zeros(num_classes, dtype=np.float32)\n    safe_index = int(np.clip(index, 0, num_classes - 1))\n    vec[safe_index] = 1.0\n    return vec\n\n# --- Dataset Class (Minimal for feature extraction) ---\n# --- (Paste the full PIEDatasetInference class definition from previous corrected script here) ---\nclass PIEDatasetInference:\n    def __init__(self, pie_database, set_names_for_data, pose_data_dir, yolop_data_dir, scalers=None):\n        self.pie_db = pie_database; self.set_names_for_data = set_names_for_data; self.pose_data_dir = pose_data_dir; self.yolop_data_dir = yolop_data_dir; self.scalers = scalers or {}; self.all_pose_data = {}; self.all_yolop_data = {}; self._input_sizes_for_error = self._get_input_sizes_dict()\n        if 'pose' in MODEL_ACTIVE_STREAMS: self._load_pose_data(self.set_names_for_data)\n        if 'yolop' in MODEL_ACTIVE_STREAMS: self._load_yolop_data(self.set_names_for_data)\n    def _get_input_sizes_dict(self):\n        input_sizes = {}; special_cases = {'TRAFFIC_LIGHT': 'TL_STATE', 'STATIC_CONTEXT': 'STATIC', 'EGO_SPEED': 'EGO_SPEED', 'EGO_ACC': 'EGO_ACC', 'EGO_GYRO': 'EGO_GYRO', 'PED_ACTION': 'PED_ACTION', 'PED_LOOK': 'PED_LOOK', 'PED_OCCLUSION': 'PED_OCC', 'YOLOP':'YOLOP'}\n        for stream in ALL_POSSIBLE_STREAMS:\n            size_constant_name = f'INPUT_SIZE_{stream.upper()}'; stream_upper_key = stream.upper(); suffix = special_cases.get(stream_upper_key)\n            if suffix: size_constant_name = f'INPUT_SIZE_{suffix}'\n            elif stream == 'bbox': size_constant_name = 'INPUT_SIZE_BBOX'\n            elif stream == 'pose': size_constant_name = 'INPUT_SIZE_POSE'\n            if size_constant_name in globals(): input_sizes[stream] = globals()[size_constant_name]\n            else: input_sizes[stream] = 1\n        return input_sizes\n    def _load_pose_data(self, set_names_to_load):\n        print(f\"Loading POSE data relevant to sets: {set_names_to_load}...\"); loaded_count = 0\n        for set_id in set_names_to_load:\n            if set_id not in self.all_pose_data: self.all_pose_data[set_id] = {}\n            pose_set_path = os.path.join(self.pose_data_dir, set_id)\n            if not os.path.isdir(pose_set_path): continue\n            pkl_files = [f for f in os.listdir(pose_set_path) if f.startswith(f\"{set_id}_\") and f.endswith(\"_poses.pkl\")]\n            for pkl_filename in pkl_files:\n                pkl_file_path = os.path.join(pose_set_path, pkl_filename)\n                try:\n                    with open(pkl_file_path, 'rb') as f: loaded_pkl = pickle.load(f)\n                    if len(loaded_pkl) != 1: continue\n                    key, data = list(loaded_pkl.items())[0]; vid = \"_\".join(key.split('_')[1:])\n                    if vid in self.pie_db.get(set_id, {}): self.all_pose_data[set_id][vid] = data; loaded_count+=1\n                except Exception as e: print(f\"Err pose PKL {pkl_file_path}: {e}\")\n        print(f\"POSE data loading attempt complete for helper (Loaded {loaded_count} files).\")\n    def _load_yolop_data(self, set_names_to_load):\n        print(f\"Loading YOLOP data relevant to sets: {set_names_to_load}...\"); loaded_count = 0\n        for set_id in set_names_to_load:\n            if set_id not in self.all_yolop_data: self.all_yolop_data[set_id] = {}\n        if not os.path.isdir(self.yolop_data_dir): print(f\"Err: YOLOP dir not found: {self.yolop_data_dir}\"); return\n        try: all_pkls = [f for f in os.listdir(self.yolop_data_dir) if f.endswith(\"_yolop_features.pkl\")]\n        except Exception as e: print(f\"Err listing YOLOP dir {self.yolop_data_dir}: {e}\"); return\n        for pkl_filename in all_pkls:\n            try: parts = pkl_filename.replace(\"_yolop_features.pkl\", \"\").split('_'); sid_file = parts[0]; vid = \"_\".join(parts[1:])\n            except IndexError: continue\n            if sid_file in set_names_to_load:\n                pkl_path = os.path.join(self.yolop_data_dir, pkl_filename)\n                try:\n                    with open(pkl_path, 'rb') as f: loaded_pkl = pickle.load(f)\n                    if len(loaded_pkl) != 1: continue\n                    key, data = list(loaded_pkl.items())[0]\n                    self.all_yolop_data[sid_file][vid] = data; loaded_count+=1\n                except Exception as e: print(f\"Err YOLOP PKL {pkl_path}: {e}\")\n        print(f\"YOLOP data loading attempt complete for helper (Loaded {loaded_count} files).\")\n    def get_feature_sequence(self, set_id, video_id, ped_id, current_pred_frame_num, seq_len, streams_to_extract):\n        last_obs_frame = current_pred_frame_num - PRED_LEN\n        start_obs_frame = last_obs_frame - seq_len + 1\n        frame_nums = list(range(start_obs_frame, last_obs_frame + 1))\n        if frame_nums[0] < 0: return None\n        video_db = self.pie_db.get(set_id, {}).get(video_id, {}); ped_db = video_db.get('ped_annotations', {}).get(ped_id, {}); ego_db = video_db.get('vehicle_annotations', {}); traffic_db = video_db.get('traffic_annotations', {}); ped_attributes = ped_db.get('attributes', {})\n        feature_sequences = {stream: [] for stream in streams_to_extract}; valid_sequence = True\n        static_vec = None\n        if 'static_context' in streams_to_extract:\n            sig_idx = ped_attributes.get('signalized', 0); int_idx = ped_attributes.get('intersection', 0); age_idx = ped_attributes.get('age', 2); gen_idx = ped_attributes.get('gender', 0)\n            td_val = ped_attributes.get('traffic_direction', 0); td_idx = int(td_val); nl_val = ped_attributes.get('num_lanes', 2); nl_cat_idx = LANE_CATEGORIES.get(nl_val, LANE_CATEGORIES[max(LANE_CATEGORIES.keys())])\n            static_features_list = [ to_one_hot(sig_idx, NUM_SIGNALIZED_CATS), to_one_hot(int_idx, NUM_INTERSECTION_CATS), to_one_hot(age_idx, NUM_AGE_CATS), to_one_hot(gen_idx, NUM_GENDER_CATS), to_one_hot(td_idx, NUM_TRAFFIC_DIR_CATS), to_one_hot(nl_cat_idx, NUM_LANE_CATS) ]\n            static_vec = np.concatenate(static_features_list)\n            if static_vec.shape[0] != INPUT_SIZE_STATIC: static_vec = np.zeros(INPUT_SIZE_STATIC, dtype=np.float32)\n        for frame_num in frame_nums:\n            frame_db_idx = -1\n            if 'frames' in ped_db:\n                 try: frame_db_idx = ped_db['frames'].index(frame_num)\n                 except ValueError: valid_sequence = False; break\n            else: valid_sequence = False; break\n            if not valid_sequence: break\n            ego_frame_data = ego_db.get(frame_num)\n            if ego_frame_data is None and any(s in streams_to_extract for s in ['ego_speed', 'ego_acc', 'ego_gyro']): valid_sequence = False; break\n            # --- (Paste the full feature extraction logic from PIEDataset.__getitem__ here, using streams_to_extract) ---\n            if 'bbox' in streams_to_extract:\n                bbox_norm = np.zeros(INPUT_SIZE_BBOX, dtype=np.float32);\n                if frame_db_idx != -1 and 'bbox' in ped_db and len(ped_db['bbox']) > frame_db_idx:\n                     try:\n                         x1, y1, x2, y2 = ped_db['bbox'][frame_db_idx];\n                         img_w = video_db.get('width', 1920);\n                         img_h = video_db.get('height', 1080)\n                         if img_w > 0 and img_h > 0:\n                             cx = ((x1 + x2) / 2) / img_w;\n                             cy = ((y1 + y2) / 2) / img_h;\n                             w = (x2 - x1) / img_w;\n                             h = (y2 - y1) / img_h;\n                         if w>0 and h>0 and 0<=cx<=1 and 0<=cy<=1:\n                             bbox_norm = np.array([cx, cy, w, h], dtype=np.float32)\n                     except Exception: pass\n                feature_sequences['bbox'].append(bbox_norm)\n            if 'pose' in streams_to_extract:\n                pose_vector = np.zeros(INPUT_SIZE_POSE, dtype=np.float32); vid_pose_data = self.all_pose_data.get(set_id, {}).get(video_id, {}); frame_pose_data = vid_pose_data.get(frame_num, {}); loaded_pose = frame_pose_data.get(ped_id)\n                if loaded_pose is not None and isinstance(loaded_pose, np.ndarray) and loaded_pose.shape == (INPUT_SIZE_POSE,): pose_vector = loaded_pose\n                feature_sequences['pose'].append(pose_vector)\n            if 'ego_speed' in streams_to_extract:\n                speed = ego_frame_data.get('OBD_speed', 0.0);\n                if speed == 0.0: speed = ego_frame_data.get('GPS_speed', 0.0)\n                speed_scaled = (speed - self.scalers.get('ego_speed_mean', 0.0)) / self.scalers.get('ego_speed_std', 1.0)\n                feature_sequences['ego_speed'].append([speed_scaled])\n            if 'ego_acc' in streams_to_extract:\n                accX = ego_frame_data.get('accX', 0.0); accY = ego_frame_data.get('accY', 0.0)\n                accX_scaled = (accX - self.scalers.get('accX_mean', 0.0)) / self.scalers.get('accX_std', 1.0)\n                accY_scaled = (accY - self.scalers.get('accY_mean', 0.0)) / self.scalers.get('accY_std', 1.0)\n                feature_sequences['ego_acc'].append([accX_scaled, accY_scaled])\n            if 'ego_gyro' in streams_to_extract:\n                gyroZ = ego_frame_data.get('gyroZ', 0.0)\n                gyroZ_scaled = (gyroZ - self.scalers.get('gyroZ_mean', 0.0)) / self.scalers.get('gyroZ_std', 1.0)\n                feature_sequences['ego_gyro'].append([gyroZ_scaled])\n            if 'ped_action' in streams_to_extract:\n                action = 0;\n                if frame_db_idx != -1 and 'behavior' in ped_db and 'action' in ped_db['behavior'] and len(ped_db['behavior']['action']) > frame_db_idx: action = ped_db['behavior']['action'][frame_db_idx]\n                feature_sequences['ped_action'].append([float(action)])\n            if 'ped_look' in streams_to_extract:\n                look = 0;\n                if frame_db_idx != -1 and 'behavior' in ped_db and 'look' in ped_db['behavior'] and len(ped_db['behavior']['look']) > frame_db_idx: look = ped_db['behavior']['look'][frame_db_idx]\n                feature_sequences['ped_look'].append([float(look)])\n            if 'ped_occlusion' in streams_to_extract:\n                occ = 0.0;\n                if frame_db_idx != -1 and 'occlusion' in ped_db and len(ped_db['occlusion']) > frame_db_idx: occ_val = ped_db['occlusion'][frame_db_idx]; occ = float(occ_val) / 2.0;\n                feature_sequences['ped_occlusion'].append([occ])\n            if 'traffic_light' in streams_to_extract:\n                state_int = 0;\n                for obj_id_tl, obj_data in traffic_db.items():\n                     if obj_data.get('obj_class') == 'traffic_light' and 'frames' in obj_data and 'state' in obj_data:\n                          try:\n                              tl_frame_idx = obj_data['frames'].index(frame_num);\n                              state_val = obj_data['state'][tl_frame_idx];\n                              if state_val != 0: state_int = state_val; break\n                          except (ValueError, IndexError): continue\n                feature_sequences['traffic_light'].append(to_one_hot(state_int, NUM_TL_STATES))\n            if 'static_context' in streams_to_extract:\n                feature_sequences['static_context'].append(static_vec if static_vec is not None else np.zeros(INPUT_SIZE_STATIC, dtype=np.float32))\n            if 'yolop' in streams_to_extract:\n                yolop_vector = np.zeros(INPUT_SIZE_YOLOP, dtype=np.float32)\n                if set_id in self.all_yolop_data and video_id in self.all_yolop_data[set_id]:\n                    frame_yolop_data = self.all_yolop_data[set_id][video_id].get(frame_num, {})\n                    loaded_yolop = frame_yolop_data.get(ped_id)\n                    if loaded_yolop is not None and isinstance(loaded_yolop, np.ndarray) and loaded_yolop.shape == (INPUT_SIZE_YOLOP,):\n                        yolop_vector = loaded_yolop\n                feature_sequences['yolop'].append(yolop_vector)\n\n        if not valid_sequence: return None\n        features = {}\n        try:\n            for stream_name in streams_to_extract:\n                 features[stream_name] = torch.tensor(np.array(feature_sequences[stream_name], dtype=np.float32), dtype=torch.float32).unsqueeze(0) # Add batch dim\n        except Exception as e: print(f\"Error converting feature sequence to tensor: {e}\"); return None\n        return features\n\n# --- Model Architecture (Paste MultiStreamWeightedAvgLSTM and Attention here) ---\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim, attention_dim): super(Attention, self).__init__(); self.attention_net = nn.Sequential(nn.Linear(hidden_dim, attention_dim), nn.Tanh(), nn.Linear(attention_dim, 1))\n    def forward(self, lstm_output): att_scores = self.attention_net(lstm_output).squeeze(2); att_weights = torch.softmax(att_scores, dim=1); context_vector = torch.sum(lstm_output * att_weights.unsqueeze(2), dim=1); return context_vector, att_weights\nclass MultiStreamWeightedAvgLSTM(nn.Module):\n    def __init__(self, input_sizes, lstm_hidden_size, num_lstm_layers, num_classes,\n                 attention_dim, dropout_rate, stream_names=['bbox', 'pose']):\n        super(MultiStreamWeightedAvgLSTM, self).__init__()\n\n        if not stream_names:\n             raise ValueError(\"stream_names cannot be empty.\")\n\n        self.stream_names = stream_names\n        self.lstms = nn.ModuleDict()\n        self.attentions = nn.ModuleDict()\n        self.num_active_streams = len(self.stream_names)\n        self.lstm_output_dim = lstm_hidden_size * 2\n\n        print(f\"Initializing Weighted Avg model with streams: {self.stream_names}\")\n\n        for name in self.stream_names:\n            if name not in input_sizes:\n                 raise KeyError(f\"Input size for stream '{name}' not provided.\")\n            # --- CORRECTED ASSIGNMENT ---\n            current_input_size = input_sizes[name]\n            # --- END CORRECTION ---\n            print(f\"  - Adding stream '{name}' with input size {current_input_size}\")\n            self.lstms[name] = nn.LSTM(current_input_size, lstm_hidden_size, num_lstm_layers,\n                                       batch_first=True, dropout=dropout_rate if num_lstm_layers > 1 else 0,\n                                       bidirectional=True)\n            self.attentions[name] = Attention(self.lstm_output_dim , attention_dim)\n\n        self.fusion_weights = nn.Parameter(torch.ones(self.num_active_streams))\n        fused_feature_dim = self.lstm_output_dim\n        print(f\"  Fused feature dimension (Weighted Avg): {fused_feature_dim}\")\n        self.dropout = nn.Dropout(dropout_rate)\n        intermediate_dim = max(num_classes * 4, fused_feature_dim // 2)\n        self.fc1 = nn.Linear(fused_feature_dim, intermediate_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(intermediate_dim, num_classes)\n\n    # ... (forward method remains the same) ...\n    def forward(self, x):\n        stream_context_vectors = []\n        for name in self.stream_names:\n            if name not in x:\n                zero_ctx = torch.zeros(x[list(x.keys())[0]].shape[0], self.lstm_output_dim, device=x[list(x.keys())[0]].device)\n                stream_context_vectors.append(zero_ctx)\n                continue\n            lstm_out, _ = self.lstms[name](x[name])\n            context_vector, _ = self.attentions[name](lstm_out)\n            stream_context_vectors.append(context_vector)\n        if len(stream_context_vectors) != self.num_active_streams:\n            raise RuntimeError(f\"Context vectors ({len(stream_context_vectors)}) != active streams ({self.num_active_streams}).\")\n        stacked_context = torch.stack(stream_context_vectors, dim=1)\n        normalized_weights = torch.softmax(self.fusion_weights, dim=0)\n        weights_reshaped = normalized_weights.view(1, self.num_active_streams, 1)\n        fused_features = torch.sum(stacked_context * weights_reshaped, dim=1)\n        out = self.dropout(fused_features)\n        out = self.relu(self.fc1(out))\n        out = self.dropout(out)\n        logits = self.fc2(out)\n        return logits\n        \n# --- Main Visualization Generation Block ---\nif __name__ == '__main__':\n\n    # --- Load PIE Database ---\n    print(\"\\nLoading PIE database cache...\")\n    if not os.path.exists(PIE_DATABASE_CACHE_PATH):\n        raise FileNotFoundError(\"PIE database cache not found. Generate it first.\")\n    try:\n        with open(PIE_DATABASE_CACHE_PATH, 'rb') as f: pie_database = pickle.load(f)\n        print(\" -> PIE database loaded successfully.\")\n    except Exception as e: raise RuntimeError(f\"Failed to load PIE database: {e}\")\n\n    # --- Load Scalers ---\n    print(f\"\\nLoading scalers from: {SCALERS_PKL_PATH}\")\n    try:\n        with open(SCALERS_PKL_PATH, 'rb') as f: scalers = pickle.load(f)\n        print(\" -> Scalers loaded successfully.\")\n    except FileNotFoundError: print(f\"ERROR: Scalers file not found: {SCALERS_PKL_PATH}. Cannot run inference.\"); exit()\n    except Exception as e: print(f\"Error loading scalers: {e}\"); exit()\n\n    # --- Initialize Minimal Dataset Object (for feature extraction method) ---\n    print(\"\\nInitializing helper dataset object...\")\n    dataset_helper = PIEDatasetInference(pie_database, [TARGET_SET_ID], POSE_DATA_DIR, YOLOP_FEATURE_DIR, scalers)\n    print(\"Helper dataset initialized.\")\n\n    # --- Initialize Model ---\n    print(\"\\nInitializing model architecture...\")\n    current_input_sizes = {}\n    for stream in MODEL_ACTIVE_STREAMS:\n        size_constant_name = f'INPUT_SIZE_{stream.upper()}'\n        special_cases = {'TRAFFIC_LIGHT': 'TL_STATE','STATIC_CONTEXT': 'STATIC','EGO_SPEED': 'EGO_SPEED','EGO_ACC': 'EGO_ACC','EGO_GYRO': 'EGO_GYRO','PED_ACTION': 'PED_ACTION','PED_LOOK': 'PED_LOOK','PED_OCCLUSION': 'PED_OCC','YOLOP':'YOLOP'}\n        stream_upper_key = stream.upper(); suffix = special_cases.get(stream_upper_key)\n        if suffix: size_constant_name = f'INPUT_SIZE_{suffix}'\n        elif stream == 'bbox': size_constant_name = 'INPUT_SIZE_BBOX'\n        elif stream == 'pose': size_constant_name = 'INPUT_SIZE_POSE'\n        if size_constant_name in globals(): current_input_sizes[stream] = globals()[size_constant_name]\n        else: raise ValueError(f\"Input size constant {size_constant_name} not found for model stream {stream}\")\n\n    model = MultiStreamWeightedAvgLSTM( # Or MultiStreamAdaptiveLSTM if that's what your .pth file is\n        input_sizes=current_input_sizes, lstm_hidden_size=LSTM_HIDDEN_SIZE,\n        num_lstm_layers=NUM_LSTM_LAYERS, num_classes=NUM_CLASSES, attention_dim=ATTENTION_DIM,\n        dropout_rate=DROPOUT_RATE, stream_names=MODEL_ACTIVE_STREAMS\n    ).to(DEVICE)\n\n    # --- Load Trained Weights ---\n    print(f\"\\nLoading trained model weights from: {MODEL_PATH}\")\n    if not os.path.exists(MODEL_PATH): raise FileNotFoundError(f\"Model file not found: {MODEL_PATH}\")\n    try:\n        # Set weights_only=True if your PyTorch version supports it and you trust the source\n        # For older PyTorch or untrusted sources, weights_only=False (default) is used but shows a warning.\n        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE, weights_only=False))\n        model.eval() # Set model to evaluation mode\n        print(\" -> Model weights loaded successfully.\")\n    except RuntimeError as e:\n        print(f\"RuntimeError loading model weights: {e}\")\n        print(\"This often indicates a mismatch between the model architecture defined in the script\")\n        print(\"and the architecture stored in the .pth file (e.g., different stream names, layer names).\")\n        print(\"Ensure MODEL_ACTIVE_STREAMS matches the training configuration of the loaded model.\")\n        exit()\n    except Exception as e:\n        raise RuntimeError(f\"General error loading model weights: {e}\")\n\n\n    # --- Get Frames with Annotations for the Target Video ---\n    print(f\"\\nFinding annotated frames for {TARGET_SET_ID}/{TARGET_VIDEO_ID}...\")\n    ped_annotations_vid = pie_database.get(TARGET_SET_ID, {}).get(TARGET_VIDEO_ID, {}).get('ped_annotations', {})\n    if not ped_annotations_vid: raise ValueError(f\"No pedestrian annotations for {TARGET_SET_ID}/{TARGET_VIDEO_ID}.\")\n\n    # Collect all frames where ANY pedestrian is annotated and a prediction can be made\n    all_annotated_target_frames = set()\n    for ped_id, ped_data in ped_annotations_vid.items():\n        if 'frames' in ped_data:\n            ped_frames_sorted = sorted(ped_data['frames'])\n            for i in range(len(ped_frames_sorted)):\n                # Prediction is for frame: ped_frames_sorted[i]\n                # Observation ends at: ped_frames_sorted[i] - PRED_LEN\n                last_obs_frame = ped_frames_sorted[i] - PRED_LEN\n                start_obs_frame = last_obs_frame - SEQ_LEN + 1\n                if start_obs_frame >= 0: # Ensure observation window is valid\n                    # Check continuity for the observation window\n                    # This requires finding the index of start_obs_frame in ped_frames_sorted\n                    try:\n                        start_obs_idx_in_list = ped_frames_sorted.index(start_obs_frame)\n                        last_obs_idx_in_list = ped_frames_sorted.index(last_obs_frame)\n                        if (last_obs_idx_in_list - start_obs_idx_in_list + 1) == SEQ_LEN and \\\n                           ped_frames_sorted[last_obs_idx_in_list] - ped_frames_sorted[start_obs_idx_in_list] == SEQ_LEN -1:\n                            all_annotated_target_frames.add(ped_frames_sorted[i]) # Add frame for which prediction is made\n                    except ValueError:\n                        pass # A frame in the constructed range not in ped's actual frames\n\n    sorted_target_prediction_frames = sorted(list(all_annotated_target_frames))\n    if not sorted_target_prediction_frames:\n        print(f\"Warning: No frames suitable for prediction found for {TARGET_SET_ID}/{TARGET_VIDEO_ID} after checking sequence validity.\")\n        # Continue to write blank video if no frames to predict\n        # exit() # Or exit if you prefer\n\n    # --- Generate Predictions for these Target Frames ---\n    print(\"\\nGenerating predictions for target frames...\")\n    predictions_map = {} # {target_frame_num: {ped_id: prediction (0 or 1)}}\n    frames_with_errors = 0\n\n    with torch.no_grad():\n        for target_frame_num in tqdm(sorted_target_prediction_frames, desc=\"Predicting Target Frames\"):\n            predictions_map.setdefault(target_frame_num, {})\n            # Find pedestrians whose tracks are long enough to make a prediction for this target_frame_num\n            for ped_id, ped_data in ped_annotations_vid.items():\n                if 'frames' not in ped_data: continue\n                if target_frame_num not in ped_data['frames']: continue # Pedestrian must be present at the target prediction time\n\n                input_features = dataset_helper.get_feature_sequence(\n                    TARGET_SET_ID, TARGET_VIDEO_ID, ped_id,\n                    target_frame_num, # Pass the frame for which prediction is made\n                    SEQ_LEN, MODEL_ACTIVE_STREAMS\n                )\n                if input_features is not None:\n                    try:\n                        input_features_dev = {name: feat.to(DEVICE) for name, feat in input_features.items()}\n                        logits = model(input_features_dev)\n                        prediction = torch.argmax(logits, dim=1).item()\n                        predictions_map[target_frame_num][ped_id] = prediction\n                    except Exception as e:\n                         print(f\"Error model inference F_target:{target_frame_num} P:{ped_id}: {e}\")\n                         predictions_map[target_frame_num][ped_id] = -1 # Indicate error\n                         frames_with_errors += 1\n                else:\n                    # Sequence was not valid for this ped/frame combination\n                    predictions_map[target_frame_num][ped_id] = -2 # Indicate no valid sequence\n\n    print(f\"Predictions generated. Errors/invalid sequences for {frames_with_errors} instances.\")\n\n    # --- Create Output Video ---\n    print(f\"\\nCreating output video: {OUTPUT_VIDEO_PATH}\")\n    # Construct the full path to the input video\n    target_video_set_folder_name = f\"pie-{TARGET_SET_ID}\"\n    input_video_path = os.path.join(VIDEO_INPUT_DIR, target_video_set_folder_name, f\"{TARGET_VIDEO_ID}.mp4\")\n\n    if not os.path.exists(input_video_path):\n        raise FileNotFoundError(f\"Input video not found: {input_video_path}\")\n\n    cap_read = cv2.VideoCapture(input_video_path)\n    if not cap_read.isOpened():\n        raise IOError(f\"Could not open input video: {input_video_path}\")\n\n    vid_width_in = int(cap_read.get(cv2.CAP_PROP_FRAME_WIDTH))\n    vid_height_in = int(cap_read.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    # Use database num_frames if available and seems more reliable\n    total_video_frames = pie_database.get(TARGET_SET_ID, {}).get(TARGET_VIDEO_ID, {}).get('num_frames', 0)\n    if total_video_frames <= 0 : # Fallback if not in DB\n        total_video_frames = int(cap_read.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    fps_in = cap_read.get(cv2.CAP_PROP_FPS) if cap_read.get(cv2.CAP_PROP_FPS) > 0 else 30.0\n    print(f\" -> Input video properties: {vid_width_in}x{vid_height_in}, ~{total_video_frames} frames, {fps_in:.2f} FPS\")\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    video_writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps_in, (vid_width_in, vid_height_in))\n    if not video_writer.isOpened():\n        raise IOError(f\"Error opening video writer for {OUTPUT_VIDEO_PATH}\")\n\n    color_not_crossing_box = (0, 200, 0)  # Green\n    color_crossing_box = (0, 0, 200)      # Red\n    color_unknown_pred = (200, 200, 0) # Yellow for unknown/error\n    text_color = (255, 255, 255) # White text\n\n    # --- Iterate through ONLY the frames for which we have predictions ---\n    print(f\"Writing {len(sorted_target_prediction_frames)} annotated/predicted frames to video...\")\n    for current_frame_to_write in tqdm(sorted_target_prediction_frames, desc=\"Writing Video Frames\"):\n        frame_idx_cv = current_frame_to_write - 1 # Convert to 0-based for video read\n        if not (0 <= frame_idx_cv < total_video_frames):\n            print(f\"Warning: Target frame {current_frame_to_write} (idx {frame_idx_cv}) out of bounds. Skipping.\")\n            continue\n\n        cap_read.set(cv2.CAP_PROP_POS_FRAMES, frame_idx_cv)\n        ret_read, frame_image = cap_read.read()\n        if not ret_read:\n            print(f\"Warning: Could not read frame {frame_idx_cv} from input video for writing.\")\n            continue\n\n        frame_predictions_for_peds = predictions_map.get(current_frame_to_write, {})\n        for ped_id, prediction in frame_predictions_for_peds.items():\n            ped_data_for_frame = ped_annotations_vid.get(ped_id, {})\n            if 'frames' in ped_data_for_frame:\n                try:\n                    f_db_idx = ped_data_for_frame['frames'].index(current_frame_to_write)\n                    if 'bbox' in ped_data_for_frame and len(ped_data_for_frame['bbox']) > f_db_idx:\n                        bbox = ped_data_for_frame['bbox'][f_db_idx]\n                        try:\n                            x1, y1, x2, y2 = map(int, bbox)\n                            box_color = color_unknown; label_text = \"NoPred\"\n                            if prediction == 0: box_color = color_not_crossing_box; label_text = \"Not Crossing\"\n                            elif prediction == 1: box_color = color_crossing_box; label_text = \"Crossing\"\n                            elif prediction == -1: label_text = \"Seq Err\"\n                            elif prediction == -2: label_text = \"No Valid Seq\"\n\n\n                            cv2.rectangle(frame_image, (x1, y1), (x2, y2), box_color, 2)\n                            # Display text above the bounding box\n                            text_size, _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n                            text_x = x1\n                            text_y = y1 - 7 if y1 - 7 > text_size[1] else y1 + text_size[1] + 3\n                            # Optional: Add a background rectangle for text for better visibility\n                            # cv2.rectangle(frame_image, (text_x, text_y - text_size[1] - 2), (text_x + text_size[0], text_y + 2), (0,0,0), -1)\n                            cv2.putText(frame_image, label_text, (text_x, text_y),\n                                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1, cv2.LINE_AA)\n                        except ValueError: pass # Invalid bbox coords\n                except (ValueError, IndexError): pass # Ped not in this exact frame in DB list\n\n        cv2.putText(frame_image, f\"Frame: {current_frame_to_write}\", (10, 30),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2, cv2.LINE_AA)\n        video_writer.write(frame_image)\n\n    cap_read.release()\n    video_writer.release()\n    print(f\"\\nOutput video saved successfully to: {OUTPUT_VIDEO_PATH}\")\n    print(\"\\n--- Visualization Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:52:01.094675Z","iopub.execute_input":"2025-05-26T23:52:01.095008Z"}},"outputs":[{"name":"stdout","text":"Model uses Active Streams: ['bbox', 'ego_acc', 'ego_speed', 'ped_action', 'ped_look', 'static_context']\n--- Generating Prediction Video for set04/video_0012 (Annotated Frames Only) ---\nUsing Model: /kaggle/input/mslstm-pid/pytorch/default/1/best_model_weighted_bbox_ego_acc_ego_speed_ped_action_ped_look_static_context_ep2.pth\nOutput Video: /kaggle/working/set04_video_0012_annotated_preds.mp4\nUsing device: cuda\n\nLoading PIE database cache...\n -> PIE database loaded successfully.\n\nLoading scalers from: /kaggle/working/scalers.pkl\n -> Scalers loaded successfully.\n\nInitializing helper dataset object...\nHelper dataset initialized.\n\nInitializing model architecture...\nInitializing Weighted Avg model with streams: ['bbox', 'ego_acc', 'ego_speed', 'ped_action', 'ped_look', 'static_context']\n  - Adding stream 'bbox' with input size 4\n  - Adding stream 'ego_acc' with input size 2\n  - Adding stream 'ego_speed' with input size 1\n  - Adding stream 'ped_action' with input size 1\n  - Adding stream 'ped_look' with input size 1\n  - Adding stream 'static_context' with input size 23\n  Fused feature dimension (Weighted Avg): 512\n\nLoading trained model weights from: /kaggle/input/mslstm-pid/pytorch/default/1/best_model_weighted_bbox_ego_acc_ego_speed_ped_action_ped_look_static_context_ep2.pth\n -> Model weights loaded successfully.\n\nFinding annotated frames for set04/video_0012...\n\nGenerating predictions for target frames...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting Target Frames:   0%|          | 0/11343 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49a336853f0f40298ae88df35d237a8d"}},"metadata":{}},{"name":"stdout","text":"Predictions generated. Errors/invalid sequences for 0 instances.\n\nCreating output video: /kaggle/working/set04_video_0012_annotated_preds.mp4\n -> Input video properties: 1920x1080, ~21600 frames, 30.00 FPS\nWriting 11343 annotated/predicted frames to video...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Writing Video Frames:   0%|          | 0/11343 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4927262d5c2940158e42f0e6a3134e08"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}